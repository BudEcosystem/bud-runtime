# Default values for litellm_container.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
chartName: "litellm-container"
namespace: "litellm-container"
serviceType: ClusterIP
containerPort: 4000
pullPolicy: IfNotPresent
nodes: []
proxyConfig:
  model_list:
    # At least one model must exist for the proxy to start.
    - model_name: openai-gpt-3.5
      litellm_params:
        model: openai/gpt-3.5-turbo
        api_key: fake-api-key
  general_settings:
    master_key: fake-master-key
ingressHost: "litellm-container.litellm-container.svc.cluster.local"

modelName: "" # will be replaced by the deployment handler
