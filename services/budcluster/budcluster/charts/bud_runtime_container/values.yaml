# Bud Runtime Container Helm Chart Values
# This chart deploys AI/ML model inference runtimes with support for multiple engines.

chartName: "bud-runtime-container"
namespace: bud-runtime # will be replaced by the deployment handler
platform: "kubernetes"
pullPolicy: Always
hfToken: "" # will be replaced by the deployment handler
containerPort: "8000" # will be replaced by the deployment handler
ingressHost: "bud-runtime-inference.apps.partner.lab.com" # will be replaced by the deployment handler

volumeType: "local" # nfs, local
pvcName: "models-pvc"
accessMode: ReadWriteOnce
nfs:
  server: ""
  path: "/exports"

nodes: [] # will be replaced by the deployment handler
skipMasterNodeForCpu: true  # skip master/control-plane nodes for CPU deployments

imagePullSecrets:
  name: bud-registry-secret
  auth: {} # replaced with the actual auth

modelName: "" # will be replaced by the deployment handler
adapters: []

# Pod Autoscaler Configuration
# The autoscaler uses engine-specific metrics for scaling decisions.
# Metrics are collected by runtime-sidecar and exposed with bud: prefix.
#
# Supported vLLM/SGLang metrics (from BudApp ScalingMetricEnum):
#   - bud:gpu_cache_usage_perc_average (default)
#   - bud:time_to_first_token_seconds_average
#   - bud:e2e_request_latency_seconds_average
#   - bud:time_per_output_token_seconds_average
#
# Supported LatentBud metrics (for embedding models):
#   - embedding_batch_queue_size (default)
#   - embedding_requests_active
podscaler:
  enabled: false
  type: "metrics" # metrics or optimizer
  minReplicas: 1
  maxReplicas: 2
  upFluctuationTolerance: 1.5
  downFluctuationTolerance: 0.5
  window: 30s
  # targetMetric is set dynamically based on engine_type if not specified
  targetMetric: bud:gpu_cache_usage_perc_average
  targetValue: 0.5

# Engine Types Reference:
# -----------------------
# vllm (default):
#   - For LLM text generation models
#   - Supports: LoRA, tool calling, reasoning
#   - Entrypoint: python -m vllm.entrypoints.openai.api_server
#   - Metrics: bud:gpu_cache_usage_perc_average, bud:time_to_first_token_seconds_average,
#              bud:e2e_request_latency_seconds_average, bud:time_per_output_token_seconds_average
#
# latentbud:
#   - For embedding models (sentence transformers, etc.)
#   - Optimized for high-throughput embedding inference
#   - Entrypoint: python -m latentbud.server
#   - Metrics: embedding_batch_queue_size, embedding_requests_active
#
# sglang (future):
#   - For LLM models with SGLang runtime
#   - Supports: LoRA, tool calling
#   - Entrypoint: python -m sglang.launch_server
#   - Metrics: Same as vLLM (via runtime-sidecar)
