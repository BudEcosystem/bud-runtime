{{- define "bud-runtime-container.multi-node" }}
apiVersion: orchestration.aibrix.ai/v1alpha1
kind: RayClusterFleet
metadata:
  name: {{ .Values.modelName }}
  namespace: {{ .Values.namespace }}
  labels:
    app.kubernetes.io/name: aibrix
    app.kubernetes.io/managed-by: kustomize
    model.aibrix.ai/name: {{ .Values.modelName }}
spec:
  replicas: {{ .node.replicas }}
  selector:
    matchLabels:
      model.aibrix.ai/name: {{ .Values.modelName }}
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
  template:
    metadata:
      labels:
        model.aibrix.ai/name: {{ .Values.modelName }}
        {{- if .node.engine_type }}
        engine_type: {{ .node.engine_type }}
        {{- end }}
      annotations:
        ray.io/overwrite-container-cmd: "true"
    spec:
      rayVersion: "2.10.0"
      headGroupSpec:
        rayStartParams:
          dashboard-host: "0.0.0.0"
        template:
          metadata:
            labels:
              model.aibrix.ai/name: {{ .Values.modelName }}
              {{- if and (eq .node.type "cpu_high") (eq .node.hardware_mode "shared") }}
              bud.studio/shared-cpu: "true"
              {{- end }}
          spec:
            nodeSelector:
              {{- if .node.node_selector }}
              {{- range $key, $value := .node.node_selector }}
              {{ $key }}: "{{ $value }}"
              {{- end }}
              {{- else }}
              # Fallback for legacy deployments without node_selector
              {{- if eq .node.type "cuda" }}
              nvidia.com/gpu.present: "true"
              {{- end }}
              {{- end }}
            {{- if or (and (eq ($.Values.accessMode | default "ReadWriteOnce") "ReadWriteOnce") (gt (.node.replicas | int) 1)) (eq .node.type "cpu_high") }}
            affinity:
              {{- if or (and (eq ($.Values.accessMode | default "ReadWriteOnce") "ReadWriteOnce") (gt (.node.replicas | int) 1)) (and (eq .node.type "cpu_high") (eq .node.hardware_mode "shared")) }}
              podAffinity:
                {{- if and (eq ($.Values.accessMode | default "ReadWriteOnce") "ReadWriteOnce") (gt (.node.replicas | int) 1) }}
                # Pod affinity to ensure all replicas are scheduled on the same node for ReadWriteOnce volumes
                requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchLabels:
                      model.aibrix.ai/name: {{ $.Values.modelName }}
                  topologyKey: kubernetes.io/hostname
                {{- end }}
                {{- if and (eq .node.type "cpu_high") (eq .node.hardware_mode "shared") }}
                {{- include "bud-runtime-container.shared-cpu-pod-affinity" . | nindent 16 }}
                {{- end }}
              {{- end }}
              {{- if eq .node.type "cpu_high" }}
              {{- include "bud-runtime-container.cpu-node-affinity" . | nindent 14 }}
              {{- end }}
            {{- end }}
            imagePullSecrets:
            - name: {{ $.Values.imagePullSecrets.name }}
            {{- if eq .node.type "cuda" }}
            runtimeClassName: nvidia
            schedulerName: hami-scheduler
            {{- end }}
            containers:
            {{/*
              Engine Selection Logic for Multi-Node:
              Note: Multi-node deployments are typically for large LLMs requiring
              pipeline parallelism. Embedding models (latentbud) usually don't
              need multi-node, but we support it for consistency.
            */}}
            {{- $engineType := .node.engine_type | default "vllm" }}
            {{- if eq $engineType "latentbud" }}
            {{- include "bud-runtime-container.engine-latentbud" (dict "Values" $.Values "device" .node) | nindent 12 }}
            {{- else }}
            {{/* Default to vLLM engine for multi-node */}}
            {{- include "bud-runtime-container.engine-vllm" (dict "Values" $.Values "device" .node) | nindent 12 }}
            {{- end }}
            {{/* Sidecar containers - engine-aware */}}
            {{- include "bud-runtime-container.sidecar" (dict "Values" $.Values "node" .node) | nindent 12 }}
            {{- include "bud-runtime-container.aibrix-runtime" (dict "Values" $.Values "node" .node) | nindent 12 }}
            volumes:
            - name: shm
              emptyDir:
                medium: "Memory"
                sizeLimit: "2Gi"
            - name: model-registry
            {{- if eq $.Values.volumeType "nfs" }}
              nfs:
                server: "{{ $.Values.nfs.server }}"
                path: "{{ $.Values.nfs.path }}"
            {{- else if eq $.Values.volumeType "local" }}
              persistentVolumeClaim:
                claimName: "{{ $.Values.pvcName }}"
            {{- end }}
      workerGroupSpecs:
        - groupName: "{{ .Values.modelName }}-group"
          replicas: {{ sub (.node.pp_size | int) 1 }}
          minReplicas: {{ sub (.node.pp_size | int) 1 }}
          rayStartParams: {}
          template:
            metadata:
              labels:
                model.aibrix.ai/name: {{ .Values.modelName }}
                {{- if and (eq .node.type "cpu_high") (eq .node.hardware_mode "shared") }}
                bud.studio/shared-cpu: "true"
                {{- end }}
            spec:
              nodeSelector:
                {{- if .node.node_selector }}
                {{- range $key, $value := .node.node_selector }}
                {{ $key }}: "{{ $value }}"
                {{- end }}
                {{- else }}
                # Fallback for legacy deployments without node_selector
                {{- if eq .node.type "cuda" }}
                nvidia.com/gpu.present: "true"
                {{- end }}
                {{- end }}
              affinity:
                podAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                  - labelSelector:
                      matchLabels:
                        model.aibrix.ai/name: {{ $.Values.modelName }}
                    topologyKey: kubernetes.io/hostname
                  {{- if and (eq .node.type "cpu_high") (eq .node.hardware_mode "shared") }}
                  {{- include "bud-runtime-container.shared-cpu-pod-affinity" . | nindent 18 }}
                  {{- end }}
                {{- if eq .node.type "cpu_high" }}
                {{- include "bud-runtime-container.cpu-node-affinity" . | nindent 16 }}
                {{- end }}
              imagePullSecrets:
              - name: {{ $.Values.imagePullSecrets.name }}
              {{- if eq .node.type "cuda" }}
              runtimeClassName: nvidia
              schedulerName: hami-scheduler
              {{- end }}
              containers:
                - name: ray-worker
                  image: {{ .node.image }}
                  volumeMounts:
                    - name: model-registry
                      mountPath: /data/models-registry
                      readOnly: false
                    - name: shm
                      mountPath: "/dev/shm"
                  env:
                    - name: MY_POD_IP
                      valueFrom:
                        fieldRef:
                          fieldPath: status.podIP
                  command: [ "/bin/bash", "-c" ]
                  args:
                    - >
                      ulimit -n 65536 &&
                      eval "$KUBERAY_GEN_RAY_START_CMD --node-ip-address=$MY_POD_IP" &&
                      tail -f /dev/null
                  lifecycle:
                    preStop:
                      exec:
                        command: [ "/bin/sh", "-c", "ray stop" ]
                  resources:
                    limits:
                      cpu: "4"
                      nvidia.com/gpu: {{ .node.tp_size }}
                    requests:
                      cpu: "4"
                      nvidia.com/gpu: {{ .node.tp_size }}
              volumes:
              - name: shm
                emptyDir:
                  medium: "Memory"
                  sizeLimit: "2Gi"
              - name: model-registry
              {{- if eq $.Values.volumeType "nfs" }}
                nfs:
                  server: "{{ $.Values.nfs.server }}"
                  path: "{{ $.Values.nfs.path }}"
              {{- else if eq $.Values.volumeType "local" }}
                persistentVolumeClaim:
                  claimName: "{{ $.Values.pvcName }}"
              {{- end }}
{{- end }}
