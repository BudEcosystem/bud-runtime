{{- define "bud-runtime-container.single-node" }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ $.Values.modelName }}
  namespace: {{ $.Values.namespace }}
  labels:
    model.aibrix.ai/name: {{ $.Values.modelName }}
    model.aibrix.ai/port: "{{ $.Values.containerPort }}"
spec:
  replicas: {{ .node.replicas }}
  selector:
    matchLabels:
      model.aibrix.ai/name: {{ $.Values.modelName }}
  template:
    metadata:
      labels:
        model.aibrix.ai/name: {{ $.Values.modelName }}
        device_name: {{ .node.name }}
        concurrency: "{{ .node.concurrency }}"
        {{- if .node.engine_type }}
        engine_type: {{ .node.engine_type }}
        {{- end }}
        {{- if and (eq .node.type "cpu_high") (eq .node.hardware_mode "shared") }}
        bud.studio/shared-cpu: "true"
        {{- end }}
    spec:
      nodeSelector:
        {{- if .node.node_selector }}
        {{- range $key, $value := .node.node_selector }}
        {{ $key }}: "{{ $value }}"
        {{- end }}
        {{- else }}
        # Fallback for legacy deployments without node_selector
        {{- if eq .node.type "cuda" }}
        nvidia.com/gpu.present: "true"
        {{- end }}
        {{- end }}
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                model.aibrix.ai/name: {{ $.Values.modelName }}
            topologyKey: kubernetes.io/hostname
          {{- if and (eq .node.type "cpu_high") (eq .node.hardware_mode "shared") }}
          {{- include "bud-runtime-container.shared-cpu-pod-affinity" . | nindent 10 }}
          {{- end }}
        {{- if eq .node.type "cpu_high" }}
        {{- include "bud-runtime-container.cpu-node-affinity" . | nindent 8 }}
        {{- end }}
      imagePullSecrets:
      - name: {{ $.Values.imagePullSecrets.name }}
      {{- if eq .node.type "cuda" }}
      runtimeClassName: nvidia
      schedulerName: hami-scheduler
      {{- end }}
      containers:
      {{/*
        Engine Selection Logic:
        1. First check engine_type from node config (set by BudSim)
        2. If not specified, default to vllm for backward compatibility

        Supported engines:
        - vllm: For LLM text generation models
        - latentbud: For embedding models
        - sglang: For LLM models (future)
      */}}
      {{- $engineType := .node.engine_type | default "vllm" }}
      {{- if eq $engineType "latentbud" }}
      {{- include "bud-runtime-container.engine-latentbud" (dict "Values" $.Values "device" .node) | nindent 6 }}
      {{- else if eq $engineType "sglang" }}
      {{/* Future: Add sglang engine template */}}
      {{- include "bud-runtime-container.engine-vllm" (dict "Values" $.Values "device" .node) | nindent 6 }}
      {{- else }}
      {{/* Default to vLLM engine */}}
      {{- include "bud-runtime-container.engine-vllm" (dict "Values" $.Values "device" .node) | nindent 6 }}
      {{- end }}
      {{/* Sidecar containers - engine-aware */}}
      {{- include "bud-runtime-container.sidecar" (dict "Values" $.Values "node" .node) | nindent 6 }}
      {{- include "bud-runtime-container.aibrix-runtime" (dict "Values" $.Values "node" .node) | nindent 6 }}
      volumes:
      - name: shm
        emptyDir:
          medium: "Memory"
          sizeLimit: "2Gi"
      - name: model-registry
      {{- if eq $.Values.volumeType "nfs" }}
        nfs:
          server: "{{ $.Values.nfs.server }}"
          path: "{{ $.Values.nfs.path }}"
      {{- else if eq $.Values.volumeType "local" }}
        persistentVolumeClaim:
          claimName: "{{ $.Values.pvcName }}"
      {{- end }}
{{- end }}
