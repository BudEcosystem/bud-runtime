{{- define "bud-runtime-container.single-node" }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ $.Values.modelName }}
  namespace: {{ $.Values.namespace }}
  labels:
    model.aibrix.ai/name: {{ $.Values.modelName }}
    model.aibrix.ai/port: "{{ $.Values.containerPort }}"
spec:
  replicas: {{ .node.replicas }}
  selector:
    matchLabels:
      model.aibrix.ai/name: {{ $.Values.modelName }}
  template:
    metadata:
      labels:
        model.aibrix.ai/name: {{ $.Values.modelName }}
        device_name: {{ .node.name }}
        concurrency: "{{ .node.concurrency }}"
        {{- if and (eq .node.type "cpu_high") (eq .node.hardware_mode "shared") }}
        bud.studio/shared-cpu: "true"
        {{- end }}
    spec:
      nodeSelector:
        # kubernetes.io/hostname: {{ .node.name }}
        {{- if .node.node_selector }}
        {{- range $key, $value := .node.node_selector }}
        {{ $key }}: "{{ $value }}"
        {{- end }}
        {{- else }}
        # Fallback for legacy deployments without node_selector
        {{- if eq .node.type "cuda" }}
        nvidia.com/gpu.present: "true"
        {{- end }}
        {{- end }}
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                model.aibrix.ai/name: {{ $.Values.modelName }}
            topologyKey: kubernetes.io/hostname
          {{- if and (eq .node.type "cpu_high") (eq .node.hardware_mode "shared") }}
          {{- include "bud-runtime-container.shared-cpu-pod-affinity" . | nindent 10 }}
          {{- end }}
        {{- if eq .node.type "cpu_high" }}
        {{- include "bud-runtime-container.cpu-node-affinity" . | nindent 8 }}
        {{- end }}
      imagePullSecrets:
      - name: {{ $.Values.imagePullSecrets.name }}
      {{- if eq .node.type "cuda" }}
      runtimeClassName: nvidia
      schedulerName: hami-scheduler
      {{- end }}
      containers:
      {{- if eq .node.type "cpu_high" }}
      {{- include "bud-runtime-container.cpu-container" (dict "Values" $.Values "device" .node) | nindent 6 }}
      {{- else if eq .node.type "cuda" }}
      {{- include "bud-runtime-container.cuda-container" (dict "Values" $.Values "device" .node) | nindent 6 }}
      {{- else if eq .node.type "hpu" }}
      {{- include "bud-runtime-container.hpu-container" (dict "Values" $.Values "device" .node) | nindent 6 }}
      {{- end }}
      - name: bud-runtime-sidecar
        image: budstudio/runtime-sidecar:latest
        ports:
          - containerPort: 9090
            protocol: TCP
        env: # Pass configuration via environment variables
          - name: ENGINE_METRICS_URL
            value: "http://localhost:8000/metrics" # Point to the vLLM container in the same pod
          - name: LISTEN_PORT
            value: "9090"
          - name: SCRAPE_INTERVAL_SECONDS
            value: "0.5"
          - name: CALCULATION_WINDOW_SECONDS
            value: "1"
        livenessProbe:
          httpGet:
            path: /metrics
            port: 9090
          initialDelaySeconds: 120
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /metrics
            port: 9090
          initialDelaySeconds: 60
          periodSeconds: 10
      - name: aibrix-runtime
        image: aibrix/runtime:v0.2.1
        command:
          - aibrix_runtime
          - --port
          - "8080"
        env:
          - name: INFERENCE_ENGINE
            value: vllm
          - name: INFERENCE_ENGINE_ENDPOINT
            value: http://localhost:8000
        ports:
          - containerPort: 8080
            protocol: TCP
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 3
          periodSeconds: 2
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 10
      volumes:
      - name: shm
        emptyDir:
          medium: "Memory"
          sizeLimit: "2Gi"
      - name: model-registry
      {{- if eq $.Values.volumeType "nfs" }}
        nfs:
          server: "{{ $.Values.nfs.server }}"
          path: "{{ $.Values.nfs.path }}"
      {{- else if eq $.Values.volumeType "local" }}
        persistentVolumeClaim:
          claimName: "{{ $.Values.pvcName }}"
      {{- end }}
{{- end }}
