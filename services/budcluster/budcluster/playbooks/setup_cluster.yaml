---
- name: Setup Cluster with NFD, GPU/HPU Operators, and Aibrix Components
  hosts: localhost
  connection: local
  gather_facts: false

  vars_files:
    - vars/common.yaml

  vars:
    nfd_chart_path: "{{ charts_dir }}/nfd"
    nfd_release_name: node-feature-discovery
    prometheus_release_name: bud-metrics
    prometheus_chart_path: "{{ charts_dir }}/prometheus-stack"
    # Default namespace for infrastructure components
    namespace: "{{ namespace | default('bud-system') }}"
    # Deployment control flags
    update_existing_components: "{{ update_existing | default(false) }}"
    force_reinstall_components: "{{ force_reinstall | default(false) }}"
    skip_healthy_components: "{{ skip_healthy | default(true) }}"

  roles:
    - create_kubeconfig

  tasks:
    # NFD Deployment Section
    - name: Check if NFD is already deployed
      kubernetes.core.helm_info:
        name: "{{ nfd_release_name }}"
        namespace: "{{ namespace }}"
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
      register: nfd_helm_info
      ignore_errors: true

    - name: Check NFD pods status
      kubernetes.core.k8s_info:
        kind: Pod
        namespace: "{{ namespace }}"
        label_selectors:
          - app.kubernetes.io/name=node-feature-discovery
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
      register: nfd_pods_check
      ignore_errors: true
      when: nfd_helm_info is succeeded

    - name: Determine NFD deployment action
      set_fact:
        nfd_needs_deployment: >-
          {{ nfd_helm_info.failed or
             nfd_helm_info.status is not defined or
             nfd_helm_info.status.status | default('') != 'deployed' or
             nfd_pods_check.resources | default([]) | selectattr('status.phase', 'ne', 'Running') | list | length > 0 or
             force_reinstall_components or
             update_existing_components }}

    - name: Skip NFD if already deployed and healthy
      debug:
        msg: "NFD is already deployed and healthy. Skipping deployment."
      when:
        - not nfd_needs_deployment | default(false)
        - skip_healthy_components

    # Clean up any existing problematic NFD deployment
    - name: Check for and remove problematic NFD deployment
      shell: |
        helm list -n {{ namespace }} --kubeconfig {{ kubeconfig_path }} {% if not validate_certs %}--insecure-skip-tls-verify {% endif %}| grep -q {{ nfd_release_name }} && \
        helm uninstall {{ nfd_release_name }} -n {{ namespace }} --kubeconfig {{ kubeconfig_path }} {% if not validate_certs %}--insecure-skip-tls-verify {% endif %}|| true
      when:
        - nfd_helm_info is failed
        - nfd_needs_deployment | default(true)
      ignore_errors: true

    # Clean up cluster-scoped resources from previous installations
    - name: Clean up existing NFD ClusterRoles and ClusterRoleBindings
      shell: |
        kubectl --kubeconfig {{ kubeconfig_path }} {% if not validate_certs %}--insecure-skip-tls-verify {% endif %}delete clusterrole node-feature-discovery --ignore-not-found=true
        kubectl --kubeconfig {{ kubeconfig_path }} {% if not validate_certs %}--insecure-skip-tls-verify {% endif %}delete clusterrolebinding node-feature-discovery --ignore-not-found=true
        # Also clean up from old bud-runtime namespace if exists
        helm list -n bud-runtime --kubeconfig {{ kubeconfig_path }} {% if not validate_certs %}--insecure-skip-tls-verify {% endif %}| grep -q {{ nfd_release_name }} && \
        helm uninstall {{ nfd_release_name }} -n bud-runtime --kubeconfig {{ kubeconfig_path }} {% if not validate_certs %}--insecure-skip-tls-verify {% endif %}|| true
        # Remove any leftover ClusterRoles/ClusterRoleBindings with NFD in the name
        kubectl --kubeconfig {{ kubeconfig_path }} {% if not validate_certs %}--insecure-skip-tls-verify {% endif %}delete clusterrole -l app.kubernetes.io/name=node-feature-discovery --ignore-not-found=true
        kubectl --kubeconfig {{ kubeconfig_path }} {% if not validate_certs %}--insecure-skip-tls-verify {% endif %}delete clusterrolebinding -l app.kubernetes.io/name=node-feature-discovery --ignore-not-found=true
      when: nfd_needs_deployment | default(true)
      ignore_errors: true

    - name: Update NFD chart dependencies
      shell: helm dependency update "{{ nfd_chart_path }}"
      when: nfd_needs_deployment | default(true)

    - name: Deploy NFD from local chart
      kubernetes.core.helm:
        release_name: "{{ nfd_release_name }}"
        chart_ref: "{{ nfd_chart_path }}"
        release_namespace: "{{ namespace }}"
        create_namespace: true
        atomic: true
        wait: true
        wait_timeout: "{{ helm_timeout }}"
        force: true
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
        # Use values from local chart
        values:
          global:
            namespace: "{{ namespace }}"
          nfd:
            enabled: true
      when: nfd_needs_deployment | default(true)

    - name: Wait for NFD master to be ready
      kubernetes.core.k8s_info:
        kind: Deployment
        namespace: "{{ namespace }}"
        name: "node-feature-discovery-master"
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
      register: nfd_master
      retries: 3  # Reduced retries to avoid long hangs
      delay: 10
      until: nfd_master.resources[0].status.availableReplicas is defined and nfd_master.resources[0].status.availableReplicas > 0
      when: nfd_needs_deployment | default(true)
      ignore_errors: true  # Continue even if master isn't ready

    - name: Check NFD master status
      debug:
        msg: >
          NFD Master status:
          {{ 'Ready' if (nfd_master.resources[0].status.availableReplicas | default(0)) > 0 else 'Not Ready - check pod events for details' }}
      when: nfd_needs_deployment | default(true)

    - name: Wait for NFD workers to be ready
      kubernetes.core.k8s_info:
        kind: DaemonSet
        namespace: "{{ namespace }}"
        name: "node-feature-discovery-worker"
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
      register: nfd_worker
      retries: 3  # Reduced retries to avoid long hangs
      delay: 10
      until: nfd_worker.resources[0].status.numberReady is defined and nfd_worker.resources[0].status.numberReady > 0
      when: nfd_needs_deployment | default(true)
      ignore_errors: true  # Continue even if workers aren't ready

    - name: Check NFD worker status
      debug:
        msg: >
          NFD Worker status:
          {{ 'Ready' if (nfd_worker.resources[0].status.numberReady | default(0)) > 0 else 'Not Ready - check pod events for details' }}
      when: nfd_needs_deployment | default(true)

    - name: Brief wait for NFD to start labeling
      pause:
        seconds: 10  # Reduced from 30 to 10 seconds
      when:
        - nfd_needs_deployment | default(true)
        - (nfd_master.resources[0].status.availableReplicas | default(0)) > 0

    # GPU Detection and Operator Deployment
    - name: Check for NVIDIA GPUs in cluster via NFD labels
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Node
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
      register: cluster_nodes

    - name: Debug node labels for GPU detection
      debug:
        msg: "Node labels: {{ item.metadata.labels | default({}) | dict2items | selectattr('key', 'match', '.*pci.*') | list }}"
      loop: "{{ cluster_nodes.resources }}"
      when: cluster_nodes.resources is defined

    - name: Set GPU detection fact
      set_fact:
        has_nvidia_gpus: >-
          {{
            cluster_nodes.resources |
            selectattr('metadata.labels', 'defined') |
            selectattr('metadata.labels.nvidia\\.com/gpu\\.present', 'defined') |
            selectattr('metadata.labels.nvidia\\.com/gpu\\.present', 'equalto', 'true') |
            list | length > 0 or
            cluster_nodes.resources |
            selectattr('metadata.labels', 'defined') |
            selectattr('metadata.labels.feature\\.node\\.kubernetes\\.io/pci-10de\\.present', 'defined') |
            selectattr('metadata.labels.feature\\.node\\.kubernetes\\.io/pci-10de\\.present', 'equalto', 'true') |
            list | length > 0 or
            cluster_nodes.resources |
            selectattr('metadata.labels', 'defined') |
            map(attribute='metadata.labels') |
            map('dict2items') |
            flatten |
            selectattr('key', 'match', '^feature\\.node\\.kubernetes\\.io/pci-.*10de\\.present$') |
            selectattr('value', 'equalto', 'true') |
            list | length > 0
          }}

    - name: Debug GPU detection result
      debug:
        msg: "NVIDIA GPUs detected: {{ has_nvidia_gpus }}"

    - name: Add NVIDIA Helm repository
      kubernetes.core.helm_repository:
        name: nvidia
        repo_url: https://helm.ngc.nvidia.com/nvidia
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
      when: has_nvidia_gpus | default(false)

    - name: Deploy GPU Operator if NVIDIA GPUs detected
      kubernetes.core.helm:
        release_name: "nvidia-gpu-operator"
        chart_ref: "nvidia/gpu-operator"
        release_namespace: "gpu-operator"
        create_namespace: true
        atomic: true
        wait: true
        wait_timeout: 600
        force: true
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
        values:
          operator:
            defaultRuntime: containerd
          driver:
            enabled: true
          toolkit:
            enabled: true
          devicePlugin:
            enabled: true
          gfd:
            enabled: true
          migManager:
            enabled: false
          nodeStatusExporter:
            enabled: true
      when: has_nvidia_gpus | default(false)

    # HAMI GPU Time-Slicing Installation
    - name: Check if HAMI is already deployed
      kubernetes.core.helm_info:
        name: hami
        namespace: kube-system
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
      register: hami_helm_info
      ignore_errors: true
      when: has_nvidia_gpus | default(false)

    - name: Add HAMI Helm repository
      kubernetes.core.helm_repository:
        name: hami-charts
        repo_url: https://project-hami.github.io/HAMi/
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
      when:
        - has_nvidia_gpus | default(false)
        - hami_helm_info.failed or hami_helm_info.status.status | default('') != 'deployed'

    - name: Deploy HAMI for GPU Time-Slicing
      kubernetes.core.helm:
        release_name: hami
        chart_ref: hami-charts/hami
        release_namespace: kube-system
        create_namespace: false
        atomic: true
        wait: true
        wait_timeout: 300
        force: true
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
        values:
          devicePlugin:
            enabled: true
            runtimeClassName: nvidia
            securityContext:
              privileged: true
            nodeSelector:
              gpu: "on"
            extraVolumes:
              - name: dev
                hostPath:
                  path: /dev
            extraVolumeMounts:
              - name: dev
                mountPath: /dev
      when:
        - has_nvidia_gpus | default(false)
        - hami_helm_info.failed or hami_helm_info.status.status | default('') != 'deployed'

    - name: Wait for HAMI scheduler to be ready
      kubernetes.core.k8s_info:
        kind: Deployment
        namespace: kube-system
        name: hami-scheduler
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
      register: hami_scheduler
      retries: 6
      delay: 10
      until: >
        hami_scheduler.resources is defined and
        hami_scheduler.resources | length > 0 and
        hami_scheduler.resources[0].status.availableReplicas is defined and
        hami_scheduler.resources[0].status.availableReplicas > 0
      when:
        - has_nvidia_gpus | default(false)
        - hami_helm_info.failed or hami_helm_info.status.status | default('') != 'deployed'

    # HPU Detection and Device Plugin Deployment
    - name: Check for Intel Gaudi HPUs via NFD labels
      set_fact:
        has_intel_hpus: >-
          {{
            cluster_nodes.resources |
            selectattr('metadata.labels', 'defined') |
            selectattr('metadata.labels.feature\\.node\\.kubernetes\\.io/pci-8086\\.device-1020', 'defined') |
            selectattr('metadata.labels.feature\\.node\\.kubernetes\\.io/pci-8086\\.device-1020', 'equalto', 'true') |
            list | length > 0 or
            cluster_nodes.resources |
            selectattr('metadata.labels', 'defined') |
            selectattr('metadata.labels.feature\\.node\\.kubernetes\\.io/pci-8086\\.device-1021', 'defined') |
            selectattr('metadata.labels.feature\\.node\\.kubernetes\\.io/pci-8086\\.device-1021', 'equalto', 'true') |
            list | length > 0 or
            cluster_nodes.resources |
            selectattr('metadata.labels', 'defined') |
            selectattr('metadata.labels.feature\\.node\\.kubernetes\\.io/pci-8086\\.device-1022', 'defined') |
            selectattr('metadata.labels.feature\\.node\\.kubernetes\\.io/pci-8086\\.device-1022', 'equalto', 'true') |
            list | length > 0 or
            cluster_nodes.resources |
            selectattr('metadata.labels', 'defined') |
            map(attribute='metadata.labels') |
            map('dict2items') |
            flatten |
            selectattr('key', 'match', '^feature\\.node\\.kubernetes\\.io/pci-.*8086\\.device-(1020|1021|1022)$') |
            selectattr('value', 'equalto', 'true') |
            list | length > 0
          }}

    - name: Deploy Intel Device Plugin if HPUs detected
      kubernetes.core.helm:
        release_name: "intel-device-plugins"
        chart_ref: "{{ nfd_chart_path }}"
        release_namespace: "{{ namespace }}"
        atomic: true
        wait: true
        wait_timeout: 300
        force: true
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
        values:
          global:
            namespace: "{{ namespace }}"
          nfd:
            enabled: false  # NFD already deployed
          gpuOperator:
            enabled: false
          intelDevicePlugin:
            enabled: true
            gaudiEnabled: true
      when: has_intel_hpus | default(false)

    # Aibrix Components Deployment Section
    - name: Check if Aibrix CRDs are installed
      kubernetes.core.k8s_info:
        api_version: apiextensions.k8s.io/v1
        kind: CustomResourceDefinition
        name: modeladapters.model.aibrix.ai
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
      register: aibrix_crd_check
      ignore_errors: true

    - name: Check if Aibrix controller is deployed
      kubernetes.core.k8s_info:
        kind: Deployment
        namespace: aibrix-system
        name: aibrix-controller-manager
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
      register: aibrix_controller_check
      ignore_errors: true

    - name: Determine Aibrix deployment action
      set_fact:
        aibrix_needs_deployment: >-
          {{ aibrix_crd_check.failed or
             not aibrix_crd_check.resources or
             aibrix_crd_check.resources | length == 0 or
             aibrix_controller_check.failed or
             not aibrix_controller_check.resources or
             aibrix_controller_check.resources | length == 0 or
             (aibrix_controller_check.resources[0].status.availableReplicas | default(0)) == 0 or
             force_reinstall_components or
             update_existing_components }}

    - name: Skip Aibrix if already deployed and healthy
      debug:
        msg: "Aibrix components are already deployed and healthy. Skipping deployment."
      when:
        - not aibrix_needs_deployment
        - skip_healthy_components

    - name: Install Aibrix dependencies
      kubernetes.core.k8s:
        state: present
        src: https://github.com/BudEcosystem/aibrix/releases/download/0.2.0/aibrix-dependency-v0.2.0.yaml
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
      when: aibrix_needs_deployment | default(true)

    - name: Install Aibrix core components
      kubernetes.core.k8s:
        state: present
        src: https://github.com/BudEcosystem/aibrix/releases/download/0.2.0/aibrix-core-v0.2.0.yaml
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
      when: aibrix_needs_deployment | default(true)

    - name: Wait for Aibrix controller to be ready
      kubernetes.core.k8s_info:
        kind: Deployment
        namespace: aibrix-system
        name: aibrix-controller-manager
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
      register: aibrix_controller
      retries: 6
      delay: 10
      until: >
        aibrix_controller.resources is defined and
        aibrix_controller.resources | length > 0 and
        aibrix_controller.resources[0].status.availableReplicas is defined and
        aibrix_controller.resources[0].status.availableReplicas > 0
      when: aibrix_needs_deployment | default(true)
      ignore_errors: true

    # Prometheus Stack Deployment Section
    - name: Check if Prometheus is already deployed
      kubernetes.core.helm_info:
        name: "{{ prometheus_release_name }}"
        namespace: "{{ namespace }}"
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
      register: prometheus_helm_info
      ignore_errors: true

    - name: Check Prometheus pods status
      kubernetes.core.k8s_info:
        kind: Pod
        namespace: "{{ namespace }}"
        label_selectors:
          - app.kubernetes.io/name=prometheus
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
      register: prometheus_pods_check
      ignore_errors: true
      when: prometheus_helm_info is succeeded

    - name: Determine Prometheus deployment action
      set_fact:
        prometheus_needs_deployment: >-
          {{ prometheus_helm_info.failed or
             prometheus_helm_info.status is not defined or
             prometheus_helm_info.status.status | default('') != 'deployed' or
             prometheus_pods_check.resources | default([]) | selectattr('status.phase', 'ne', 'Running') | list | length > 0 or
             force_reinstall_components or
             update_existing_components }}

    - name: Skip Prometheus if already deployed and healthy
      debug:
        msg: "Prometheus stack is already deployed and healthy. Skipping deployment."
      when:
        - not prometheus_needs_deployment | default(false)
        - skip_healthy_components

    - name: Update Prometheus chart dependencies
      shell: helm dependency update "{{ prometheus_chart_path }}"
      when: prometheus_needs_deployment | default(true)

    - name: Deploy Prometheus Stack from local chart
      kubernetes.core.helm:
        release_name: "{{ prometheus_release_name }}"
        chart_ref: "{{ prometheus_chart_path }}"
        release_namespace: "{{ namespace }}"
        create_namespace: true
        atomic: true
        wait: true
        wait_timeout: "{{ helm_timeout }}"
        force: "{{ force_reinstall_components | default(false) }}"
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
        values:
          kube-prometheus-stack:
            prometheus:
              enabled: true
              prometheusSpec:
                externalLabels:
                  cluster: "{{ cluster_id | default('unknown-cluster') }}"
                  cluster_name: "{{ cluster_name | default('') }}"
                # Remote write disabled - metrics collected via pull mechanism
                # remoteWrite: []
                retention: "{{ prometheus_retention | default('24h') }}"
                resources:
                  requests:
                    memory: "{{ prometheus_memory_request | default('1Gi') }}"
                    cpu: "{{ prometheus_cpu_request | default('250m') }}"
                  limits:
                    memory: "{{ prometheus_memory_limit | default('2Gi') }}"
                    cpu: "{{ prometheus_cpu_limit | default('500m') }}"
                storageSpec:
                  volumeClaimTemplate:
                    spec:
                      accessModes: ["ReadWriteOnce"]
                      resources:
                        requests:
                          storage: "{{ prometheus_storage_size | default('20Gi') }}"
                serviceMonitorSelectorNilUsesHelmValues: false
                podMonitorSelectorNilUsesHelmValues: false
                ruleSelectorNilUsesHelmValues: false
            prometheus-node-exporter:
              enabled: true
              service:
                port: 9100
                targetPort: 9100
              resources:
                requests:
                  memory: "32Mi"
                  cpu: "50m"
                limits:
                  memory: "64Mi"
                  cpu: "100m"
            kube-state-metrics:
              enabled: true
              resources:
                requests:
                  memory: "64Mi"
                  cpu: "100m"
                limits:
                  memory: "128Mi"
                  cpu: "200m"
            grafana:
              enabled: false
            alertmanager:
              enabled: false
      when: prometheus_needs_deployment | default(true)

    - name: Wait for Prometheus StatefulSet to be ready
      kubernetes.core.k8s_info:
        kind: StatefulSet
        namespace: "{{ namespace }}"
        name: "prometheus-{{ prometheus_release_name }}-kube-prometheu-prometheus"
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
      register: prometheus_statefulset
      retries: 6
      delay: 10
      until: >
        prometheus_statefulset.resources is defined and
        prometheus_statefulset.resources | length > 0 and
        prometheus_statefulset.resources[0].status.readyReplicas is defined and
        prometheus_statefulset.resources[0].status.readyReplicas > 0
      when: prometheus_needs_deployment | default(true)
      ignore_errors: true

    - name: Check Prometheus StatefulSet status
      debug:
        msg: >
          Prometheus status:
          {{ 'Ready' if (prometheus_statefulset.resources[0].status.readyReplicas | default(0)) > 0 else 'Not Ready - check pod events for details' }}
      when: prometheus_needs_deployment | default(true)

    - name: Wait for node-exporter DaemonSet to be ready
      kubernetes.core.k8s_info:
        kind: DaemonSet
        namespace: "{{ namespace }}"
        name: "{{ prometheus_release_name }}-prometheus-node-exporter"
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
      register: node_exporter_daemonset
      retries: 6
      delay: 10
      until: >
        node_exporter_daemonset.resources is defined and
        node_exporter_daemonset.resources | length > 0 and
        node_exporter_daemonset.resources[0].status.numberReady is defined and
        node_exporter_daemonset.resources[0].status.numberReady > 0
      when: prometheus_needs_deployment | default(true)
      ignore_errors: true

    - name: Check node-exporter status
      debug:
        msg: >
          Node Exporter status:
          {{ 'Ready on ' + (node_exporter_daemonset.resources[0].status.numberReady | string) + ' nodes' if (node_exporter_daemonset.resources[0].status.numberReady | default(0)) > 0 else 'Not Ready - check pod events for details' }}
      when: prometheus_needs_deployment | default(true)

    # Deploy DCGM Exporter for GPU metrics if GPUs are detected
    - name: Deploy DCGM Exporter for GPU metrics
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Service
          metadata:
            name: dcgm-exporter
            namespace: "{{ namespace }}"
            labels:
              app: dcgm-exporter
          spec:
            type: ClusterIP
            ports:
              - name: metrics
                port: 9400
                targetPort: 9400
                protocol: TCP
            selector:
              app: dcgm-exporter
      when:
        - has_nvidia_gpus | default(false)
        - prometheus_needs_deployment | default(true)

    - name: Create DCGM Exporter DaemonSet
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: apps/v1
          kind: DaemonSet
          metadata:
            name: dcgm-exporter
            namespace: "{{ namespace }}"
          spec:
            selector:
              matchLabels:
                app: dcgm-exporter
            template:
              metadata:
                labels:
                  app: dcgm-exporter
              spec:
                nodeSelector:
                  nvidia.com/gpu.present: "true"
                containers:
                  - name: dcgm-exporter
                    image: nvcr.io/nvidia/k8s/dcgm-exporter:3.3.0-3.2.0-ubuntu20.04
                    ports:
                      - name: metrics
                        containerPort: 9400
                    securityContext:
                      runAsNonRoot: false
                      runAsUser: 0
                      capabilities:
                        add:
                          - SYS_ADMIN
                    volumeMounts:
                      - name: pod-gpu-resources
                        mountPath: /var/lib/kubelet/pod-resources
                        readOnly: true
                volumes:
                  - name: pod-gpu-resources
                    hostPath:
                      path: /var/lib/kubelet/pod-resources
      when:
        - has_nvidia_gpus | default(false)
        - prometheus_needs_deployment | default(true)

    - name: Create ServiceMonitor for DCGM Exporter
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: monitoring.coreos.com/v1
          kind: ServiceMonitor
          metadata:
            name: dcgm-exporter
            namespace: "{{ namespace }}"
            labels:
              app: dcgm-exporter
          spec:
            selector:
              matchLabels:
                app: dcgm-exporter
            endpoints:
              - port: metrics
                interval: 30s
                path: /metrics
      when:
        - has_nvidia_gpus | default(false)
        - prometheus_needs_deployment | default(true)

    # Wait for NFD to label nodes
    - name: Wait for NFD to label nodes
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Node
        kubeconfig: "{{ kubeconfig_path }}"
        validate_certs: "{{ validate_certs }}"
      register: labeled_nodes
      retries: 12
      delay: 5
      until: >
        labeled_nodes.resources | length > 0 and
        labeled_nodes.resources | map(attribute='metadata.labels') |
        select('defined') |
        map('dict2items') |
        flatten |
        selectattr('key', 'match', '^(feature\.node\.kubernetes\.io/|nfd\.node\.kubernetes\.io/)') |
        list | length > 0

    # Summary
    - name: Display deployment summary
      debug:
        msg: |
          Cluster Setup Complete:
          - NFD Status: {{ 'Deployed' if nfd_needs_deployment | default(true) else 'Already exists' }}
          - NVIDIA GPUs Detected: {{ has_nvidia_gpus | default(false) }}
          - Intel HPUs Detected: {{ has_intel_hpus | default(false) }}
          - GPU Operator: {{ 'Deployed' if has_nvidia_gpus | default(false) else 'Not needed' }}
          - Intel Device Plugin: {{ 'Deployed' if has_intel_hpus | default(false) else 'Not needed' }}
          - Aibrix Components: {{ 'Deployed' if aibrix_needs_deployment | default(true) else 'Already exists' }}
          - Prometheus Stack: {{ 'Deployed' if prometheus_needs_deployment | default(true) else 'Already exists' }}
          - DCGM Exporter: {{ 'Deployed' if (has_nvidia_gpus | default(false) and prometheus_needs_deployment | default(true)) else 'Not needed' }}

          Monitoring Status:
          - Prometheus collecting metrics from all nodes
          - Node Exporter monitoring hardware resources
          - Prometheus endpoint available for metrics collection
          {{ '- GPU metrics collection enabled via DCGM Exporter' if has_nvidia_gpus | default(false) else '' }}

          Next Steps:
          - Use get_node_info.yaml to retrieve hardware information
          - NFD labels will be automatically updated every 60 seconds
          - Aibrix autoscaler and model adapters are ready for use
          - Hardware metrics are being collected by Prometheus for pull-based collection

  post_tasks:
    - name: Cleanup kubeconfig file
      ansible.builtin.include_role:
        name: cleanup_kubeconfig
