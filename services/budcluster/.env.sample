APP_NAME=budcluster
NAMESPACE=development
LOG_LEVEL=DEBUG
CONFIGSTORE_NAME=configstore
CRYPTO_NAME=local-crypto
SECRETSTORE_NAME=local-secretstore
STATESTORE_NAME=statestore
APP_PORT=9081
DAPR_HTTP_PORT=3510
DAPR_GRPC_PORT=50001
DAPR_API_TOKEN=
DAPR_PLACEMENT_HOST=0.0.0.0
DAPR_PLACEMENT_PORT=50006
DAPR_METRICS_PORT=9190
DAPR_SCHEDULER_HOST=0.0.0.0
DAPR_SCHEDULER_PORT=50007

SECRETS_REDIS_URI=redis:6379
SECRETS_REDIS_PASSWORD=

VLLM_CPU_IMAGE=
VLLM_CUDA_IMAGE=

PSQL_HOST=127.0.0.1
PSQL_PORT=5432
PSQL_DB_NAME=
PSQL_USER=
PSQL_PASSWORD=

# Generate keys using `openssl`
# mkdir -p keys
# Generate a private RSA key, 4096-bit keys
# openssl genpkey -algorithm RSA -pkeyopt rsa_keygen_bits:4096 -out keys/rsa-private-key.pem
# Generate a 256-bit key for AES
# openssl rand -out keys/symmetric-key-256 32
RSA_KEY_NAME=<file name ending in .pem>
AES_SYMMETRIC_KEY_NAME=<any name>

VALIDATE_CERTS=true

# Node-info-collector removed - NFD is now the only method for hardware detection

QUANTIZATION_JOB_IMAGE=budimages.azurecr.io/budecosystem/bud-quantization:0.01
ENGINE_CONTAINER_PORT=8080

# NFD Configuration (Node Feature Discovery - required for hardware detection)
ENABLE_NFD_DETECTION=true
# NFD_FALLBACK_TO_CONFIGMAP is deprecated - NFD is now the only method
NFD_DETECTION_TIMEOUT=30
NFD_NAMESPACE=node-feature-discovery

# CPU optimization: skip master/control-plane nodes for CPU deployments (true/false)
SKIP_MASTER_NODE_FOR_CPU=true

REGISTRY_SERVER=registry-dev.bud.studio
REGISTRY_USERNAME=admin
REGISTRY_PASSWORD=

NOTIFY_SERVICE_NAME=notify
NOTIFY_SERVICE_TOPIC=

# Budserve
BUD_SERVE_URL=http://localhost:8050

# Minio
MINIO_ENDPOINT=bud-store.bud.studio
MINIO_ACCESS_KEY=
MINIO_SECRET_KEY=
MINIO_BUCKET=models-registry
MINIO_SECURE=true

# Model registry volume
VOLUME_TYPE=local
MODEL_REGISTRY_PATH=
NFS_SERVER=
NFS_PATH=

# Prometheus Configuration (Local to each cluster)
PROMETHEUS_ENABLED=true
PROMETHEUS_RETENTION=24h
PROMETHEUS_MEMORY_REQUEST=1Gi
PROMETHEUS_MEMORY_LIMIT=2Gi
PROMETHEUS_CPU_REQUEST=250m
PROMETHEUS_CPU_LIMIT=500m
PROMETHEUS_STORAGE_SIZE=20Gi
PROMETHEUS_SERVICE_NAME=bud-metrics-kube-prometheu-prometheus
PROMETHEUS_NAMESPACE=bud-system
PROMETHEUS_PORT=9090

# Metrics Collection Configuration
METRICS_COLLECTION_ENABLED=true
DEFAULT_COLLECTION_INTERVAL=300  # seconds (5 minutes)
METRICS_BATCH_SIZE=5000
METRICS_QUERY_TIMEOUT=30  # seconds

# OpenTelemetry Collector Configuration
OTEL_COLLECTOR_ENDPOINT=http://localhost:4318  # OTel Collector HTTP endpoint
OTEL_CONFIG_PATH=/etc/otel/config.yaml  # Path to OTel Collector configuration

# ClickHouse Configuration - ONLY for local OTel Collector container (docker-compose-dev.yaml)
# NOTE: budcluster does NOT connect to ClickHouse directly. It sends metrics to OTel Collector,
# which then writes to ClickHouse. These variables are used by the OTel Collector container.
CLICKHOUSE_HOST=localhost
CLICKHOUSE_PORT=9000
CLICKHOUSE_DATABASE=metrics
CLICKHOUSE_USER=default
CLICKHOUSE_PASSWORD=

# Metrics collection configuration
METRICS_COLLECTION_ENABLED=true  # Global flag to enable/disable metrics collection
METRICS_COLLECTION_TIMEOUT=30  # Query timeout in seconds
METRICS_BATCH_SIZE=20000  # Number of metrics per batch for OTel

# Legacy - kept for backward compatibility
PROMETHEUS_URL=http://localhost:9090

# HAMI GPU Time-Slicing Configuration
# HAMI (HAMi GPU Device Plugin) enables GPU sharing via time-slicing and is automatically
# installed during cluster onboarding when NVIDIA GPUs are detected
ENABLE_HAMI_METRICS=false  # Set to true to enable HAMI metrics collection and device enrichment (auto-enabled for GPU clusters)
HAMI_SCHEDULER_PORT=31993  # NodePort for HAMI scheduler metrics endpoint (device allocation metrics)
HAMI_DEVICE_PLUGIN_PORT=31992  # NodePort for HAMI device plugin monitor (per-container memory limit/usage)
HAMI_UTILIZATION_THRESHOLD=80  # Percentage threshold for considering a time-sliced GPU available

# DCGM GPU Hardware Metrics Configuration
# DCGM Exporter provides hardware-level GPU metrics (temperature, power, utilization, clocks)
# that complement HAMI's scheduling/allocation metrics
ENABLE_DCGM_METRICS=true  # Set to true to enable DCGM metrics collection
DCGM_EXPORTER_NAMESPACE=gpu-operator  # Namespace where DCGM Exporter is deployed
DCGM_EXPORTER_SERVICE=nvidia-dcgm-exporter  # DCGM Exporter service name
DCGM_EXPORTER_PORT=9400  # Port for DCGM Exporter metrics endpoint
