#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------


"""Implements core services and business logic that power the microservices, including key functionality and integrations."""

import math
import uuid
import warnings
from concurrent.futures import ProcessPoolExecutor
from copy import deepcopy
from dataclasses import asdict
from typing import Any, Dict, List, Optional, Tuple, Union

from budmicroframe.commons import logging
from budmicroframe.commons.constants import WorkflowStatus
from budmicroframe.commons.schemas import (
    ErrorResponse,
    NotificationContent,
    NotificationRequest,
    PaginatedResponse,
)
from budmicroframe.shared.dapr_service import DaprService
from budmicroframe.shared.dapr_workflow import DaprWorkflow
from pydantic import ValidationError
from sqlalchemy.orm import Session

from ..commons.config import app_settings
from ..commons.device_utils import normalize_device_type
from ..engine_ops import (
    get_compatible_engines,
    get_engine_args_and_envs,
    get_minimal_engine_args_and_envs,
)
from .direct_search import DirectSearchOptimizer
from .evolution import Evolution
from .models import SimulationResultsCRUD, SimulationResultsSchema
from .schemas import (
    BenchmarkConfigRequest,
    BenchmarkConfigResponse,
    ClusterInfo,
    ClusterMetrics,
    ClusterRecommendationRequest,
    ClusterRecommendationResponse,
    DeploymentConfigurationRequest,
    DeploymentConfigurationResponse,
    DeviceConfiguration,
    DeviceTypeConfiguration,
    DeviceTypeMetrics,
    ModelMemoryInfo,
    NodeConfiguration,
    NodeConfigurationRequest,
    NodeConfigurationResponse,
    NodeGroupConfiguration,
    SimulationMethod,
    SimulationMetrics,
    TPPPOption,
)


logger = logging.get_logger(__name__)


def calculate_available_gpu_memory(device: Dict[str, Any]) -> Tuple[float, float, float]:
    """Calculate available GPU memory for shared hardware mode.

    This helper function calculates the available GPU memory by subtracting
    allocated memory from total memory. It's used consistently across workflow
    and service logic for GPU time-slicing scenarios.

    Args:
        device: Device dictionary containing memory information with keys:
            - mem_per_GPU_in_GB or memory_gb: Total GPU memory
            - memory_allocated_gb: Currently allocated memory

    Returns:
        Tuple of (total_memory_gb, memory_allocated_gb, available_memory_gb)

    Note:
        For shared mode, 100% of available memory is used with no safety margin,
        as users typically add their own buffer in the total required memory.
    """
    total_memory_gb = device.get("mem_per_GPU_in_GB") or device.get("memory_gb") or 0.0
    memory_allocated_gb = device.get("memory_allocated_gb") or 0.0
    available_memory_gb = total_memory_gb - memory_allocated_gb
    return total_memory_gb, memory_allocated_gb, available_memory_gb


def ensure_json_serializable(obj):
    """Recursively ensure all values in the object are JSON serializable."""
    if isinstance(obj, dict):
        return {k: ensure_json_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [ensure_json_serializable(item) for item in obj]
    elif isinstance(obj, tuple):
        return list(obj)
    elif isinstance(obj, bool):
        return bool(obj)  # Ensure it's a Python bool, not numpy bool
    elif obj is None or isinstance(obj, (str, int, float)):
        return obj
    else:
        return str(obj)


dapr_workflow = DaprWorkflow()


def _is_heuristic_config(config: Dict[str, Any]) -> bool:
    """Check if a configuration was generated by heuristic mode.

    Heuristic configs only contain essential parameters:
    model, tensor_parallel_size, pipeline_parallel_size, target_device, concurrency.
    Evolution configs contain additional optimized parameters like
    scheduler_delay_factor, max_num_seqs, enable_chunked_prefill, etc.
    """
    heuristic_params = {"model", "tensor_parallel_size", "pipeline_parallel_size", "target_device", "concurrency"}

    # Check if config only contains heuristic parameters (or subset)
    config_params = set(config.keys())
    extra_params = config_params - heuristic_params

    # If there are no extra evolution parameters, it's from heuristic mode
    evolution_params = {
        "scheduler_delay_factor",
        "max_num_seqs",
        "enable_chunked_prefill",
        "enable_prefix_caching",
        "block_size",
    }

    has_evolution_params = bool(extra_params & evolution_params)
    return not has_evolution_params


class SimulationService:
    @staticmethod
    def _group_devices_by_type_across_cluster(
        cluster_info: List[Dict[str, Any]], cluster_topology: Dict[str, Any], user_hardware_mode: str
    ) -> Dict[str, Dict[str, Any]]:
        """Group devices by type across the entire cluster for PP-aware optimization.

        This method creates device groups that span across nodes and clusters,
        enabling better pipeline parallel planning by considering all available
        devices of the same type together.

        Args:
            cluster_info: List of cluster dictionaries with node and device information
            cluster_topology: Pre-analyzed cluster topology information
            user_hardware_mode: Hardware mode selected by user ("dedicated" or "shared")

        Returns:
            Dict mapping device type to device group information including:
                - devices: List of all devices of this type
                - cluster_id: ID of the cluster (assumes single cluster for now)
                - node_distribution: Count of devices per node
                - devices_by_node: Devices organized by node for PP planning

        Raises:
            ValueError: If shared mode is requested but no HAMI metrics are available
        """
        device_groups = {}

        for cluster in cluster_info:
            cluster_id = cluster["id"]

            for node in cluster.get("nodes", []):
                node_id = node["id"]
                node_name = node["name"]

                for device in node.get("devices", []):
                    device_type = device["type"]
                    available_count = device["available_count"]

                    # Skip master/control-plane nodes for CPU deployments only (if configured)
                    device_type_lower = device_type.lower()
                    if app_settings.skip_master_node_for_cpu and device_type_lower in ("cpu", "cpu_high"):
                        is_master = node.get("is_master", False)
                        if is_master:
                            logger.info(f"Skipping master node {node_name} for CPU deployment")
                            continue

                    # Filter devices based on user's hardware mode preference
                    if user_hardware_mode == "dedicated":
                        # cpu_high dedicated mode check using utilized_cores/utilized_memory_gb
                        # Note: Only cpu_high type is considered for utilization-based filtering
                        if device_type_lower == "cpu_high":
                            total_cores = device.get("cores") or 0
                            utilized_cores = device.get("utilized_cores") or 0.0
                            total_memory_gb = device.get("memory_gb") or device.get("mem_per_GPU_in_GB") or 0.0
                            utilized_memory_gb = device.get("utilized_memory_gb") or 0.0

                            # Calculate utilization percentage
                            core_util_percent = (utilized_cores / total_cores * 100) if total_cores > 0 else 100.0

                            # cpu_high dedicated threshold: < 5% core utilization
                            CPU_HIGH_DEDICATED_CORE_THRESHOLD_PERCENT = 5.0

                            if core_util_percent >= CPU_HIGH_DEDICATED_CORE_THRESHOLD_PERCENT:
                                logger.debug(
                                    f"Dedicated mode: Skipping cpu_high {device.get('name')} - "
                                    f"core utilization: {core_util_percent:.1f}% >= {CPU_HIGH_DEDICATED_CORE_THRESHOLD_PERCENT}% threshold "
                                    f"(utilized: {utilized_cores}/{total_cores} cores)"
                                )
                                continue

                            # Store available memory for later model weight validation
                            available_memory_gb = total_memory_gb - utilized_memory_gb
                            device["available_memory_gb"] = available_memory_gb

                            logger.info(
                                f"Dedicated mode: cpu_high {device.get('name')} available - "
                                f"core utilization: {core_util_percent:.1f}%, "
                                f"memory: {available_memory_gb:.1f}GB free of {total_memory_gb:.1f}GB"
                            )

                        elif device_type_lower == "cpu":
                            # Regular cpu type: skip utilization filtering, use available_count fallback
                            if available_count <= 0:
                                logger.debug(
                                    f"Dedicated mode: Skipping cpu {device.get('name')} - "
                                    f"available_count: {available_count}"
                                )
                                continue

                        else:
                            # GPU/HPU dedicated mode check (existing HAMI logic)
                            core_util = device.get("core_utilization_percent") or 0.0
                            memory_util = device.get("memory_utilization_percent") or 0.0

                            # If utilization metrics are available, enforce strict 0% requirement
                            if "core_utilization_percent" in device or "memory_utilization_percent" in device:
                                if core_util > 0 or memory_util > 0:
                                    logger.debug(
                                        f"Dedicated mode: Skipping device {device.get('name')} - "
                                        f"core utilization: {core_util}%, memory utilization: {memory_util}% "
                                        f"(requires 0% for both)"
                                    )
                                    continue
                            # If no utilization metrics, use available_count (backward compatibility)
                            elif available_count <= 0:
                                logger.debug(
                                    f"Dedicated mode: Skipping device {device.get('name')} - "
                                    f"available_count: {available_count}"
                                )
                                continue

                    elif user_hardware_mode == "shared":
                        device_type_lower = device_type.lower()

                        # CPU shared mode: skip utilization check, allow CPU devices with available memory
                        if device_type_lower in ("cpu", "cpu_high"):
                            total_cores = device.get("cores") or 0
                            utilized_cores = device.get("utilized_cores") or 0.0
                            total_memory_gb = device.get("memory_gb") or device.get("mem_per_GPU_in_GB") or 0.0
                            utilized_memory_gb = device.get("utilized_memory_gb") or 0.0

                            # Calculate available memory
                            available_memory_gb = total_memory_gb - utilized_memory_gb
                            device["available_memory_gb"] = available_memory_gb

                            if available_memory_gb <= 0:
                                logger.debug(
                                    f"Shared mode: Skipping {device_type_lower} {device.get('name')} - "
                                    f"no available memory (total: {total_memory_gb}GB, utilized: {utilized_memory_gb}GB)"
                                )
                                continue

                            logger.info(
                                f"Shared mode: {device_type_lower} {device.get('name')} available - "
                                f"memory: {available_memory_gb:.1f}GB free of {total_memory_gb:.1f}GB "
                                f"(cores: {total_cores - utilized_cores:.1f}/{total_cores} available)"
                            )
                            # Set available_count to 1 for shared mode
                            available_count = 1
                        else:
                            # GPU/HPU shared mode: Require HAMI metrics to calculate available memory
                            memory_allocated_value = device.get("memory_allocated_gb")
                            if memory_allocated_value is None:
                                logger.debug(
                                    f"Shared mode: Skipping device {device.get('name')} - "
                                    f"HAMI metrics (memory_allocated_gb) not available or None"
                                )
                                continue

                            # For shared mode, we'll use 100% of available memory (no threshold)
                            # Memory calculation will be done later in get_topk_engine_configs_per_cluster
                            total_memory_gb, _, available_memory_gb = calculate_available_gpu_memory(device)
                            memory_allocated_gb = memory_allocated_value  # Already validated as not None

                            if available_memory_gb <= 0:
                                logger.debug(
                                    f"Shared mode: Skipping device {device.get('name')} - "
                                    f"no available memory (total: {total_memory_gb}GB, "
                                    f"allocated: {memory_allocated_gb}GB)"
                                )
                                continue

                            logger.debug(
                                f"Shared mode: Device {device.get('name')} has {available_memory_gb:.2f}GB "
                                f"available (total: {total_memory_gb:.2f}GB, allocated: {memory_allocated_gb:.2f}GB)"
                            )
                            # Set available_count to 1 for shared mode since we can use partial GPU
                            available_count = 1

                    # Handle backward compatibility: map generic 'gpu' to specific type
                    if device_type == "gpu":
                        # Try to determine specific type from vendor if available
                        vendor = device.get("vendor", "").lower()
                        if "amd" in vendor or "ati" in vendor:
                            device_type = "rocm"
                        elif "intel" in vendor:
                            device_type = "hpu"
                        else:
                            # Default to cuda for generic GPU (most common)
                            device_type = "cuda"
                        logger.info(f"Mapped generic 'gpu' type to '{device_type}' based on vendor: {vendor}")

                    logger.debug(
                        f"Processing device {device.get('name', 'unknown')} in node {node_id}: "
                        f"type={device_type}, available_count={available_count}"
                    )

                    # Skip devices with 0 available count
                    if available_count <= 0:
                        logger.warning(
                            f"Skipping device {device.get('name', 'unknown')} in node {node_id}: "
                            f"available_count = {available_count}"
                        )
                        continue

                    # CPU devices are now supported for LLM inference
                    # The CPU compatibility checks in vllm.py will enforce proper constraints

                    # Initialize device group if not exists
                    if device_type not in device_groups:
                        device_groups[device_type] = {
                            "devices": [],
                            "cluster_id": cluster_id,
                            "node_distribution": {},
                            "devices_by_node": {},
                        }

                    # Add device spec to group (one record per device type per node)
                    device_copy = deepcopy(device)
                    device_copy["node_id"] = node_id
                    device_copy["node_name"] = node_name
                    device_copy["cluster_id"] = cluster_id
                    device_groups[device_type]["devices"].append(device_copy)

                    # Track node distribution (available_count represents actual physical devices)
                    if node_id not in device_groups[device_type]["node_distribution"]:
                        device_groups[device_type]["node_distribution"][node_id] = 0
                        device_groups[device_type]["devices_by_node"][node_id] = []

                    device_groups[device_type]["node_distribution"][node_id] += available_count
                    device_groups[device_type]["devices_by_node"][node_id].append(device_copy)

        # Add summary statistics to each device group
        for device_type, group in device_groups.items():
            group["total_nodes_with_device"] = len(group["node_distribution"])
            group["max_devices_per_node"] = (
                max(group["node_distribution"].values()) if group["node_distribution"] else 0
            )
            group["min_devices_per_node"] = (
                min(group["node_distribution"].values()) if group["node_distribution"] else 0
            )
            # Add total_devices field for proper validation later
            group["total_devices"] = sum(group["node_distribution"].values()) if group["node_distribution"] else 0

            logger.debug(
                f"Device type {device_type}: {len(group['devices'])} devices across "
                f"{group['total_nodes_with_device']} nodes, max_per_node={group['max_devices_per_node']}, "
                f"total_devices={group['total_devices']}, node_distribution={group['node_distribution']}"
            )

        return device_groups

    @staticmethod
    def analyze_cluster_topology(cluster_info: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze cluster topology for TP+PP planning.

        This method analyzes the cluster structure to determine:
        - Node capacities and device distributions
        - Maximum TP size per node (intra-node parallelism)
        - Maximum PP size (inter-node parallelism)
        - Total cluster resources

        Args:
            cluster_info: List of cluster dictionaries with node and device information

        Returns:
            Dict containing cluster topology analysis for TP+PP optimization
        """
        node_capacities = []
        total_nodes = 0
        max_tp_per_node = 0
        total_cluster_devices = 0

        for cluster in cluster_info:
            for node in cluster.get("nodes", []):
                total_nodes += 1
                device_types = {}
                node_total_devices = 0

                # Group devices by type within this node
                for device in node.get("devices", []):
                    device_type = device["type"]
                    available_count = device["available_count"]

                    if device_type not in device_types:
                        device_types[device_type] = 0
                    device_types[device_type] += available_count
                    node_total_devices += available_count

                # Get bandwidth from first device if available, otherwise use defaults
                devices = node.get("devices", [])
                inter_node_bandwidth = 200  # Default
                intra_node_bandwidth = 300  # Default

                if devices:
                    inter_node_bandwidth = devices[0].get("inter_node_bandwidth_in_GB_per_sec", 200)
                    intra_node_bandwidth = devices[0].get("intra_node_bandwidth_in_GB_per_sec", 300)

                node_capacities.append(
                    {
                        "cluster_id": cluster["id"],
                        "node_id": node["id"],
                        "node_name": node["name"],
                        "device_types": device_types,
                        "total_devices": node_total_devices,
                        "inter_node_bandwidth": inter_node_bandwidth,
                        "intra_node_bandwidth": intra_node_bandwidth,
                    }
                )

                # Track maximums
                max_tp_per_node = max(max_tp_per_node, node_total_devices)
                total_cluster_devices += node_total_devices

        return {
            "total_nodes": total_nodes,
            "node_capacities": node_capacities,
            "max_tp_per_node": max_tp_per_node,
            "total_cluster_devices": total_cluster_devices,
            "max_pp_size": total_nodes,  # PP can span up to all nodes
        }

    @staticmethod
    def get_simulation_method(request: Optional[ClusterRecommendationRequest] = None) -> SimulationMethod:
        """Determine which simulation method to use based on request and configuration.

        Args:
            request: The cluster recommendation request that may contain simulation_method

        Returns:
            SimulationMethod: The method to use for simulation (regressor or heuristic)
        """
        if request is not None and request.simulation_method is not None:
            return request.simulation_method

        # Fall back to configuration default
        default_method = app_settings.default_simulation_method.lower()
        if default_method == "heuristic":
            return SimulationMethod.HEURISTIC
        else:
            return SimulationMethod.REGRESSOR

    @staticmethod
    def get_eta(
        current_step: str,
        cluster_count: int = 1,
        step_time: int = None,
        simulation_method: Optional[SimulationMethod] = None,
        total_devices: int = None,
        cluster_info: List[Dict[str, Any]] = None,
    ):
        """Calculate the estimated time to completion for workflow steps.

        Enhanced ETA calculation that considers simulation method, device count,
        and actual algorithm performance characteristics.

        Args:
            current_step (str): The current step in the workflow process.
            cluster_count (int): Number of clusters being processed.
            step_time (int, optional): Override the default time for the current step.
            simulation_method (SimulationMethod, optional): The simulation method being used.
            total_devices (int, optional): Total number of devices across all clusters.
            cluster_info (List[Dict], optional): Detailed cluster information for device counting.

        Returns:
            int: The estimated time in minutes for remaining steps.
        """
        # Base step times in seconds
        base_step_times = {
            "validation": 2,  # Quick validation
            "performance_estimation": 10,  # Base time, scaled by method and devices
            "ranking": 3,  # Quick ranking and sorting
        }

        # Calculate actual device count if not provided
        if total_devices is None and cluster_info:
            total_devices = SimulationService._count_total_devices(cluster_info)

        if total_devices is None:
            total_devices = cluster_count * 2  # Fallback estimate

        # Determine simulation method if not provided
        if simulation_method is None:
            simulation_method = SimulationService.get_simulation_method()

        # Calculate performance estimation time based on simulation method and devices
        if simulation_method == SimulationMethod.HEURISTIC:
            # DirectSearch: ~2-3 seconds per device (much faster)
            device_time = 2.5
            # Account for parallel processing (assume 4 concurrent threads)
            parallel_factor = min(total_devices / 4, 4)
            performance_time = max(device_time * parallel_factor, 5)
        else:
            # Evolution: ~15-30 seconds per device (50 generations Ã— 10 population)
            device_time = 20
            # Evolution is more CPU intensive, less parallel benefit
            parallel_factor = min(total_devices / 2, 8)
            performance_time = max(device_time * parallel_factor, 15)

        step_times = base_step_times.copy()
        step_times["performance_estimation"] = int(performance_time)

        # Override with custom step time if provided
        if step_time is not None:
            step_times[current_step] = step_time

        # Define the order of steps
        step_order = ["validation", "performance_estimation", "ranking"]

        # Calculate total time for current and future steps
        total_time = 0
        current_step_index = step_order.index(current_step) if current_step in step_order else 0

        for i in range(current_step_index, len(step_order)):
            step = step_order[i]
            total_time += step_times.get(step, 30)  # Default 30 seconds for unknown steps

        # Add buffer for database operations and network overhead (10-20%)
        buffer_factor = 1.15
        total_time_seconds = int(total_time * buffer_factor)

        # Convert to minutes and round up (UI expects minutes)
        total_time_minutes = max(1, int((total_time_seconds + 59) // 60))  # Round up to nearest minute

        logger.debug(
            f"ETA calculation: step={current_step}, method={simulation_method}, "
            f"devices={total_devices}, clusters={cluster_count}, estimated_time={total_time_seconds}s ({total_time_minutes}m)"
        )

        return total_time_minutes

    @staticmethod
    def _count_total_devices(cluster_info: List[Dict[str, Any]]) -> int:
        """Count total devices across all clusters."""
        total_devices = 0
        for cluster in cluster_info:
            for node in cluster.get("nodes", []):
                for device in node.get("devices", []):
                    available_count = device.get("available_count", 0)
                    if available_count > 0:
                        total_devices += available_count
        return total_devices

    @staticmethod
    def get_topk_engine_configs(
        engine_name: str,
        pretrained_model_uri: str,
        input_tokens: int,
        output_tokens: int,
        concurrency: int,
        target_ttft: float,
        target_throughput_per_user: float,
        target_e2e_latency: float,
        device_config: Dict[str, Any],
        quantization_type: str,
        engine_image: str,
        simulation_method: SimulationMethod,
        model_uri: Optional[str] = None,
        engine_version: Optional[str] = None,
        tool_calling_parser_type: Optional[str] = None,
        reasoning_parser_type: Optional[str] = None,
        architecture_family: Optional[str] = None,
        chat_template: Optional[str] = None,
        supports_lora: bool = False,
        supports_pipeline_parallelism: bool = False,
        hardware_mode: str = "dedicated",
        **kwargs: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Generate the top K deployment configurations based on the provided parameters.

        Args:
            engine_name (str): The name of the engine to be used for deployment.
            pretrained_model_uri (str): The URI of the pretrained model.
            input_tokens (int): The number of input tokens.
            output_tokens (int): The number of output tokens.
            concurrency (int): The level of concurrency for the deployment.
            target_throughput (int): The target throughput for the deployment.
            target_ttft (int): The target time to first token (TTFT).
            target_throughput_per_user (int): The target throughput per user.
            device_config (Dict[str, Any]): Configuration details for the device.
            **kwargs (Dict[str, Any]): Additional keyword arguments.

        Returns:
            Dict[str, Any]: A dictionary containing the top K configurations, the engine name, and the device configuration.
        """
        try:
            if simulation_method == SimulationMethod.HEURISTIC:
                # Use DirectSearchOptimizer for heuristic mode
                logger.info("Using DirectSearchOptimizer for heuristic mode")
                optimizer = DirectSearchOptimizer(
                    model=pretrained_model_uri,
                    model_uri=model_uri,  # Pass the HuggingFace model identifier
                    input_tokens=input_tokens,
                    output_tokens=output_tokens,
                    max_concurrency=concurrency,
                    target_ttft=target_ttft,
                    target_throughput_per_user=target_throughput_per_user,
                    target_e2e_latency=target_e2e_latency,
                    engine_name=engine_name,
                    device_config=device_config,
                    benchmark_predictor_models_dir=app_settings.benchmark_predictor_models_dir,
                    dtype=quantization_type,
                    use_heuristic=True,  # DirectSearchOptimizer uses heuristic calculations
                    supports_pipeline_parallelism=supports_pipeline_parallelism,
                    hardware_mode=hardware_mode,
                    is_quantization=bool(quantization_type),
                    supports_lora=supports_lora,
                )
                top_k_configs = optimizer.search()
                # Convert SearchResult dataclasses to dicts for JSON serialization in workflow mode
                top_k_configs = [asdict(result) for result in top_k_configs]
            else:
                # Use Evolution for regressor mode
                logger.info("Using Evolution for regressor mode")
                evolution = Evolution(
                    model=pretrained_model_uri,
                    input_tokens=input_tokens,
                    output_tokens=output_tokens,
                    max_concurrency=concurrency,
                    target_ttft=target_ttft,
                    target_throughput_per_user=target_throughput_per_user,
                    target_e2e_latency=target_e2e_latency,
                    engine_name=engine_name,
                    device_config=device_config,
                    benchmark_predictor_models_dir=app_settings.benchmark_predictor_models_dir,
                    generation=app_settings.generation_count,
                    population_size=app_settings.population_size,
                    dtype=quantization_type,
                    use_heuristic=False,  # Evolution uses ML regressor
                    supports_pipeline_parallelism=supports_pipeline_parallelism,
                )
                top_k_configs = evolution.evolve()
                # Convert EvaluationResult dataclasses to dicts for JSON serialization in workflow mode
                top_k_configs = [asdict(result) for result in top_k_configs]

            # Add hardware_mode to each config for downstream processing (common to both paths)
            for config in top_k_configs:
                config["hardware_mode"] = hardware_mode

            # Handle case where no valid configurations were found
            if not top_k_configs:
                logger.warning(
                    f"No valid configurations found for model {pretrained_model_uri} "
                    f"on device {device_config.get('name', 'Unknown')} with {device_config.get('mem_per_GPU_in_GB', 0):.2f}GB memory"
                )
                # Return empty result for this device
                return ensure_json_serializable(
                    {
                        "top_k_configs": [],
                        "engine": engine_name,
                        "engine_image": engine_image,
                        "engine_version": engine_version,
                        "tool_calling_parser_type": tool_calling_parser_type,
                        "reasoning_parser_type": reasoning_parser_type,
                        "architecture_family": architecture_family,
                        "chat_template": chat_template,
                        "supports_lora": supports_lora,
                        "supports_pipeline_parallelism": supports_pipeline_parallelism,
                        "device_config": device_config,
                        "error": "No valid configurations found - device may not have enough memory for this model",
                    }
                )

            # Handle both cluster-wide and legacy device configurations
            if "node_distribution" in device_config:
                # Cluster-wide configuration: create results for each device
                results = []
                devices_by_node = device_config.get("devices_by_node", {})

                for node_id, devices in devices_by_node.items():
                    for device in devices:
                        device_result = {
                            "device_id": device.get("id", str(uuid.uuid4())),
                            "device_type": device_config["device_type"],
                            "device_name": device.get("name", device.get("id")),
                            "node_id": node_id,
                            "node_name": device.get("node_name", node_id),
                            "cluster_id": device_config["cluster_id"],
                            "available_count": device.get("available_count", 1),
                            **{
                                k.lower(): v
                                for k, v in device.items()
                                if k
                                not in ["id", "type", "name", "node_id", "node_name", "cluster_id", "available_count"]
                            },
                        }

                        results.append(
                            {
                                "top_k_configs": top_k_configs,
                                "engine": engine_name,
                                "engine_image": engine_image,
                                "engine_version": engine_version,
                                "tool_calling_parser_type": tool_calling_parser_type,
                                "reasoning_parser_type": reasoning_parser_type,
                                "architecture_family": architecture_family,
                                "chat_template": chat_template,
                                "supports_lora": supports_lora,
                                "supports_pipeline_parallelism": supports_pipeline_parallelism,
                                "device_config": device_result,
                            }
                        )

                return ensure_json_serializable(
                    results
                    if results
                    else [
                        {
                            "top_k_configs": top_k_configs,
                            "engine": engine_name,
                            "engine_image": engine_image,
                            "engine_version": engine_version,
                            "tool_calling_parser_type": tool_calling_parser_type,
                            "reasoning_parser_type": reasoning_parser_type,
                            "architecture_family": architecture_family,
                            "chat_template": chat_template,
                            "supports_lora": supports_lora,
                            "supports_pipeline_parallelism": supports_pipeline_parallelism,
                            "device_config": device_config,
                        }
                    ]
                )
            else:
                # Legacy per-device configuration
                device_config["device_id"] = device_config.pop("id", str(uuid.uuid4()))
                device_config["device_type"] = device_config.pop("type")
                device_config["device_name"] = device_config.pop("name", device_config["device_id"])
                # Ensure available_count is present for legacy configs
                if "available_count" not in device_config:
                    device_config["available_count"] = 1
                device_config = {k.lower(): v for k, v in device_config.items()}

                return ensure_json_serializable(
                    {
                        "top_k_configs": top_k_configs,
                        "engine": engine_name,
                        "engine_image": engine_image,
                        "engine_version": engine_version,
                        "tool_calling_parser_type": tool_calling_parser_type,
                        "reasoning_parser_type": reasoning_parser_type,
                        "architecture_family": architecture_family,
                        "chat_template": chat_template,
                        "supports_lora": supports_lora,
                        "supports_pipeline_parallelism": supports_pipeline_parallelism,
                        "device_config": device_config,
                    }
                )
        except Exception as e:
            logger.exception(
                "Error running simulation for %s with device type %s: %s",
                engine_name,
                device_config.get("device_type") or device_config.get("type"),
                str(e),
            )
            # Return error result instead of None so it can be properly handled
            return ensure_json_serializable(
                {
                    "top_k_configs": [],
                    "engine": engine_name,
                    "engine_image": engine_image,
                    "engine_version": engine_version,
                    "tool_calling_parser_type": tool_calling_parser_type,
                    "reasoning_parser_type": reasoning_parser_type,
                    "architecture_family": architecture_family,
                    "chat_template": chat_template,
                    "device_config": device_config,
                    "error": f"Simulation failed: {str(e)}",
                }
            )

    @staticmethod
    def get_topk_proprietary_engine_configs(
        engine_name: str,
        pretrained_model_uri: str,
        input_tokens: int,
        output_tokens: int,
        concurrency: int,
        device_config: Dict[str, Any],
        engine_image: str,
        simulation_method: SimulationMethod,
        model_uri: Optional[str] = None,
        engine_version: Optional[str] = None,
        tool_calling_parser_type: Optional[str] = None,
        reasoning_parser_type: Optional[str] = None,
        architecture_family: Optional[str] = None,
        chat_template: Optional[str] = None,
        **kwargs: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Generate the top K deployment configurations based on the provided parameters.

        Args:
            engine_name (str): The name of the engine to be used for deployment.
            pretrained_model_uri (str): The URI of the pretrained model.
            input_tokens (int): The number of input tokens.
            output_tokens (int): The number of output tokens.
            concurrency (int): The level of concurrency for the deployment.
            target_throughput (int): The target throughput for the deployment.
            target_ttft (int): The target time to first token (TTFT).
            target_throughput_per_user (int): The target throughput per user.
            device_config (Dict[str, Any]): Configuration details for the device.
            **kwargs (Dict[str, Any]): Additional keyword arguments.

        Returns:
            Dict[str, Any]: A dictionary containing the top K configurations, the engine name, and the device configuration.
        """
        import random

        from ..engine_ops import get_engine_max_concurrency
        from .evolution import EvaluationResult

        top_k_configs = [
            EvaluationResult(
                config={"model": pretrained_model_uri},
                total_memory=0.0,
                kv_cache_memory=512,
                ttft=0,
                e2e_latency=0,
                throughput_per_user=0,
                concurrency=get_engine_max_concurrency(engine_name),
                fitness=(0, 0, 0),
                error_rate=0,
                cost_per_million_tokens=random.random(),
            )
        ]
        # Convert EvaluationResult dataclasses to dicts for JSON serialization in workflow mode
        top_k_configs = [asdict(result) for result in top_k_configs]

        device_config["device_id"] = device_config.pop("id", str(uuid.uuid4()))
        device_config["device_type"] = device_config.pop("type")
        device_config["device_name"] = device_config.pop("name", device_config["device_id"])
        device_config = {k.lower(): v for k, v in device_config.items()}
        return {
            "top_k_configs": top_k_configs,
            "engine": engine_name,
            "engine_image": engine_image,
            "engine_version": engine_version,
            "tool_calling_parser_type": tool_calling_parser_type,
            "reasoning_parser_type": reasoning_parser_type,
            "architecture_family": architecture_family,
            "chat_template": chat_template,
            "device_config": device_config,
        }

    @staticmethod
    def get_topk_quantization_engine_configs(
        engine_name: str,
        pretrained_model_uri: str,
        device_config: Dict[str, Any],
        quantization_method: str,
        quantization_type: str,
        engine_image: str,
        simulation_method: SimulationMethod,
        model_uri: Optional[str] = None,
        engine_version: Optional[str] = None,
        tool_calling_parser_type: Optional[str] = None,
        reasoning_parser_type: Optional[str] = None,
        architecture_family: Optional[str] = None,
        chat_template: Optional[str] = None,
        **kwargs: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Generate the top K quantization engine configurations based on the provided parameters.

        Args:
            engine_name (str): The name of the engine to be used for quantization.
            pretrained_model_uri (str): The URI of the pretrained model.
            device_config (Dict[str, Any]): Configuration details for the device.
            method (str): The quantization method to be used (e.g., "RTN", "AWQ").
            **kwargs (Dict[str, Any]): Additional keyword arguments.

        Returns:
            Dict[str, Any]: A dictionary containing the top K configurations, the engine name, and the device configuration.
        """
        from .evolution import EvaluationResult
        from .hardware import CostCalculator

        cost_calculator = CostCalculator()

        top_k_configs = [
            EvaluationResult(
                config={
                    "model": pretrained_model_uri,
                    "quantization_method": quantization_method,
                    "quantization_type": quantization_type,
                },
                total_memory=0.0,
                kv_cache_memory=512,
                ttft=0,
                e2e_latency=0,
                throughput_per_user=0,
                concurrency=1,
                fitness=(0, 0, 0),
                error_rate=0,
                cost_per_million_tokens=cost_calculator.get_quantization_cost(
                    pretrained_model_uri, quantization_method, device_config
                ),
            )
        ]
        # Convert EvaluationResult dataclasses to dicts for JSON serialization in workflow mode
        top_k_configs = [asdict(result) for result in top_k_configs]

        device_config["device_id"] = device_config.pop("id", str(uuid.uuid4()))
        device_config["device_type"] = device_config.pop("type")
        device_config["device_name"] = device_config.pop("name", device_config["device_id"])
        device_config = {k.lower(): v for k, v in device_config.items()}
        return {
            "top_k_configs": top_k_configs,
            "engine": engine_name,
            "engine_image": engine_image,
            "engine_version": engine_version,
            "tool_calling_parser_type": tool_calling_parser_type,
            "reasoning_parser_type": reasoning_parser_type,
            "architecture_family": architecture_family,
            "chat_template": chat_template,
            "device_config": device_config,
        }

    @staticmethod
    def validate_cluster_info(cluster_info: Union[List[Dict[str, Any]], str]) -> List[Dict[str, Any]]:
        """Validate and normalize cluster information.

        This function validates the provided cluster information against the ClusterInfo model
        and returns a normalized JSON representation of the data.

        Args:
            cluster_info (Union[List[Dict[str, Any]], str]): The cluster information to validate,
                either as a JSON string or a list of dictionaries.

        Returns:
            List[Dict[str, Any]]: A validated and normalized list of cluster information dictionaries.

        Raises:
            ValidationError: If the provided cluster information does not conform to the ClusterInfo model.
        """
        model = (
            ClusterInfo.model_validate_json(cluster_info)
            if isinstance(cluster_info, str)
            else ClusterInfo.model_validate(cluster_info)
        )

        return model.model_dump(mode="json")

    @staticmethod
    def get_available_clusters(
        workflow_id: str,
        notification_request: NotificationRequest,
        target_topic_name: Optional[str] = None,
        target_name: Optional[str] = None,
        cluster_id: Optional[uuid.UUID] = None,
    ) -> List[Dict[str, Any]]:
        """Retrieve available clusters for simulation.

        This function fetches cluster information from the state store, validates it,
        and filters by cluster_id if provided. It sends notifications about the
        validation process and any errors encountered.

        Args:
            workflow_id (str): The ID of the workflow.
            notification_request (NotificationRequest): The notification request object.
            target_topic_name (Optional[str], optional): The target topic name for notifications. Defaults to None.
            target_name (Optional[str], optional): The target name for notifications. Defaults to None.
            cluster_id (Optional[uuid.UUID], optional): The specific cluster ID to filter by. Defaults to None.

        Returns:
            List[Dict[str, Any]]: A list of available cluster information dictionaries.

        Raises:
            ValidationError: If the cluster information fails validation.
            ValueError: If no cluster information is available.
            Exception: For any other errors encountered during the process.
        """
        notification_req = notification_request.model_copy(deep=True)
        notification_req.payload.event = "validation"

        notification_req.payload.content = NotificationContent(
            title="Requirement validation",
            message="Validating the requirements for the simulation",
            status=WorkflowStatus.STARTED,
        )
        dapr_workflow.publish_notification(
            workflow_id=workflow_id,
            notification=notification_req,
            target_topic_name=target_topic_name,
            target_name=target_name,
        )

        try:
            with DaprService() as dapr_service:
                cluster_info = dapr_service.get_state(
                    app_settings.statestore_name, app_settings.cluster_info_state_key
                )

            cluster_info = SimulationService.validate_cluster_info(cluster_info.data.decode("utf-8"))

            # Filter out inactive nodes in each cluster
            # Only include nodes that are explicitly marked as active (status=True)
            # and not in problematic states
            for cluster in cluster_info:
                active_nodes = []
                for node in cluster.get("nodes", []):
                    # Check if node has an explicit status
                    node_status = node.get("status")

                    # Skip nodes that are:
                    # - Explicitly marked as inactive (status=False)
                    # - Missing status information (safer to exclude)
                    # - In problematic states (NotReady, Unschedulable, UnderPressure)
                    # - Have no devices available
                    if node_status is True:
                        # Check if node has devices
                        node_devices = node.get("devices", [])
                        if not node_devices:
                            logger.warning(f"Node {node.get('name', 'unknown')} has no devices, skipping")
                            continue

                        # Additional check for node condition if available
                        node_condition = node.get("condition", "").lower()
                        if node_condition not in ["notready", "unschedulable", "underpressure"]:
                            active_nodes.append(node)

                cluster["nodes"] = active_nodes

            if cluster_id:
                cluster_info = [cluster for cluster in cluster_info if cluster["id"] == str(cluster_id)]

            if not len(cluster_info):
                raise ValueError("Cluster info is required to run the simulation")

            # Validate that clusters have available nodes after filtering
            clusters_with_nodes = []
            for cluster in cluster_info:
                if len(cluster.get("nodes", [])) > 0:
                    clusters_with_nodes.append(cluster)
                else:
                    logger.warning(f"Cluster {cluster['id']} has no available nodes after filtering")

            if not clusters_with_nodes:
                raise ValueError(
                    "No clusters with available nodes found. All nodes are either inactive, tainted, or in problematic states."
                )

            # Log summary of available resources
            total_nodes = sum(len(cluster.get("nodes", [])) for cluster in clusters_with_nodes)
            logger.info(f"Found {len(clusters_with_nodes)} clusters with {total_nodes} available nodes for simulation")

            return clusters_with_nodes
        except ValidationError as e:
            logger.exception("Error validating cluster info: %s", str(e))
            notification_req.payload.content = NotificationContent(
                title="Invalid cluster info",
                message="Fix: Contact support",
                status=WorkflowStatus.FAILED,
            )
            dapr_workflow.publish_notification(
                workflow_id=workflow_id,
                notification=notification_req,
                target_topic_name=target_topic_name,
                target_name=target_name,
            )
            raise e
        except Exception as e:
            logger.exception("Error getting cluster info: %s", str(e))
            # Provide more specific error message for node availability issues
            if "No clusters with available nodes found" in str(e):
                notification_req.payload.content = NotificationContent(
                    title="No available nodes in cluster",
                    message="All nodes are inactive, tainted, or in problematic states. Please check cluster health.",
                    status=WorkflowStatus.FAILED,
                    primary_action="retry",
                )
            else:
                notification_req.payload.content = NotificationContent(
                    title="No clusters found" if cluster_id is None else "The cluster is not available",
                    message="Fix: Go to cluster management to add clusters",
                    status=WorkflowStatus.FAILED,
                    primary_action="retry",
                )
            dapr_workflow.publish_notification(
                workflow_id=workflow_id,
                notification=notification_req,
                target_topic_name=target_topic_name,
                target_name=target_name,
            )
            raise e

    @staticmethod
    def get_compatible_engines(
        workflow_id: str,
        pretrained_model_uri: str,
        cluster_info: List[Dict[str, Any]],
        notification_request: NotificationRequest,
        model_uri: Optional[str] = None,
        target_topic_name: Optional[str] = None,
        target_name: Optional[str] = None,
        proprietary_only: bool = False,
        cluster_id: Optional[uuid.UUID] = None,
        model_endpoints: Optional[str] = None,
    ) -> List[Dict[str, str]]:
        """Identify compatible engines for a given model and cluster configuration.

        This method determines which inference engines are compatible with the specified
        pretrained model, considering the available clusters. It sends notifications about
        the progress and results of the compatibility check.

        Args:
            workflow_id (str): The ID of the current workflow.
            pretrained_model_uri (str): URI of the pretrained model to check compatibility for.
            cluster_info (List[Dict[str, Any]]): Information about available clusters.
            notification_request (NotificationRequest): Template for notifications to be sent.
            target_topic_name (Optional[str], optional): Topic name for notifications. Defaults to None.
            target_name (Optional[str], optional): Target name for notifications. Defaults to None.
            proprietary_only (bool, optional): Whether to check only proprietary engines. Defaults to False.
            cluster_id (Optional[uuid.UUID], optional): Specific cluster ID to check. Defaults to None.
            model_endpoints (Optional[str], optional): Comma-separated endpoint types (e.g., "EMBEDDING", "LLM").

        Returns:
            List[Dict[str, str]]: List of compatible engine configurations, each containing
                                 engine_name and device information.

        Raises:
            Exception: If compatibility check fails, with appropriate notification sent.
        """
        notification_req = notification_request.model_copy(deep=True)

        try:
            # Notify the ETA based on simulation method, clusters, and devices
            simulation_method = SimulationService.get_simulation_method()
            eta_minutes = SimulationService.get_eta(
                current_step="validation",
                cluster_count=len(cluster_info),
                simulation_method=simulation_method,
                cluster_info=cluster_info,
            )

            notification_req.payload.event = "eta"
            notification_req.payload.content = NotificationContent(
                title="Estimated time to completion",
                message=f"{eta_minutes}",  # UI expects just the number in minutes
                status=WorkflowStatus.RUNNING,
            )
            dapr_workflow.publish_notification(
                workflow_id=workflow_id,
                notification=notification_req,
                target_topic_name=target_topic_name,
                target_name=target_name,
            )

            compatible_engines = get_compatible_engines(
                pretrained_model_uri, model_uri, proprietary_only, model_endpoints
            )

            if len(compatible_engines) == 0:
                raise ValueError("No compatible engines found")

            notification_req.payload.event = "validation"
            notification_req.payload.content = NotificationContent(
                title="Identified compatible clusters" if cluster_id is None else "Cluster is compatible",
                message="All requirements and runtime compatibility are met for the cluster(s)",
                status=WorkflowStatus.COMPLETED,
            )
            dapr_workflow.publish_notification(
                workflow_id=workflow_id,
                notification=notification_req,
                target_topic_name=target_topic_name,
                target_name=target_name,
            )
            return compatible_engines
        except Exception as e:
            notification_req.payload.event = "validation"
            notification_req.payload.content = NotificationContent(
                title="Model is not compatible with engine",
                message="Fix: Try a different model",
                status=WorkflowStatus.FAILED,
                primary_action="retry",
            )
            dapr_workflow.publish_notification(
                workflow_id=workflow_id,
                notification=notification_req,
                target_topic_name=target_topic_name,
                target_name=target_name,
            )
            raise e

    def get_topk_engine_configs_per_cluster(
        self,
        workflow_id: str,
        request: ClusterRecommendationRequest,
        compatible_engines: List[Tuple[str, str]],
        cluster_info: List[Dict[str, Any]],
        notification_request: NotificationRequest,
        cluster_id: Optional[uuid.UUID] = None,
    ) -> List[Dict[str, Any]]:
        """Get the top-k engine configurations for each cluster.

        This function processes each cluster and its nodes to find the best engine configurations
        based on the compatible engines and device types. It uses parallel processing to evaluate
        multiple configurations simultaneously.

        Args:
            workflow_id (str): The ID of the workflow.
            request (ClusterRecommendationRequest): The request containing parameters for recommendation.
            compatible_engines (List[Tuple[str, str]]): List of compatible engine and device combinations.
            cluster_info (List[Dict[str, Any]]): Information about available clusters.
            notification_request (NotificationRequest): The notification request to update workflow status.
            cluster_id (Optional[uuid.UUID], optional): Specific cluster ID to focus on. Defaults to None.

        Returns:
            List[Dict[str, Any]]: List of top engine configurations for each cluster.

        Raises:
            Exception: If there's an error during configuration generation, with appropriate notification sent.
        """
        notification_req = notification_request.model_copy(deep=True)
        notification_req.payload.event = "performance_estimation"

        # Calculate updated ETA for performance estimation step
        simulation_method = self.get_simulation_method(request)
        eta_minutes = self.get_eta(
            current_step="performance_estimation",
            cluster_count=len(cluster_info),
            simulation_method=simulation_method,
            cluster_info=cluster_info,
        )

        notification_req.payload.content = NotificationContent(
            title="Generating best configurations",
            message=f"Estimating performance metrics using {simulation_method.value} method (~{eta_minutes}m remaining)",
            status=WorkflowStatus.STARTED,
        )
        dapr_workflow.publish_notification(
            workflow_id=workflow_id,
            notification=notification_req,
            target_topic_name=request.source_topic,
            target_name=request.source,
        )

        method = (
            self.get_topk_engine_configs
            if not request.is_proprietary_model
            else self.get_topk_proprietary_engine_configs
        )

        if request.is_quantization:
            method = self.get_topk_quantization_engine_configs

        # Determine the simulation method to use
        simulation_method = self.get_simulation_method(request)

        # Analyze cluster topology for TP+PP optimization
        cluster_topology = self.analyze_cluster_topology(cluster_info)
        logger.debug(
            f"Cluster topology: {cluster_topology['total_nodes']} nodes, {cluster_topology['total_cluster_devices']} total devices"
        )

        # Group devices by type across entire cluster for PP-aware optimization
        # Pass user's hardware mode preference for device filtering
        user_hardware_mode = (
            request.hardware_mode.value if hasattr(request.hardware_mode, "value") else request.hardware_mode
        )
        device_groups = self._group_devices_by_type_across_cluster(cluster_info, cluster_topology, user_hardware_mode)

        # Filter out invalid device groups (no devices or no nodes)
        valid_device_groups = {}
        for device_type, group in device_groups.items():
            if group["max_devices_per_node"] > 0 and group["total_nodes_with_device"] > 0:
                valid_device_groups[device_type] = group
            else:
                logger.warning(
                    f"Skipping device type {device_type}: "
                    f"max_devices_per_node={group['max_devices_per_node']}, "
                    f"total_nodes_with_device={group['total_nodes_with_device']}"
                )

        device_groups = valid_device_groups
        logger.debug(f"Found {len(device_groups)} valid device type groups for optimization")

        if not device_groups:
            logger.error(f"No valid device groups found for optimization with hardware_mode={user_hardware_mode}")
            if user_hardware_mode == "shared":
                raise ValueError(
                    "Shared hardware mode requires HAMI-enabled clusters with real-time utilization metrics. "
                    "No devices with HAMI metrics (memory_allocated_gb) were found. "
                    "Please ensure the cluster has HAMI installed or use dedicated hardware mode."
                )
            elif user_hardware_mode == "dedicated":
                raise ValueError(
                    "Dedicated hardware mode requires devices with 0% utilization (both core and memory). "
                    "No available devices with 0% utilization were found. "
                    "Please wait for devices to become available or use shared hardware mode."
                )
            else:
                raise ValueError(
                    "No devices available for simulation - all device types have 0 available count or 0 nodes"
                )

        try:
            with ProcessPoolExecutor() as executor:
                futures = []

                # Run evolution once per device type with full cluster context
                for device_type, device_group in device_groups.items():
                    for engine_device_combo in compatible_engines:
                        # Normalize both device types for comparison (e.g., cpu_high -> cpu, CPU -> cpu)
                        if normalize_device_type(engine_device_combo["device"]) == normalize_device_type(device_type):
                            logger.debug(
                                f"Processing engine-device combination: Engine={engine_device_combo['engine_name']}, "
                                f"Device={device_type}"
                            )
                            # Calculate total available devices across all nodes
                            total_available_count = sum(
                                device["available_count"] for device in device_group["devices"]
                            )

                            # Create cluster-wide device configuration for this device type
                            # Use first device as representative for specs but override counts
                            representative_device = device_group["devices"][0].copy()
                            representative_device["available_count"] = total_available_count

                            # Calculate available memory based on user's hardware mode
                            logger.debug(
                                f"Memory override check: user_hardware_mode={user_hardware_mode}, "
                                f"device_type={device_type}, "
                                f"representative_device has mem_per_GPU_in_GB={representative_device.get('mem_per_GPU_in_GB')}, "
                                f"memory_allocated_gb={representative_device.get('memory_allocated_gb')}"
                            )

                            if user_hardware_mode == "shared":
                                # For shared mode, use unutilized memory (100% of available, no safety margin)
                                total_memory_gb, memory_allocated_gb, available_memory_gb = (
                                    calculate_available_gpu_memory(representative_device)
                                )

                                # Use 100% of available memory (user adds buffer in total required)
                                logger.info(
                                    f"Shared mode GPU memory calculation for {device_type}: "
                                    f"total={total_memory_gb:.2f}GB, allocated={memory_allocated_gb:.2f}GB, "
                                    f"available={available_memory_gb:.2f}GB (no safety margin applied)"
                                )

                                # Override memory fields with available memory for optimization
                                representative_device["mem_per_GPU_in_GB"] = available_memory_gb
                                representative_device["memory_gb"] = available_memory_gb
                                representative_device["available_memory_gb"] = available_memory_gb
                                representative_device["total_memory_gb_original"] = (
                                    total_memory_gb  # Keep for reference
                                )
                            else:
                                logger.debug(
                                    f"Dedicated mode: Using full device memory (no override), "
                                    f"mem_per_GPU_in_GB={representative_device.get('mem_per_GPU_in_GB'):.2f}GB"
                                )
                            # For dedicated mode, use full device memory (existing behavior)

                            # Create base config from representative device
                            cluster_device_config = {
                                **representative_device,  # Device specs first
                                # Override with cluster-specific values (these take precedence)
                                "device_type": device_type,
                                "cluster_id": device_group["cluster_id"],
                                "total_devices": total_available_count,  # Fixed: Total available devices, not device entries
                                "node_distribution": device_group["node_distribution"],
                                "devices_by_node": device_group["devices_by_node"],
                                "cluster_topology": cluster_topology,
                                "max_devices_per_node": device_group["max_devices_per_node"],
                                "total_nodes_with_device": device_group["total_nodes_with_device"],
                                "available_count": total_available_count,  # Ensure this is correct
                            }

                            logger.debug(
                                f"Creating cluster config for {device_type}: "
                                f"max_devices_per_node={cluster_device_config['max_devices_per_node']}, "
                                f"total_available_count={total_available_count}, "
                                f"total_nodes_with_device={cluster_device_config['total_nodes_with_device']}, "
                                f"total_devices={cluster_device_config['total_devices']}"
                            )

                            # Final safety check before creating Evolution
                            if cluster_device_config["max_devices_per_node"] <= 0:
                                logger.error(
                                    f"Invalid cluster_device_config for {device_type}: "
                                    f"max_devices_per_node = {cluster_device_config['max_devices_per_node']}"
                                )
                                continue  # Skip this invalid configuration

                            # Prepare request data without simulation_method to avoid conflicts
                            request_data = request.model_dump()
                            # Remove simulation_method from request data if it exists
                            request_data.pop("simulation_method", None)

                            futures.append(
                                executor.submit(
                                    method,
                                    device_config=cluster_device_config,
                                    **request_data,
                                    engine_name=engine_device_combo["engine_name"],
                                    engine_image=engine_device_combo["image"],
                                    simulation_method=simulation_method,
                                    supports_lora=engine_device_combo.get("supports_lora", False),
                                    supports_pipeline_parallelism=engine_device_combo.get(
                                        "supports_pipeline_parallelism", False
                                    ),
                                )
                            )

                # Collect results with progress tracking
                total_count = len(futures)
                raw_results = []

                for completed_count, future in enumerate(futures, 1):
                    result = future.result()
                    raw_results.append(result)

                    # Send progress notification every 25% or for small batches
                    if completed_count % max(1, total_count // 4) == 0 or completed_count == total_count:
                        progress_pct = int((completed_count / total_count) * 100)
                        remaining_minutes = max(0, int((total_count - completed_count) * eta_minutes / total_count))

                        time_msg = f" (~{remaining_minutes}m remaining)" if remaining_minutes > 0 else ""

                        progress_notification = notification_req.model_copy(deep=True)
                        progress_notification.payload.content = NotificationContent(
                            title=f"Performance estimation progress: {progress_pct}%",
                            message=f"Completed {completed_count}/{total_count} device configurations{time_msg}",
                            status=WorkflowStatus.RUNNING,
                        )
                        dapr_workflow.publish_notification(
                            workflow_id=workflow_id,
                            notification=progress_notification,
                            target_topic_name=request.source_topic,
                            target_name=request.source,
                        )
                results = []
                for result in raw_results:
                    if isinstance(result, list):
                        results.extend(result)
                    elif result is not None:
                        results.append(result)

            notification_req.payload.content = NotificationContent(
                title="Generated best configurations for each cluster"
                if cluster_id is None
                else "Generated best configurations for the cluster",
                message="All performance metrics are estimated",
                status=WorkflowStatus.COMPLETED,
            )
            dapr_workflow.publish_notification(
                workflow_id=workflow_id,
                notification=notification_req,
                target_topic_name=request.source_topic,
                target_name=request.source,
            )
            return results

        except Exception as e:
            # Extract meaningful error message from exception
            error_detail = str(e)
            if "cannot run on any of the" in error_detail and "available device(s)" in error_detail:
                # Model doesn't fit on any device
                fix_message = "Fix: Use a smaller model or add devices with more memory"
            elif "No valid configurations found" in error_detail:
                fix_message = "Fix: Model requires more memory than available on devices"
            else:
                fix_message = (
                    f"Fix: {error_detail[:100]}" if len(error_detail) < 100 else "Fix: Check logs for details"
                )

            notification_req.payload.content = NotificationContent(
                title="Failed to generate best configurations",
                message=fix_message,
                status=WorkflowStatus.FAILED,
                primary_action="retry",
            )
            dapr_workflow.publish_notification(
                workflow_id=workflow_id,
                notification=notification_req,
                target_topic_name=request.source_topic,
                target_name=request.source,
            )
            raise e

    def rank_configs(
        self,
        workflow_id: str,
        request: ClusterRecommendationRequest,
        topk_engine_configs: List[Dict[str, Any]],
        notification_request: NotificationRequest,
        serialize: bool = False,
        cluster_id: Optional[uuid.UUID] = None,
    ) -> Union[List[ClusterInfo], List[Dict[str, Any]]]:
        """Rank the configurations based on performance metrics.

        This method processes the top K engine configurations, stores them in the database,
        and ranks them based on performance metrics. It publishes notifications about the
        ranking process status.

        Args:
            workflow_id (str): The ID of the workflow.
            request (ClusterRecommendationRequest): The request containing model and performance requirements.
            topk_engine_configs (List[Dict[str, Any]]): The top K engine configurations to rank.
            notification_request (NotificationRequest): The notification request template.
            serialize (bool, optional): Whether to serialize the results. Defaults to False.
            cluster_id (Optional[uuid.UUID], optional): The specific cluster ID to rank. Defaults to None.

        Returns:
            Union[List[ClusterInfo], List[Dict[str, Any]]]: The ranked configurations, either as model objects
                or serialized dictionaries based on the serialize parameter.

        Raises:
            ValueError: If there are no simulation results to process.
            Exception: If any error occurs during the ranking process.
        """
        notification_req = notification_request.model_copy(deep=True)
        notification_req.payload.event = "ranking"
        recommendations = []

        notification_req.payload.content = NotificationContent(
            title="Ranking the clusters based on performance"
            if cluster_id is None
            else "Ranking the configurations based on performance",
            message="Finding the most suitable clusters based on performance metrics"
            if cluster_id is None
            else "Finding the most suitable configurations based on performance metrics",
            status=WorkflowStatus.STARTED,
        )
        dapr_workflow.publish_notification(
            workflow_id=workflow_id,
            notification=notification_req,
            target_topic_name=request.source_topic,
            target_name=request.source,
        )

        try:
            kwargs = {
                "workflow_id": workflow_id,
                "model_name": request.pretrained_model_uri,
                "model_version": "latest",
                "input_tokens": request.input_tokens,
                "output_tokens": request.output_tokens,
                "target_concurrency": request.concurrency,
                "target_ttft": request.target_ttft,
                "target_throughput_per_user": request.target_throughput_per_user,
                "target_e2e_latency": request.target_e2e_latency,
            }
            # Filter out results with no valid configurations and count devices that couldn't run the model
            valid_results = [r for r in topk_engine_configs if r is not None and r.get("top_k_configs")]
            invalid_results = [r for r in topk_engine_configs if r is not None and not r.get("top_k_configs")]

            if invalid_results:
                logger.warning(
                    f"Found {len(invalid_results)} device(s) that cannot run the model {request.pretrained_model_uri}:"
                )
                for result in invalid_results:
                    device_info = result.get("device_config", {})
                    # Try multiple fields to get device identification
                    device_name = (
                        device_info.get("device_name")
                        or device_info.get("model")
                        or device_info.get("name")
                        or device_info.get("id", "Unknown")
                    )
                    # Handle memory in MB or GB
                    memory_gb = device_info.get("mem_per_GPU_in_GB", 0)
                    if memory_gb == 0 and "memory" in device_info:
                        # Convert from MB to GB if needed
                        memory_mb = device_info.get("memory") or 0
                        memory_gb = memory_mb / 1024.0 if memory_mb > 0 else 0

                    error_msg = result.get("error", "Model cannot fit in device memory")
                    logger.warning(f"  - Device: {device_name} (Memory: {memory_gb:.2f}GB) - {error_msg}")

            records = []
            for result in valid_results:
                for config in result["top_k_configs"]:
                    # Convert config object to dict, handling any non-serializable types
                    config_dict = config.__dict__.copy() if hasattr(config, "__dict__") else config

                    # Clean up config dict to ensure JSON serialization
                    if isinstance(config_dict, dict):
                        # Remove or convert non-serializable types
                        cleaned_config = {}
                        for key, value in config_dict.items():
                            if value is None or isinstance(value, (str, int, float, bool, list, dict)):
                                cleaned_config[key] = value
                            elif hasattr(value, "__dict__"):
                                # Convert objects to dicts
                                cleaned_config[key] = value.__dict__
                            else:
                                # Convert to string as fallback
                                cleaned_config[key] = str(value)
                        config_dict = cleaned_config

                    # Also clean device_config
                    device_config_cleaned = {}
                    for key, value in result["device_config"].items():
                        if value is None or isinstance(value, (str, int, float, bool)):
                            device_config_cleaned[key] = value
                        elif isinstance(value, (list, dict)):
                            # Keep lists and dicts but ensure they're JSON serializable
                            import json

                            try:
                                json.dumps(value)
                                device_config_cleaned[key] = value
                            except (TypeError, ValueError):
                                logger.warning(f"Skipping non-serializable device_config field: {key}")
                        else:
                            # Convert to appropriate type
                            device_config_cleaned[key] = str(value)

                    record = {
                        **kwargs,
                        **device_config_cleaned,
                        "engine": result["engine"],
                        "engine_image": result["engine_image"],
                        "engine_version": result.get("engine_version"),
                        "tool_calling_parser_type": result.get("tool_calling_parser_type"),
                        "reasoning_parser_type": result.get("reasoning_parser_type"),
                        "architecture_family": result.get("architecture_family"),
                        "chat_template": result.get("chat_template"),
                        "supports_lora": result.get("supports_lora"),
                        "supports_pipeline_parallelism": result.get("supports_pipeline_parallelism"),
                        "top_k_configs": config_dict,
                    }
                    records.append(record)

            with SimulationResultsCRUD() as crud:
                if len(records):
                    crud.delete(conditions={"workflow_id": workflow_id})
                    crud.bulk_insert(records)
                else:
                    # More informative error when no devices can run the model
                    total_devices = len(topk_engine_configs)
                    error_msg = (
                        f"No valid simulation results to process. "
                        f"Model {request.pretrained_model_uri} cannot run on any of the {total_devices} available device(s). "
                        f"Consider using a smaller model or devices with more memory."
                    )
                    logger.error(error_msg)
                    raise ValueError(error_msg)

            recommendations = []
            result = self.get_topk_cluster_recommendations(workflow_id=workflow_id, limit=10)
            for item in result.items:
                recommendations.append(item.model_dump(mode="json") if serialize else item)

            # Check if any recommendations failed to meet target concurrency
            target_concurrency = request.concurrency
            concurrency_warnings = []

            for item in result.items:
                actual_concurrency = item.metrics.concurrency
                if actual_concurrency < target_concurrency:
                    concurrency_warnings.append(
                        {
                            "cluster_id": str(item.cluster_id),
                            "cluster_name": str(item.cluster_id),
                            "target": target_concurrency,
                            "actual": actual_concurrency,
                        }
                    )

            # Fail workflow if concurrency target cannot be met
            if concurrency_warnings:
                # Build detailed error message
                warning_details = [
                    f"{w['cluster_name']}: achieved {w['actual']}/{w['target']} concurrency"
                    for w in concurrency_warnings
                ]

                error_message = (
                    f"Target concurrency {target_concurrency} cannot be achieved. "
                    f"Details: {'; '.join(warning_details)}. "
                    f"This may be due to limited available devices, memory constraints, or TP/PP requirements. "
                    f"Recommended actions: (1) Add more devices to the cluster(s), "
                    f"(2) Reduce concurrent_requests parameter, "
                    f"or (3) Relax performance targets (ttft, throughput, e2e_latency)."
                )

                # Send ERROR notification
                notification_req.payload.content = NotificationContent(
                    title="Concurrency Target Cannot Be Met",
                    message=error_message,
                    status=WorkflowStatus.FAILED,
                    primary_action="retry",
                )
                dapr_workflow.publish_notification(
                    workflow_id=workflow_id,
                    notification=notification_req,
                    target_topic_name=request.source_topic,
                    target_name=request.source,
                )

                logger.error(f"Failing workflow {workflow_id}: {error_message}")

                # Fail the workflow by raising exception
                raise ValueError(error_message)

            notification_req.payload.content = NotificationContent(
                title="Ranked the clusters based on performance"
                if cluster_id is None
                else "Ranked the configurations based on performance",
                message=f"Found {len(recommendations)} suitable cluster(s)",
                status=WorkflowStatus.COMPLETED,
            )
            dapr_workflow.publish_notification(
                workflow_id=workflow_id,
                notification=notification_req,
                target_topic_name=request.source_topic,
                target_name=request.source,
            )

            return recommendations
        except Exception as e:
            # Extract meaningful error message from exception
            error_detail = str(e)
            if "cannot run on any of the" in error_detail and "available device(s)" in error_detail:
                # Model doesn't fit on any device
                fix_message = "Fix: Use a smaller model or add devices with more memory"
            elif "No valid simulation results" in error_detail:
                fix_message = "Fix: Check hardware requirements and model compatibility"
            else:
                fix_message = (
                    f"Fix: {error_detail[:100]}" if len(error_detail) < 100 else "Fix: Check logs for details"
                )

            notification_req.payload.content = NotificationContent(
                title="Failed to rank the clusters" if cluster_id is None else "Failed to rank the configurations",
                message=fix_message,
                status=WorkflowStatus.FAILED,
                primary_action="retry",
            )
            dapr_workflow.publish_notification(
                workflow_id=workflow_id,
                notification=notification_req,
                target_topic_name=request.source_topic,
                target_name=request.source,
            )
            raise e

    def __call__(
        self, request: ClusterRecommendationRequest, workflow_id: Optional[str] = None
    ) -> ClusterRecommendationResponse:
        """Execute the simulation process based on the provided deployment configuration.

        This method retrieves cluster information, determines compatible engines,
        and generates the top K deployment configurations. It also logs the results
        and publishes the recommendations to a specified topic if provided.

        Args:
            config (ClusterRecommendationRequest): The deployment configuration
            containing model URI, input/output tokens, concurrency, and target
            performance metrics.
            workflow_id (Optional[str]): An optional workflow ID for tracking the
            simulation process.

        Raises:
            ValueError: If the cluster information is not available or empty.

        Returns:
            List[Dict[str, Any]]: A sorted list of deployment recommendations based
            on error rates and device types.
        """
        workflow_name = "get_cluster_recommendations"
        notification_request = NotificationRequest.from_cloud_event(
            cloud_event=request,
            name=workflow_name,
            workflow_id=workflow_id,
        )
        response = ClusterRecommendationResponse(workflow_id=workflow_id, recommendations=[])

        cluster_info = self.get_available_clusters(
            workflow_id, notification_request, request.source_topic, request.source, request.cluster_id
        )
        compatible_engines = self.get_compatible_engines(
            workflow_id,
            request.pretrained_model_uri,
            cluster_info,
            notification_request,
            request.model_uri,  # Pass the HuggingFace model identifier
            request.source_topic,
            request.source,
            proprietary_only=request.is_proprietary_model,
            cluster_id=request.cluster_id,
            model_endpoints=request.model_endpoints,
        )
        topk_engine_configs = self.get_topk_engine_configs_per_cluster(
            workflow_id, request, compatible_engines, cluster_info, notification_request, cluster_id=request.cluster_id
        )
        recommendations = self.rank_configs(
            workflow_id, request, topk_engine_configs, notification_request, cluster_id=request.cluster_id
        )

        response.recommendations = recommendations

        notification_request.payload.event = "results"
        notification_request.payload.content = NotificationContent(
            title="Deployment Recommendation Results",
            message="The deployment recommendation results are ready",
            result=response.model_dump(),
            status=WorkflowStatus.COMPLETED,
        )

        dapr_workflow.publish_notification(
            workflow_id=workflow_id,
            notification=notification_request,
            target_topic_name=request.source_topic,
            target_name=request.source,
        )

        return response

    def get_topk_cluster_recommendations(
        self,
        workflow_id: str,
        cluster_id: Optional[str] = None,
        concurrency: Optional[int] = None,
        error_rate_threshold: float = 0.5,
        page: int = 1,
        limit: int = 1,
        session: Optional[Session] = None,
    ) -> PaginatedResponse[ClusterMetrics]:
        """Get top-k cluster recommendations based on performance metrics."""
        import time

        start_time = time.time()

        results, total_count = SimulationResultsCRUD().fetch_topk_configs_by_cluster(
            workflow_id=workflow_id,
            cluster_id=cluster_id,
            error_rate_threshold=error_rate_threshold,
            limit=limit,
            skip=(page - 1) * limit,
            session=session,
        )

        query_time = time.time() - start_time
        logger.info("Database query took %.3f seconds for workflow %s", query_time, workflow_id)
        logger.info("Found %s/%s deployment configurations", len(results), total_count)
        logger.info("topk_cluster_recommendations Results: %s", results)

        recommendations = {}

        for result in results:
            recommendation = ClusterMetrics(
                cluster_id="",
                metrics=SimulationMetrics(
                    device_types=[],
                    replica=0,
                    concurrency=0,
                    ttft=0,
                    throughput_per_user=0,
                    e2e_latency=0,
                    error_rate=0,
                    cost_per_million_tokens=0,
                ),
                # Include engine metadata from the simulation result
                engine_version=getattr(result, "engine_version", None),
                tool_calling_parser_type=getattr(result, "tool_calling_parser_type", None),
                reasoning_parser_type=getattr(result, "reasoning_parser_type", None),
                architecture_family=getattr(result, "architecture_family", None),
                chat_template=getattr(result, "chat_template", None),
                supports_lora=getattr(result, "supports_lora", None),
                supports_pipeline_parallelism=getattr(result, "supports_pipeline_parallelism", None),
            )
            deployment_config = self.optimal_search_node_group_config([result], concurrency)
            if deployment_config is not None:
                device_types = {}
                # Process node groups instead of nodes.devices
                for node_group in deployment_config.node_groups:
                    if node_group.type not in device_types:
                        device_types[node_group.type] = DeviceTypeMetrics(
                            device_type=node_group.type,
                            num_replicas=node_group.replicas,
                            # Concurrency is calculated from the node group's base concurrency per replica
                            concurrency=node_group.replicas,  # For node groups, this represents total capacity
                            cost_per_million_tokens=node_group.cost_per_million_tokens,
                        )
                    else:
                        device_types[node_group.type].num_replicas += node_group.replicas
                        device_types[node_group.type].concurrency += node_group.replicas
                        device_types[node_group.type].cost_per_million_tokens += node_group.cost_per_million_tokens

                recommendation.cluster_id = deployment_config.id
                recommendation.metrics.device_types = list(device_types.values())
                recommendation.metrics.replica = deployment_config.replica
                recommendation.metrics.concurrency = deployment_config.concurrency
                recommendation.metrics.ttft = deployment_config.ttft
                recommendation.metrics.throughput_per_user = deployment_config.throughput_per_user
                recommendation.metrics.e2e_latency = deployment_config.e2e_latency
                recommendation.metrics.error_rate = deployment_config.error_rate
                recommendation.metrics.cost_per_million_tokens = deployment_config.cost_per_million_tokens

                if (
                    recommendation.cluster_id not in recommendations
                    or recommendations[recommendation.cluster_id].metrics.cost_per_million_tokens
                    > deployment_config.cost_per_million_tokens
                ):
                    recommendations[recommendation.cluster_id] = recommendation
        logger.info("topk_cluster_recommendations Recommendations: %s", recommendations)
        return PaginatedResponse(
            object="cluster_recommendations",
            items=sorted(recommendations.values(), key=lambda x: x.metrics.cost_per_million_tokens),
            total_items=total_count,
            page=page,
            limit=limit,
        )

    @staticmethod
    def _should_use_node_groups(simulation_results: List[SimulationResultsSchema]) -> bool:
        """Determine if node groups should be used instead of legacy node structure.

        NOTE: Always returns True now - we always use the new node group format
        for consistency and to support the new device mapping functionality.
        The legacy nodes[].devices[] format is deprecated.
        """
        return True  # Always use new node group format

    @staticmethod
    def _group_devices_by_type(simulation_results: List[SimulationResultsSchema]) -> Dict[str, List]:
        """Group simulation results by device type for node group creation."""
        device_groups = {}
        for result in simulation_results:
            device_key = result.device_name or result.device_type
            if device_key not in device_groups:
                device_groups[device_key] = []
            device_groups[device_key].append(result)
        return device_groups

    @staticmethod
    def _create_node_group_config(
        device_group_results: List[SimulationResultsSchema],
        device_type: str,
        replica_count: int,
        target_concurrency: int,
    ) -> Optional[NodeGroupConfiguration]:
        """Create a node group configuration from grouped device results."""
        logger.info(f"Creating node group config for device_type={device_type}, replica_count={replica_count}")

        if not device_group_results:
            logger.warning(f"No device group results for {device_type}")
            return None

        # Use the first result as the template for the group
        template_result = device_group_results[0]
        logger.info(f"Template result type: {type(template_result)}")

        # Check top_k_configs structure
        top_k_configs = getattr(template_result, "top_k_configs", None)
        if top_k_configs is None:
            logger.error(f"top_k_configs is None for device_type {device_type}")
            return None

        if not isinstance(top_k_configs, dict):
            logger.error(f"top_k_configs is not a dict for device_type {device_type}, got {type(top_k_configs)}")
            return None

        logger.info(f"top_k_configs structure: {top_k_configs}")
        engine_config = top_k_configs.get("config", {})

        if not isinstance(engine_config, dict):
            logger.error(f"engine_config is not a dict, got {type(engine_config)}")
            return None

        engine_config["model"] = template_result.model_name
        logger.info(f"Engine config: {engine_config}")

        try:
            # Get args and envs for the engine configuration
            from ..engine_ops import get_engine_args_and_envs, get_minimal_engine_args_and_envs

            if _is_heuristic_config(engine_config):
                args_and_envs = get_minimal_engine_args_and_envs(template_result.engine, engine_config)
            else:
                args_and_envs = get_engine_args_and_envs(template_result.engine, engine_config)
        except Exception:
            logger.exception("Failed to get engine args and envs for %s", engine_config)
            return None

        # Calculate parallelism parameters
        tp_size = engine_config.get("tensor_parallel_size", 1)
        pp_size = engine_config.get("pipeline_parallel_size", 1)

        # Override TP/PP to 1 for shared hardware mode (defensive check)
        hardware_mode = top_k_configs.get("hardware_mode", "dedicated")
        if hardware_mode == "shared":
            if tp_size != 1 or pp_size != 1:
                logger.warning(
                    f"Shared hardware mode detected but TP={tp_size}, PP={pp_size}. "
                    "Overriding to TP=1, PP=1 for time-slicing compatibility."
                )
            tp_size = 1
            pp_size = 1
            logger.info("Shared hardware mode: Using TP=1, PP=1 for time-slicing (no tensor/pipeline parallelism)")
        else:
            # Skip validation - TP/PP values from top_k_configs are already validated during optimization
            # The Evolution and DirectSearch optimizers validate these combinations before storing them
            logger.info(f"Dedicated hardware mode: Using pre-validated parallelism TP={tp_size}, PP={pp_size}")

        # Create labels for Kubernetes node selection
        labels = {"device_name": device_type, "concurrency": str(top_k_configs.get("concurrency", 1))}

        # Add device identification based on device type
        device_name = template_result.device_name
        device_model = getattr(template_result, "device_model", None)
        raw_name = getattr(template_result, "raw_name", None)

        return NodeGroupConfiguration(
            config_id=str(template_result.id),
            name=device_type,
            labels=labels,
            type=template_result.device_type,
            engine_type=template_result.engine,  # Engine type for Helm chart selection
            tp_size=tp_size,
            pp_size=pp_size,
            envs=args_and_envs.get("envs", {}),
            args=args_and_envs.get("args", {}),
            replicas=replica_count,
            image=template_result.engine_image,
            memory=top_k_configs.get("total_memory", 0) / (1024**3),  # Convert bytes to GB
            weight_memory_gb=top_k_configs.get("weight_memory", 0) / (1024**3),
            kv_cache_memory_gb=top_k_configs.get("kv_cache_memory", 0) / (1024**3),
            hardware_mode=top_k_configs.get("hardware_mode", "dedicated"),
            ttft=float(top_k_configs.get("ttft", 0)),
            throughput_per_user=float(top_k_configs.get("throughput_per_user", 0)),
            e2e_latency=float(top_k_configs.get("e2e_latency", 0)),
            error_rate=float(top_k_configs.get("error_rate", 0)),
            cost_per_million_tokens=float(top_k_configs.get("cost_per_million_tokens", 0)) * replica_count,
            device_name=device_name,
            device_model=device_model,
            raw_name=raw_name,
            engine_version=getattr(template_result, "engine_version", None),
            tool_calling_parser_type=getattr(template_result, "tool_calling_parser_type", None),
            reasoning_parser_type=getattr(template_result, "reasoning_parser_type", None),
            architecture_family=getattr(template_result, "architecture_family", None),
            chat_template=getattr(template_result, "chat_template", None),
            supports_lora=getattr(template_result, "supports_lora", None),
            supports_pipeline_parallelism=getattr(template_result, "supports_pipeline_parallelism", None),
            # CPU cores for cpu/cpu_high deployments
            # Shared mode: use total cores (pods burst to full capacity, K8s handles scheduling)
            # Dedicated mode: subtract utilized cores to get truly available (min 1 core)
            cores=(
                (
                    int(getattr(template_result, "cores", 0))
                    if top_k_configs.get("hardware_mode", "dedicated") == "shared"
                    else max(
                        1,
                        int(
                            getattr(template_result, "cores", 0) - (getattr(template_result, "utilized_cores", 0) or 0)
                        ),
                    )
                )
                if template_result.device_type in ("cpu", "cpu_high") and getattr(template_result, "cores", None)
                else None
            ),
            max_loras=top_k_configs.get("max_loras"),
        )

    @staticmethod
    def optimal_search_node_group_config(
        simulation_results: List[SimulationResultsSchema], target_concurrency: int = None
    ) -> DeploymentConfigurationResponse:
        """Perform optimal search for deployment configuration using node groups."""
        if not simulation_results:
            logger.warning("optimal_search_node_group_config called with empty simulation_results")
            return None

        try:
            logger.info(f"Starting optimal_search_node_group_config with {len(simulation_results)} results")

            target_concurrency = target_concurrency or simulation_results[0].target_concurrency
            logger.info(f"Target concurrency: {target_concurrency}")

            # Group devices by type for node group creation
            device_groups = SimulationService._group_devices_by_type(simulation_results)
            logger.info(f"Device groups created: {list(device_groups.keys())}")

            config = DeploymentConfigurationResponse(
                id=simulation_results[0].cluster_id,
                nodes=None,  # Using new node groups structure
                node_groups=[],
                replica=0,
                concurrency=0,
                ttft=0,
                throughput_per_user=0,
                e2e_latency=0,
                error_rate=0,
                cost_per_million_tokens=0,
            )
        except Exception as e:
            logger.error(f"Error in optimal_search_node_group_config initialization: {str(e)}", exc_info=True)
            return None

        try:
            total_cost = 0
            total_concurrency = 0

            for device_type, group_results in device_groups.items():
                logger.info(f"Processing device type: {device_type} with {len(group_results)} results")

                if not group_results:
                    logger.warning(f"No results for device type {device_type}, skipping")
                    continue

                template_result = group_results[0]
                logger.info(f"Template result type: {type(template_result)}")

                # Check top_k_configs structure
                top_k_configs = getattr(template_result, "top_k_configs", None)
                if top_k_configs is None:
                    logger.error(f"top_k_configs is None for device type {device_type}")
                    continue

                logger.info(f"top_k_configs for {device_type}: {top_k_configs}")

                if not isinstance(top_k_configs, dict):
                    logger.error(
                        f"top_k_configs is not a dict for device type {device_type}, got {type(top_k_configs)}"
                    )
                    continue

                engine_config = top_k_configs.get("config", {})
                if not isinstance(engine_config, dict):
                    logger.error(
                        f"engine_config is not a dict for device type {device_type}, got {type(engine_config)}"
                    )
                    continue

                tp_size = engine_config.get("tensor_parallel_size", 1)
                logger.info(f"TP size for {device_type}: {tp_size}")

                # Use per-device count for replica calculation
                available_devices = template_result.available_count
                max_replicas = available_devices // tp_size if tp_size > 0 else 0
                logger.info(f"Available devices per node: {available_devices}, max replicas: {max_replicas}")

                if max_replicas <= 0:
                    logger.warning(f"No replicas possible for device type {device_type}")
                    continue

                target_concurrency_per_result = top_k_configs.get("concurrency", 1)
                needed_replicas = math.ceil(target_concurrency / target_concurrency_per_result)
                replica_count = min(max_replicas, needed_replicas)

                # Validate if target concurrency can be achieved
                if replica_count < needed_replicas:
                    logger.warning(
                        f"Cannot fully meet target concurrency {target_concurrency} on {device_type}. "
                        f"Need {needed_replicas} replicas but only {max_replicas} available. "
                        f"Actual concurrency will be {replica_count * target_concurrency_per_result}"
                    )

                logger.info(f"Replica count for {device_type}: {replica_count}")

                # Create node group configuration
                node_group = SimulationService._create_node_group_config(
                    group_results, device_type, replica_count, target_concurrency
                )

                if node_group:
                    logger.info(f"Successfully created node group for {device_type}")
                    config.node_groups.append(node_group)
                    total_cost += node_group.cost_per_million_tokens
                    device_concurrency = top_k_configs.get("concurrency", 0) * replica_count
                    total_concurrency += device_concurrency
                    config.replica += replica_count

                    # Detailed logging for concurrency scaling
                    logger.info(
                        f"Concurrency scaling for {device_type}: "
                        f"optimized_concurrency_per_replica={top_k_configs.get('concurrency', 0)}, "
                        f"replicas={replica_count}, "
                        f"device_total_concurrency={device_concurrency}, "
                        f"cumulative_total_concurrency={total_concurrency}, "
                        f"target_concurrency={target_concurrency}"
                    )
                    config.ttft += node_group.ttft
                    config.throughput_per_user += node_group.throughput_per_user
                    config.e2e_latency = max(config.e2e_latency, node_group.e2e_latency)
                    config.error_rate = max(config.error_rate, node_group.error_rate)
                else:
                    logger.error(f"Failed to create node group for {device_type}")
        except Exception as e:
            logger.error(f"Error processing device groups: {str(e)}", exc_info=True)
            return None

        config.concurrency = total_concurrency
        config.cost_per_million_tokens = total_cost

        # Validate if target concurrency was achieved across all device types
        if total_concurrency < target_concurrency:
            logger.error(
                f"Target concurrency {target_concurrency} cannot be met. "
                f"Maximum achievable: {total_concurrency}. "
                f"Consider relaxing performance targets or using more devices."
            )
        else:
            logger.info(f"Successfully met target concurrency: {total_concurrency} >= {target_concurrency}")

        if not config.node_groups:
            logger.error("No node groups created, returning None")
            return None

        # Validate that we can meet the requested concurrency
        if total_concurrency < target_concurrency:
            logger.error(
                f"Insufficient resources to meet requested concurrency. "
                f"Requested: {target_concurrency}, achievable: {total_concurrency}. "
                f"Cannot deploy - need {math.ceil(target_concurrency / total_concurrency) if total_concurrency > 0 else 'infinite'} "
                f"times more resources or reduce concurrency requirement."
            )
            return None

        # Populate engine metadata from the first node group (they should all have the same metadata)
        if config.node_groups:
            first_group = config.node_groups[0]
            config.engine_version = first_group.engine_version
            config.tool_calling_parser_type = first_group.tool_calling_parser_type
            config.reasoning_parser_type = first_group.reasoning_parser_type
            config.architecture_family = first_group.architecture_family
            config.chat_template = first_group.chat_template

        logger.info(f"Successfully created config with {len(config.node_groups)} node groups")
        return config

    @staticmethod
    def greedy_search_deployment_config(
        simulation_results: List[SimulationResultsSchema], target_concurrency: int = None
    ) -> DeploymentConfigurationResponse:
        """Perform greedy search for optimal deployment configuration."""
        nodes = {}
        device_ids = set()
        config = DeploymentConfigurationResponse(
            id=simulation_results[0].cluster_id,
            nodes=[],
            replica=0,
            concurrency=0,
            ttft=0,
            throughput_per_user=0,
            e2e_latency=0,
            error_rate=0,
            cost_per_million_tokens=0,
        )
        target_concurrency = target_concurrency or simulation_results[0].target_concurrency

        for entry in sorted(
            simulation_results,
            key=lambda x: x.top_k_configs["cost_per_million_tokens"] / x.top_k_configs["concurrency"],
        ):
            node_id = entry.node_id
            if node_id not in nodes:
                nodes[node_id] = NodeConfiguration(
                    id=node_id,
                    name=entry.node_name,
                    devices=[],
                )

            engine_config = entry.top_k_configs["config"]
            engine_config["model"] = entry.model_name
            if entry.device_id in device_ids or engine_config.get("tensor_parallel_size", 1) > entry.available_count:
                continue
            try:
                # Use minimal args for heuristic configs to avoid including unoptimized defaults
                if _is_heuristic_config(engine_config):
                    args_and_envs = get_minimal_engine_args_and_envs(entry.engine, engine_config)
                else:
                    args_and_envs = get_engine_args_and_envs(entry.engine, engine_config)
            except Exception:
                logger.exception("Failed to get engine args and envs for %s", engine_config)
                continue

            device_info = DeviceConfiguration(
                config_id=str(entry.id),
                name=entry.device_name,
                type=entry.device_type,
                image=entry.engine_image,
                memory=entry.top_k_configs["total_memory"],
                num_cpus=args_and_envs["envs"].get("NUM_CPUS", -1),
                args=args_and_envs["args"],
                envs=args_and_envs["envs"],
                tp_size=engine_config.get("tensor_parallel_size", 1),
                replica=0,
                concurrency=0,
                ttft=float(entry.top_k_configs["ttft"]),
                throughput_per_user=float(entry.top_k_configs["throughput_per_user"]),
                e2e_latency=float(entry.top_k_configs["e2e_latency"]),
                error_rate=float(entry.top_k_configs["error_rate"]),
                cost_per_million_tokens=0,
            )

            config.ttft += device_info.ttft
            config.throughput_per_user += device_info.throughput_per_user
            config.e2e_latency += device_info.e2e_latency
            config.error_rate += device_info.error_rate

            for _ in range(1, entry.available_count + 1, device_info.tp_size):
                config.concurrency += int(entry.top_k_configs["concurrency"])
                config.cost_per_million_tokens += float(entry.top_k_configs["cost_per_million_tokens"])
                config.replica += 1

                device_info.concurrency += int(entry.top_k_configs["concurrency"])
                device_info.cost_per_million_tokens += float(entry.top_k_configs["cost_per_million_tokens"])
                device_info.replica += 1

                if config.concurrency >= target_concurrency:
                    break

            nodes[node_id].devices.append(device_info)
            device_ids.add(entry.device_id)

            if config.concurrency >= target_concurrency:
                break

        if config.concurrency >= target_concurrency:
            num_workers = len(device_ids)
            config.ttft /= num_workers
            config.throughput_per_user /= num_workers
            config.e2e_latency /= num_workers
            config.error_rate /= num_workers
            config.nodes = list(filter(lambda x: x.devices, nodes.values()))
        else:
            config = None

        return config

    @staticmethod
    def optimal_search_deployment_config(
        simulation_results: List[SimulationResultsSchema], target_concurrency: int = None
    ) -> DeploymentConfigurationResponse:
        """Create deployment configuration using legacy structure.

        DEPRECATED: Use optimal_search_node_group_config instead.
        Legacy function that creates deployment configuration using the old
        nodes[].devices[] structure. This is deprecated in favor of the new
        node group structure that uses device groups (A100, V100, etc.).
        """
        warnings.warn(
            "optimal_search_deployment_config is deprecated. Use optimal_search_node_group_config instead.",
            DeprecationWarning,
            stacklevel=2,
        )
        target_concurrency = target_concurrency or simulation_results[0].target_concurrency

        device_args_and_envs = {}
        valid_combination = None
        for i, entry in enumerate(simulation_results):
            if entry.device_id not in device_args_and_envs:
                engine_config = entry.top_k_configs["config"]
                engine_config["model"] = entry.model_name
                if engine_config.get("tensor_parallel_size", 1) > entry.available_count:
                    continue

                try:
                    # Use minimal args for heuristic configs to avoid including unoptimized defaults
                    if _is_heuristic_config(engine_config):
                        args_and_envs = get_minimal_engine_args_and_envs(entry.engine, engine_config)
                    else:
                        args_and_envs = get_engine_args_and_envs(entry.engine, engine_config)

                    device_args_and_envs[entry.device_id] = {
                        **args_and_envs,
                        "tp_size": engine_config.get("tensor_parallel_size", 1),
                    }
                except Exception:
                    logger.exception("Failed to get engine args and envs for %s", engine_config)
                    continue

            combos = []
            tp_size = device_args_and_envs[entry.device_id]["tp_size"]
            replicas = min(
                entry.available_count // tp_size,
                math.ceil(target_concurrency / entry.top_k_configs["concurrency"]),
            )
            combos.append((i, replicas))
            cur_concurrency = entry.top_k_configs["concurrency"] * replicas
            cur_cost = entry.top_k_configs["cost_per_million_tokens"] * replicas
            last_best_entry = None

            if cur_concurrency >= target_concurrency and (
                valid_combination is None or cur_cost < valid_combination[1]
            ):
                valid_combination = (combos, cur_cost, cur_concurrency)
                continue

            for j, _entry in enumerate(simulation_results):
                if i == j:
                    continue

                if _entry.device_id not in device_args_and_envs:
                    engine_config = _entry.top_k_configs["config"]
                    engine_config["model"] = _entry.model_name
                    if engine_config.get("tensor_parallel_size", 1) > _entry.available_count:
                        continue

                    try:
                        # Use minimal args for heuristic configs to avoid including unoptimized defaults
                        if _is_heuristic_config(engine_config):
                            args_and_envs = get_minimal_engine_args_and_envs(_entry.engine, engine_config)
                        else:
                            args_and_envs = get_engine_args_and_envs(_entry.engine, engine_config)

                        device_args_and_envs[_entry.device_id] = {
                            **args_and_envs,
                            "tp_size": engine_config.get("tensor_parallel_size", 1),
                        }
                    except Exception:
                        logger.exception("Failed to get engine args and envs for %s", engine_config)
                        continue

                tp_size = device_args_and_envs[_entry.device_id]["tp_size"]
                replicas = min(
                    _entry.available_count // tp_size,
                    max(1, math.ceil((target_concurrency - cur_concurrency) / _entry.top_k_configs["concurrency"])),
                )

                if cur_concurrency + _entry.top_k_configs["concurrency"] * replicas > target_concurrency:
                    last_best_cost = (
                        simulation_results[last_best_entry[0]].top_k_configs["cost_per_million_tokens"]
                        * last_best_entry[1]
                        if last_best_entry is not None
                        else float("inf")
                    )
                    if last_best_cost > _entry.top_k_configs["cost_per_million_tokens"] * replicas:
                        last_best_entry = (j, replicas)

                    continue

                combos.append((j, replicas))
                cur_concurrency += _entry.top_k_configs["concurrency"] * replicas
                cur_cost += _entry.top_k_configs["cost_per_million_tokens"] * replicas
                if cur_concurrency == target_concurrency:
                    break

            if last_best_entry is not None and cur_concurrency < target_concurrency:
                combos.append(last_best_entry)
                cur_concurrency += (
                    simulation_results[last_best_entry[0]].top_k_configs["concurrency"] * last_best_entry[1]
                )
                cur_cost += (
                    simulation_results[last_best_entry[0]].top_k_configs["cost_per_million_tokens"]
                    * last_best_entry[1]
                )

            if cur_concurrency >= target_concurrency and (
                valid_combination is None or cur_cost < valid_combination[1]
            ):
                valid_combination = (combos, cur_cost, cur_concurrency)

        if valid_combination is None:
            return None

        config = DeploymentConfigurationResponse(
            id=simulation_results[0].cluster_id,
            nodes=[],
            replica=0,
            concurrency=valid_combination[2],
            ttft=0,
            throughput_per_user=0,
            e2e_latency=0,
            error_rate=0,
            cost_per_million_tokens=valid_combination[1],
        )
        nodes = {}
        for idx, replica in valid_combination[0]:
            entry = simulation_results[idx]
            node_id = entry.node_id
            if node_id not in nodes:
                nodes[node_id] = NodeConfiguration(
                    id=node_id,
                    name=entry.node_name,
                    devices=[],
                )

            device_info = DeviceConfiguration(
                config_id=str(entry.id),
                name=entry.device_name,
                type=entry.device_type,
                image=entry.engine_image,
                memory=entry.top_k_configs["total_memory"],
                num_cpus=device_args_and_envs[entry.device_id]["envs"].get("NUM_CPUS", -1),
                args=device_args_and_envs[entry.device_id]["args"],
                envs=device_args_and_envs[entry.device_id]["envs"],
                tp_size=device_args_and_envs[entry.device_id]["tp_size"],
                replica=replica,
                concurrency=int(entry.top_k_configs["concurrency"]) * replica,
                ttft=float(entry.top_k_configs["ttft"]),
                throughput_per_user=float(entry.top_k_configs["throughput_per_user"]),
                e2e_latency=float(entry.top_k_configs["e2e_latency"]),
                error_rate=float(entry.top_k_configs["error_rate"]),
                cost_per_million_tokens=float(entry.top_k_configs["cost_per_million_tokens"]) * replica,
            )
            nodes[node_id].devices.append(device_info)

            config.ttft += device_info.ttft
            config.throughput_per_user += device_info.throughput_per_user
            config.e2e_latency += device_info.e2e_latency
            config.error_rate += device_info.error_rate
            config.replica += replica

        num_workers = len(valid_combination[0])
        config.ttft /= num_workers
        config.throughput_per_user /= num_workers
        config.e2e_latency /= num_workers
        config.error_rate /= num_workers
        config.nodes = list(nodes.values())

        return config if len(config.nodes) else None

    def get_deployment_configs(
        self,
        request: DeploymentConfigurationRequest,
        session: Optional[Session] = None,
    ) -> DeploymentConfigurationResponse:
        """Get deployment configurations based on request parameters."""
        if request.feedback:
            try:
                SimulationResultsCRUD().update_feedback(request.feedback, session)
            except Exception:
                return ErrorResponse(message="Failed to update feedback", code=500)

        results, total_count = SimulationResultsCRUD().fetch_topk_configs_by_cluster(
            workflow_id=request.workflow_id,
            cluster_id=request.cluster_id,
            error_rate_threshold=request.error_rate_threshold,
            limit=1,
            skip=0,
            session=session,
        )

        config = None

        for page in range(1, total_count + 1):
            for result in results:
                # Always use the new node group structure
                # Legacy optimal_search_deployment_config is deprecated
                config = self.optimal_search_node_group_config([result], request.concurrency)

            if config is not None:
                break

            results, _ = SimulationResultsCRUD().fetch_topk_configs_by_cluster(
                workflow_id=request.workflow_id,
                cluster_id=request.cluster_id,
                error_rate_threshold=request.error_rate_threshold,
                limit=1,
                skip=page,
                session=session,
            )

        return (
            config
            if config is not None
            else ErrorResponse(
                message="No deployment configuration found",
                code=400,
            )
        )

    @staticmethod
    def get_node_configurations(request: NodeConfigurationRequest) -> NodeConfigurationResponse:
        """Get valid TP/PP configuration options for selected nodes.

        This method analyzes the selected nodes and returns available device types
        with valid TP/PP combinations and maximum replica counts.

        Args:
            request: NodeConfigurationRequest containing cluster_id, model_id, hostnames,
                    hardware_mode, and token configuration

        Returns:
            NodeConfigurationResponse with device configurations and model memory info

        Raises:
            ValueError: If cluster or nodes not found
        """
        # 1. Get cluster info from state store
        try:
            with DaprService() as dapr_service:
                cluster_info_raw = dapr_service.get_state(
                    app_settings.statestore_name, app_settings.cluster_info_state_key
                )

            if not cluster_info_raw or not cluster_info_raw.data:
                raise ValueError("No cluster information available in state store")

            cluster_info = SimulationService.validate_cluster_info(cluster_info_raw.data.decode("utf-8"))
        except Exception as e:
            logger.exception(f"Failed to retrieve cluster info: {e}")
            raise ValueError(f"Failed to retrieve cluster information: {str(e)}") from e

        # 2. Filter to selected cluster
        cluster_info = [c for c in cluster_info if c["id"] == str(request.cluster_id)]
        if not cluster_info:
            raise ValueError(f"Cluster {request.cluster_id} not found in state store")

        cluster = cluster_info[0]

        # 3. Filter nodes by hostname
        selected_nodes = []
        for node in cluster.get("nodes", []):
            node_name = node.get("name") or node.get("hostname")
            if node_name in request.hostnames:
                # Check node is active
                node_status = node.get("status")
                if node_status is True:
                    selected_nodes.append(node)
                else:
                    logger.warning(f"Node {node_name} is not active (status={node_status}), skipping")

        if not selected_nodes:
            raise ValueError(f"No active nodes found matching hostnames: {request.hostnames}")

        logger.info(f"Found {len(selected_nodes)} active nodes matching hostnames")

        # 4. Create filtered cluster info for device grouping
        filtered_cluster_info = [{"id": cluster["id"], "nodes": selected_nodes}]

        # 5. Analyze topology for PP constraints
        cluster_topology = SimulationService.analyze_cluster_topology(filtered_cluster_info)

        # 6. Group devices by type using existing method
        hardware_mode_str = (
            request.hardware_mode.value if hasattr(request.hardware_mode, "value") else str(request.hardware_mode)
        )
        device_groups = SimulationService._group_devices_by_type_across_cluster(
            filtered_cluster_info, cluster_topology, hardware_mode_str
        )

        if not device_groups:
            raise ValueError("No available devices found on selected nodes for the specified hardware mode")

        # 7. Calculate model memory requirements
        model_memory_info = SimulationService._calculate_model_memory_for_config(
            model_uri=request.model_uri,
            input_tokens=request.input_tokens,
            output_tokens=request.output_tokens,
            device_groups=device_groups,
        )

        # 8. Calculate valid TP/PP options for each device type
        device_configurations = []
        for device_type, group in device_groups.items():
            config = SimulationService._calculate_tp_pp_options_for_device(
                device_type=device_type,
                device_group=group,
                model_weight_gb=model_memory_info.estimated_weight_memory_gb,
                hardware_mode=hardware_mode_str,
            )
            if config:
                device_configurations.append(config)

        if not device_configurations:
            raise ValueError("No valid configurations found for the selected nodes and hardware mode")

        return NodeConfigurationResponse(
            cluster_id=request.cluster_id,
            model_info=model_memory_info,
            device_configurations=device_configurations,
            selected_nodes=request.hostnames,
            hardware_mode=hardware_mode_str,
        )

    @staticmethod
    def _calculate_model_memory_for_config(
        model_uri: Optional[str],
        input_tokens: int,
        output_tokens: int,
        device_groups: Dict[str, Any],
    ) -> ModelMemoryInfo:
        """Calculate model memory requirements for configuration options.

        Args:
            model_uri: Model URI/path for memory calculation
            input_tokens: Expected input tokens
            output_tokens: Expected output tokens
            device_groups: Device groups from _group_devices_by_type_across_cluster

        Returns:
            ModelMemoryInfo with estimated memory requirements
        """
        estimated_weight_memory_gb = 0.0
        min_tp_for_model = 1

        if model_uri:
            try:
                # Try to use llm-memory-calculator for accurate estimation
                from llm_memory_calculator import calculate_memory

                seq_length = int((input_tokens + output_tokens) * 1.1)
                memory_report = calculate_memory(
                    model_id_or_config=model_uri,
                    batch_size=1,
                    seq_length=seq_length,
                    precision="bf16",
                    tensor_parallel=1,
                )

                if hasattr(memory_report, "total_memory_gb"):
                    estimated_weight_memory_gb = memory_report.total_memory_gb
                elif hasattr(memory_report, "model_weights_memory_gb"):
                    estimated_weight_memory_gb = memory_report.model_weights_memory_gb
                else:
                    # Fallback: estimate from parameter count
                    if hasattr(memory_report, "parameter_count"):
                        # BF16: 2 bytes per parameter
                        estimated_weight_memory_gb = (memory_report.parameter_count * 2) / (1024**3)

                logger.info(f"Model memory calculated via llm-memory-calculator: {estimated_weight_memory_gb:.2f} GB")
            except ImportError:
                logger.warning("llm-memory-calculator not available, using fallback estimation")
            except Exception as e:
                logger.warning(f"Failed to calculate model memory: {e}, using fallback estimation")

        # Fallback: estimate from model name if available
        if estimated_weight_memory_gb == 0.0 and model_uri:
            # Simple heuristic based on model name patterns
            model_lower = model_uri.lower()
            if "70b" in model_lower:
                estimated_weight_memory_gb = 140.0  # ~140GB for 70B model in BF16
            elif "40b" in model_lower or "34b" in model_lower:
                estimated_weight_memory_gb = 80.0
            elif "13b" in model_lower:
                estimated_weight_memory_gb = 26.0
            elif "7b" in model_lower or "8b" in model_lower:
                estimated_weight_memory_gb = 16.0
            elif "3b" in model_lower:
                estimated_weight_memory_gb = 6.0
            elif "1b" in model_lower:
                estimated_weight_memory_gb = 2.0
            else:
                # Default to a small model estimate
                estimated_weight_memory_gb = 10.0

        # Calculate minimum TP required based on available device memory
        if device_groups and estimated_weight_memory_gb > 0:
            # Get maximum memory per device across all device types
            max_device_memory = 0.0
            for group in device_groups.values():
                devices = group.get("devices", [])
                if devices:
                    device_mem = devices[0].get("mem_per_GPU_in_GB", 0)
                    max_device_memory = max(max_device_memory, device_mem)

            if max_device_memory > 0:
                # Use 80% of device memory for model weights (leave room for KV cache)
                usable_memory = max_device_memory * 0.8
                min_tp_for_model = max(1, math.ceil(estimated_weight_memory_gb / usable_memory))
                # Round up to power of 2
                if min_tp_for_model > 1:
                    min_tp_for_model = 2 ** math.ceil(math.log2(min_tp_for_model))

        return ModelMemoryInfo(
            model_id=uuid.uuid4(),  # Placeholder, actual model_id comes from budapp
            model_name=None,
            model_uri=model_uri,
            estimated_weight_memory_gb=estimated_weight_memory_gb,
            min_tp_for_model=min_tp_for_model,
        )

    @staticmethod
    def _calculate_tp_pp_options_for_device(
        device_type: str,
        device_group: Dict[str, Any],
        model_weight_gb: float,
        hardware_mode: str,
    ) -> Optional[DeviceTypeConfiguration]:
        """Calculate valid TP/PP combinations for a device type.

        Constraints:
        - TP cannot exceed max_devices_per_node (intra-node)
        - PP cannot exceed nodes_with_device (inter-node)
        - TP must be power of 2
        - PP must be >= 1
        - Replicas = total_devices / (TP * PP)
        - Shared mode forces TP=1, PP=1
        - CPU devices don't support PP > 1

        Args:
            device_type: Device type (cuda, hpu, cpu, cpu_high)
            device_group: Device group info from _group_devices_by_type_across_cluster
            model_weight_gb: Estimated model weight in GB
            hardware_mode: "dedicated" or "shared"

        Returns:
            DeviceTypeConfiguration or None if no valid configs
        """
        total_devices = device_group.get("total_devices", 0)
        max_devices_per_node = device_group.get("max_devices_per_node", 1)
        nodes_count = device_group.get("total_nodes_with_device", 1)

        if total_devices == 0:
            return None

        # Get first device for memory info
        devices = device_group.get("devices", [])
        if not devices:
            return None

        first_device = devices[0]
        memory_per_device = first_device.get("mem_per_GPU_in_GB", 0)
        device_name = first_device.get("name", device_type)
        device_model = first_device.get("device_model")

        # Calculate minimum TP required for model
        min_tp = 1
        if memory_per_device > 0 and model_weight_gb > 0:
            usable_memory = memory_per_device * 0.8
            min_tp = max(1, math.ceil(model_weight_gb / usable_memory))
            # Round up to power of 2
            if min_tp > 1:
                min_tp = 2 ** math.ceil(math.log2(min_tp))

        # Determine if PP is supported
        device_type_lower = device_type.lower()
        supports_pp = device_type_lower not in ("cpu", "cpu_high")

        # Generate valid TP/PP options
        tp_pp_options = []

        if hardware_mode == "shared":
            # Shared mode: only TP=1, PP=1
            if total_devices >= 1:
                tp_pp_options.append(
                    TPPPOption(
                        tp_size=1,
                        pp_size=1,
                        max_replicas=total_devices,
                        total_devices_needed=1,
                        description="Single device per replica (shared mode)",
                    )
                )
        else:
            # Dedicated mode: explore TP/PP combinations
            # Generate valid TP sizes (powers of 2)
            valid_tps = []
            tp = 1
            while tp <= max_devices_per_node:
                if tp >= min_tp:
                    valid_tps.append(tp)
                tp *= 2

            # If no valid TPs found but min_tp fits, include it
            if not valid_tps and min_tp <= max_devices_per_node:
                valid_tps = [min_tp]

            max_pp = nodes_count if supports_pp else 1

            for tp in valid_tps:
                for pp in range(1, max_pp + 1):
                    devices_per_replica = tp * pp

                    # Check if we have enough devices
                    if devices_per_replica > total_devices:
                        continue

                    # Validate that PP distribution is feasible
                    # Each PP stage needs TP devices on a single node
                    if pp > 1 and tp > max_devices_per_node:
                        continue

                    max_replicas = total_devices // devices_per_replica
                    if max_replicas > 0:
                        desc = f"TP={tp}"
                        if pp > 1:
                            desc += f", PP={pp} across {pp} nodes"
                        else:
                            desc += ", PP=1"
                        desc += f" ({devices_per_replica} device{'s' if devices_per_replica > 1 else ''}/replica)"

                        tp_pp_options.append(
                            TPPPOption(
                                tp_size=tp,
                                pp_size=pp,
                                max_replicas=max_replicas,
                                total_devices_needed=devices_per_replica,
                                description=desc,
                            )
                        )

        if not tp_pp_options:
            logger.warning(f"No valid TP/PP options for device type {device_type}")
            return None

        return DeviceTypeConfiguration(
            device_type=device_type,
            device_name=device_name,
            device_model=device_model,
            total_devices=total_devices,
            nodes_count=nodes_count,
            max_devices_per_node=max_devices_per_node,
            memory_per_device_gb=memory_per_device,
            tp_pp_options=tp_pp_options,
            min_tp_required=min_tp,
            supports_pipeline_parallelism=supports_pp,
        )

    @staticmethod
    def generate_benchmark_config(
        request: BenchmarkConfigRequest,
    ) -> Union[BenchmarkConfigResponse, ErrorResponse]:
        """Generate deployment configuration for benchmark with user-selected parameters.

        Unlike get_deployment_configs which fetches from saved simulation results,
        this method generates configuration directly from user selections.

        Args:
            request: BenchmarkConfigRequest containing cluster_id, model_id, model_uri,
                    hostnames, device_type, tp_size, pp_size, replicas, and token configuration.

        Returns:
            BenchmarkConfigResponse with node_groups array containing full deployment configs.

        Raises:
            ValueError: If cluster or nodes not found, or configuration is invalid.
        """
        # 1. Get cluster info from state store
        try:
            with DaprService() as dapr_service:
                cluster_info_raw = dapr_service.get_state(
                    app_settings.statestore_name, app_settings.cluster_info_state_key
                )

            if not cluster_info_raw or not cluster_info_raw.data:
                raise ValueError("No cluster information available in state store")

            cluster_info = SimulationService.validate_cluster_info(cluster_info_raw.data.decode("utf-8"))
        except Exception as e:
            logger.exception(f"Failed to retrieve cluster info: {e}")
            raise ValueError(f"Failed to retrieve cluster information: {str(e)}") from e

        # 2. Filter to selected cluster
        cluster_info = [c for c in cluster_info if c["id"] == str(request.cluster_id)]
        if not cluster_info:
            raise ValueError(f"Cluster {request.cluster_id} not found in state store")

        cluster = cluster_info[0]

        # 3. Filter nodes by hostname and find matching device
        selected_device = None
        device_memory_gb = 0.0
        device_name = request.device_type
        device_model = None
        raw_name = None
        # CPU-specific fields
        device_cores = None
        device_physical_cores = None
        device_utilized_cores = None

        for node in cluster.get("nodes", []):
            node_name = node.get("name") or node.get("hostname")
            if node_name in request.hostnames:
                # Find device matching requested type
                # Use exact matching since user selected the exact device type from /node-configurations
                for device in node.get("devices", []):
                    device_stored_type = device.get("type", "").lower()
                    if device_stored_type == request.device_type.lower():
                        selected_device = device
                        device_memory_gb = device.get("mem_per_GPU_in_GB", 0)
                        device_name = device.get("name", request.device_type)
                        device_model = device.get("device_model")
                        raw_name = device.get("raw_name")
                        # Extract CPU cores info for cpu/cpu_high devices
                        if device_stored_type in ("cpu", "cpu_high"):
                            device_cores = device.get("cores")
                            device_physical_cores = device.get("physical_cores")
                            device_utilized_cores = device.get("utilized_cores", 0.0)
                        break
                if selected_device:
                    break

        if not selected_device:
            raise ValueError(f"No device of type {request.device_type} found on selected nodes")

        # 4. Calculate model memory requirements
        estimated_weight_memory_gb = 0.0
        kv_cache_memory_gb = 0.0

        if request.model_uri:
            try:
                from llm_memory_calculator import calculate_memory

                seq_length = int((request.input_tokens + request.output_tokens) * 1.1)
                memory_report = calculate_memory(
                    model_id_or_config=request.model_uri,
                    batch_size=request.concurrency,
                    seq_length=seq_length,
                    precision="bf16",
                    tensor_parallel=request.tp_size,
                )

                if hasattr(memory_report, "weight_memory_gb"):
                    estimated_weight_memory_gb = memory_report.weight_memory_gb
                if hasattr(memory_report, "kv_cache_gb"):
                    kv_cache_memory_gb = memory_report.kv_cache_gb

                logger.info(
                    f"Model memory calculated: weights={estimated_weight_memory_gb:.2f}GB, "
                    f"kv_cache={kv_cache_memory_gb:.2f}GB"
                )
            except Exception as e:
                logger.warning(f"Failed to calculate model memory: {e}, using device memory")
                estimated_weight_memory_gb = device_memory_gb * 0.7  # Conservative estimate

        total_memory_gb = estimated_weight_memory_gb + kv_cache_memory_gb
        if total_memory_gb < 1.0:
            # Fallback: use 70% of device memory
            total_memory_gb = device_memory_gb * 0.7
        # Round up to integer for K8s resource specification compatibility
        total_memory_gb = math.ceil(total_memory_gb)

        # 5. Get engine image from BudConnect API (same as simulator/run)
        device_type_lower = request.device_type.lower()
        target_device = "cpu" if device_type_lower in ("cpu", "cpu_high") else device_type_lower

        # Call BudConnect API to get compatible engines with image
        image = None
        try:
            compatible_engines = get_compatible_engines(request.model_uri)
            # Find engine matching our device type
            for engine in compatible_engines:
                if (
                    engine.get("engine_name") == "vllm"
                    and normalize_device_type(engine.get("device", "")) == target_device
                ):
                    image = engine.get("image")
                    logger.info(f"Got image from BudConnect API: {image}")
                    break
        except Exception as e:
            logger.warning(f"Failed to get compatible engines from BudConnect: {e}")

        # 6. Build args and envs using full engine args (same as simulator/run)
        engine_config = {
            "model": request.model_uri,
            "tensor_parallel_size": request.tp_size,
            "pipeline_parallel_size": request.pp_size,
            "target_device": target_device,
        }

        args_and_envs = get_minimal_engine_args_and_envs("vllm", engine_config)
        args = args_and_envs.get("args", {})
        envs = args_and_envs.get("envs", {})

        # 7. Build labels for handler to extract concurrency
        labels = {
            "concurrency": str(request.concurrency),
            "device_name": device_name,
        }

        # 8. Determine hardware mode string
        hardware_mode_str = (
            request.hardware_mode.value if hasattr(request.hardware_mode, "value") else str(request.hardware_mode)
        )

        # 9. Calculate CPU cores for cpu/cpu_high devices
        available_cores = None
        if device_type_lower in ("cpu", "cpu_high"):
            # Priority: cores (threads) > physical_cores
            total_cores = device_cores or device_physical_cores
            if total_cores is not None:
                if hardware_mode_str == "shared":
                    # Shared mode: use total cores (each pod can burst to full capacity)
                    # Don't subtract utilized cores - let K8s handle scheduling
                    available_cores = int(total_cores)
                    logger.info(
                        f"CPU cores (shared mode): total={total_cores}, "
                        f"setting cores={available_cores} (full capacity for bursting)"
                    )
                else:
                    # Dedicated mode: subtract utilized cores to get truly available
                    utilized = device_utilized_cores or 0.0
                    available_cores = int(total_cores - utilized)
                    # Ensure at least 1 core is available
                    available_cores = max(1, available_cores)
                    logger.info(
                        f"CPU cores (dedicated mode): total={total_cores}, utilized={utilized:.1f}, "
                        f"available={available_cores}"
                    )
            else:
                logger.warning(
                    f"No cores info found for {device_type_lower} device {device_name}. "
                    "Deployment will use default core count."
                )

        # 10. Create NodeGroupConfiguration
        config_id = str(uuid.uuid4())

        node_group = NodeGroupConfiguration(
            config_id=config_id,
            name=device_name,
            labels=labels,
            type=device_type_lower,
            tp_size=request.tp_size,
            pp_size=request.pp_size,
            envs=envs,
            args=args,
            replicas=request.replicas,
            image=image,
            memory=int(total_memory_gb * (1024**3)),  # Convert GB to bytes for deployment handler PVC sizing
            weight_memory_gb=estimated_weight_memory_gb,
            kv_cache_memory_gb=kv_cache_memory_gb,
            hardware_mode=hardware_mode_str,
            # Performance metrics - placeholder values for benchmarks
            ttft=0.0,
            throughput_per_user=0.0,
            e2e_latency=0.0,
            error_rate=0.0,
            cost_per_million_tokens=0.0,
            # Device identification
            device_name=device_name,
            device_model=device_model,
            raw_name=raw_name,
            # CPU cores for cpu/cpu_high deployments
            cores=available_cores,
        )

        logger.info(
            f"Generated benchmark config: device={device_type_lower}, tp={request.tp_size}, "
            f"pp={request.pp_size}, replicas={request.replicas}, memory={total_memory_gb:.2f}GB, "
            f"weight_memory={estimated_weight_memory_gb:.2f}GB, kv_cache={kv_cache_memory_gb:.2f}GB"
            + (f", cores={available_cores}" if available_cores else "")
        )

        return BenchmarkConfigResponse(
            cluster_id=request.cluster_id,
            model_id=request.model_id,
            node_groups=[node_group],
        )
