{
    "manifest_version": "1.0.8",
    "last_updated": "2025-10-24T11:44:12Z",
    "schema_version": "1.0",
    "repository": {
        "name": "Bud Evaluation Datasets",
        "description": "Official evaluation datasets for model testing - 446 datasets from OpenCompass",
        "maintainer": "Bud Ecosystem",
        "base_url": "https://eval-datasets.bud.eco/v2",
        "bundle_url": null,
        "bundle_checksum": null,
        "bundle_size_mb": null
    },
    "version_info": {
        "current_version": "1.0.8",
        "previous_versions": [
            {
                "version": "1.0.0",
                "deprecated": false,
                "migration_required": false,
                "updated_at": "2025-10-24T07:02:14Z"
            },
            {
                "version": "1.0.1",
                "deprecated": false,
                "migration_required": false,
                "updated_at": "2025-10-24T07:02:55Z"
            },
            {
                "version": "1.0.2",
                "deprecated": false,
                "migration_required": false,
                "updated_at": "2025-10-24T07:18:48Z"
            },
            {
                "version": "1.0.3",
                "deprecated": false,
                "migration_required": false,
                "updated_at": "2025-10-24T07:36:22Z"
            },
            {
                "version": "1.0.4",
                "deprecated": false,
                "migration_required": false,
                "updated_at": "2025-10-24T09:30:50Z"
            },
            {
                "version": "1.0.5",
                "deprecated": false,
                "migration_required": false,
                "updated_at": "2025-10-24T09:53:53Z"
            },
            {
                "version": "1.0.6",
                "deprecated": false,
                "migration_required": false,
                "updated_at": "2025-10-24T10:35:39Z"
            },
            {
                "version": "1.0.7",
                "deprecated": false,
                "migration_required": false,
                "updated_at": "2025-10-24T11:12:11Z"
            }
        ]
    },
    "traits": {
        "version": "1.0.0",
        "checksum": "sha256:913a03cac67794ef",
        "url": "traits/traits_v1.json",
        "count": 6,
        "definitions": [
            {
                "name": "Agent Topic",
                "description": "Evaluating the capability of large language models as intelligent agents when invoking tools.",
                "slogan": "ğŸ® Large language models do more than talk â€“ they wield tools",
                "icon": "https://opencompass.oss-cn-shanghai.aliyuncs.com/image/compass-hub/agent.png"
            },
            {
                "name": "Math Topic",
                "description": "An integrated evaluation of the model's proficiency in addressing problems across the spectrum of mathematical disciplines, from elementary to advanced levels.",
                "slogan": "Does your large language model serve as the mathematics representative in its class?",
                "icon": "https://opencompass.oss-cn-shanghai.aliyuncs.com/image/compass-hub/evaluate.png"
            },
            {
                "name": "Code Topic",
                "description": "Assessing the model's ability to understand, interpret, generate, optimize code, and understand coding problems.",
                "slogan": "ğŸ¥¹ Oh no! I've been replaced by a large language model",
                "icon": "https://opencompass.oss-cn-shanghai.aliyuncs.com/image/compass-hub/code.png"
            },
            {
                "name": "Safety Topic",
                "description": "The ability to avoid generating harmful, misleading, biased, or inappropriate content.",
                "slogan": "ğŸ«¢ In Chinese language contexts, ensuring legal, compliant, and value-aligned content generation by large models is crucial",
                "icon": "https://opencompass.oss-cn-shanghai.aliyuncs.com/image/compass-hub/security.png"
            },
            {
                "name": "Creation Topic",
                "description": "An assessment of the model's adeptness in elaborating, extending, and revising written works across a diverse range of subjects.",
                "slogan": "Who is the Champion Writer? âœ’ï¸",
                "icon": "https://opencompass.oss-cn-shanghai.aliyuncs.com/image/compass-hub/security.png"
            },
            {
                "name": "Long-Context Topic",
                "description": "Evaluating the model's accuracy in understanding and generating exceptionally long texts.",
                "slogan": "ğŸ˜… The model canâ€™t afford to lose track in long conversations",
                "icon": "https://opencompass.oss-cn-shanghai.aliyuncs.com/image/compass-hub/long-text.png"
            }
        ]
    },
    "datasets": {
        "opencompass": {
            "version": "2.0.0",
            "license": "Various - See individual dataset licenses",
            "source": "OpenCompass Evaluation Platform",
            "checksum": "sha256:placeholder_opencompass_v2",
            "count": 446,
            "datasets": [
                {
                    "id": "opencompass_2152",
                    "name": "InternData-N1",
                    "version": "1.0.0",
                    "description": "InternData-N1: A high-quality navigation dataset with the most diverse scenes and extensive randomization across embodiments/viewpoints, including 3k+ scenes and 830k VLN data.",
                    "url": "opencompass/opencompass_2152.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2152",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2152",
                        "name": "InternData-N1",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "",
                        "officialWebsiteLink": "https://huggingface.co/datasets/InternRobotics/InternData-N1",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50207630",
                            "name": null,
                            "avatar": null,
                            "nickname": "fak111"
                        },
                        "lookNum": "293",
                        "top": true,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-08-15 16:39:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-08-15 16:39:55",
                        "createDate": "2025-08-15 16:39:42",
                        "desc": {
                            "cn": "InternData-N1ï¼šä¸€ä¸ªé«˜è´¨é‡å¯¼èˆªæ•°æ®é›†ï¼Œå…·æœ‰æœ€ä¸°å¯Œçš„åœºæ™¯å’Œè·¨å…·èº«/è§†è§’çš„å¹¿æ³›éšæœºåŒ–ï¼ŒåŒ…å« 3000+ åœºæ™¯å’Œ 83 ä¸‡æ¡ VLN æ•°æ®ã€‚",
                            "en": "InternData-N1: A high-quality navigation dataset with the most diverse scenes and extensive randomization across embodiments/viewpoints, including 3k+ scenes and 830k VLN data."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/interndata_n1'. Error: Path opencompass/interndata_n1 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1922",
                    "name": "MMSI-Bench",
                    "version": "1.0.0",
                    "description": "MMSI-Bench is a novel Visual Question Answering (VQA) benchmark specifically designed to evaluate Multi-image Spatial Intelligence in multimodal large language models (MLLMs). Unlike traditional datasets that focus on spatial reasoning within a single image, MMSI-Bench emphasizes real-world inspired",
                    "url": "opencompass/opencompass_1922.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1922",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1922",
                        "name": "MMSI-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "ç©ºé—´æ™ºèƒ½",
                                "en": "ç©ºé—´æ™ºèƒ½"
                            },
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "å¤šæ¨¡æ€"
                            },
                            {
                                "cn": "è·¨è§†è§’",
                                "en": "è·¨è§†è§’"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OpenRobotLab/MMSI-Bench",
                        "paperLink": "https://arxiv.org/pdf/2505.23764",
                        "officialWebsiteLink": "https://runsenxu.com/projects/MMSI_Bench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "155355",
                            "name": "kennyutc",
                            "avatar": null,
                            "nickname": "kennyutc"
                        },
                        "lookNum": "619",
                        "top": true,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-11 14:16:32",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-11 14:16:32",
                        "createDate": "2025-06-11 14:11:18",
                        "desc": {
                            "cn": "MMSI-Bench æ˜¯ä¸€ä¸ªå…¨æ–°çš„å¤šæ¨¡æ€ç©ºé—´æ™ºèƒ½è§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æ•°æ®é›†ï¼Œä¸“ä¸ºè¯„ä¼°å¤šå›¾åƒç©ºé—´æ¨ç†èƒ½åŠ›è€Œè®¾è®¡ã€‚ä¸ä¸“æ³¨äºå•å›¾åƒå…³ç³»æ¨ç†çš„ä¼ ç»Ÿæ•°æ®é›†ä¸åŒï¼ŒMMSI-Bench æ›´è´´è¿‘ç°å®ä¸–ç•Œï¼Œèšç„¦äºéœ€åœ¨å¤šå¼ å›¾åƒä¹‹é—´è¿›è¡Œé€»è¾‘æ¨ç†çš„å¤æ‚åœºæ™¯ã€‚è¯¥æ•°æ®é›†ç”±å…­ä½ä¸‰ç»´è§†è§‰ä¸“å®¶è€—æ—¶è¶…è¿‡300å°æ—¶æ„å»ºï¼Œç²¾å¿ƒæ•´ç†å‡ºåŒ…å«1,000ä¸ªé«˜è´¨é‡é€‰æ‹©é¢˜çš„é—®é¢˜é›†ï¼Œé¢˜ç›®æ¥è‡ª12ä¸‡ä½™å¼ å›¾åƒï¼Œå¹¶é™„æœ‰ç²¾å¿ƒè®¾è®¡çš„è¯¯å¯¼é€‰é¡¹ä¸é€æ­¥æ¨ç†è¿‡ç¨‹ã€‚å¯¹34ä¸ªä¸»æµå¼€æºå’Œé—­æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å®è¯è¯„ä¼°æ˜¾ç¤ºï¼šæœ€å…ˆè¿›çš„æ¨¡å‹å‡†ç¡®ç‡ä»…ä¸º30%è‡³40%ï¼Œè€Œäººç±»è¡¨ç°é«˜è¾¾97%ï¼Œæ­ç¤ºè¯¥ä»»åŠ¡çš„å·¨å¤§æŒ‘æˆ˜æ€§ä¸æ¨¡å‹å‘å±•ç©ºé—´ã€‚æ­¤å¤–ï¼ŒMMSI-Bench é…å¥—æä¾›è‡ªåŠ¨åŒ–é”™è¯¯åˆ†æ",
                            "en": "MMSI-Bench is a novel Visual Question Answering (VQA) benchmark specifically designed to evaluate Multi-image Spatial Intelligence in multimodal large language models (MLLMs). Unlike traditional datasets that focus on spatial reasoning within a single image, MMSI-Bench emphasizes real-world inspired"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmsi_bench'. Error: Path opencompass/mmsi_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1942",
                    "name": "OmniDocBench",
                    "version": "1.0.0",
                    "description": "OmniDocBench is a comprehensive benchmark for evaluating document parsing in real-world scenarios. It includes 981 PDF pages across 9 document types, annotated with dense paragraph-level bboxes with text and attributes. Along with its designed evaluation methods, it provides Fine-grained results.",
                    "url": "opencompass/opencompass_1942.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1942",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1942",
                        "name": "OmniDocBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "é•¿æ–‡æœ¬",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Document content extraction",
                                "en": "Document content extraction"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/opendatalab/OmniDocBench/tree/main?tab=readme-ov-file",
                        "paperLink": "https://arxiv.org/pdf/2412.07626",
                        "officialWebsiteLink": "https://huggingface.co/datasets/opendatalab/OmniDocBench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "032787",
                            "name": "ouyanglinke",
                            "avatar": null,
                            "nickname": "ouyanglinke"
                        },
                        "lookNum": "346",
                        "top": true,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-16 17:13:23",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-16 17:13:23",
                        "createDate": "2025-06-16 14:34:38",
                        "desc": {
                            "cn": "OmniDocBenchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°çœŸå®åœºæ™¯ä¸‹å¤šæ ·æ€§æ–‡æ¡£è§£ææ•ˆæœçš„è¯„æµ‹é›†ï¼Œå®ƒåŒ…å«äº†981ä¸ªé¡µé¢ï¼Œè¦†ç›–9ç§æ–‡æ¡£ç±»å‹ï¼ˆåŒ…æ‹¬ç ”æŠ¥ã€æ•™æã€æŠ¥çº¸ã€æ‰‹å†™ç¬”è®°ã€æ‚å¿—ç­‰ï¼‰ï¼Œå…·æœ‰æ®µè½çº§åˆ«çš„ä½ç½®æ ‡æ³¨å’Œå†…å®¹æ ‡æ³¨ï¼Œè¿˜æœ‰é˜…è¯»é¡ºåºæ ‡æ³¨å’Œå±æ€§æ ‡æ³¨ï¼Œå¹¶å¼€å‘äº†é…å¥—çš„è¯„æµ‹æ–¹æ³•ï¼Œä½¿å…¶æ—¢å…·å¤‡å•æ¨¡å—çš„è¯„æµ‹èƒ½åŠ›ï¼ˆåŒ…æ‹¬å¸ƒå±€æ£€æµ‹ï¼Œå…¬å¼è¯†åˆ«ï¼Œè¡¨æ ¼è¯†åˆ«ï¼Œæ–‡æœ¬è¯†åˆ«ï¼‰ï¼Œåˆå…·å¤‡ç«¯åˆ°ç«¯çš„è¯„æµ‹èƒ½åŠ›ï¼Œé’ˆå¯¹ä¸åŒå…ƒç´ æä¾›äº†åˆ†é¡µé¢ä»¥åŠåˆ†å±æ€§çš„ç²¾ç»†åŒ–è¯„æµ‹ç»“æœï¼Œç²¾å‡†å®šä½æ¨¡å‹æ–‡æ¡£è§£æçš„ç—›ç‚¹é—®é¢˜ã€‚",
                            "en": "OmniDocBench is a comprehensive benchmark for evaluating document parsing in real-world scenarios. It includes 981 PDF pages across 9 document types, annotated with dense paragraph-level bboxes with text and attributes. Along with its designed evaluation methods, it provides Fine-grained results."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/omnidocbench'. Error: Path opencompass/omnidocbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1206",
                    "name": "MMBench",
                    "version": "1.0.0",
                    "description": "MMBench is a collection of benchmarks to evaluate the multi-modal understanding capability of large vision language models (LVLMs). This benchmark contains 3,000 multiple-choice questions covering 20 fine-grained assessment dimensions.",
                    "url": "opencompass/opencompass_1206.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1206",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1206",
                        "name": "MMBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 3,
                        "githubLink": "https://github.com/open-compass/MMBench",
                        "paperLink": "https://arxiv.org/abs/2307.06281",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1912",
                        "top": true,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-30 16:30:49",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-30 16:30:49",
                        "createDate": "2024-12-30 16:16:18",
                        "desc": {
                            "cn": "MMBenchæ˜¯OpenCompass ç ”ç©¶å›¢é˜Ÿè‡ªå»ºçš„è§†è§‰è¯­è¨€æ¨¡å‹è¯„æµ‹æ•°æ®é›†ï¼Œå¯å®ç°ä»æ„ŸçŸ¥åˆ°è®¤çŸ¥èƒ½åŠ›é€çº§ç»†åˆ†è¯„ä¼°ã€‚æ­¤è¯„æµ‹åŸºå‡†åŒ…å«3000 é“å•é¡¹é€‰æ‹©é¢˜ ï¼Œè¦†ç›– 20ä¸ªç»†ç²’åº¦è¯„ä¼°ç»´åº¦ã€‚",
                            "en": "MMBench is a collection of benchmarks to evaluate the multi-modal understanding capability of large vision language models (LVLMs). This benchmark contains 3,000 multiple-choice questions covering 20 fine-grained assessment dimensions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmbench'. Error: Path opencompass/mmbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1694",
                    "name": "MaritimeBench",
                    "version": "1.0.0",
                    "description": "MaritimeBench builds a scientific, fair maritime knowledge assessment system. With 1,888 MCQs based on industry standards, we evaluate models' capabilities across shipping domains.",
                    "url": "opencompass/opencompass_1694.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1694",
                    "sample_count": 1000,
                    "traits": [
                        "Examination",
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1694",
                        "name": "MaritimeBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å­¦ç§‘",
                                "en": "Examination"
                            },
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "èˆªè¿",
                                "en": "èˆªè¿"
                            },
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "çŸ¥è¯†"
                            },
                            {
                                "cn": "æµ·è¿",
                                "en": "æµ·è¿"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 4,
                        "githubLink": "",
                        "paperLink": "",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "067099",
                            "name": "wangxiangyu",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/067099-bd016c2a-6b89-4ba3-aaa1-6cc12a7ca88f.png",
                            "nickname": "Hi-Dolphin"
                        },
                        "lookNum": "779",
                        "top": true,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-22 11:41:39",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-22 11:41:39",
                        "createDate": "2025-04-01 09:13:05",
                        "desc": {
                            "cn": "MaritimeBench è‡´åŠ›äºæ„å»ºä¸€å¥—ç§‘å­¦ã€å…¬å¹³ä¸”ä¸¥è°¨çš„èˆªè¿çŸ¥è¯†è¯„ä¼°ä½“ç³»ã€‚åŸºäºè¡Œä¸šæƒå¨æ ‡å‡†ï¼Œæˆ‘ä»¬æŒç»­ç»´æŠ¤å¹¶æ›´æ–°é«˜è´¨é‡çš„èˆªè¿æ•°æ®é›†â€”â€”å…¶ä¸­åŒ…å«1,888é“å®¢è§‚é€‰æ‹©é¢˜ï¼ˆMCQæ ¼å¼ï¼‰ï¼Œä»¥å…¨é¢ã€å¤šç»´åº¦åœ°é‡åŒ–æ¨¡å‹åœ¨èˆªè¿å„é¢†åŸŸçš„èƒ½åŠ›è¡¨ç°ã€‚",
                            "en": "MaritimeBench builds a scientific, fair maritime knowledge assessment system. With 1,888 MCQs based on industry standards, we evaluate models' capabilities across shipping domains."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/maritimebench'. Error: Path opencompass/maritimebench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1558",
                    "name": "MM-AlignBench",
                    "version": "1.0.0",
                    "description": "A benchmark for evaluating MLLMs' alignment with human preferences. It includes 252 high-quality, human-annotated samples with diverse image types and open-ended questions. Modeled after Arena-style benchmarks, it uses GPT-4o as the judge model and Claude-Sonnet-3 as the reference model.",
                    "url": "opencompass/opencompass_1558.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1558",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1558",
                        "name": "MM-AlignBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/PhoenixZ810/OmniAlign-V",
                        "paperLink": "https://arxiv.org/abs/2502.18411",
                        "officialWebsiteLink": "https://phoenixz810.github.io/OmniAlign-V/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1137",
                        "top": true,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-05 09:55:00",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-05 09:55:00",
                        "createDate": "2025-03-05 09:54:39",
                        "desc": {
                            "cn": "ç”¨äºè¯„ä¼° MLLM ä¸äººç±»åå¥½çš„ä¸€è‡´æ€§çš„åŸºå‡†ã€‚å®ƒåŒ…å« 252 ä¸ªé«˜è´¨é‡ã€äººç±»æ ‡æ³¨çš„æ ·æœ¬ï¼Œå…·æœ‰ä¸åŒçš„å›¾åƒç±»å‹å’Œå¼€æ”¾å¼é—®é¢˜ã€‚å®ƒä»¿ç…§ Arena é£æ ¼çš„åŸºå‡†ï¼Œä½¿ç”¨ GPT-4o ä½œä¸ºè¯„åˆ¤æ¨¡å‹ï¼ŒClaude-Sonnet-3 ä½œä¸ºå‚è€ƒæ¨¡å‹ã€‚",
                            "en": "A benchmark for evaluating MLLMs' alignment with human preferences. It includes 252 high-quality, human-annotated samples with diverse image types and open-ended questions. Modeled after Arena-style benchmarks, it uses GPT-4o as the judge model and Claude-Sonnet-3 as the reference model."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mm_alignbench'. Error: Path opencompass/mm_alignbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1509",
                    "name": "MVBench",
                    "version": "1.0.0",
                    "description": "MVBench can test MLLMs' temporal understanding in the dynamic video tasks. It covers 20 challenging video tasks that cannot be effectively solved with a single frame.",
                    "url": "opencompass/opencompass_1509.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1509",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1509",
                        "name": "MVBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/OpenGVLab/Ask-Anything",
                        "paperLink": "https://arxiv.org/abs/2311.17005",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "690",
                        "top": true,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-02-19 14:28:53",
                        "supportOnlineEval": false,
                        "updateDate": "2025-02-19 14:28:53",
                        "createDate": "2025-02-17 15:17:00",
                        "desc": {
                            "cn": "MVBenchç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨åŠ¨æ€è§†é¢‘ä»»åŠ¡ä¸­çš„æ—¶é—´ç†è§£èƒ½åŠ›ï¼Œç”±20ä¸ªå•å¸§å†…å®¹æ— æ³•æœ‰æ•ˆè§£å†³çš„æŒ‘æˆ˜æ€§çš„è§†é¢‘ä»»åŠ¡ç»„æˆã€‚",
                            "en": "MVBench can test MLLMs' temporal understanding in the dynamic video tasks. It covers 20 challenging video tasks that cannot be effectively solved with a single frame."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mvbench'. Error: Path opencompass/mvbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1375",
                    "name": "VBench",
                    "version": "1.0.0",
                    "description": "VBench is a comprehensive benchmark evaluates video generation quality. It comprises 16 dimensions in video generation, and also provides a dataset of human preference annotations. ",
                    "url": "opencompass/opencompass_1375.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1375",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Creation"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1375",
                        "name": "VBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "åˆ›ä½œ",
                                "en": "Creation"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Vchitect/VBench",
                        "paperLink": "https://arxiv.org/abs/2311.17982",
                        "officialWebsiteLink": "https://vchitect.github.io/VBench-project/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "670",
                        "top": true,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:36",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:36",
                        "createDate": "2025-01-10 16:36:24",
                        "desc": {
                            "cn": "VBenchç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„è§†é¢‘ç”Ÿæˆè´¨é‡ï¼ŒåŒ…å«16ä¸ªè§†é¢‘ç”Ÿæˆç»´åº¦åŠ1ä¸ªäººç±»åå¥½æ³¨é‡Šæ•°æ®é›†ã€‚",
                            "en": "VBench is a comprehensive benchmark evaluates video generation quality. It comprises 16 dimensions in video generation, and also provides a dataset of human preference annotations. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/vbench'. Error: Path opencompass/vbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1397",
                    "name": "LiveMathBench",
                    "version": "1.0.0",
                    "description": "LiveMathBench can capture LLM capabilities in complex reasoning tasks, including challenging latest question sets from various mathematical competitions. ",
                    "url": "opencompass/opencompass_1397.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1397",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Math"
                    ],
                    "eval_type": {
                        "gen": "LiveMathBench_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1397",
                        "name": "LiveMathBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 3,
                        "githubLink": "https://github.com/open-compass/GPassK",
                        "paperLink": "https://arxiv.org/abs/2412.13147",
                        "officialWebsiteLink": "https://open-compass.github.io/GPassK/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1365",
                        "top": true,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-16 20:30:57",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-16 20:30:57",
                        "createDate": "2025-01-15 18:21:51",
                        "desc": {
                            "cn": "LiveMathBenchç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†æ–¹é¢çš„è¡¨ç°ï¼Œç”±æå…·æŒ‘æˆ˜æ€§çš„ç°ä»£æ•°å­¦é—®é¢˜ç»„æˆã€‚",
                            "en": "LiveMathBench can capture LLM capabilities in complex reasoning tasks, including challenging latest question sets from various mathematical competitions. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/livemathbench'. Error: Path opencompass/livemathbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1370",
                    "name": "MathVision",
                    "version": "1.0.0",
                    "description": "MathVision measures multimodal mathematical reasoning capabilities through a meticulously curated collection of 3,040 high-quality mathematical problems spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty.",
                    "url": "opencompass/opencompass_1370.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1370",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1370",
                        "name": "MathVision",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/mathllm/MATH-V",
                        "paperLink": "https://arxiv.org/abs/2402.14804",
                        "officialWebsiteLink": "https://mathllm.github.io/mathvision/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "908",
                        "top": true,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-10 18:23:49",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-10 18:23:49",
                        "createDate": "2025-01-10 18:19:40",
                        "desc": {
                            "cn": "MathVisionç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œç”±æ¶µç›–16ä¸ªæ•°å­¦é¢†åŸŸã€è·¨è¶Š5ä¸ªéš¾åº¦çº§åˆ«çš„3040ä¸ªé«˜è´¨é‡æ•°å­¦é—®é¢˜ç»„æˆã€‚",
                            "en": "MathVision measures multimodal mathematical reasoning capabilities through a meticulously curated collection of 3,040 high-quality mathematical problems spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mathvision'. Error: Path opencompass/mathvision is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1371",
                    "name": "MathVerse",
                    "version": "1.0.0",
                    "description": "MathVerse is intended for evaluating MLLMs' visual math problem-solving, containing 2,612 high-quality, multi-subject math problems with diagrams. ",
                    "url": "opencompass/opencompass_1371.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1371",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1371",
                        "name": "MathVerse",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/ZrrSkywalker/MathVerse",
                        "paperLink": "https://arxiv.org/abs/2403.14624",
                        "officialWebsiteLink": "https://mathverse-cuhk.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "459",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-10 18:00:01",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-10 18:00:01",
                        "createDate": "2025-01-10 14:29:47",
                        "desc": {
                            "cn": "MathVerseç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„è§†è§‰æ•°å­¦é—®é¢˜è§£å†³èƒ½åŠ›ï¼ŒåŒ…å«2612ä¸ªé«˜è´¨é‡ã€å¤šä¸»é¢˜çš„æ•°å­¦é—®é¢˜ã€‚",
                            "en": "MathVerse is intended for evaluating MLLMs' visual math problem-solving, containing 2,612 high-quality, multi-subject math problems with diagrams. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mathverse'. Error: Path opencompass/mathverse is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1374",
                    "name": "DynaMath",
                    "version": "1.0.0",
                    "description": "DynaMath is a dynamic visual math benchmark designed for in-depth assessment of VLMs. It includes 501 high-quality, multi-topic seed questions, each represented as a Python program enabling the automatic generation of a much larger set of concrete questions.",
                    "url": "opencompass/opencompass_1374.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1374",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1374",
                        "name": "DynaMath",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/DynaMath/DynaMath",
                        "paperLink": "https://arxiv.org/abs/2411.00836",
                        "officialWebsiteLink": "https://dynamath.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "321",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-10 18:25:13",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-10 18:25:13",
                        "createDate": "2025-01-10 18:24:27",
                        "desc": {
                            "cn": " DynaMathç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ•°å­¦èƒ½åŠ›ï¼ŒåŒ…æ‹¬501ä¸ªé«˜è´¨é‡ã€å¤šä¸»é¢˜çš„ç§å­é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½ä»¥Pythonç¨‹åºè¡¨ç¤ºï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå¤§é‡å…·ä½“çš„å¤šæ ·åŒ–é—®é¢˜ã€‚",
                            "en": "DynaMath is a dynamic visual math benchmark designed for in-depth assessment of VLMs. It includes 501 high-quality, multi-topic seed questions, each represented as a Python program enabling the automatic generation of a much larger set of concrete questions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/dynamath'. Error: Path opencompass/dynamath is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_539",
                    "name": "BBH",
                    "version": "1.0.0",
                    "description": "BIG-Bench Hard (BBH) is a subset of the BIG-Bench, a diverse evaluation suite for language models. BBH focuses on a suite of 23 challenging tasks from BIG-Bench that were found to be beyond the capabilities of current language models.",
                    "url": "opencompass/opencompass_539.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_539",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "539",
                        "name": "BBH",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "reasoning",
                                "en": "reasoning"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/suzgunmirac/BIG-Bench-Hard",
                        "paperLink": "https://arxiv.org/pdf/2210.09261.pdf",
                        "officialWebsiteLink": "https://github.com/suzgunmirac/BIG-Bench-Hard",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "17498",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:31:41",
                        "supportOnlineEval": true,
                        "updateDate": "2024-09-12 19:31:41",
                        "createDate": "2024-09-12 19:26:57",
                        "desc": {
                            "cn": "BIG Bench-Hardï¼ˆBBHï¼‰æ˜¯BIG Benchçš„ä¸€ä¸ªå­é›†ï¼Œå®ƒæ˜¯ä¸€ä¸ªç”¨äºè¯­è¨€æ¨¡å‹çš„å¤šæ ·åŒ–è¯„ä¼°å¥—ä»¶ã€‚BBHä¸“æ³¨äºBIG Benchçš„23é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡è¢«å‘ç°è¶…å‡ºäº†å½“å‰è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚",
                            "en": "BIG-Bench Hard (BBH) is a subset of the BIG-Bench, a diverse evaluation suite for language models. BBH focuses on a suite of 23 challenging tasks from BIG-Bench that were found to be beyond the capabilities of current language models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/bbh'. Error: No data found in /home/budadmin/.cache/opencompass/./data/BBH/data for split 'test'. Please check if the split exists and contains data files.\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_534",
                    "name": "MATH",
                    "version": "1.0.0",
                    "description": "MATH is a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution.",
                    "url": "opencompass/opencompass_534.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_534",
                    "sample_count": 5000,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {
                        "gen": "math_gen",
                        "agent": "math_agent"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 100,
                        "estimated_output_tokens": 50
                    },
                    "original_data": {
                        "id": "534",
                        "name": "MATH",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "math",
                                "en": "math"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/hendrycks/math",
                        "paperLink": "https://arxiv.org/pdf/2103.03874.pdf",
                        "officialWebsiteLink": "https://huggingface.co/datasets/hendrycks/competition_math",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "13270",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:31:29",
                        "supportOnlineEval": true,
                        "updateDate": "2024-09-12 19:31:29",
                        "createDate": "2024-09-12 19:28:19",
                        "desc": {
                            "cn": "MATH æ˜¯ä¸€ä¸ªåŒ…å« 12,500 ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç«èµ›æ•°å­¦é—®é¢˜çš„æ–°æ•°æ®é›†ã€‚ MATH ä¸­çš„æ¯ä¸ªé—®é¢˜éƒ½æœ‰å®Œæ•´çš„åˆ†æ­¥è§£å†³æ–¹æ¡ˆã€‚",
                            "en": "MATH is a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "{\"problem\": \"Simplify\\n\\\\[\\\\frac{\\\\sec x}{\\\\sin x} - \\\\frac{\\\\sin x}{\\\\cos x}.\\\\]\", \"solution\": \"We can write\\n\\\\begin{align*}\\n\\\\frac{\\\\sec x}{\\\\sin x} - \\\\frac{\\\\sin x}{\\\\cos x} &= \\\\frac{1}{\\\\cos x",
                                    "difficulty": "moderate",
                                    "task_type": "Mathematical simplification, Trigonometric identity application"
                                },
                                {
                                    "question": "{\"problem\": \"Compute $\\\\sin 120^\\\\circ$.\", \"solution\": \"Let $P$ be the point on the unit circle that is $120^\\\\circ$ counterclockwise from $(1,0)$, and let $D$ be the foot of the altitude from $P$ to ",
                                    "difficulty": "moderate",
                                    "task_type": "Mathematical Calculation, Conceptual Application"
                                },
                                {
                                    "question": "{\"problem\": \"Find the units digit of $13^{19} \\\\cdot 19^{13}$\", \"solution\": \"Since the tens digits don't matter to us, the problem is the same as finding the units digit of $3^{19} \\\\cdot 9^{13} = 3^{",
                                    "difficulty": "moderate",
                                    "task_type": "Problem Solving, Mathematical Reasoning"
                                }
                            ],
                            "total_questions": 5000,
                            "question_format": "Conceptual Application, Math problem solving, Mathematical Calculation, Mathematical Reasoning, Mathematical simplification, Problem Solving, Trigonometric identity application",
                            "difficulty_levels": [
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Tests domain-specific knowledge in Mathematical Analysis, Mathematics, Geometry",
                                "Evaluates Spatial Reasoning, Critical Thinking, Algebraic manipulation skills",
                                "Diverse task types: Math problem solving, Conceptual Application, Problem Solving"
                            ],
                            "disadvantages": [
                                "Requires knowledge across multiple specialized domains"
                            ]
                        }
                    },
                    "analysis_file": "analysis/opencompass_534_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 5,
                        "successful": 5,
                        "failed": 0
                    }
                },
                {
                    "id": "opencompass_542",
                    "name": "LongBench",
                    "version": "1.0.0",
                    "description": "LongBench is a benchmark for bilingual, multitask, and comprehensive assessment of long context understanding capabilities of large language models.",
                    "url": "opencompass/opencompass_542.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_542",
                    "sample_count": 1000,
                    "traits": [
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "542",
                        "name": "LongBench",
                        "emoji": "ğŸª‘",
                        "dimensions": [
                            {
                                "cn": "é•¿æ–‡æœ¬",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "long-context",
                                "en": "long-context"
                            }
                        ],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/THUDM/LongBench",
                        "paperLink": "https://arxiv.org/pdf/2308.14508.pdf",
                        "officialWebsiteLink": "https://huggingface.co/datasets/THUDM/LongBench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "8912",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:32:08",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-12 19:32:08",
                        "createDate": "2024-09-12 19:26:26",
                        "desc": {
                            "cn": "LongBench æ˜¯ä¸€ä¸ªå¤šä»»åŠ¡ã€ä¸­è‹±åŒè¯­ã€é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹é•¿æ–‡æœ¬ç†è§£èƒ½åŠ›çš„è¯„æµ‹åŸºå‡†ã€‚",
                            "en": "LongBench is a benchmark for bilingual, multitask, and comprehensive assessment of long context understanding capabilities of large language models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/longbench'. Error: Path opencompass/longbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_535",
                    "name": "GSM8K",
                    "version": "1.0.0",
                    "description": "GSM8K is a dataset of 8,500 high quality linguistically diverse grade school math word problems created by human problem writers. The dataset is segmented into 7,500 training problems and 1,000 test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ âˆ’ Ã— Ã·) to reach the final answer.",
                    "url": "opencompass/opencompass_535.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_535",
                    "sample_count": 1319,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {
                        "gen": "gsm8k_gen",
                        "agent": "gsm8k_agent"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 55,
                        "estimated_output_tokens": 95
                    },
                    "original_data": {
                        "id": "535",
                        "name": "GSM8K",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "math",
                                "en": "math"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/openai/grade-school-math",
                        "paperLink": "https://arxiv.org/pdf/2110.14168.pdf",
                        "officialWebsiteLink": "https://huggingface.co/datasets/gsm8k",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "8894",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:31:32",
                        "supportOnlineEval": true,
                        "updateDate": "2024-09-12 19:31:32",
                        "createDate": "2024-09-12 19:27:59",
                        "desc": {
                            "cn": "GSM8K æ˜¯ä¸€ä¸ªåŒ…å« 8,500 ä¸ªé«˜è´¨é‡ã€è¯­è¨€å¤šæ ·åŒ–çš„å°å­¦æ•°å­¦å•è¯é—®é¢˜çš„æ•°æ®é›†ï¼Œç”±äººç±»é—®é¢˜ç¼–å†™è€…åˆ›å»ºã€‚è¯¥æ•°æ®é›†åˆ†ä¸º 7,500 ä¸ªè®­ç»ƒé—®é¢˜å’Œ 1,000 ä¸ªæµ‹è¯•é—®é¢˜ã€‚è¿™äº›é—®é¢˜çš„è§£é¢˜æ­¥éª¤åœ¨ 2 åˆ° 8 æ­¥ä¹‹é—´ï¼Œè§£é¢˜è¿‡ç¨‹ä¸»è¦æ¶‰åŠä½¿ç”¨åŸºæœ¬ç®—æœ¯è¿ç®—ï¼ˆ+ - Ã— Ã·ï¼‰è¿›è¡Œä¸€è¿ä¸²çš„åŸºæœ¬è®¡ç®—ï¼Œä»è€Œå¾—å‡ºæœ€ç»ˆç­”æ¡ˆã€‚ä¸€ä¸ªèªæ˜çš„åˆä¸­ç”Ÿåº”è¯¥èƒ½å¤Ÿè§£å†³æ¯ä¸€ä¸ªé—®é¢˜ã€‚å®ƒå¯ç”¨äºå¤šæ­¥æ•°å­¦æ¨ç†ã€‚",
                            "en": "GSM8K is a dataset of 8,500 high quality linguistically diverse grade school math word problems created by human problem writers. The dataset is segmented into 7,500 training problems and 1,000 test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ âˆ’ Ã— Ã·) to reach the final answer."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "Jared is trying to increase his typing speed. He starts with 47 words per minute (WPM). After some lessons the next time he tests his typing speed it has increased to 52 WPM. If he continues to increa",
                                    "difficulty": "easy",
                                    "task_type": "math_problem"
                                },
                                {
                                    "question": "Jordan has 2 children who wear diapers.  Each child requires 5 diaper changes per day.  Jordan's wife changes half of the diapers.  How many diapers does Jordan change per day?",
                                    "difficulty": "easy",
                                    "task_type": "arithmetic, word_problem"
                                },
                                {
                                    "question": "A wooden bridge can carry no more than 5000 pounds. A delivery truck filled with identical boxes, each weighing 15 pounds, will pass over the bridge. The combined weight of the driver and the empty tr",
                                    "difficulty": "moderate",
                                    "task_type": "Problem Solving"
                                }
                            ],
                            "total_questions": 1319,
                            "question_format": "Mathematical Problem Solving, Problem Solving, Quantitative reasoning, arithmetic, math_problem, word_problem",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Tests domain-specific knowledge in Mathematics",
                                "Evaluates arithmetic, Basic arithmetic, problem_solving skills",
                                "Covers multiple difficulty levels: Beginner, Intermediate",
                                "Diverse task types: arithmetic, math_problem, word_problem"
                            ],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": "analysis/opencompass_535_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 5,
                        "successful": 5,
                        "failed": 0
                    }
                },
                {
                    "id": "opencompass_512",
                    "name": "TriviaQA",
                    "version": "1.0.0",
                    "description": "TriviaqQA is a reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaqQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions.",
                    "url": "opencompass/opencompass_512.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_512",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {
                        "gen": "TriviaQA_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "512",
                        "name": "TriviaQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/mandarjoshi90/triviaqa",
                        "paperLink": "https://arxiv.org/abs/1705.03551",
                        "officialWebsiteLink": "http://nlp.cs.washington.edu/triviaqa/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "6251",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": true,
                        "updateDate": "2024-08-02 09:45:49",
                        "createDate": "2024-01-11 14:10:43",
                        "desc": {
                            "cn": "TriviaqQAæ˜¯ä¸€ä¸ªé˜…è¯»ç†è§£æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡65ä¸‡ä¸ªé—®é¢˜-ç­”æ¡ˆ-è¯æ®ä¸‰å…ƒç»„ã€‚å…¶åŒ…æ‹¬95Kä¸ªé—®ç­”å¯¹ï¼Œç”±å†·çŸ¥è¯†çˆ±å¥½è€…å’Œç‹¬ç«‹æ”¶é›†çš„äº‹å®æ€§æ–‡æ¡£æ’°å†™ï¼Œå¹³å‡æ¯ä¸ªé—®é¢˜6ä¸ªï¼Œä¸ºå›ç­”é—®é¢˜æä¾›é«˜è´¨é‡çš„è¿œç¨‹ç›‘ç£ã€‚\n",
                            "en": "TriviaqQA is a reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaqQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/triviaqa'. Error: Path opencompass/triviaqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_540",
                    "name": "T-Eval",
                    "version": "1.0.0",
                    "description": "T-Eval evaluates the tool utilization capabilities of LLMs and decomposing them into instruction following, planning, reasoning, retrieval, understanding, and review.",
                    "url": "opencompass/opencompass_540.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_540",
                    "sample_count": 1000,
                    "traits": [
                        "agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "540",
                        "name": "T-Eval",
                        "emoji": "ğŸ—½",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 3,
                        "githubLink": "https://github.com/open-compass/T-Eval",
                        "paperLink": "https://arxiv.org/abs/2312.14033",
                        "officialWebsiteLink": "https://open-compass.github.io/T-Eval/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "5850",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-01-25 15:15:04",
                        "createDate": "2024-01-25 15:15:04",
                        "desc": {
                            "cn": "T-Eval è¯„ä¼°äº† LLM çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œå¹¶å°†å…¶åˆ†è§£ä¸ºæŒ‡ä»¤éµå¾ªã€è§„åˆ’ã€æ¨ç†ã€æ£€ç´¢ã€ç†è§£å’Œå®¡æŸ¥ç­‰å­èƒ½åŠ›",
                            "en": "T-Eval evaluates the tool utilization capabilities of LLMs and decomposing them into instruction following, planning, reasoning, retrieval, understanding, and review."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/t_eval'. Error: Path opencompass/t_eval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_895",
                    "name": "Fin-Eva",
                    "version": "1.0.0",
                    "description": "Ant Group and Shanghai University of Finance and Economics jointly launched the financial benchmarkï¼ŒFin-Eva Version 1.0, covering multiple financial scenarios and subjects such as wealth management, insurance, investment research. The number of this benchmark's questions reaches 13,000+.",
                    "url": "opencompass/opencompass_895.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_895",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "895",
                        "name": "Fin-Eva",
                        "emoji": "ğŸ“ˆ",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "knowledge;Finance",
                                "en": "knowledge;Finance"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 4,
                        "githubLink": "https://github.com/alipay/financial_evaluation_dataset",
                        "paperLink": "",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50074087",
                            "name": "Ant_Group",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50074087-492c3cdd-1529-4e49-84ad-8cc2f8c86893.png",
                            "nickname": "èš‚èšé›†å›¢"
                        },
                        "lookNum": "5750",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:40:45",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-12 19:40:45",
                        "createDate": "2024-09-12 19:36:26",
                        "desc": {
                            "cn": "èš‚èšé›†å›¢ã€ä¸Šæµ·è´¢ç»å¤§å­¦è”åˆæ¨å‡ºé‡‘èè¯„æµ‹é›†Fin-Eva Version 1.0ï¼Œè¦†ç›–è´¢å¯Œç®¡ç†ã€ä¿é™©ã€æŠ•èµ„ç ”ç©¶ç­‰å¤šä¸ªé‡‘èåœºæ™¯ä»¥åŠé‡‘èä¸“ä¸šä¸»é¢˜å­¦ç§‘ï¼Œæ€»è¯„æµ‹é¢˜æ•°ç›®è¾¾åˆ°13,000+ã€‚",
                            "en": "Ant Group and Shanghai University of Finance and Economics jointly launched the financial benchmarkï¼ŒFin-Eva Version 1.0, covering multiple financial scenarios and subjects such as wealth management, insurance, investment research. The number of this benchmark's questions reaches 13,000+."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/fin_eva'. Error: Path opencompass/fin_eva is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_498",
                    "name": "MMLU",
                    "version": "1.0.0",
                    "description": "MMLU (Massive Multitask Language Understanding) is a new benchmark designed to measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the benchmark more challenging and more similar to how we evaluate humans. The benchmark covers 57 subjects across STEM, the humanities, the social sciences, and more. It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and probl",
                    "url": "opencompass/opencompass_498.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_498",
                    "sample_count": 14042,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 202,
                        "estimated_output_tokens": 1
                    },
                    "original_data": {
                        "id": "498",
                        "name": "MMLU",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å­¦ç§‘",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/hendrycks/test",
                        "paperLink": "https://arxiv.org/abs/2009.03300",
                        "officialWebsiteLink": "https://github.com/hendrycks/test",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "5070",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": true,
                        "updateDate": "2024-08-02 09:45:42",
                        "createDate": "2024-01-11 14:09:29",
                        "desc": {
                            "cn": "MMLU (Massive Multitask Language Understanding) æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡åœ¨é›¶æ¬¡å­¦ä¹ å’Œå°‘æ¬¡å­¦ä¹ çš„ç¯å¢ƒä¸­è¯„ä¼°æ¨¡å‹æ¥æµ‹é‡é¢„è®­ç»ƒæœŸé—´è·å¾—çš„çŸ¥è¯†ã€‚è¿™ä½¿å¾—åŸºå‡†æµ‹è¯•æ›´å…·æŒ‘æˆ˜æ€§ï¼Œä¸”æ›´æ¥è¿‘æˆ‘ä»¬è¯„ä¼°äººç±»çš„æ–¹å¼ã€‚è¯¥åŸºå‡†æµ‹è¯•æ¶µç›–äº†STEMã€äººæ–‡å­¦ç§‘ã€ç¤¾ä¼šç§‘å­¦ç­‰57ä¸ªä¸»é¢˜ã€‚å…¶éš¾åº¦èŒƒå›´ä»å°å­¦çº§åˆ«åˆ°ä¸“ä¸šçº§åˆ«ï¼Œæ—¨åœ¨æµ‹è¯•ä¸–ç•ŒçŸ¥è¯†å’Œè§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚æµ‹è¯•ä¸»é¢˜èŒƒå›´ä»ä¼ ç»Ÿé¢†åŸŸï¼Œå¦‚æ•°å­¦å’Œå†å²ï¼Œåˆ°æ›´ä¸“ä¸šçš„é¢†åŸŸï¼Œå¦‚æ³•å¾‹å’Œä¼¦ç†å­¦ã€‚é¢˜ç›®çš„ç²¾ç»†åº¦å’Œå¹¿åº¦ä½¿è¯¥åŸºå‡†æµ‹è¯•æˆä¸ºè¯†åˆ«æ¨¡å‹ç›²ç‚¹çš„ç†æƒ³é€‰æ‹©ã€‚",
                            "en": "MMLU (Massive Multitask Language Understanding) is a new benchmark designed to measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the benchmark more challenging and more similar to how we evaluate humans. The benchmark covers 57 subjects across STEM, the humanities, the social sciences, and more. It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and problem solving ability. Subjects range from traditional areas, such as mathematics and history, to more specialized areas like law and ethics. The granularity and breadth of the subjects makes the benchmark ideal for identifying a modelâ€™s blind spots."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "A manufacturer of appliances put a washing machine of a new and advanced design on the market. Two years of experimental use prior to commercial marketing had demonstrated that the machine laundered c",
                                    "difficulty": "moderate",
                                    "task_type": "QA, Legal Analysis"
                                },
                                {
                                    "question": "A man was in an accident while driving his car on July 3. The other driver sent him a notice of injuries and demanded damages. The man notified his insurance carrier but was told that his policy had l",
                                    "difficulty": "moderate",
                                    "task_type": "Legal Analysis, Contract Interpretation"
                                },
                                {
                                    "question": "Airmail Express charges for shipping small packages by integer values of weight. The charges for a weight w in pounds are as follows:\n 0 < w â‰¤ 2 $4.00\n 2 < w â‰¤ 5 $8.00\n 5 < w â‰¤ 20 $15.00\n The company ",
                                    "difficulty": "moderate",
                                    "task_type": "qa, problem_solving"
                                }
                            ],
                            "total_questions": 14042,
                            "question_format": "Contract Interpretation, Legal Analysis, QA, problem_solving, qa",
                            "difficulty_levels": [
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Tests domain-specific knowledge in Legal Studies, Product Liability, Physics",
                                "Evaluates Legal Reasoning, abstract_reasoning, Organizational understanding skills",
                                "Diverse task types: QA, problem_solving, qa"
                            ],
                            "disadvantages": [
                                "Requires knowledge across multiple specialized domains"
                            ]
                        }
                    },
                    "analysis_file": "analysis/opencompass_498_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 5,
                        "successful": 5,
                        "failed": 0
                    }
                },
                {
                    "id": "opencompass_496",
                    "name": "C-Eval",
                    "version": "1.0.0",
                    "description": "C-Eval is a comprehensive Chinese evaluation suite for foundation models. It consists of 13948 multi-choice questions spanning 52 diverse disciplines and four difficulty levels.",
                    "url": "opencompass/opencompass_496.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_496",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "496",
                        "name": "C-Eval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å­¦ç§‘",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/SJTU-LIT/ceval",
                        "paperLink": "https://arxiv.org/abs/2305.08322",
                        "officialWebsiteLink": "https://cevalbenchmark.com/index.html",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "5034",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": true,
                        "updateDate": "2024-01-11 14:09:22",
                        "createDate": "2024-01-11 14:09:22",
                        "desc": {
                            "cn": "C-Eval æ˜¯ä¸€ä¸ªå…¨é¢çš„ä¸­æ–‡åŸºç¡€æ¨¡å‹è¯„ä¼°å¥—ä»¶ã€‚å®ƒåŒ…å«äº†13948ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ï¼Œæ¶µç›–äº†52ä¸ªä¸åŒçš„å­¦ç§‘å’Œå››ä¸ªéš¾åº¦çº§åˆ«ã€‚",
                            "en": "C-Eval is a comprehensive Chinese evaluation suite for foundation models. It consists of 13948 multi-choice questions spanning 52 diverse disciplines and four difficulty levels."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/c_eval'. Error: Path opencompass/c_eval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_541",
                    "name": "L-Eval",
                    "version": "1.0.0",
                    "description": "L-Eval is a comprehensive Long Context Language Models (LCLMs) evaluation suite with 20 sub-tasks, 508 long documents, and over 2,000 human-labeled query-response pairs encompassing diverse question styles, domains, and input length (3kï½200k tokens).",
                    "url": "opencompass/opencompass_541.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_541",
                    "sample_count": 1000,
                    "traits": [
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "541",
                        "name": "L-Eval",
                        "emoji": "ğŸ¦¾",
                        "dimensions": [
                            {
                                "cn": "é•¿æ–‡æœ¬",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "long-context",
                                "en": "long-context"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/OpenLMLab/LEval",
                        "paperLink": "https://arxiv.org/pdf/2307.11088",
                        "officialWebsiteLink": "https://huggingface.co/datasets/L4NLP/LEval",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "5031",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:32:04",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-12 19:32:04",
                        "createDate": "2024-09-12 19:26:40",
                        "desc": {
                            "cn": "L-Eval æ˜¯ä¸€ä¸ªå…¨é¢çš„é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹ï¼ˆLCLMsï¼‰è¯„ä¼°å¥—ä»¶ï¼ŒåŒ…æ‹¬ 20 ä¸ªå­ä»»åŠ¡ã€508 ä¸ªé•¿æ–‡æ¡£å’Œè¶…è¿‡ 2,000 ä¸ªäººå·¥æ ‡è®°çš„æŸ¥è¯¢-å“åº”å¯¹ã€‚å®ƒæ¶µç›–äº†å¤šç§é—®ç­”é£æ ¼ã€é¢†åŸŸå’Œè¾“å…¥é•¿åº¦ï¼ˆ3,000 è‡³ 200,000 ä¸ª tokenï¼‰ã€‚",
                            "en": "L-Eval is a comprehensive Long Context Language Models (LCLMs) evaluation suite with 20 sub-tasks, 508 long documents, and over 2,000 human-labeled query-response pairs encompassing diverse question styles, domains, and input length (3kï½200k tokens)."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/l_eval'. Error: Path opencompass/l_eval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_948",
                    "name": "SecBench",
                    "version": "1.0.0",
                    "description": "Tencent Zhuque Lab and Tencent Security Keen Lab, together with Tencent Huyuan Team, Professor Jiang Yong/Professor Xia Shutao's team from Tsinghua University, Professor Luo Xiapu's research team from Hong Kong Polytechnic University, and OpenCompass team from Shanghai Artificial Intelligence Laboratory, have jointly built a safety benchmark, SecBench. We provide fair, impartial, objective, and comprehensive evaluation capabilities and promote the construction of large model in security dimensio",
                    "url": "opencompass/opencompass_948.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_948",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "948",
                        "name": "SecBench",
                        "emoji": "ğŸ”",
                        "dimensions": [
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "Safety",
                                "en": "Safety"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 4,
                        "githubLink": "",
                        "paperLink": "",
                        "officialWebsiteLink": "https://secbench.org/dataset",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50163763",
                            "name": "Tencent",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50163763-2065e33a-96ba-4182-9708-b1967a29e6a4.png",
                            "nickname": "Tencent"
                        },
                        "lookNum": "4545",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:40:48",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-12 19:40:48",
                        "createDate": "2024-09-12 19:35:12",
                        "desc": {
                            "cn": "è…¾è®¯æœ±é›€å®éªŒå®¤å’Œè…¾è®¯å®‰å…¨ç§‘æ©å®éªŒå®¤è”åˆè…¾è®¯æ··å…ƒå¤§æ¨¡å‹å›¢é˜Ÿã€æ¸…åå¤§å­¦æ±Ÿå‹‡æ•™æˆ/å¤æ ‘æ¶›æ•™æˆå›¢é˜Ÿã€é¦™æ¸¯ç†å·¥å¤§å­¦ç½—å¤æœ´æ•™æˆç ”ç©¶å›¢é˜Ÿä»¥åŠä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤OpenCompasså›¢é˜Ÿï¼Œé€šè¿‡å»ºè®¾å®‰å…¨å¤§æ¨¡å‹è¯„æµ‹åŸºå‡†SecBenchï¼Œä¸ºå®‰å…¨å¤§æ¨¡å‹ç ”å‘æä¾›å…¬å¹³ã€å…¬æ­£ã€å®¢è§‚ã€å…¨é¢çš„è¯„æµ‹èƒ½åŠ›ï¼Œæ¨åŠ¨å®‰å…¨å¤§æ¨¡å‹å»ºè®¾ã€‚",
                            "en": "Tencent Zhuque Lab and Tencent Security Keen Lab, together with Tencent Huyuan Team, Professor Jiang Yong/Professor Xia Shutao's team from Tsinghua University, Professor Luo Xiapu's research team from Hong Kong Polytechnic University, and OpenCompass team from Shanghai Artificial Intelligence Laboratory, have jointly built a safety benchmark, SecBench. We provide fair, impartial, objective, and comprehensive evaluation capabilities and promote the construction of large model in security dimension."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/secbench'. Error: Path opencompass/secbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_537",
                    "name": "HumanEval",
                    "version": "1.0.0",
                    "description": "The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases.",
                    "url": "opencompass/opencompass_537.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_537",
                    "sample_count": 164,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 132,
                        "estimated_output_tokens": 50
                    },
                    "original_data": {
                        "id": "537",
                        "name": "HumanEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "code",
                                "en": "code"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/openai/human-eval",
                        "paperLink": "https://arxiv.org/pdf/2107.03374.pdf",
                        "officialWebsiteLink": "https://huggingface.co/datasets/openai_humaneval",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "3963",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:31:35",
                        "supportOnlineEval": true,
                        "updateDate": "2024-09-12 19:31:35",
                        "createDate": "2024-09-12 19:27:36",
                        "desc": {
                            "cn": "è¿™æ˜¯ \"Evaluating Large Language Models Trained on Code\" è®ºæ–‡ä¸­æè¿°çš„ HumanEval é—®é¢˜è§£å†³æ•°æ®é›†çš„è¯„ä¼°å·¥å…·åŒ…ã€‚å®ƒç”¨äºæµ‹é‡ä»æ–‡æ¡£è„šæœ¬åˆæˆç¨‹åºçš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚å®ƒç”± 164 ä¸ªåŸå§‹ç¼–ç¨‹é—®é¢˜ç»„æˆï¼Œè¯„ä¼°è¯­è¨€ç†è§£èƒ½åŠ›ã€ç®—æ³•å’Œç®€å•æ•°å­¦ï¼Œå…¶ä¸­ä¸€äº›é—®é¢˜ä¸ç®€å•çš„è½¯ä»¶é¢è¯•é¢˜ç±»ä¼¼ã€‚",
                            "en": "The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "\n\ndef correct_bracketing(brackets: str):\n    \"\"\" brackets is a string of \"(\" and \")\".\n    return True if every opening bracket has a corresponding closing bracket.\n\n    >>> correct_bracketing(\"(\")\n   ",
                                    "difficulty": "moderate",
                                    "task_type": "coding"
                                },
                                {
                                    "question": "\ndef unique_digits(x):\n    \"\"\"Given a list of positive integers x. return a sorted list of all \n    elements that hasn't any even digit.\n\n    Note: Returned list should be sorted in increasing order.\n",
                                    "difficulty": "moderate",
                                    "task_type": "coding"
                                },
                                {
                                    "question": "\ndef by_length(arr):\n    \"\"\"\n    Given an array of integers, sort the integers that are between 1 and 9 inclusive,\n    reverse the resulting array, and then replace each digit by its corresponding nam",
                                    "difficulty": "moderate",
                                    "task_type": "coding"
                                }
                            ],
                            "total_questions": 164,
                            "question_format": "coding, function_calling, problem_solving",
                            "difficulty_levels": [
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Tests domain-specific knowledge in Mathematics, Algorithms, Computer Science",
                                "Evaluates problem_solving, basic_programming, attention_to_detail skills",
                                "Diverse task types: problem_solving, function_calling, coding"
                            ],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": "analysis/opencompass_537_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 5,
                        "successful": 5,
                        "failed": 0
                    }
                },
                {
                    "id": "opencompass_945",
                    "name": "Flames",
                    "version": "1.0.0",
                    "description": "Flames is a highly adversarial benchmark in Chinese for LLM's value alignment evaluation developed by Shanghai AI Lab and Fudan NLP Group. Flames meticulously designs a dataset of 2,251 highly adversarial, manually crafted prompts, each tailored to probe a specific value dimension (i.e., Fairness, Safety, Morality, Legality, Data protection). Currently,  Flames releases 1,000 prompts for public use (Flames_1k_Chinese).",
                    "url": "opencompass/opencompass_945.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_945",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "945",
                        "name": "Flames",
                        "emoji": "ğŸ’¡",
                        "dimensions": [
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "Safety",
                                "en": "Safety"
                            }
                        ],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NAACL 2024",
                                "en": "NAACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 4,
                        "githubLink": "https://github.com/AIFlames/Flames",
                        "paperLink": "https://arxiv.org/abs/2311.06899",
                        "officialWebsiteLink": "https://flames.opencompass.org.cn/leaderboard",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "3536",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-16 11:02:53",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-16 11:02:53",
                        "createDate": "2024-10-15 15:34:25",
                        "desc": {
                            "cn": "Flames æ˜¯ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤å’Œå¤æ—¦å¤§å­¦ NLPå›¢é˜Ÿå¼€å‘çš„ LLM ä»·å€¼å¯¹é½æ–¹å‘çš„ä¸­æ–‡é«˜åº¦å¯¹æŠ—æ€§åŸºå‡†ã€‚Flames ç²¾å¿ƒè®¾è®¡äº†ä¸€ä¸ªç”± 2,251 ä¸ªé«˜åº¦å¯¹æŠ—æ€§ã€äººå·¥åˆ›å»ºçš„æç¤ºè¯æˆçš„è¯„æµ‹é›†ï¼Œæ¯ä¸ªæç¤ºè¯éƒ½ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥æ¢ç©¶ç‰¹å®šçš„ä»·å€¼ç»´åº¦ï¼ˆå³å…¬å¹³ã€å®‰å…¨ã€é“å¾·ã€åˆæ³•ã€æ•°æ®ä¿æŠ¤ï¼‰ã€‚ç›®å‰ï¼ŒFlames å‘å¸ƒäº† 1,000 ä¸ªæç¤ºè¯ä¾›å…¬ä¼—ä½¿ç”¨ï¼ˆFlames_1k_Chineseï¼‰ã€‚",
                            "en": "Flames is a highly adversarial benchmark in Chinese for LLM's value alignment evaluation developed by Shanghai AI Lab and Fudan NLP Group. Flames meticulously designs a dataset of 2,251 highly adversarial, manually crafted prompts, each tailored to probe a specific value dimension (i.e., Fairness, Safety, Morality, Legality, Data protection). Currently,  Flames releases 1,000 prompts for public use (Flames_1k_Chinese)."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/flames'. Error: Path opencompass/flames is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_499",
                    "name": "CMMLU",
                    "version": "1.0.0",
                    "description": "CMMLU is a comprehensive Chinese evaluation benchmark specifically designed to assess the knowledge and reasoning abilities of language models in the context of the Chinese language. CMMLU covers 67 topics ranging from basic subjects to advanced professional levels. It includes tasks that require calculations and reasoning in natural sciences, as well as tasks involving knowledge from humanities, social sciences, and practical aspects like Chinese driving rules. Moreover, many tasks within CMMLU",
                    "url": "opencompass/opencompass_499.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_499",
                    "sample_count": 11649,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 100,
                        "estimated_output_tokens": 50
                    },
                    "original_data": {
                        "id": "499",
                        "name": "CMMLU",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å­¦ç§‘",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/haonan-li/CMMLU/tree/1b8ce5813be6a011b1042f2d8fc7f0806e2ac0bc",
                        "paperLink": "https://arxiv.org/abs/2306.09212",
                        "officialWebsiteLink": "https://github.com/haonan-li/CMMLU/tree/1b8ce5813be6a011b1042f2d8fc7f0806e2ac0bc",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "3028",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": true,
                        "updateDate": "2024-08-02 09:46:14",
                        "createDate": "2024-01-11 14:09:33",
                        "desc": {
                            "cn": "CMMLUæ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„ä¸­æ–‡è¯„ä¼°åŸºå‡†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨ä¸­æ–‡è¯­å¢ƒä¸‹çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚CMMLUæ¶µç›–äº†ä»åŸºç¡€å­¦ç§‘åˆ°é«˜çº§ä¸“ä¸šæ°´å¹³çš„67ä¸ªä¸»é¢˜ã€‚å®ƒåŒ…æ‹¬ï¼šéœ€è¦è®¡ç®—å’Œæ¨ç†çš„è‡ªç„¶ç§‘å­¦ï¼Œéœ€è¦çŸ¥è¯†çš„äººæ–‡ç§‘å­¦å’Œç¤¾ä¼šç§‘å­¦,ä»¥åŠéœ€è¦ç”Ÿæ´»å¸¸è¯†çš„ä¸­å›½é©¾é©¶è§„åˆ™ç­‰ã€‚æ­¤å¤–ï¼ŒCMMLUä¸­çš„è®¸å¤šä»»åŠ¡å…·æœ‰ä¸­å›½ç‰¹å®šçš„ç­”æ¡ˆï¼Œå¯èƒ½åœ¨å…¶ä»–åœ°åŒºæˆ–è¯­è¨€ä¸­å¹¶ä¸æ™®éé€‚ç”¨ã€‚å› æ­¤æ˜¯ä¸€ä¸ªå®Œå…¨ä¸­å›½åŒ–çš„ä¸­æ–‡æµ‹è¯•åŸºå‡†ã€‚",
                            "en": "CMMLU is a comprehensive Chinese evaluation benchmark specifically designed to assess the knowledge and reasoning abilities of language models in the context of the Chinese language. CMMLU covers 67 topics ranging from basic subjects to advanced professional levels. It includes tasks that require calculations and reasoning in natural sciences, as well as tasks involving knowledge from humanities, social sciences, and practical aspects like Chinese driving rules. Moreover, many tasks within CMMLU have answers specific to China, which might not be universally applicable in other regions or languages. As a result, CMMLU serves as a fully localized Chinese evaluation benchmark."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "{\"0\": \"62\", \"1\": \"åœ¨ç»Ÿè®¡è¯­è¨€æ¨¡å‹ä¸­ï¼Œé€šå¸¸ä»¥æ¦‚ç‡çš„å½¢å¼æè¿°ä»»æ„è¯­å¥çš„å¯èƒ½æ€§ï¼Œåˆ©ç”¨æœ€å¤§ç›¸ä¼¼åº¦ä¼°è®¡è¿›è¡Œåº¦é‡ï¼Œå¯¹äºä¸€äº›ä½é¢‘è¯ï¼Œæ— è®ºå¦‚ä½•æ‰©å¤§è®­ç»ƒæ•°æ®ï¼Œå‡ºç°çš„é¢‘åº¦ä»ç„¶å¾ˆä½ï¼Œä¸‹åˆ—å“ªç§æ–¹æ³•èƒ½è§£å†³è¿™ä¸€é—®é¢˜\", \"2\": \"æ•°æ®å¹³æ»‘\", \"3\": \"Nå…ƒæ–‡æ³•\", \"4\": \"ä¸€å…ƒæ–‡æ³•\", \"5\": \"ä¸€å…ƒåˆ‡åˆ†\", \"6\": \"A\"}",
                                    "difficulty": "moderate",
                                    "task_type": "qa"
                                },
                                {
                                    "question": "{\"0\": \"63\", \"1\": \"1994å¹´åˆ°l997å¹´4å¹´æ—¶é—´ï¼Œè´¢æ”¿æ”¶å…¥å GDPçš„æ¯”é‡ä»…l0%å¤šä¸€ç‚¹ï¼Œåˆ°2011å¹´æ¥è¿‘22%ï¼Œå¦‚æœè€ƒè™‘åˆ°åœŸåœ°å‡ºè®©å’Œå…¶ä»–æ”¿åºœåŸºé‡‘æ€§æ”¶å…¥ï¼Œæœ‰ä¸“å®¶ä¼°è®¡æ”¿åºœæ”¶å…¥å GDPçš„æ¯”lé‡è¶…è¿‡30%ã€‚è€ŒåŒæœŸï¼Œä¼ä¸šæˆ–è€…æ˜¯èµ„æœ¬æ”¶å…¥å æ¯”ä¹Ÿå¤§å¹…åº¦æé«˜ã€‚ä¸€ä¸ªä½è¯æ˜¯ï¼Œ2002å¹´åº•ï¼Œä¼ä¸šå­˜æ¬¾ä¸º64298ï¼47äº¿å…ƒï¼Œä½äºå½“æ—¶çš„ä¸ªäººå­˜æ¬¾ï¼Œè€Œåˆ°2011å¹´åº•ï¼Œä¼ä¸šå­˜æ¬¾(å¤®å“¥æ–°çš„ç»Ÿè®¡ç§‘æ—§ä¸ºâ€œå•ä½å­˜æ¬¾â€ï¼Œå£",
                                    "difficulty": "moderate",
                                    "task_type": "question_answering, critical_thinking"
                                },
                                {
                                    "question": "{\"0\": \"119\", \"1\": \"å¤©å…ƒæ€§æ„é€ çš„ä¸¤æ€§ç”Ÿç†å­¦å·®å¼‚æ€§æ„æ¶å†³å®šäº†ç”·å¥³èº«å¿ƒå‘å±•å­˜åœ¨ç€å®¢è§‚çš„\", \"2\": \"å·®å¼‚æ€§\", \"3\": \"åŒå¼‚æ€§\", \"4\": \"åŒä¸€æ€§\", \"5\": \"ç»Ÿä¸€æ€§\", \"6\": \"A\"}",
                                    "difficulty": "moderate",
                                    "task_type": "multiple_choice_question"
                                }
                            ],
                            "total_questions": 11649,
                            "question_format": "QA, critical_thinking, multiple_choice_question, qa, question_answering",
                            "difficulty_levels": [
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Tests domain-specific knowledge in Gender Studies, Statistics, Criminal Law",
                                "Evaluates Legal analysis, domain_knowledge, Political theory comprehension skills",
                                "Diverse task types: question_answering, QA, multiple_choice_question"
                            ],
                            "disadvantages": [
                                "Requires knowledge across multiple specialized domains"
                            ]
                        }
                    },
                    "analysis_file": "analysis/opencompass_499_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 5,
                        "successful": 5,
                        "failed": 0
                    }
                },
                {
                    "id": "opencompass_1003",
                    "name": "S-Eval",
                    "version": "1.0.0",
                    "description": "S-Eval is a new comprehensive, multi-dimensional and open-ended safety evaluation benchmark for LLMs consisting of 220,000 evaluation prompts (still in active expansion) across 102 risk subcategories and 10 advanced jailbreak attacks.",
                    "url": "opencompass/opencompass_1003.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1003",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1003",
                        "name": "S-Eval",
                        "emoji": "âš–ï¸",
                        "dimensions": [
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "LLM",
                                "en": "LLM"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 4,
                        "githubLink": "https://github.com/IS2Lab/S-Eval",
                        "paperLink": "https://dl.acm.org/doi/10.1145/3728971",
                        "officialWebsiteLink": "https://s-eval.github.io",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50151947",
                            "name": "IS2Lab",
                            "avatar": null,
                            "nickname": "IS2Lab"
                        },
                        "lookNum": "2725",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-10-15 16:28:34",
                        "supportOnlineEval": false,
                        "updateDate": "2025-10-15 16:28:34",
                        "createDate": "2025-10-14 20:59:45",
                        "desc": {
                            "cn": "S-Eval æ˜¯ä¸€ä¸ªé’ˆå¯¹ LLM çš„å…¨æ–°å…¨é¢ã€å¤šç»´ã€å¼€æ”¾å¼å®‰å…¨è¯„ä¼°åŸºå‡†ï¼ŒåŒ…å« 102 ä¸ªé£é™©å­ç±»åˆ«çš„ 220,000 ä¸ªè¯„ä¼°æç¤ºï¼ˆä»åœ¨ç§¯ææ‰©å±•ä¸­ï¼‰å’Œ 10 ä¸ªé«˜çº§è¶Šç‹±æ”»å‡»ã€‚",
                            "en": "S-Eval is a new comprehensive, multi-dimensional and open-ended safety evaluation benchmark for LLMs consisting of 220,000 evaluation prompts (still in active expansion) across 102 risk subcategories and 10 advanced jailbreak attacks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/s_eval'. Error: Path opencompass/s_eval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_531",
                    "name": "HellaSwag",
                    "version": "1.0.0",
                    "description": "HellaSwag is a challenge dataset for evaluating commonsense natural language inference, which is specially hard for state-of-the-art models, though its questions are trivial for humans (>95% accuracy). It consists of 70k multiple choice questions, each with a scenario and four possible endings, which requires to select the most reasonable ending. These questions come from two domains:activitynet and wikihow, involving video and text scenarios respectively. The correct answers of these questions ",
                    "url": "opencompass/opencompass_531.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_531",
                    "sample_count": 10042,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {
                        "gen": "hellaswag_gen",
                        "ppl": "hellaswag_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 43,
                        "estimated_output_tokens": 50
                    },
                    "original_data": {
                        "id": "531",
                        "name": "HellaSwag",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "reasoning",
                                "en": "reasoning"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/rowanz/hellaswag",
                        "paperLink": "https://arxiv.org/abs/1905.07830",
                        "officialWebsiteLink": "https://allenai.org/data/hellaswag",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "2352",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:31:26",
                        "supportOnlineEval": true,
                        "updateDate": "2024-09-12 19:31:26",
                        "createDate": "2024-09-12 19:28:41",
                        "desc": {
                            "cn": "HellaSwag æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¸¸è¯†æ€§è‡ªç„¶è¯­è¨€æ¨ç†çš„æ•°æ®é›†ï¼ŒHellaSwagçš„é—®é¢˜å¯¹äºæœ€å…ˆè¿›çš„æ¨¡å‹æ¥è¯´æ˜¯ç‰¹åˆ«å›°éš¾çš„ï¼Œå°½ç®¡å®ƒçš„é—®é¢˜å¯¹äºäººç±»æ¥è¯´éå¸¸è½»æ¾å°±èƒ½å›ç­”çš„ï¼ˆ> 95% çš„å‡†ç¡®ç‡ï¼‰ã€‚å®ƒç”±7ä¸‡å¤šé“å¤šé¡¹é€‰æ‹©é¢˜ç»„æˆï¼Œæ¯é“é¢˜éƒ½æœ‰ä¸€ä¸ªåœºæ™¯å’Œå››ç§å¯èƒ½çš„ç­”æ¡ˆï¼Œéœ€è¦é€‰æ‹©æœ€åˆç†çš„ç­”æ¡ˆã€‚è¿™äº›é—®é¢˜æ¥è‡ªä¸¤ä¸ªé¢†åŸŸï¼šactivitynetå’Œwikihowï¼Œåˆ†åˆ«æ¶‰åŠè§†é¢‘å’Œæ–‡æœ¬åœºæ™¯ã€‚è¿™äº›é—®é¢˜çš„æ­£ç¡®ç­”æ¡ˆæ˜¯ä¸‹ä¸€ä¸ªäº‹ä»¶çš„çœŸå®å¥å­ï¼Œè€Œé”™è¯¯ç­”æ¡ˆæ˜¯é€šè¿‡å¯¹æŠ—æŠ€æœ¯ç”Ÿæˆçš„å¹¶ç»è¿‡äººç±»éªŒè¯ï¼Œè¿™äº›ç­”æ¡ˆå¯ä»¥æ¬ºéª—æœºå™¨ä½†ä¸èƒ½æ¬ºéª—äººç±»ã€‚",
                            "en": "HellaSwag is a challenge dataset for evaluating commonsense natural language inference, which is specially hard for state-of-the-art models, though its questions are trivial for humans (>95% accuracy). It consists of 70k multiple choice questions, each with a scenario and four possible endings, which requires to select the most reasonable ending. These questions come from two domains:activitynet and wikihow, involving video and text scenarios respectively. The correct answers of these questions are the real sentences for the next event, while the incorrect answers are adversarially generated and human verified, so as to fool machines but not humans."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the righ",
                                    "difficulty": "easy",
                                    "task_type": "QA, Instructional Guidance"
                                },
                                {
                                    "question": "Washing face: A girl stands in front of a bathroom mirror and vigorously rubs her face. The girl turns on the faucet. The girl",
                                    "difficulty": "easy",
                                    "task_type": "procedural_instruction, comprehension"
                                },
                                {
                                    "question": "Home and Garden: How to paint basement stairs. Remove any carpet or overlaid material from your basement stairs. Remove staples left from the carpet installation with pliers. Look over all areas of th",
                                    "difficulty": "moderate",
                                    "task_type": "Procedural Guidance, DIY Tutorial"
                                }
                            ],
                            "total_questions": 10042,
                            "question_format": "Advisory, Behavioral analysis, DIY Tutorial, Instructional Guidance, Problem-solving, Procedural Guidance, QA, comprehension, procedural_instruction, qa, summarisation",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Tests domain-specific knowledge in Home Improvement, Developmental Psychology, Education",
                                "Evaluates fine_motor_skills, Behavioral intervention strategies, Observational skills skills",
                                "Covers multiple difficulty levels: Beginner, Intermediate",
                                "Diverse task types: Instructional Guidance, comprehension, QA"
                            ],
                            "disadvantages": [
                                "Requires knowledge across multiple specialized domains"
                            ]
                        }
                    },
                    "analysis_file": "analysis/opencompass_531_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 5,
                        "successful": 5,
                        "failed": 0
                    }
                },
                {
                    "id": "opencompass_692",
                    "name": "ChemBench",
                    "version": "1.0.0",
                    "description": "ChemBench is a large-scale chemistry competency evaluation benchmark for language models, which includes nine chemistry core tasks and 4100 high-quality single-choice questions and answers.",
                    "url": "opencompass/opencompass_692.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_692",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "692",
                        "name": "ChemBench",
                        "emoji": "ğŸ§ª",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "Name_Conversion",
                                "en": "Name_Conversion"
                            },
                            {
                                "cn": "Property_Prediction",
                                "en": "Property_Prediction"
                            },
                            {
                                "cn": "Mol2caption",
                                "en": "Mol2caption"
                            },
                            {
                                "cn": "Caption2mol",
                                "en": "Caption2mol"
                            },
                            {
                                "cn": "Product_Prediction",
                                "en": "Product_Prediction"
                            },
                            {
                                "cn": "Retrosynthesis",
                                "en": "Retrosynthesis"
                            },
                            {
                                "cn": "Yield_Prediction",
                                "en": "Yield_Prediction"
                            },
                            {
                                "cn": "Temperature_Prediction",
                                "en": "Temperature_Prediction"
                            },
                            {
                                "cn": "Solvent_Prediction",
                                "en": "Solvent_Prediction"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 4,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2402.06852",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50029510",
                            "name": "OpenScienceLab",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50029510-baa30340-1e57-432f-9b2b-6f656752c65e.png",
                            "nickname": "OpenScienceLab"
                        },
                        "lookNum": "2296",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:51:38",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-12 19:51:38",
                        "createDate": "2024-09-12 19:49:11",
                        "desc": {
                            "cn": "ChemBenchæ˜¯ä¸€ä¸ªåŒ…å«äº†ä¹é¡¹åŒ–å­¦æ ¸å¿ƒä»»åŠ¡ï¼Œ4100ä¸ªé«˜è´¨é‡å•é€‰é—®ç­”çš„å¤§è¯­è¨€æ¨¡å‹åŒ–å­¦èƒ½åŠ›è¯„æµ‹åŸºå‡†.",
                            "en": "ChemBench is a large-scale chemistry competency evaluation benchmark for language models, which includes nine chemistry core tasks and 4100 high-quality single-choice questions and answers."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/chembench'. Error: Path opencompass/chembench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_538",
                    "name": "MBPP",
                    "version": "1.0.0",
                    "description": "The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases.",
                    "url": "opencompass/opencompass_538.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_538",
                    "sample_count": 974,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {
                        "gen": "MBPP_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 13,
                        "estimated_output_tokens": 50
                    },
                    "original_data": {
                        "id": "538",
                        "name": "MBPP",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "code",
                                "en": "code"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/google-research/google-research/tree/master/mbpp",
                        "paperLink": "https://arxiv.org/pdf/2108.07732v1",
                        "officialWebsiteLink": "https://huggingface.co/datasets/mbpp",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "2291",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:31:38",
                        "supportOnlineEval": true,
                        "updateDate": "2024-09-12 19:31:38",
                        "createDate": "2024-09-12 19:27:17",
                        "desc": {
                            "cn": "è¯¥åŸºå‡†æµ‹è¯•ç”±å¤§çº¦1000ä¸ªå…¥é—¨çº§ç¨‹åºå‘˜å¯ä»¥è§£å†³çš„ä¼—åŒ…Pythonç¼–ç¨‹é—®é¢˜ç»„æˆï¼Œæ¶µç›–ç¼–ç¨‹åŸºç¡€çŸ¥è¯†ã€æ ‡å‡†åº“åŠŸèƒ½ç­‰ã€‚æ¯ä¸ªé—®é¢˜éƒ½ç”±ä»»åŠ¡æè¿°ã€ä»£ç è§£å†³æ–¹æ¡ˆå’Œ3ä¸ªè‡ªåŠ¨åŒ–æµ‹è¯•ç”¨ä¾‹ç»„æˆã€‚",
                            "en": "The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "Write a function to check if the triangle is valid or not.",
                                    "difficulty": "moderate",
                                    "task_type": "coding"
                                },
                                {
                                    "question": "Write a function to sort the given array by using counting sort.",
                                    "difficulty": "moderate",
                                    "task_type": "coding"
                                },
                                {
                                    "question": "Write a function to find the list of lists with maximum length.",
                                    "difficulty": "moderate",
                                    "task_type": "coding"
                                }
                            ],
                            "total_questions": 974,
                            "question_format": "coding",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Tests domain-specific knowledge in Computer Science, Data Structures, Programming",
                                "Evaluates abstract_reasoning, problem_solving, programming skills",
                                "Covers multiple difficulty levels: Beginner, Intermediate",
                                "Diverse task types: coding"
                            ],
                            "disadvantages": [
                                "Requires knowledge across multiple specialized domains"
                            ]
                        }
                    },
                    "analysis_file": "analysis/opencompass_538_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 5,
                        "successful": 5,
                        "failed": 0
                    }
                },
                {
                    "id": "opencompass_497",
                    "name": "AGIEval",
                    "version": "1.0.0",
                    "description": "AGIEval is a human-centric benchmark specifically designed to evaluate the general abilities of foundation models in tasks pertinent to human cognition and problem-solving. This benchmark is derived from 20 official, public, and high-standard admission and qualification exams intended for general human test-takers, such as general college admission tests (e.g., Chinese College Entrance Exam (Gaokao) and American SAT), law school admission tests, math competitions, lawyer qualification tests, and",
                    "url": "opencompass/opencompass_497.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_497",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "497",
                        "name": "AGIEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å­¦ç§‘",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "examination",
                                "en": "examination"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/ruixiangcui/AGIEval",
                        "paperLink": "https://arxiv.org/pdf/2304.06364",
                        "officialWebsiteLink": "https://github.com/ruixiangcui/AGIEval",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1885",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:31:13",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-12 19:31:13",
                        "createDate": "2024-09-12 19:30:02",
                        "desc": {
                            "cn": "AGIEvalæ˜¯ä¸€ä¸ªä»¥äººä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°åŸºç¡€æ¨¡å‹åœ¨æ¶‰åŠäººç±»è®¤çŸ¥å’Œé—®é¢˜è§£å†³çš„ä»»åŠ¡ä¸­çš„ä¸€èˆ¬èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•æºè‡ª20ä¸ªå®˜æ–¹ã€å…¬å¼€å’Œé«˜æ ‡å‡†çš„å…¥å­¦å’Œèµ„æ ¼è€ƒè¯•ï¼Œä¾‹å¦‚æ™®é€šå¤§å­¦å…¥å­¦è€ƒè¯•ï¼ˆä¾‹å¦‚ä¸­å›½é«˜è€ƒå’Œç¾å›½SATï¼‰ã€æ³•å­¦é™¢å…¥å­¦è€ƒè¯•ã€æ•°å­¦ç«èµ›ã€å¾‹å¸ˆèµ„æ ¼è€ƒè¯•ä»¥åŠå›½å®¶å…¬åŠ¡å‘˜è€ƒè¯•",
                            "en": "AGIEval is a human-centric benchmark specifically designed to evaluate the general abilities of foundation models in tasks pertinent to human cognition and problem-solving. This benchmark is derived from 20 official, public, and high-standard admission and qualification exams intended for general human test-takers, such as general college admission tests (e.g., Chinese College Entrance Exam (Gaokao) and American SAT), law school admission tests, math competitions, lawyer qualification tests, and national civil service exams."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "No valid url for ./data/AGIEval/data/v1/!\nPlease make sure  `./data/AGIEval/data/v1/` is correct"
                    }
                },
                {
                    "id": "opencompass_505",
                    "name": "CHID",
                    "version": "1.0.0",
                    "description": " CHID is a chinese idiom reading comprehension task, which requires to select the correct idiom to fill in the blank according to the context, with 10 candidate idioms.",
                    "url": "opencompass/opencompass_505.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_505",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "505",
                        "name": "CHID",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/chujiezheng/ChID-Dataset",
                        "paperLink": "https://arxiv.org/abs/1906.01265",
                        "officialWebsiteLink": "https://github.com/chujiezheng/ChID-Dataset",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1844",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:48:03",
                        "createDate": "2024-01-11 14:10:02",
                        "desc": {
                            "cn": "CHIDæ˜¯ä¸€ä¸ªä¸­æ–‡æˆè¯­é˜…è¯»ç†è§£ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ä¸Šä¸‹æ–‡é€‰æ‹©æ­£ç¡®çš„æˆè¯­å¡«ç©ºï¼Œå…±æœ‰10ä¸ªå€™é€‰æˆè¯­ã€‚",
                            "en": " CHID is a chinese idiom reading comprehension task, which requires to select the correct idiom to fill in the blank according to the context, with 10 candidate idioms."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/chid'. Error: Path opencompass/chid is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_502",
                    "name": "ARC-c",
                    "version": "1.0.0",
                    "description": "The AI2â€™s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. ",
                    "url": "opencompass/opencompass_502.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_502",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {
                        "gen": "ARC-c_gen",
                        "ppl": "ARC-c_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "502",
                        "name": "ARC-c",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å­¦ç§‘",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://allenai.org/data/arc",
                        "paperLink": "https://arxiv.org/pdf/1803.05457.pdf",
                        "officialWebsiteLink": "https://allenai.org/data/arc",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1815",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:48:39",
                        "createDate": "2024-01-11 14:09:48",
                        "desc": {
                            "cn": "AI2çš„æ¨ç†æŒ‘æˆ˜ï¼ˆARCï¼‰æ•°æ®é›†æ˜¯ä¸€ä¸ªå¤šé¡¹é€‰æ‹©é—®é¢˜å›ç­”æ•°æ®é›†ï¼ŒåŒ…å«äº†ä»ä¸‰å¹´çº§åˆ°ä¹å¹´çº§çš„ç§‘å­¦è€ƒè¯•ä¸­æå–çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šç®€å•å’ŒæŒ‘æˆ˜ï¼Œå…¶ä¸­åè€…åŒ…å«äº†éœ€è¦æ¨ç†èƒ½åŠ›çš„æ›´éš¾çš„é—®é¢˜ã€‚å¤§å¤šæ•°é—®é¢˜æœ‰4ä¸ªç­”æ¡ˆé€‰é¡¹ï¼Œä»…æœ‰ä¸åˆ°1ï¼…çš„é—®é¢˜æœ‰3ä¸ªæˆ–5ä¸ªç­”æ¡ˆé€‰é¡¹ã€‚",
                            "en": "The AI2â€™s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/arc_c'. Error: Path opencompass/arc_c is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_564",
                    "name": "LV-Eval",
                    "version": "1.0.0",
                    "description": "LV-Eval is a challenging long-context benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up to 256k words. The average number of words is 102,380, and the Min/Max number of words is 11,896/387,406. It features two main tasks, single-hop QA and multi-hop QA, comprising 11 bilingual datasets.",
                    "url": "opencompass/opencompass_564.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_564",
                    "sample_count": 1000,
                    "traits": [
                        "long-context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "564",
                        "name": "LV-Eval",
                        "emoji": "ğŸ—½",
                        "dimensions": [
                            {
                                "cn": "é•¿æ–‡æœ¬",
                                "en": "long-context"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "Question Answering",
                                "en": "Question Answering"
                            },
                            {
                                "cn": "Synthetic",
                                "en": "Synthetic"
                            },
                            {
                                "cn": "Confusing Evidence",
                                "en": "Confusing Evidence"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 4,
                        "githubLink": "https://github.com/infinigence/LVEval",
                        "paperLink": "https://arxiv.org/abs/2402.05136",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "066910",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-KGJYN5OmL"
                        },
                        "lookNum": "1782",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 10:32:21",
                        "createDate": "2024-02-18 11:35:12",
                        "desc": {
                            "cn": "LV-Evalæ˜¯ä¸€ä¸ªå…·å¤‡5ä¸ªé•¿åº¦ç­‰çº§ï¼ˆ16kã€32kã€64kã€128kå’Œ256kï¼‰ã€æœ€å¤§æ–‡æœ¬æµ‹è¯•é•¿åº¦è¾¾åˆ°256kçš„é•¿æ–‡æœ¬è¯„æµ‹åŸºå‡†ã€‚LV-Evalçš„å¹³å‡æ–‡æœ¬é•¿åº¦è¾¾åˆ°102,380å­—ï¼Œæœ€å°/æœ€å¤§æ–‡æœ¬é•¿åº¦ä¸º11,896/387,406å­—ã€‚LV-Evalä¸»è¦æœ‰ä¸¤ç±»è¯„æµ‹ä»»åŠ¡â€”â€”å•è·³QAå’Œå¤šè·³QAï¼Œå…±åŒ…å«11ä¸ªæ¶µç›–ä¸­è‹±æ–‡çš„è¯„æµ‹æ•°æ®å­é›†ã€‚",
                            "en": "LV-Eval is a challenging long-context benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up to 256k words. The average number of words is 102,380, and the Min/Max number of words is 11,896/387,406. It features two main tasks, single-hop QA and multi-hop QA, comprising 11 bilingual datasets."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/lv_eval'. Error: Path opencompass/lv_eval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_529",
                    "name": "COPA",
                    "version": "1.0.0",
                    "description": "COPA is a causal inference task, which requires to select the correct causal relation based on the given premise.",
                    "url": "opencompass/opencompass_529.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_529",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {
                        "gen": "COPA_gen",
                        "ppl": "COPA_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "529",
                        "name": "COPA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://people.ict.usc.edu/~gordon/copa.html",
                        "paperLink": "https://arxiv.org/abs/1905.00537",
                        "officialWebsiteLink": "https://people.ict.usc.edu/~gordon/copa.html",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1746",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:51:45",
                        "createDate": "2024-01-11 14:11:52",
                        "desc": {
                            "cn": "COPAæ˜¯ä¸€ä¸ªå› æœæ¨æ–­ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ç»™å®šçš„å‰æï¼Œé€‰æ‹©æ­£ç¡®çš„å› æœå…³ç³»ã€‚\n",
                            "en": "COPA is a causal inference task, which requires to select the correct causal relation based on the given premise."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/copa'. Error: Path opencompass/copa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_631",
                    "name": "OpenFinData",
                    "version": "1.0.0",
                    "description": "OpenFinData is an open-source financial evaluation dataset jointly released by EastMoney and Shanghai Artificial Intelligence Laboratory. This dataset represents the most realistic industrial scenario requirements and is currently the most comprehensive and professional financial evaluation dataset.",
                    "url": "opencompass/opencompass_631.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_631",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "631",
                        "name": "OpenFinData",
                        "emoji": "ğŸ¦¾",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "Finance",
                                "en": "Finance"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 4,
                        "githubLink": "https://github.com/open-compass/OpenFinData",
                        "paperLink": "",
                        "officialWebsiteLink": "https://openfindata.org/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50029256",
                            "name": "eastmoney",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50029256-0729a0fd-9e97-4c38-b9f9-a580ad6f6a54.png",
                            "nickname": "eastmoney"
                        },
                        "lookNum": "1700",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:40:43",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-12 19:40:43",
                        "createDate": "2024-09-12 19:39:10",
                        "desc": {
                            "cn": "OpenFinDataæ˜¯ç”±ä¸œæ–¹è´¢å¯Œä¸ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤è”åˆå‘å¸ƒçš„å¼€æºé‡‘èè¯„æµ‹æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†ä»£è¡¨äº†æœ€çœŸå®çš„äº§ä¸šåœºæ™¯éœ€æ±‚ï¼Œæ˜¯ç›®å‰åœºæ™¯æœ€å…¨ã€ä¸“ä¸šæ€§æœ€æ·±çš„é‡‘èè¯„æµ‹æ•°æ®é›†ã€‚å®ƒåŸºäºä¸œæ–¹è´¢å¯Œå®é™…é‡‘èä¸šåŠ¡çš„å¤šæ ·åŒ–ä¸°å¯Œåœºæ™¯ï¼Œæ—¨åœ¨ä¸ºé‡‘èç§‘æŠ€é¢†åŸŸçš„ç ”ç©¶è€…å’Œå¼€å‘è€…æä¾›ä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®èµ„æºã€‚",
                            "en": "OpenFinData is an open-source financial evaluation dataset jointly released by EastMoney and Shanghai Artificial Intelligence Laboratory. This dataset represents the most realistic industrial scenario requirements and is currently the most comprehensive and professional financial evaluation dataset."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/openfindata'. Error: Path opencompass/openfindata is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_518",
                    "name": "OpenbookQA",
                    "version": "1.0.0",
                    "description": "OpenBookQA contains questions that require multi-step reasoning, application of common-sense knowledge, and in-depth comprehension of text. It is a new type of question-answering dataset, modeled after open-book exams, designed to assess human understanding of a specific topic.",
                    "url": "opencompass/opencompass_518.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_518",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {
                        "gen": "openbookqa_gen",
                        "ppl": "openbookqa_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "518",
                        "name": "OpenbookQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/allenai/OpenBookQA",
                        "paperLink": "https://arxiv.org/abs/1809.02789",
                        "officialWebsiteLink": "https://allenai.org/data/open-book-qa",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1661",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:48:08",
                        "createDate": "2024-01-11 14:11:07",
                        "desc": {
                            "cn": "OpenBookQAåŒ…å«éœ€è¦å¤šæ­¥æ¨ç†ã€è¿ç”¨å¸¸è¯†çŸ¥è¯†ã€æ·±å…¥ç†è§£æ–‡æœ¬ç­‰èƒ½åŠ›çš„é—®é¢˜ï¼Œæ˜¯ä¸€ç§æ–°å‹çš„é—®ç­”æ•°æ®é›†ï¼Œå…¶æ¨¡å¼å€Ÿé‰´äº†å¼€æ”¾å¼ä¹¦æœ¬è€ƒè¯•ï¼Œç”¨äºè¯„ä¼°äººç±»å¯¹æŸä¸€ä¸»é¢˜ç†è§£çš„ç¨‹åº¦ã€‚",
                            "en": "OpenBookQA contains questions that require multi-step reasoning, application of common-sense knowledge, and in-depth comprehension of text. It is a new type of question-answering dataset, modeled after open-book exams, designed to assess human understanding of a specific topic."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/openbookqa'. Error: Path opencompass/openbookqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_511",
                    "name": "CommonSenseQA",
                    "version": "1.0.0",
                    "description": "\nCommonsenseQA is a multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers. \n",
                    "url": "opencompass/opencompass_511.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_511",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "511",
                        "name": "CommonSenseQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/jonathanherzig/commonsenseqa",
                        "paperLink": "https://arxiv.org/abs/1811.00937",
                        "officialWebsiteLink": "https://www.tau-nlp.sites.tau.ac.il/commonsenseqa",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1582",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:48:32",
                        "createDate": "2024-01-11 14:10:39",
                        "desc": {
                            "cn": "CommonsenseQAæ˜¯ä¸€ä¸ªé€‰æ‹©é¢˜æ•°æ®é›†ï¼Œå®ƒéœ€è¦ä¸åŒç±»å‹çš„å¸¸è¯†çŸ¥è¯†æ¥é¢„æµ‹æ­£ç¡®ç­”æ¡ˆã€‚å®ƒåŒ…å«12,102ä¸ªé—®é¢˜ï¼Œæœ‰ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆå’Œå››ä¸ªå¹²æ‰°ç­”æ¡ˆã€‚\n",
                            "en": "\nCommonsenseQA is a multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers. \n"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/commonsenseqa'. Error: Path opencompass/commonsenseqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_513",
                    "name": "NQ",
                    "version": "1.0.0",
                    "description": "NQ (NaturalQuestion) corpus contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question. The inclusion of real user questions, and the requirement that solutions should read an entire page to find the answer, cause NQ to be a more realistic and challenging task than prior QA datasets.",
                    "url": "opencompass/opencompass_513.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_513",
                    "sample_count": 1000,
                    "traits": [
                        "knowledge"
                    ],
                    "eval_type": {
                        "gen": "NQ_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "513",
                        "name": "NQ",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/google-research-datasets/natural-questions",
                        "paperLink": "https://storage.googleapis.com/pub-tools-public-publication-data/pdf/b8c26e4347adc3453c15d96a09e6f7f102293f71.pdf",
                        "officialWebsiteLink": "https://ai.google.com/research/NaturalQuestions/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1543",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": true,
                        "updateDate": "2024-08-02 09:48:17",
                        "createDate": "2024-01-11 14:10:47",
                        "desc": {
                            "cn": "NQ æ•°æ®é›†æ¥è‡ªäºçœŸå®ç”¨æˆ·çš„é—®é¢˜ï¼Œå®ƒè¦æ±‚ QA ç³»ç»Ÿé˜…è¯»å’Œç†è§£æ•´ä¸ªç»´åŸºç™¾ç§‘æ–‡ç« ï¼Œè¿™äº›æ–‡ç« å¯èƒ½åŒ…å«ä¹Ÿå¯èƒ½ä¸åŒ…å«é—®é¢˜çš„ç­”æ¡ˆã€‚ç”±çœŸå®ç”¨æˆ·é—®é¢˜æ„æˆï¼Œä»¥åŠéœ€è¦é˜…è¯»æ•´ä¸ªé¡µé¢æ‰èƒ½æ‰¾åˆ°ç­”æ¡ˆçš„è¦æ±‚ï¼Œæ¯”ä»¥å¾€çš„ QA æ•°æ®é›†æ›´ç°å®å’Œæ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚",
                            "en": "NQ (NaturalQuestion) corpus contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question. The inclusion of real user questions, and the requirement that solutions should read an entire page to find the answer, cause NQ to be a more realistic and challenging task than prior QA datasets."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/nq'. Error: Path opencompass/nq is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_514",
                    "name": "C3",
                    "version": "1.0.0",
                    "description": "A free-form multiple-Choice Chinese machine reading Comprehension dataset (C3), containing 13,369 documents (dialogues or more formally written mixed-genre texts) and their associated 19,577 multiple-choice free-form questions collected from Chinese-as-a-second-language examinations",
                    "url": "opencompass/opencompass_514.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_514",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {
                        "gen": "C3_gen",
                        "ppl": "C3_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "514",
                        "name": "C3",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "understanding",
                                "en": "understanding"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/nlpdata/c3",
                        "paperLink": "https://arxiv.org/abs/1904.09679",
                        "officialWebsiteLink": "https://github.com/nlpdata/c3",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1473",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:31:23",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-12 19:31:23",
                        "createDate": "2024-09-12 19:29:14",
                        "desc": {
                            "cn": "ä¸€ä¸ªè‡ªç”±å½¢å¼çš„å¤šé¡¹é€‰æ‹©ä¸­æ–‡æœºå™¨é˜…è¯»ç†è§£æ•°æ®é›†ï¼ˆC3ï¼‰ï¼ŒåŒ…å«13369ç¯‡æ–‡çŒ®ï¼ˆå¯¹è¯æˆ–æ›´æ­£å¼çš„æ··åˆä½“è£æ–‡æœ¬ï¼‰åŠå…¶ç›¸å…³çš„19577é“è‡ªç”±é€‰æ‹©é¢˜ï¼Œè¿™äº›é—®é¢˜éƒ½æ˜¯ä»æ±‰è¯­ä½œä¸ºç¬¬äºŒè¯­è¨€çš„è€ƒè¯•ä¸­æ”¶é›†åˆ°çš„",
                            "en": "A free-form multiple-Choice Chinese machine reading Comprehension dataset (C3), containing 13,369 documents (dialogues or more formally written mixed-genre texts) and their associated 19,577 multiple-choice free-form questions collected from Chinese-as-a-second-language examinations"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/c3'. Error: Path opencompass/c3 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_510",
                    "name": "BoolQ",
                    "version": "1.0.0",
                    "description": "BoolQ is a question answering dataset for yes/no questions containing 15942 examples. These questions are naturally occurring ---they are generated in unprompted and unconstrained settings. Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context.",
                    "url": "opencompass/opencompass_510.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_510",
                    "sample_count": 3270,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {
                        "gen": "BoolQ_gen",
                        "ppl": "BoolQ_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 9,
                        "estimated_output_tokens": 1
                    },
                    "original_data": {
                        "id": "510",
                        "name": "BoolQ",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "knowledge",
                                "en": "knowledge"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/google-research-datasets/boolean-questions",
                        "paperLink": "https://arxiv.org/abs/1905.10044",
                        "officialWebsiteLink": "https://github.com/google-research-datasets/boolean-questions",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1410",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:31:18",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-12 19:31:18",
                        "createDate": "2024-09-12 19:29:42",
                        "desc": {
                            "cn": "BoolQæ˜¯ä¸€ä¸ªåŒ…å«15942ä¸ªç¤ºä¾‹çš„æ˜¯/å¦é—®é¢˜çš„é—®ç­”æ•°æ®é›†ã€‚è¿™äº›é—®é¢˜æ˜¯è‡ªç„¶ç”Ÿæˆçš„â€”â€”å³åœ¨æ— promptå’Œæ— çº¦æŸçš„ç¯å¢ƒä¸­äº§ç”Ÿçš„ã€‚æ¯ä¸ªä¾‹å­éƒ½æ˜¯ä¸€ä¸ªä¸‰å…ƒç»„(é—®é¢˜ã€æ®µè½ã€ç­”æ¡ˆ)ï¼Œé¡µé¢æ ‡é¢˜æ˜¯å¯é€‰çš„é™„åŠ ä¸Šä¸‹æ–‡ã€‚",
                            "en": "BoolQ is a question answering dataset for yes/no questions containing 15942 examples. These questions are naturally occurring ---they are generated in unprompted and unconstrained settings. Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "is ncis new orleans over for the season",
                                    "difficulty": "easy",
                                    "task_type": "qa"
                                },
                                {
                                    "question": "can i have a beard in the military",
                                    "difficulty": "moderate",
                                    "task_type": "QA, RAG"
                                },
                                {
                                    "question": "can you contest a scrum in rugby league",
                                    "difficulty": "moderate",
                                    "task_type": "QA, Procedural knowledge"
                                }
                            ],
                            "total_questions": 3270,
                            "question_format": "Procedural knowledge, QA, RAG, qa, question_answering",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Tests domain-specific knowledge in Grooming Standards, Military Policy, Geometry",
                                "Evaluates abstract_reasoning, Strategic thinking, Teamwork skills",
                                "Covers multiple difficulty levels: Beginner, Intermediate",
                                "Diverse task types: question_answering, QA, qa"
                            ],
                            "disadvantages": [
                                "Requires knowledge across multiple specialized domains"
                            ]
                        }
                    },
                    "analysis_file": "analysis/opencompass_510_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 5,
                        "successful": 5,
                        "failed": 0
                    }
                },
                {
                    "id": "opencompass_500",
                    "name": "GAOKAO-Bench",
                    "version": "1.0.0",
                    "description": "GAOKAO-bench is an evaluation framework that utilizes Chinese high school entrance examination (GAOKAO) questions as a dataset to evaluate the language understanding and logical reasoning abilities of large language models.",
                    "url": "opencompass/opencompass_500.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_500",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "500",
                        "name": "GAOKAO-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å­¦ç§‘",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/OpenLMLab/GAOKAO-Bench",
                        "paperLink": "https://arxiv.org/abs/2305.12474",
                        "officialWebsiteLink": "https://github.com/OpenLMLab/GAOKAO-Bench",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1375",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": true,
                        "updateDate": "2024-08-02 09:49:06",
                        "createDate": "2024-01-11 14:09:41",
                        "desc": {
                            "cn": "GAOKAO-benchæ˜¯ä¸€ä¸ªä»¥ä¸­å›½é«˜è€ƒé¢˜ç›®ä¸ºæ•°æ®é›†ï¼Œæ—¨åœ¨æä¾›å’Œäººç±»å¯¹é½çš„ï¼Œç›´è§‚ï¼Œé«˜æ•ˆåœ°æµ‹è¯„å¤§æ¨¡å‹è¯­è¨€ç†è§£èƒ½åŠ›ã€é€»è¾‘æ¨ç†èƒ½åŠ›çš„æµ‹è¯„æ¡†æ¶",
                            "en": "GAOKAO-bench is an evaluation framework that utilizes Chinese high school entrance examination (GAOKAO) questions as a dataset to evaluate the language understanding and logical reasoning abilities of large language models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gaokao_bench'. Error: Path opencompass/gaokao_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_504",
                    "name": "WiC",
                    "version": "1.0.0",
                    "description": "WiC is a benchmark for the evaluation of context-sensitive word embeddings. WiC is framed as a binary classification task. Each instance in WiC has a target word w, either a verb or a noun, for which two contexts are provided. Each of these contexts triggers a specific meaning of w. The task is to identify if the occurrences of w in the two contexts correspond to the same meaning or not. In fact, the dataset can also be viewed as an application of Word Sense Disambiguation in practise.",
                    "url": "opencompass/opencompass_504.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_504",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {
                        "gen": "WiC_gen",
                        "ppl": "WiC_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "504",
                        "name": "WiC",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://pilehvar.github.io/wic/",
                        "paperLink": "https://arxiv.org/abs/1808.09121",
                        "officialWebsiteLink": "https://pilehvar.github.io/wic/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1257",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:48:51",
                        "createDate": "2024-01-11 14:09:58",
                        "desc": {
                            "cn": "Word-in-Contextæ˜¯ä¸€ä¸ªè¯ä¹‰æ¶ˆæ­§ä»»åŠ¡ï¼Œè¢«è§†ä¸ºå¥å­å¯¹çš„äºŒå…ƒåˆ†ç±»ã€‚ç»™å®šä¸¤ä¸ªæ–‡æœ¬ç‰‡æ®µå’Œä¸€ä¸ªåœ¨ä¸¤ä¸ªå¥å­ä¸­éƒ½å‡ºç°çš„å¤šä¹‰è¯ï¼Œä»»åŠ¡æ˜¯ç¡®å®šè¯¥è¯åœ¨ä¸¤ä¸ªå¥å­ä¸­æ˜¯å¦å…·æœ‰ç›¸åŒçš„å«ä¹‰ã€‚",
                            "en": "WiC is a benchmark for the evaluation of context-sensitive word embeddings. WiC is framed as a binary classification task. Each instance in WiC has a target word w, either a verb or a noun, for which two contexts are provided. Each of these contexts triggers a specific meaning of w. The task is to identify if the occurrences of w in the two contexts correspond to the same meaning or not. In fact, the dataset can also be viewed as an application of Word Sense Disambiguation in practise."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/wic'. Error: Path opencompass/wic is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_509",
                    "name": "Flores",
                    "version": "1.0.0",
                    "description": "Flores is a benchmark dataset for machine translation between English and low-resource languages, which consists of sentences translated from Wikipedia, involving English and four low-resource languages, namely Nepali, Sinhala, Khmer and Pashto. Flores has two versions, we use the Flores-101 version here, which is the second version including 101 languages besides english. ",
                    "url": "opencompass/opencompass_509.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_509",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "509",
                        "name": "Flores",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/facebookresearch/flores/",
                        "paperLink": "https://arxiv.org/abs/2106.03193",
                        "officialWebsiteLink": "https://github.com/facebookresearch/flores/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1156",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:50:59",
                        "createDate": "2024-01-11 14:10:32",
                        "desc": {
                            "cn": "Floresæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°ä½èµ„æºè¯­è¨€æœºå™¨ç¿»è¯‘çš„åŸºå‡†æ•°æ®é›†ï¼Œå®ƒåŒ…å«äº†ä»ç»´åŸºç™¾ç§‘ç¿»è¯‘çš„å¥å­ï¼Œæ¶‰åŠè‹±è¯­å’Œå››ç§ä½èµ„æºè¯­è¨€ï¼Œåˆ†åˆ«æ˜¯å°¼æ³Šå°”è¯­ã€åƒ§ä¼½ç½—è¯­ã€é«˜æ£‰è¯­å’Œæ™®ä»€å›¾è¯­ã€‚Floresæœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼Œæˆ‘ä»¬è¿™é‡Œä½¿ç”¨çš„æ˜¯ç¬¬ä¸€ä¸ªç‰ˆæœ¬Flores-101ï¼Œå®ƒåŒ…å«æœ‰é™¤è‹±è¯­å¤–çš„101ç§è¯­è¨€ã€‚",
                            "en": "Flores is a benchmark dataset for machine translation between English and low-resource languages, which consists of sentences translated from Wikipedia, involving English and four low-resource languages, namely Nepali, Sinhala, Khmer and Pashto. Flores has two versions, we use the Flores-101 version here, which is the second version including 101 languages besides english. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "No valid url for ./data/flores_first100!\nPlease make sure  `./data/flores_first100` is correct"
                    }
                },
                {
                    "id": "opencompass_524",
                    "name": "CMNLI",
                    "version": "1.0.0",
                    "description": "CMNLI  is a Chinese natural language inference task, which requires to determine the logical relation between two sentences, with three relations: entailment, contradiction and neutral.",
                    "url": "opencompass/opencompass_524.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_524",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {
                        "gen": "cmnli_gen",
                        "ppl": "cmnli_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "524",
                        "name": "CMNLI",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/CLUEbenchmark/CLUE",
                        "paperLink": "https://arxiv.org/abs/2004.05986",
                        "officialWebsiteLink": "https://www.cluebenchmarks.com/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1139",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:49:37",
                        "createDate": "2024-01-11 14:11:34",
                        "desc": {
                            "cn": "CMNLIæ˜¯ä¸€ä¸ªä¸­æ–‡è‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ä¸¤ä¸ªå¥å­åˆ¤æ–­å®ƒä»¬ä¹‹é—´çš„é€»è¾‘å…³ç³»ï¼Œæœ‰ä¸‰ç§å…³ç³»ï¼šè•´å«ã€çŸ›ç›¾å’Œä¸­ç«‹ã€‚",
                            "en": "CMNLI  is a Chinese natural language inference task, which requires to determine the logical relation between two sentences, with three relations: entailment, contradiction and neutral."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cmnli'. Error: Path opencompass/cmnli is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_520",
                    "name": "LCSTS",
                    "version": "1.0.0",
                    "description": "LCSTS is a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text.",
                    "url": "opencompass/opencompass_520.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_520",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {
                        "gen": "LCSTS_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "520",
                        "name": "LCSTS",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "http://icrc.hitsz.edu.cn/Article/show/139.html",
                        "paperLink": "https://arxiv.org/abs/1506.05865",
                        "officialWebsiteLink": "http://icrc.hitsz.edu.cn/Article/show/139.html",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1131",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:49:18",
                        "createDate": "2024-01-11 14:11:16",
                        "desc": {
                            "cn": "LCSTSæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„ä¸­æ–‡çŸ­æ–‡æœ¬æ‘˜è¦æ•°æ®é›†ï¼Œä»ä¸­å›½å¾®åšç½‘ç«™æ–°æµªå¾®åšä¸­æ„å»ºè€Œæˆï¼Œå¹¶å·²å¼€æºã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡ 200 ä¸‡æ¡çœŸå®çš„ä¸­æ–‡çŸ­æ–‡æœ¬ï¼Œæ¯ä¸ªæ–‡æœ¬éƒ½æä¾›äº†ä¸€ä¸ªç®€çŸ­çš„æ‘˜è¦ã€‚",
                            "en": "LCSTS is a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/lcsts'. Error: Path opencompass/lcsts is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_543",
                    "name": "HumanEval-X",
                    "version": "1.0.0",
                    "description": "HumanEval-X is a benchmark for evaluating the multilingual ability of code generative models. It consists of 820 high-quality human-crafted data samples (each with test cases) in Python, C++, Java, JavaScript, and Go, and can be used for various tasks, such as code generation and translation.",
                    "url": "opencompass/opencompass_543.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_543",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "543",
                        "name": "HumanEval-X",
                        "emoji": "ğŸ—½",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "code",
                                "en": "code"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/THUDM/CodeGeeX",
                        "paperLink": "https://arxiv.org/abs/2303.17568",
                        "officialWebsiteLink": "https://huggingface.co/datasets/THUDM/humaneval-x",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1123",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:32:11",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-12 19:32:11",
                        "createDate": "2024-09-12 19:25:43",
                        "desc": {
                            "cn": "HumanEval-X æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°ä»£ç ç”Ÿæˆæ¨¡å‹çš„å¤šè¯­è¨€èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…å«äº†820ä¸ªé«˜è´¨é‡çš„äººå·¥åˆ¶ä½œçš„æ•°æ®æ ·æœ¬ï¼ˆæ¯ä¸ªéƒ½æœ‰æµ‹è¯•æ¡ˆä¾‹ï¼‰ï¼ŒåŒ…æ‹¬Pythonã€C++ã€Javaã€JavaScriptå’ŒGoè¯­è¨€ï¼Œå¯ç”¨äºå„ç§ä»»åŠ¡ï¼Œå¦‚ä»£ç ç”Ÿæˆå’Œç¿»è¯‘ã€‚",
                            "en": "HumanEval-X is a benchmark for evaluating the multilingual ability of code generative models. It consists of 820 high-quality human-crafted data samples (each with test cases) in Python, C++, Java, JavaScript, and Go, and can be used for various tasks, such as code generation and translation."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/humaneval_x'. Error: Path opencompass/humaneval_x is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1073",
                    "name": "SafetyBench",
                    "version": "1.0.0",
                    "description": " SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. Notably, SafetyBench also incorporates both Chinese and English data.",
                    "url": "opencompass/opencompass_1073.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1073",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1073",
                        "name": "SafetyBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/thu-coai/SafetyBench",
                        "paperLink": "https://arxiv.org/pdf/2309.07045",
                        "officialWebsiteLink": "https://llmbench.ai/safety",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "5018933",
                            "name": "thu-coai",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/5018933-17d059c5-d271-410a-82a9-ef8966927c24.png",
                            "nickname": "thu-coai"
                        },
                        "lookNum": "1086",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-27 17:50:17",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-27 17:50:17",
                        "createDate": "2024-09-27 12:52:37",
                        "desc": {
                            "cn": "SafetyBench æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å®‰å…¨æ€§ï¼ŒåŒ…å« 11,435 é“å¤šæ ·åŒ–çš„é€‰æ‹©é¢˜ï¼Œæ¶µç›– 7 ä¸ªä¸åŒçš„å®‰å…¨å…³æ³¨ç±»åˆ«ã€‚SafetyBench è¿˜åŒ…å«ä¸­æ–‡å’Œè‹±æ–‡çš„æ•°æ®ï¼Œæ–¹ä¾¿åŒè¯­è¯„ä¼°ã€‚",
                            "en": " SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. Notably, SafetyBench also incorporates both Chinese and English data."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/safetybench'. Error: Path opencompass/safetybench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1052",
                    "name": "CaLM",
                    "version": "1.0.0",
                    "description": "CaLM is the first comprehensive benchmark for evaluating the causal reasoning capabilities of language models. The CaLM framework establishes a foundational taxonomy consisting of four modules: causal target, adaptation, metric, and error.\n",
                    "url": "opencompass/opencompass_1052.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1052",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1052",
                        "name": "CaLM",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/OpenCausaLab/CaLM",
                        "paperLink": "https://arxiv.org/abs/2405.00622",
                        "officialWebsiteLink": "https://opencausalab.github.io/CaLM/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50204289",
                            "name": "OpenCausaLab",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50204289-b2e0a1f4-5d24-4237-b62a-ed8731d8834d.png",
                            "nickname": "OpenCausaLab"
                        },
                        "lookNum": "1053",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-23 16:42:32",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-23 16:42:32",
                        "createDate": "2024-09-13 13:55:09",
                        "desc": {
                            "cn": "CaLMæ˜¯ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤è”åˆåŒæµå¤§å­¦ã€ä¸Šæµ·äº¤é€šå¤§å­¦ã€åŒ—äº¬å¤§å­¦åŠå•†æ±¤ç§‘æŠ€å‘å¸ƒé¦–ä¸ªå¤§æ¨¡å‹å› æœæ¨ç†å¼€æ”¾è¯„æµ‹ä½“ç³»ã€‚é¦–æ¬¡ä»å› æœæ¨ç†è§’åº¦æå‡ºè¯„ä¼°æ¡†æ¶ï¼Œä¸ºAIç ”ç©¶è€…æ‰“é€ å¯é è¯„æµ‹å·¥å…·ï¼Œä»è€Œä¸ºæ¨è¿›å¤§æ¨¡å‹è®¤çŸ¥èƒ½åŠ›å‘äººç±»æ°´å¹³çœ‹é½æä¾›æŒ‡æ ‡å‚è€ƒã€‚",
                            "en": "CaLM is the first comprehensive benchmark for evaluating the causal reasoning capabilities of language models. The CaLM framework establishes a foundational taxonomy consisting of four modules: causal target, adaptation, metric, and error.\n"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/calm'. Error: Path opencompass/calm is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_532",
                    "name": "PIQA",
                    "version": "1.0.0",
                    "description": "PIQA is a physical interaction question answering task, which requires to select the most reasonable solution based on the given scenario and two possible solutions. This task is designed to test the model's knowledge in physical commonsense. This dataset consists of 16k training samples, 800 development samples and 2k test samples, all on English text.",
                    "url": "opencompass/opencompass_532.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_532",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {
                        "gen": "piqa_gen",
                        "ppl": "piqa_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "532",
                        "name": "PIQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/francois-rozet/piqa",
                        "paperLink": "https://arxiv.org/abs/1911.11641",
                        "officialWebsiteLink": "https://yonatanbisk.com/piqa/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1041",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:49:25",
                        "createDate": "2024-01-11 14:12:04",
                        "desc": {
                            "cn": "PIQAæ˜¯ä¸€ä¸ªç‰©ç†äº¤äº’é—®ç­”ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ç»™å®šçš„åœºæ™¯å’Œä¸¤ä¸ªå¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼Œé€‰æ‹©æœ€åˆç†çš„æ–¹æ¡ˆã€‚è¿™ä¸ªä»»åŠ¡æ˜¯ä¸ºäº†æµ‹è¯•æ¨¡å‹åœ¨ç‰©ç†å¸¸è¯†æ–¹é¢çš„çŸ¥è¯†ã€‚è¿™ä¸ªæ•°æ®é›†åŒ…å«äº†16000ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œ800ä¸ªå¼€å‘æ ·æœ¬å’Œ2000ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œæ‰€æœ‰çš„æ–‡æœ¬éƒ½æ˜¯è‹±æ–‡æ–‡æœ¬ã€‚",
                            "en": "PIQA is a physical interaction question answering task, which requires to select the most reasonable solution based on the given scenario and two possible solutions. This task is designed to test the model's knowledge in physical commonsense. This dataset consists of 16k training samples, 800 development samples and 2k test samples, all on English text."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "No valid url for ./data/piqa!\nPlease make sure  `./data/piqa` is correct"
                    }
                },
                {
                    "id": "opencompass_519",
                    "name": "CSL",
                    "version": "1.0.0",
                    "description": "CSL is a large-scale Chinese Scientific Literature dataset, which contains the titles, abstracts, keywords and academic fields of 396k papers.",
                    "url": "opencompass/opencompass_519.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_519",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "519",
                        "name": "CSL",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/ydli-ai/CSL",
                        "paperLink": "https://arxiv.org/abs/2209.05034",
                        "officialWebsiteLink": "https://github.com/ydli-ai/CSL",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1038",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:50:33",
                        "createDate": "2024-01-11 14:11:11",
                        "desc": {
                            "cn": "CSLæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„ä¸­æ–‡ç§‘æŠ€æ–‡çŒ®æ•°æ®é›†ï¼ŒåŒ…å« 39.6 ä¸‡ç¯‡è®ºæ–‡çš„æ ‡é¢˜ã€æ‘˜è¦ã€å…³é”®è¯å’Œå­¦æœ¯é¢†åŸŸä¿¡æ¯ã€‚",
                            "en": "CSL is a large-scale Chinese Scientific Literature dataset, which contains the titles, abstracts, keywords and academic fields of 396k papers."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/csl'. Error: Path opencompass/csl is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_536",
                    "name": "DROP",
                    "version": "1.0.0",
                    "description": "DROP is a QA dataset which tests comprehensive understanding of paragraphs. In this crowdsourced, adversarially-created, 96k question-answering benchmark, a system must resolve multiple references in a question, map them onto a paragraph, and perform discrete operations over them (such as addition, counting, or sorting).",
                    "url": "opencompass/opencompass_536.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_536",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {
                        "gen": "DROP_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "536",
                        "name": "DROP",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/allenai/allennlp-reading-comprehension/blob/master/allennlp_rc/eval/drop_eval.py",
                        "paperLink": "https://arxiv.org/abs/1903.00161",
                        "officialWebsiteLink": "https://allennlp.org/drop",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1031",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:50:10",
                        "createDate": "2024-01-11 14:12:19",
                        "desc": {
                            "cn": "DROP æ˜¯ä¸€ä¸ªæµ‹è¯•æ®µè½ç»¼åˆç†è§£èƒ½åŠ›çš„ QA æ•°æ®é›†ã€‚åœ¨è¿™ä¸ªä¼—åŒ…ã€å¯¹æŠ—æ€§åˆ›å»ºçš„ 96K é—®é¢˜è§£ç­”åŸºå‡†ä¸­ï¼Œç³»ç»Ÿå¿…é¡»è§£æé—®é¢˜ä¸­çš„å¤šä¸ªå¼•ç”¨ï¼Œå°†å®ƒä»¬æ˜ å°„åˆ°æ®µè½ä¸­ï¼Œå¹¶å¯¹å®ƒä»¬æ‰§è¡Œç¦»æ•£æ“ä½œï¼ˆå¦‚åŠ æ³•ã€è®¡æ•°æˆ–æ’åºï¼‰ã€‚",
                            "en": "DROP is a QA dataset which tests comprehensive understanding of paragraphs. In this crowdsourced, adversarially-created, 96k question-answering benchmark, a system must resolve multiple references in a question, map them onto a paragraph, and perform discrete operations over them (such as addition, counting, or sorting)."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/drop'. Error: Path opencompass/drop is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_533",
                    "name": "SIQA",
                    "version": "1.0.0",
                    "description": "SIQA is a social interaction question answering task, which requires to select the most reasonable behavior based on the given scenario and three possible subsequent behaviors. This task is designed to test the model's knowledge in social commonsense. This dataset consists of 38,963 training samples, 1,951 development samples and 1,960 test samples, all on English text.",
                    "url": "opencompass/opencompass_533.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_533",
                    "sample_count": 1000,
                    "traits": [
                        "reasoning"
                    ],
                    "eval_type": {
                        "gen": "siqa_gen",
                        "ppl": "siqa_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "533",
                        "name": "SIQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/pdf/1904.09728.pdf",
                        "officialWebsiteLink": "https://leaderboard.allenai.org/socialiqa/submissions/public",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1029",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:49:13",
                        "createDate": "2024-01-11 14:12:08",
                        "desc": {
                            "cn": "SIQA æ˜¯ä¸€ä¸ªç¤¾ä¼šäº¤äº’é—®ç­”ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ç»™å®šçš„åœºæ™¯å’Œä¸‰ä¸ªå¯èƒ½çš„åç»­è¡Œä¸ºï¼Œé€‰æ‹©æœ€åˆç†çš„è¡Œä¸ºã€‚è¿™ä¸ªä»»åŠ¡æ˜¯ä¸ºäº†æµ‹è¯•æ¨¡å‹åœ¨ç¤¾ä¼šå¸¸è¯†æ–¹é¢çš„çŸ¥è¯†ã€‚è¿™ä¸ªæ•°æ®é›†åŒ…å«äº† 38963 ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œ1951 ä¸ªå¼€å‘æ ·æœ¬å’Œ 1960 ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œæ‰€æœ‰çš„æ–‡æœ¬éƒ½æ˜¯è‹±æ–‡æ–‡æœ¬ã€‚",
                            "en": "SIQA is a social interaction question answering task, which requires to select the most reasonable behavior based on the given scenario and three possible subsequent behaviors. This task is designed to test the model's knowledge in social commonsense. This dataset consists of 38,963 training samples, 1,951 development samples and 1,960 test samples, all on English text."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "No valid url for ./data/siqa!\nPlease make sure  `./data/siqa` is correct"
                    }
                },
                {
                    "id": "opencompass_503",
                    "name": "ARC-e",
                    "version": "1.0.0",
                    "description": "The AI2â€™s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. ",
                    "url": "opencompass/opencompass_503.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_503",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {
                        "gen": "ARC-e_gen",
                        "ppl": "ARC-e_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "503",
                        "name": "ARC-e",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å­¦ç§‘",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://allenai.org/data/arc",
                        "paperLink": "https://arxiv.org/pdf/1803.05457.pdf",
                        "officialWebsiteLink": "https://allenai.org/data/arc",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "1017",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:49:30",
                        "createDate": "2024-01-11 14:09:54",
                        "desc": {
                            "cn": "AI2çš„æ¨ç†æŒ‘æˆ˜ï¼ˆARCï¼‰æ•°æ®é›†æ˜¯ä¸€ä¸ªå¤šé¡¹é€‰æ‹©é—®é¢˜å›ç­”æ•°æ®é›†ï¼ŒåŒ…å«äº†ä»ä¸‰å¹´çº§åˆ°ä¹å¹´çº§çš„ç§‘å­¦è€ƒè¯•ä¸­æå–çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šç®€å•å’ŒæŒ‘æˆ˜ï¼Œå…¶ä¸­åè€…åŒ…å«äº†éœ€è¦æ¨ç†èƒ½åŠ›çš„æ›´éš¾çš„é—®é¢˜ã€‚å¤§å¤šæ•°é—®é¢˜æœ‰4ä¸ªç­”æ¡ˆé€‰é¡¹ï¼Œä»…æœ‰ä¸åˆ°1ï¼…çš„é—®é¢˜æœ‰3ä¸ªæˆ–5ä¸ªç­”æ¡ˆé€‰é¡¹ã€‚",
                            "en": "The AI2â€™s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/arc_e'. Error: Path opencompass/arc_e is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_521",
                    "name": "XSum",
                    "version": "1.0.0",
                    "description": "XSum is a single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question â€œWhat is the article about?â€. The dataset collects real-world, large scale data by harvesting online articles from the British Broadcasting Corporation (BBC).",
                    "url": "opencompass/opencompass_521.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_521",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {
                        "gen": "XSum_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "521",
                        "name": "XSum",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/EdinburghNLP/XSum",
                        "paperLink": "https://arxiv.org/abs/1808.08745",
                        "officialWebsiteLink": "https://github.com/EdinburghNLP/XSum",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "955",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:50:47",
                        "createDate": "2024-01-11 14:11:21",
                        "desc": {
                            "cn": "XSumæ˜¯ä¸€ä¸ªå•æ–‡æ¡£æ‘˜è¦ä»»åŠ¡ï¼Œä¸æ”¯æŒæŠ½å–å¼ç­–ç•¥ï¼Œéœ€è¦é‡‡ç”¨æŠ½è±¡å»ºæ¨¡æ–¹æ³•ã€‚å…¶æ€æƒ³æ˜¯åˆ›å»ºä¸€ä¸ªç®€çŸ­çš„ä¸€å¥è¯æ–°é—»æ‘˜è¦ï¼Œå›ç­”â€œè¿™ç¯‡æ–‡ç« æ˜¯å…³äºä»€ä¹ˆçš„ï¼Ÿâ€çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†é€šè¿‡ä»è‹±å›½å¹¿æ’­å…¬å¸ï¼ˆBBCï¼‰æ”¶é›†åœ¨çº¿æ–‡ç« ï¼Œå¾—åˆ°äº†å¤§é‡çš„ç°å®æ•°æ®ã€‚",
                            "en": "XSum is a single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question â€œWhat is the article about?â€. The dataset collects real-world, large scale data by harvesting online articles from the British Broadcasting Corporation (BBC)."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "No valid url for ./data/Xsum/dev.jsonl!\nPlease make sure  `./data/Xsum/dev.jsonl` is correct"
                    }
                },
                {
                    "id": "opencompass_506",
                    "name": "AFQMC",
                    "version": "1.0.0",
                    "description": " AFQMC is an Ant Financial chinese semantic similarity task, which requires to judge whether two sentences have the same meaning or not.",
                    "url": "opencompass/opencompass_506.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_506",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "506",
                        "name": "AFQMC",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/IDEA-CCNL/Fengshenbang-LM/",
                        "paperLink": "https://arxiv.org/abs/2209.02970",
                        "officialWebsiteLink": "https://tianchi.aliyun.com/dataset/106411",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "945",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:50:04",
                        "createDate": "2024-01-11 14:10:07",
                        "desc": {
                            "cn": "AFQMCä¸€ä¸ªèš‚èšé‡‘æœä¸­æ–‡è¯­ä¹‰ç›¸ä¼¼åº¦ä»»åŠ¡ï¼Œè¦æ±‚åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦å…·æœ‰ç›¸åŒçš„è¯­ä¹‰ã€‚",
                            "en": " AFQMC is an Ant Financial chinese semantic similarity task, which requires to judge whether two sentences have the same meaning or not."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/afqmc'. Error: Path opencompass/afqmc is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1097",
                    "name": "HelloBench",
                    "version": "1.0.0",
                    "description": "HelloBench is a hierarchical long text generation benchmark to evaluate LLMs' performance in generating long text.  Based on Bloom's Taxonomy, HelloBench categorizes long text generation tasks into five subtasks: open-ended QA, summarization, chat, text completion, and heuristic text generation.",
                    "url": "opencompass/opencompass_1097.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1097",
                    "sample_count": 1000,
                    "traits": [
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1097",
                        "name": "HelloBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "é•¿æ–‡æœ¬",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 3,
                        "githubLink": "https://github.com/Quehry/HelloBench",
                        "paperLink": "https://arxiv.org/pdf/2409.16191",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "934",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-30 15:32:45",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-30 15:32:45",
                        "createDate": "2024-09-30 15:06:26",
                        "desc": {
                            "cn": "HelloBenchä¸ºé•¿æ–‡æœ¬ç”ŸæˆåŸºå‡†ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ã€å¼€æ”¾å¼çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°LLMåœ¨ç”Ÿæˆé•¿æ–‡æœ¬æ–¹é¢çš„æ€§èƒ½ã€‚åŸºäºBloomçš„åˆ†ç±»æ³•ï¼ŒHelloBenchå°†é•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡åˆ†ä¸ºäº”ä¸ªå­ä»»åŠ¡ï¼šå¼€æ”¾å¼QAã€æ‘˜è¦ã€èŠå¤©ã€æ–‡æœ¬å®Œæˆå’Œå¯å‘å¼æ–‡æœ¬ç”Ÿæˆã€‚",
                            "en": "HelloBench is a hierarchical long text generation benchmark to evaluate LLMs' performance in generating long text.  Based on Bloom's Taxonomy, HelloBench categorizes long text generation tasks into five subtasks: open-ended QA, summarization, chat, text completion, and heuristic text generation."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/hellobench'. Error: Path opencompass/hellobench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1253",
                    "name": "BigCodeBench",
                    "version": "1.0.0",
                    "description": "BigCodeBench is a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained tasks. ",
                    "url": "opencompass/opencompass_1253.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1253",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1253",
                        "name": "BigCodeBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/bigcode-project/bigcodebench/",
                        "paperLink": "https://arxiv.org/abs/2406.15877",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "929",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-30 16:30:39",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-30 16:30:39",
                        "createDate": "2024-12-30 16:24:34",
                        "desc": {
                            "cn": "BigCodeBenchç”¨äºè¯„ä¼°LLMçš„ä»£ç ç”Ÿæˆèƒ½åŠ›ï¼ŒåŒ…å«1140ä¸ªå¯ä»¥è°ƒç”¨139ä¸ªåº“å’Œ7ä¸ªåŸŸçš„å¤šä¸ªå‡½æ•°æ¥å®Œæˆçš„ç»†ç²’åº¦ä»»åŠ¡ã€‚",
                            "en": "BigCodeBench is a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/bigcodebench'. Error: No data found in /home/budadmin/.cache/opencompass/./data/bigcodebench/ for split 'test'. Please check if the split exists and contains data files.\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_507",
                    "name": "WSC",
                    "version": "1.0.0",
                    "description": "WSC is a pronoun disambiguation task, which requires to determine which noun the pronoun refers to according to the context.",
                    "url": "opencompass/opencompass_507.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_507",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {
                        "gen": "WSC_gen",
                        "ppl": "WSC_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "507",
                        "name": "WSC",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html",
                        "paperLink": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.729.9814&rep=rep1&type=pdf",
                        "officialWebsiteLink": "https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "917",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:51:25",
                        "createDate": "2024-01-11 14:10:14",
                        "desc": {
                            "cn": "WSCæ˜¯ä¸€ä¸ªä»£è¯æ¶ˆæ­§ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­ä»£è¯æŒ‡ä»£çš„æ˜¯å“ªä¸ªåè¯ã€‚",
                            "en": "WSC is a pronoun disambiguation task, which requires to determine which noun the pronoun refers to according to the context."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/wsc'. Error: Path opencompass/wsc is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_516",
                    "name": "RACE(High)",
                    "version": "1.0.0",
                    "description": "RACE is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle school and high school students. ",
                    "url": "opencompass/opencompass_516.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_516",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "516",
                        "name": "RACE(High)",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/qizhex/RACE_AR_baselines",
                        "paperLink": "https://arxiv.org/abs/1704.04683",
                        "officialWebsiteLink": "https://www.cs.cmu.edu/~glai1/data/race/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "916",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:49:43",
                        "createDate": "2024-01-11 14:10:58",
                        "desc": {
                            "cn": "RACE æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„é˜…è¯»ç†è§£æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡ 28,000 ä¸ªæ®µè½å’Œè¿‘ 100,000 ä¸ªé—®é¢˜ã€‚è¯¥æ•°æ®é›†æ˜¯ä»ä¸­å›½çš„è‹±è¯­è€ƒè¯•ä¸­æ”¶é›†è€Œæ¥ï¼Œè¿™äº›è€ƒè¯•æ˜¯ä¸ºä¸­å­¦å’Œé«˜ä¸­å­¦ç”Ÿè®¾è®¡çš„ã€‚\n",
                            "en": "RACE is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle school and high school students. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/race(high)'. Error: Path opencompass/race(high) is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_517",
                    "name": "RACE(Middle)",
                    "version": "1.0.0",
                    "description": "RACE is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle school and high school students. ",
                    "url": "opencompass/opencompass_517.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_517",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "517",
                        "name": "RACE(Middle)",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/qizhex/RACE_AR_baselines",
                        "paperLink": "https://arxiv.org/abs/1704.04683",
                        "officialWebsiteLink": "https://www.cs.cmu.edu/~glai1/data/race/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "906",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:52:17",
                        "createDate": "2024-01-11 14:11:03",
                        "desc": {
                            "cn": "RACE æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„é˜…è¯»ç†è§£æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡ 28,000 ä¸ªæ®µè½å’Œè¿‘ 100,000 ä¸ªé—®é¢˜ã€‚è¯¥æ•°æ®é›†æ˜¯ä»ä¸­å›½çš„è‹±è¯­è€ƒè¯•ä¸­æ”¶é›†è€Œæ¥ï¼Œè¿™äº›è€ƒè¯•æ˜¯ä¸ºä¸­å­¦å’Œé«˜ä¸­å­¦ç”Ÿè®¾è®¡çš„ã€‚\n",
                            "en": "RACE is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle school and high school students. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/race(middle)'. Error: Path opencompass/race(middle) is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_525",
                    "name": "OCNLI",
                    "version": "1.0.0",
                    "description": "OCNLI is a Chinese natural language inference task, which requires to determine the logical relation between two sentences, with three relations: entailment, contradiction and neutral.",
                    "url": "opencompass/opencompass_525.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_525",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {
                        "gen": "ocnli_gen",
                        "ppl": "ocnli_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "525",
                        "name": "OCNLI",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/cluebenchmark/OCNLI",
                        "paperLink": "https://arxiv.org/abs/2010.05444",
                        "officialWebsiteLink": "https://github.com/cluebenchmark/OCNLI",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "885",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:50:54",
                        "createDate": "2024-01-11 14:11:37",
                        "desc": {
                            "cn": "OCNLIæ˜¯ä¸€ä¸ªä¸­æ–‡è‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ä¸¤ä¸ªå¥å­åˆ¤æ–­å®ƒä»¬ä¹‹é—´çš„é€»è¾‘å…³ç³»ï¼Œæœ‰ä¸‰ç§å…³ç³»ï¼šè•´å«ã€çŸ›ç›¾å’Œä¸­ç«‹ã€‚",
                            "en": "OCNLI is a Chinese natural language inference task, which requires to determine the logical relation between two sentences, with three relations: entailment, contradiction and neutral."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ocnli'. Error: Path opencompass/ocnli is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_924",
                    "name": "CS-Bench",
                    "version": "1.0.0",
                    "description": "CS-Bench, the first bilingual (Chinese-English) benchmark dedicated to evaluating the performance of LLMs in computer science. CS-Bench comprises approximately 5K meticulously curated test samples, covering 26 subfields across 4 key areas of computer science, encompassing various task forms and divisions of knowledge and reasoning.",
                    "url": "opencompass/opencompass_924.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_924",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning",
                        "Code",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "924",
                        "name": "CS-Bench",
                        "emoji": "ğŸ˜œ",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            },
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/csbench/csbench",
                        "paperLink": "https://arxiv.org/pdf/2406.08587",
                        "officialWebsiteLink": "https://csbench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50103486",
                            "name": null,
                            "avatar": null,
                            "nickname": "LeonDiao0427"
                        },
                        "lookNum": "883",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-07-11 09:52:59",
                        "createDate": "2024-07-11 09:48:45",
                        "desc": {
                            "cn": "CS-Bench ç¬¬ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°LLMsåœ¨è®¡ç®—æœºç§‘å­¦ä¸­è¡¨ç°çš„åŒè¯­ï¼ˆä¸­è‹±æ–‡ï¼‰åŸºå‡†ã€‚CS-BenchåŒ…æ‹¬çº¦5,000ä¸ªç²¾å¿ƒç­–åˆ’çš„æµ‹è¯•æ ·æœ¬ï¼Œæ¶µç›–äº†è®¡ç®—æœºç§‘å­¦4ä¸ªå…³é”®é¢†åŸŸä¸­çš„26ä¸ªå­é¢†åŸŸï¼Œå¹¶åŒ…æ‹¬å„ç§ä»»åŠ¡å½¢å¼å’ŒçŸ¥è¯†æ¨ç†çš„åˆ’åˆ†ã€‚",
                            "en": "CS-Bench, the first bilingual (Chinese-English) benchmark dedicated to evaluating the performance of LLMs in computer science. CS-Bench comprises approximately 5K meticulously curated test samples, covering 26 subfields across 4 key areas of computer science, encompassing various task forms and divisions of knowledge and reasoning."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cs_bench'. Error: Path opencompass/cs_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1172",
                    "name": "MT-Bench-101",
                    "version": "1.0.0",
                    "description": "MT-Bench-101 is specifically designed to evaluate the finegrained abilities of LLMs in multi-turn dialogues. ",
                    "url": "opencompass/opencompass_1172.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1172",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1172",
                        "name": "MT-Bench-101",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/mtbench101/mt-bench-101",
                        "paperLink": "https://arxiv.org/pdf/2402.14762",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "874",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-27 17:23:10",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-27 17:23:10",
                        "createDate": "2024-10-21 14:33:55",
                        "desc": {
                            "cn": "MT-Bench-101 ä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼° LLMs åœ¨å¤šè½®å¯¹è¯ä¸­çš„ç»†ç²’åº¦èƒ½åŠ›ã€‚é€šè¿‡å¯¹çœŸå®å¤šè½®å¯¹è¯æ•°æ®çš„è¯¦ç»†åˆ†æï¼Œæ„å»ºäº†ä¸€ä¸ªä¸‰å±‚çº§çš„èƒ½åŠ›åˆ†ç±»æ³•ï¼Œæ¶µç›– 1388 ä¸ªå¤šè½®å¯¹è¯ä¸­çš„ 4208 ä¸ªè½®æ¬¡ï¼Œæ¶‰åŠ 13 ç§ä¸åŒçš„ä»»åŠ¡ã€‚",
                            "en": "MT-Bench-101 is specifically designed to evaluate the finegrained abilities of LLMs in multi-turn dialogues. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mt_bench_101'. Error: Path opencompass/mt_bench_101 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_508",
                    "name": "TyDiQA",
                    "version": "1.0.0",
                    "description": "TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language expresses.",
                    "url": "opencompass/opencompass_508.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_508",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "508",
                        "name": "TyDiQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/google-research-datasets/tydiqa",
                        "paperLink": "https://arxiv.org/abs/2003.05002",
                        "officialWebsiteLink": "https://ai.google.com/research/tydiqa",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "869",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-01-11 14:10:23",
                        "createDate": "2024-01-11 14:10:23",
                        "desc": {
                            "cn": "TyDi QA æ˜¯ä¸€ä¸ªæ¶µç›– 11 ç§ä¸åŒè¯­è¨€çš„é—®é¢˜å›ç­”æ•°æ®é›†ï¼ŒåŒ…å« 20.4 ä¸‡ä¸ªé—®é¢˜-ç­”æ¡ˆå¯¹ã€‚TyDi QA çš„è¯­è¨€ç§ç±»å¤šæ ·ï¼Œæ¶µç›–äº†è¯­è¨€å­¦ç‰¹å¾çš„å„ç§ç±»å‹ã€‚",
                            "en": "TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language expresses."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "No valid url for ./data/tydiqa/!\nPlease make sure  `./data/tydiqa/` is correct"
                    }
                },
                {
                    "id": "opencompass_523",
                    "name": "LAMBADA",
                    "version": "1.0.0",
                    "description": "The LAMBADA evaluates the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broad",
                    "url": "opencompass/opencompass_523.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_523",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {
                        "gen": "LAMBADA_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "523",
                        "name": "LAMBADA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://zenodo.org/record/2630551",
                        "paperLink": "https://arxiv.org/abs/1606.06031",
                        "officialWebsiteLink": "https://zenodo.org/record/2630551",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "849",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:51:18",
                        "createDate": "2024-01-11 14:11:30",
                        "desc": {
                            "cn": "LAMBADA é€šè¿‡ä¸€ä¸ªå•è¯é¢„æµ‹ä»»åŠ¡æ¥è¯„ä¼°è®¡ç®—æ¨¡å‹å¯¹æ–‡æœ¬ç†è§£çš„èƒ½åŠ›ã€‚LAMBADA æ˜¯æœ‰å¦‚ä¸‹ç‰¹ç‚¹çš„ä¸€ç»„å™è¿°æ€§æ–‡ç« ï¼šå¦‚æœé¢å¯¹æ•´ç¯‡æ–‡ç« ï¼Œäººä»¬å¯ä»¥çŒœæµ‹å®ƒä»¬çš„æœ€åä¸€ä¸ªå•è¯ï¼Œä½†å¦‚æœä»–ä»¬åªçœ‹åˆ°ç›®æ ‡å•è¯å‰é¢çš„æœ€åä¸€å¥è¯ï¼Œå°±æ— æ³•çŒœæµ‹ã€‚ä¸ºäº†åœ¨ LAMBADA ä¸Šç”±å¥½çš„æ•ˆæœï¼Œæ¨¡å‹ä¸èƒ½ä»…ä»…ä¾èµ–äºå±€éƒ¨ä¸Šä¸‹æ–‡ï¼Œè€Œå¿…é¡»èƒ½å¤Ÿè·Ÿè¸ªæ›´å¹¿æ³›çš„è¯è¯­ä¿¡æ¯ã€‚\nLAMBADA æ•°æ®é›†æ˜¯ä» BookCorpus ä¸­æå–çš„ï¼ŒåŒ…æ‹¬ 10,022 æ®µè½ï¼Œåˆ†ä¸º 4,869 ä¸ªå¼€å‘æ®µè½å’Œ 5,153 ä¸ªæµ‹è¯•æ®µè½ï¼Œå…±è®¡ 2.03 äº¿ä¸ªå•è¯ã€‚",
                            "en": "The LAMBADA evaluates the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse.\nThe LAMBADA dataset is extracted from BookCorpus and consists of 10'022 passages, divided into 4'869 development and 5'153 test passages, comprising 203 million words."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "No valid url for ./data/lambada/test.jsonl!\nPlease make sure  `./data/lambada/test.jsonl` is correct"
                    }
                },
                {
                    "id": "opencompass_1085",
                    "name": "InfoBench",
                    "version": "1.0.0",
                    "description": "InfoBench is a benchmark comprising 500 diverse instructions and 2,250 decomposed questions across multiple constraint categories.\n",
                    "url": "opencompass/opencompass_1085.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1085",
                    "sample_count": 1000,
                    "traits": [
                        "Instruct"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1085",
                        "name": "InfoBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æŒ‡ä»¤è·Ÿéš",
                                "en": "Instruct"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/qinyiwei/InfoBench",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.772.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50186103",
                            "name": "Tencent-AI-Lab",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186103-0a763557-5cd0-4d44-b53a-aa07caf31eb7.png",
                            "nickname": "Tencent-AI-Lab"
                        },
                        "lookNum": "804",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-29 16:24:48",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-29 16:24:48",
                        "createDate": "2024-09-29 13:22:54",
                        "desc": {
                            "cn": "InfoBench æ˜¯ä¸€ä¸ªæŒ‡ä»¤è¿½éšè¯„æµ‹åŸºå‡†ï¼ŒåŒ…å« 500 æ¡å¤šæ ·åŒ–çš„æŒ‡ä»¤å’Œ 2,250 ä¸ªåˆ†è§£é—®é¢˜ï¼Œæ¶µç›–å¤šä¸ªçº¦æŸç±»åˆ«ã€‚",
                            "en": "InfoBench is a benchmark comprising 500 diverse instructions and 2,250 decomposed questions across multiple constraint categories.\n"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/infobench'. Error: Path opencompass/infobench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_530",
                    "name": "ReCoRD",
                    "version": "1.0.0",
                    "description": "ReCoRD is a reading comprehension task, which requires to extract the answer from the article based on the given news article and question.",
                    "url": "opencompass/opencompass_530.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_530",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {
                        "gen": "ReCoRD_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "530",
                        "name": "ReCoRD",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://sheng-z.github.io/ReCoRD-explorer/",
                        "paperLink": "https://arxiv.org/abs/1905.00537",
                        "officialWebsiteLink": "https://sheng-z.github.io/ReCoRD-explorer/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "773",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 11:35:25",
                        "createDate": "2024-01-11 14:11:57",
                        "desc": {
                            "cn": "ReCoRDæ˜¯ä¸€ä¸ªé˜…è¯»ç†è§£ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ç»™å®šçš„æ–°é—»æ–‡ç« å’Œé—®é¢˜ï¼Œä»æ–‡ç« ä¸­æŠ½å–å‡ºç­”æ¡ˆã€‚\n",
                            "en": "ReCoRD is a reading comprehension task, which requires to extract the answer from the article based on the given news article and question."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/record'. Error: Path opencompass/record is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1136",
                    "name": "IFEval",
                    "version": "1.0.0",
                    "description": "IFEval is a straightforward and easy-to reproduce evaluation benchmark. It focuses on a set of â€œverifiable instructionsâ€ such as â€œwrite in more than 400 wordsâ€ and â€œmention the keyword of AI at least 3 timesâ€.",
                    "url": "opencompass/opencompass_1136.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1136",
                    "sample_count": 1000,
                    "traits": [
                        "Instruct"
                    ],
                    "eval_type": {
                        "gen": "IFEval_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1136",
                        "name": "IFEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æŒ‡ä»¤è·Ÿéš",
                                "en": "Instruct"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/google-research/google-research/tree/master/instruction_following_eval",
                        "paperLink": "https://arxiv.org/pdf/2311.07911",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "767",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-14 20:15:14",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-14 20:15:14",
                        "createDate": "2024-10-12 16:05:28",
                        "desc": {
                            "cn": "IFEval æ˜¯ä¸€ä¸ªç®€å•ä¸”æ˜“äºå¤ç°çš„è¯„ä¼°åŸºå‡†ã€‚å®ƒå…³æ³¨ä¸€ç»„â€œå¯éªŒè¯çš„æŒ‡ä»¤â€ï¼Œä¾‹å¦‚â€œå†™è¶…è¿‡ 400 ä¸ªå•è¯â€å’Œâ€œè‡³å°‘æåˆ°å…³é”®è¯ AI 3 æ¬¡â€ã€‚",
                            "en": "IFEval is a straightforward and easy-to reproduce evaluation benchmark. It focuses on a set of â€œverifiable instructionsâ€ such as â€œwrite in more than 400 wordsâ€ and â€œmention the keyword of AI at least 3 timesâ€."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ifeval'. Error: Path opencompass/ifeval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1155",
                    "name": "Ada-LEval",
                    "version": "1.0.0",
                    "description": "Ada-LEval is a length-adaptable benchmark for evaluating the long-context understanding\nof LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable\na more reliable evaluation of LLMsâ€™ long context capabilities.",
                    "url": "opencompass/opencompass_1155.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1155",
                    "sample_count": 1000,
                    "traits": [
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1155",
                        "name": "Ada-LEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "é•¿æ–‡æœ¬",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NAACL 2024",
                                "en": "NAACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 3,
                        "githubLink": "https://github.com/open-compass/Ada-LEval",
                        "paperLink": "https://aclanthology.org/2024.naacl-long.205.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "756",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-27 17:23:13",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-27 17:23:13",
                        "createDate": "2024-10-15 16:58:09",
                        "desc": {
                            "cn": "Ada-LEval ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹é•¿ä¸Šä¸‹æ–‡çš„ç†è§£èƒ½åŠ›ã€‚Ada-LEval åŒ…å«ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å­é›†ï¼ŒTSort å’Œ BestAnswerï¼Œèƒ½å¤Ÿæ›´å¯é åœ°è¯„ä¼° LLMs çš„é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ã€‚",
                            "en": "Ada-LEval is a length-adaptable benchmark for evaluating the long-context understanding\nof LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable\na more reliable evaluation of LLMsâ€™ long context capabilities."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ada_leval'. Error: Path opencompass/ada_leval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1096",
                    "name": "TruthfulQA",
                    "version": "1.0.0",
                    "description": "TruthfulQA is  a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics.",
                    "url": "opencompass/opencompass_1096.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1096",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1096",
                        "name": "TruthfulQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/sylinrl/TruthfulQA",
                        "paperLink": "https://arxiv.org/pdf/2109.07958",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "734",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-30 15:32:49",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-30 15:32:49",
                        "createDate": "2024-09-29 18:03:55",
                        "desc": {
                            "cn": "TruthfulQA ç”¨äºæµ‹é‡è¯­è¨€æ¨¡å‹åœ¨å›ç­”é—®é¢˜æ—¶çš„çœŸå®åº¦ã€‚è¯¥åŸºå‡†åŒ…å« 817 ä¸ªé—®é¢˜ï¼Œæ¶µç›– 38 ä¸ªç±»åˆ«ï¼ŒåŒ…æ‹¬å¥åº·ã€æ³•å¾‹ã€é‡‘èå’Œæ”¿æ²»ã€‚",
                            "en": "TruthfulQA is  a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/truthfulqa'. Error: Path opencompass/truthfulqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_557",
                    "name": "OCRBench",
                    "version": "1.0.0",
                    "description": "OCRBench provides a comprehensive evaluation of Large Multimodal Models, such as GPT4V and Gemini, in various text-related visual tasks including Text Recognition, Scene Text-Centric Visual Question Answering (VQA), Document-Oriented VQA, Key Information Extraction (KIE), and Handwritten Mathematical Expression Recognition (HMER). ",
                    "url": "opencompass/opencompass_557.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_557",
                    "sample_count": 1000,
                    "traits": [
                        "Language",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "557",
                        "name": "OCRBench",
                        "emoji": "ğŸ‘¹",
                        "dimensions": [
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "vision-language",
                                "en": "vision-language"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Yuliang-Liu/MultimodalOCR",
                        "paperLink": "https://arxiv.org/abs/2305.07895",
                        "officialWebsiteLink": "https://huggingface.co/spaces/echo840/ocrbench-leaderboard",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "40041912",
                            "name": "echo840",
                            "avatar": null,
                            "nickname": "echo840"
                        },
                        "lookNum": "733",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-01-30 10:43:27",
                        "createDate": "2024-01-30 10:43:27",
                        "desc": {
                            "cn": "OCRBenchå¯¹ GPT4V å’Œ Gemini ç­‰å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å„ç§æ–‡æœ¬ç›¸å…³çš„è§†è§‰ä»»åŠ¡ä¸­çš„è¡¨ç°è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬æ–‡æœ¬è¯†åˆ«ã€åœºæ™¯æ–‡æœ¬ä¸ºä¸­å¿ƒçš„è§†è§‰é—®ç­” (VQA)ã€é¢å‘æ–‡æ¡£çš„ VQAã€å…³é”®ä¿¡æ¯æå– (KIE) å’Œæ‰‹å†™æ•°å­¦è¡¨è¾¾å¼è¯†åˆ« (HMER)ã€‚",
                            "en": "OCRBench provides a comprehensive evaluation of Large Multimodal Models, such as GPT4V and Gemini, in various text-related visual tasks including Text Recognition, Scene Text-Centric Visual Question Answering (VQA), Document-Oriented VQA, Key Information Extraction (KIE), and Handwritten Mathematical Expression Recognition (HMER). "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ocrbench'. Error: Path opencompass/ocrbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_544",
                    "name": "DS-1000",
                    "version": "1.0.0",
                    "description": "DS-1000 is a code generation benchmark with a thousand data science questions spanning seven Python libraries that (1) reflects diverse, realistic, and practical use cases, (2) has a reliable metric, (3) defends against memorization by perturbing questions.",
                    "url": "opencompass/opencompass_544.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_544",
                    "sample_count": 1000,
                    "traits": [
                        "code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "544",
                        "name": "DS-1000",
                        "emoji": "ğŸ—½",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/xlang-ai/DS-1000",
                        "paperLink": "https://arxiv.org/pdf/2211.11501.pdf",
                        "officialWebsiteLink": "https://ds1000-code-gen.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "726",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-01-26 15:43:03",
                        "createDate": "2024-01-26 15:43:03",
                        "desc": {
                            "cn": "DS-1000 æ˜¯ä¸€ä¸ªä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä¸€åƒä¸ªæ•°æ®ç§‘å­¦é—®é¢˜ï¼Œæ¶µç›–ä¸ƒä¸ªPythonåº“ï¼Œå…¶ç‰¹ç‚¹æ˜¯ï¼ˆ1ï¼‰åæ˜ å¤šæ ·åŒ–ã€ç°å®ä¸”å®ç”¨çš„ç”¨ä¾‹ï¼Œï¼ˆ2ï¼‰å…·æœ‰å¯é çš„åº¦é‡æ ‡å‡†ï¼Œï¼ˆ3ï¼‰é€šè¿‡æ‰°ä¹±é—®é¢˜æ¥é˜²æ­¢è®°å¿†åŒ–ã€‚",
                            "en": "DS-1000 is a code generation benchmark with a thousand data science questions spanning seven Python libraries that (1) reflects diverse, realistic, and practical use cases, (2) has a reliable metric, (3) defends against memorization by perturbing questions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ds_1000'. Error: Path opencompass/ds_1000 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_522",
                    "name": "EPRSTMT",
                    "version": "1.0.0",
                    "description": "(E-commerce Product Review Dataset for Sentiment Analysis), also known as EPRSTMT, is a binary sentiment analysis dataset based on product reviews on e-commerce platform. Each sample is labelled as Positive or Negative. It collect by ICIP Lab of Beijing Normal University.",
                    "url": "opencompass/opencompass_522.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_522",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "522",
                        "name": "EPRSTMT",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/CLUEbenchmark/FewCLUE",
                        "paperLink": "https://arxiv.org/abs/2107.07498",
                        "officialWebsiteLink": "https://github.com/CLUEbenchmark/FewCLUE",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "705",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:52:03",
                        "createDate": "2024-01-11 14:11:26",
                        "desc": {
                            "cn": "EPRSTMTï¼Œä¹Ÿç§°ä½œç”µå­å•†åŠ¡äº§å“è¯„è®ºæƒ…æ„Ÿåˆ†ææ•°æ®é›†ï¼Œæ˜¯ä¸€ä¸ªåŸºäºç”µå­å•†åŠ¡å¹³å°ä¸Šçš„äº§å“è¯„è®ºçš„äºŒå…ƒæƒ…æ„Ÿåˆ†ææ•°æ®é›†ã€‚æ¯ä¸ªæ ·æœ¬éƒ½è¢«æ ‡è®°ä¸ºç§¯ææˆ–æ¶ˆæã€‚è¯¥æ•°æ®é›†ç”±åŒ—äº¬å¸ˆèŒƒå¤§å­¦ ICIP å®éªŒå®¤æ”¶é›†ã€‚",
                            "en": "(E-commerce Product Review Dataset for Sentiment Analysis), also known as EPRSTMT, is a binary sentiment analysis dataset based on product reviews on e-commerce platform. Each sample is labelled as Positive or Negative. It collect by ICIP Lab of Beijing Normal University."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/eprstmt'. Error: Path opencompass/eprstmt is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_527",
                    "name": "AX-g",
                    "version": "1.0.0",
                    "description": "AX-g is a Winogender diagnostic task, which requires to determine which noun the pronoun refers to according to the given sentence and pronoun. This task is selected from a subset of the Winogender dataset, mainly used to test the model's ability in dealing with gender bias and discrimination.",
                    "url": "opencompass/opencompass_527.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_527",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "527",
                        "name": "AX-g",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/rudinger/winogender-schemas",
                        "paperLink": "https://arxiv.org/abs/1905.00537",
                        "officialWebsiteLink": "https://github.com/rudinger/winogender-schemas",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "684",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 11:37:55",
                        "createDate": "2024-01-11 14:11:44",
                        "desc": {
                            "cn": "AX-gæ˜¯ä¸€ä¸ªWinogenderè¯Šæ–­ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ç»™å®šçš„å¥å­å’Œä»£è¯ï¼Œåˆ¤æ–­ä»£è¯æŒ‡ä»£çš„æ˜¯å“ªä¸ªåè¯ã€‚è¿™ä¸ªä»»åŠ¡æ˜¯ä»Winogenderæ•°æ®é›†ä¸­é€‰å–äº†ä¸€éƒ¨åˆ†æ•°æ®ï¼Œä¸»è¦ç”¨æ¥æµ‹è¯•æ¨¡å‹åœ¨å¤„ç†æ€§åˆ«åè§å’Œæ€§åˆ«æ­§è§†æ–¹é¢çš„èƒ½åŠ›ã€‚",
                            "en": "AX-g is a Winogender diagnostic task, which requires to determine which noun the pronoun refers to according to the given sentence and pronoun. This task is selected from a subset of the Winogender dataset, mainly used to test the model's ability in dealing with gender bias and discrimination."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ax_g'. Error: Path opencompass/ax_g is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_528",
                    "name": "RTE",
                    "version": "1.0.0",
                    "description": "RTE is a natural language inference task, which requires to determine the logical relation between the given sentence pair, with three relations: entailment, contradiction and neutral.",
                    "url": "opencompass/opencompass_528.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_528",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {
                        "gen": "RTE_gen",
                        "ppl": "RTE_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "528",
                        "name": "RTE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://tac.nist.gov//2011/RTE/index.html",
                        "paperLink": "https://arxiv.org/abs/1905.00537",
                        "officialWebsiteLink": "https://tac.nist.gov//2011/RTE/index.html",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "668",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:52:48",
                        "createDate": "2024-01-11 14:11:48",
                        "desc": {
                            "cn": "RTEæ˜¯ä¸€ä¸ªè‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ç»™å®šçš„å¥å­å¯¹ï¼Œåˆ¤æ–­å®ƒä»¬ä¹‹é—´çš„é€»è¾‘å…³ç³»ï¼Œæœ‰ä¸‰ç§å…³ç³»ï¼šè•´å«ã€çŸ›ç›¾å’Œä¸­ç«‹ã€‚",
                            "en": "RTE is a natural language inference task, which requires to determine the logical relation between the given sentence pair, with three relations: entailment, contradiction and neutral."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/rte'. Error: Path opencompass/rte is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1074",
                    "name": "NewsBench",
                    "version": "1.0.0",
                    "description": "NewsBench is a novel evaluation framework to systematically assess the capabilities of Large Language Models (LLMs) for editorial capabilities in Chinese journalism. ",
                    "url": "opencompass/opencompass_1074.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1074",
                    "sample_count": 1000,
                    "traits": [
                        "Creation"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1074",
                        "name": "NewsBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "åˆ›ä½œ",
                                "en": "Creation"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/IAAR-Shanghai/NewsBench",
                        "paperLink": "https://arxiv.org/pdf/2403.00862",
                        "officialWebsiteLink": "https://iaar-shanghai.github.io/NewsBench/#/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50187574",
                            "name": "IAAR-Shanghai",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50187574-4cf5b8da-55c3-4c75-aa9b-9c2d053b5b6d.png",
                            "nickname": "IAAR-Shanghai"
                        },
                        "lookNum": "645",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-27 17:50:27",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-27 17:50:27",
                        "createDate": "2024-09-27 13:08:42",
                        "desc": {
                            "cn": "NewsBench æ˜¯ä¸€ä¸ªæ–°é¢–çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿæ€§åœ°è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸­æ–‡æ–°é—»ç¼–è¾‘èƒ½åŠ›ä¸Šçš„è¡¨ç°ã€‚æ„å»ºçš„åŸºå‡†æ•°æ®é›†èšç„¦äºå†™ä½œèƒ½åŠ›çš„å››ä¸ªæ–¹é¢å’Œå®‰å…¨éµå¾ªçš„å…­ä¸ªæ–¹é¢ï¼ŒåŒ…å« 1,267 ä¸ªæ‰‹åŠ¨ç²¾å¿ƒè®¾è®¡çš„æµ‹è¯•æ ·æœ¬ï¼Œç±»å‹åŒ…æ‹¬é€‰æ‹©é¢˜å’Œç®€ç­”é¢˜ï¼Œæ¶µç›– 24 ä¸ªæ–°é—»é¢†åŸŸçš„äº”é¡¹ç¼–è¾‘ä»»åŠ¡ã€‚",
                            "en": "NewsBench is a novel evaluation framework to systematically assess the capabilities of Large Language Models (LLMs) for editorial capabilities in Chinese journalism. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/newsbench'. Error: Path opencompass/newsbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1121",
                    "name": "HaluEval",
                    "version": "1.0.0",
                    "description": "HaluEval evaluates the performance of LLMs in recognizing hallucination. It includes 5,000 general user queries with ChatGPT responses and 30,000 task-specific examples from three tasks, i.e., question answering, knowledge-grounded dialogue, and text summarization.",
                    "url": "opencompass/opencompass_1121.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1121",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1121",
                        "name": "HaluEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/RUCAIBox/HaluEval",
                        "paperLink": "https://arxiv.org/pdf/2305.11747",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "642",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-13 15:31:34",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-13 15:31:34",
                        "createDate": "2025-02-13 18:15:01",
                        "desc": {
                            "cn": "HaluEvalç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹è¯†åˆ«å¹»è§‰çš„èƒ½åŠ›ï¼ŒåŒ…å« 5,000 æ¡æ™®é€šç”¨æˆ·æŸ¥è¯¢åŠ ChatGPT çš„å›ç­”ï¼Œä»¥åŠæ¥è‡ªä¸‰ä¸ªä»»åŠ¡çš„ 30,000 ä¸ªç‰¹å®šä»»åŠ¡ç¤ºä¾‹ï¼Œå³é—®ç­”ã€åŸºäºçŸ¥è¯†çš„å¯¹è¯å’Œæ–‡æœ¬æ‘˜è¦ã€‚",
                            "en": "HaluEval evaluates the performance of LLMs in recognizing hallucination. It includes 5,000 general user queries with ChatGPT responses and 30,000 task-specific examples from three tasks, i.e., question answering, knowledge-grounded dialogue, and text summarization."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/halueval'. Error: Path opencompass/halueval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_526",
                    "name": "AX-b",
                    "version": "1.0.0",
                    "description": "AX-b is a broad-coverage diagnostic task, which requires to determine the logical relation between the given sentence pair, with three relations: entailment, contradiction and neutral. This task is selected from a subset of the GLUE broad-coverage diagnostic dataset, mainly used to test the model's understanding ability in grammar, semantics, world knowledge and so on.",
                    "url": "opencompass/opencompass_526.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_526",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "526",
                        "name": "AX-b",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://gluebenchmark.com/diagnostics",
                        "paperLink": "https://arxiv.org/abs/1905.00537",
                        "officialWebsiteLink": "https://gluebenchmark.com/diagnostics",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "622",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 11:38:03",
                        "createDate": "2024-01-11 14:11:41",
                        "desc": {
                            "cn": "AX-bæ˜¯ä¸€ä¸ªå¹¿è¦†ç›–è¯Šæ–­ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ç»™å®šçš„å¥å­å¯¹ï¼Œåˆ¤æ–­å®ƒä»¬ä¹‹é—´çš„é€»è¾‘å…³ç³»ï¼Œæœ‰ä¸‰ç§å…³ç³»ï¼šè•´å«ã€çŸ›ç›¾å’Œä¸­ç«‹ã€‚è¿™ä¸ªä»»åŠ¡æ˜¯ä»GLUEçš„å¹¿è¦†ç›–è¯Šæ–­æ•°æ®é›†ä¸­é€‰å–äº†ä¸€éƒ¨åˆ†æ•°æ®ï¼Œä¸»è¦ç”¨æ¥æµ‹è¯•æ¨¡å‹åœ¨è¯­æ³•ã€è¯­ä¹‰ã€ä¸–ç•ŒçŸ¥è¯†ç­‰æ–¹é¢çš„ç†è§£èƒ½åŠ›ã€‚",
                            "en": "AX-b is a broad-coverage diagnostic task, which requires to determine the logical relation between the given sentence pair, with three relations: entailment, contradiction and neutral. This task is selected from a subset of the GLUE broad-coverage diagnostic dataset, mainly used to test the model's understanding ability in grammar, semantics, world knowledge and so on."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ax_b'. Error: Path opencompass/ax_b is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1175",
                    "name": "MMStar",
                    "version": "1.0.0",
                    "description": "MMStar is an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMsâ€™ multi-modal capacities with carefully balanced and purified samples.",
                    "url": "opencompass/opencompass_1175.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1175",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1175",
                        "name": "MMStar",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MMStar-Benchmark/MMStar",
                        "paperLink": "https://arxiv.org/pdf/2403.20330",
                        "officialWebsiteLink": "https://mmstar-benchmark.github.io/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "617",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 11:27:36",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 11:27:36",
                        "createDate": "2024-10-21 16:21:15",
                        "desc": {
                            "cn": "MMStar æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åŸºå‡†ï¼ŒåŒ…å« 1,500 ä¸ªç»è¿‡äººå·¥ç²¾å¿ƒæŒ‘é€‰çš„æ ·æœ¬ã€‚MMStar è¯„ä¼° 6 é¡¹æ ¸å¿ƒèƒ½åŠ›å’Œ 18 ä¸ªè¯¦ç»†ç»´åº¦ï¼Œæ—¨åœ¨é€šè¿‡ç²¾å¿ƒå¹³è¡¡å’Œå‡€åŒ–çš„æ ·æœ¬ï¼Œè¯„ä¼° LVLM çš„å¤šæ¨¡æ€èƒ½åŠ›ã€‚",
                            "en": "MMStar is an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMsâ€™ multi-modal capacities with carefully balanced and purified samples."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmstar'. Error: Path opencompass/mmstar is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1089",
                    "name": "MathBench",
                    "version": "1.0.0",
                    "description": "MathBench, a new benchmark that rigorously assesses the mathematical capabilities of large\nlanguage models. MathBench spans a wide range of mathematical disciplines, offering a\ndetailed evaluation of both theoretical understanding and practical problem-solving skills.",
                    "url": "opencompass/opencompass_1089.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1089",
                    "sample_count": 1000,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1089",
                        "name": "MathBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 3,
                        "githubLink": "https://github.com/open-compass/MathBench",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.411.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "605",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-30 15:32:54",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-30 15:32:54",
                        "createDate": "2024-09-29 15:21:55",
                        "desc": {
                            "cn": "MathBench ä¸¥æ ¼è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°å­¦èƒ½åŠ›ã€‚MathBench æ¶‰åŠå¹¿æ³›çš„æ•°å­¦å­¦ç§‘ï¼Œæä¾›å¯¹ç†è®ºç†è§£å’Œå®é™…é—®é¢˜è§£å†³æŠ€èƒ½çš„è¯¦ç»†è¯„ä¼°ã€‚",
                            "en": "MathBench, a new benchmark that rigorously assesses the mathematical capabilities of large\nlanguage models. MathBench spans a wide range of mathematical disciplines, offering a\ndetailed evaluation of both theoretical understanding and practical problem-solving skills."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mathbench'. Error: Path opencompass/mathbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1961",
                    "name": "SFE",
                    "version": "1.0.0",
                    "description": "The Scientists' First Exam (SFE) benchmark, designed to comprehensively evaluate the scientific cognitive capabilities of MLLMs through three cognitive levels (cog-levels):Scientific Signal Perceptionã€Scientific Attribute Understanding ã€Scientific Comparative Reasoning.",
                    "url": "opencompass/opencompass_1961.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1961",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1961",
                        "name": "SFE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            },
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://huggingface.co/datasets/PrismaX/SFE",
                        "paperLink": "https://arxiv.org/abs/2506.10521",
                        "officialWebsiteLink": "https://prismax.opencompass.org.cn/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "585",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-24 15:55:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-24 15:55:33",
                        "createDate": "2025-06-24 15:55:24",
                        "desc": {
                            "cn": "The Scientists' First Exam (SFE) åŸºå‡†æµ‹è¯•æ—¨åœ¨é€šè¿‡ä¸‰ä¸ªè®¤çŸ¥å±‚çº§â€”â€”ç§‘å­¦ä¿¡å·æ„ŸçŸ¥ã€ç§‘å­¦å±æ€§ç†è§£ å’Œ ç§‘å­¦å¯¹æ¯”æ¨ç†ï¼Œå…¨é¢è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç§‘å­¦è®¤çŸ¥èƒ½åŠ›ã€‚",
                            "en": "The Scientists' First Exam (SFE) benchmark, designed to comprehensively evaluate the scientific cognitive capabilities of MLLMs through three cognitive levels (cog-levels):Scientific Signal Perceptionã€Scientific Attribute Understanding ã€Scientific Comparative Reasoning."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/sfe'. Error: Path opencompass/sfe is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1075",
                    "name": "AlignBench",
                    "version": "1.0.0",
                    "description": "ALIGNBENCH is a comprehensive multidimensional benchmark for evaluating LLMsâ€™ alignment in Chinese. We tailor a humanin-the-loop data curation pipeline, containing 8 main categories, 683 real-scenario rooted queries and corresponding human verified references.",
                    "url": "opencompass/opencompass_1075.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1075",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1075",
                        "name": "AlignBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/THUDM/AlignBench",
                        "paperLink": "https://aclanthology.org/2024.acl-long.624.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "043910",
                            "name": "THUDM",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/043910-ad5eb16e-0c3f-4e1b-b1d6-e9dc8ab355a7.png",
                            "nickname": "æ™ºè°±.AI"
                        },
                        "lookNum": "584",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-11-01 13:39:39",
                        "supportOnlineEval": false,
                        "updateDate": "2024-11-01 13:39:39",
                        "createDate": "2024-10-29 17:11:05",
                        "desc": {
                            "cn": "AlignBench æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹å¯¹é½æ€§èƒ½çš„å…¨é¢ã€å¤šç»´åº¦çš„è¯„æµ‹åŸºå‡†ã€‚AlignBench æ„å»ºäº†äººç±»å‚ä¸çš„æ•°æ®æ„å»ºæµç¨‹ï¼Œæ¥ä¿è¯è¯„æµ‹æ•°æ®çš„åŠ¨æ€æ›´æ–°ã€‚AlignBench é‡‡ç”¨å¤šç»´åº¦ã€è§„åˆ™æ ¡å‡†çš„æ¨¡å‹è¯„ä»·æ–¹æ³•ï¼ˆLLM-as-Judgeï¼‰ï¼Œå¹¶ä¸”ç»“åˆæ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰ç”Ÿæˆå¯¹æ¨¡å‹å›å¤çš„å¤šç»´åº¦åˆ†æå’Œæœ€ç»ˆçš„ç»¼åˆè¯„åˆ†ï¼Œå¢å¼ºäº†è¯„æµ‹çš„é«˜å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚",
                            "en": "ALIGNBENCH is a comprehensive multidimensional benchmark for evaluating LLMsâ€™ alignment in Chinese. We tailor a humanin-the-loop data curation pipeline, containing 8 main categories, 683 real-scenario rooted queries and corresponding human verified references."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/alignbench'. Error: Path opencompass/alignbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_930",
                    "name": "MR-Ben-Meta-Reasoning-Benchmark",
                    "version": "1.0.0",
                    "description": "Welcome to the dataset page for the Meta-Reasoning Benchmark associated with our recent publication `Mr-Ben: A Comprehensive Meta-Reasoning Benchmark for Large Language Models`. We have provided a demo evaluate script for you to try out benchmark in mere two steps. We encourage everyone to try out our benchmark in the SOTA models and return its results to us. We would be happy to include it in the eval_results and update the evaluation tables below for you.",
                    "url": "opencompass/opencompass_930.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_930",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "930",
                        "name": "MR-Ben-Meta-Reasoning-Benchmark",
                        "emoji": "ğŸ—½",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "long-context",
                                "en": "long-context"
                            },
                            {
                                "cn": "understanding",
                                "en": "understanding"
                            },
                            {
                                "cn": "knowledge",
                                "en": "knowledge"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/dvlab-research/Mr-Ben",
                        "paperLink": "https://arxiv.org/abs/2406.13975",
                        "officialWebsiteLink": "https://randolph-zeng.github.io/Mr-Ben.github.io/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50104209",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-lFJ83HfdP"
                        },
                        "lookNum": "578",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-07-12 13:38:55",
                        "createDate": "2024-07-12 11:20:05",
                        "desc": {
                            "cn": "æœ¬å·¥ä½œè”åˆMIT,æ¸…å,å‰‘æ¡¥ç­‰çŸ¥åé™¢æ ¡, æå‡ºäº†ä¸€ä¸ªè¯„æµ‹å¤§è¯­è¨€æ¨¡å‹å¯¹å¤æ‚é—®é¢˜çš„æ¨ç†è¿‡ç¨‹çš„â€œé˜…å·â€æ‰¹æ”¹èƒ½åŠ›çš„è¯„æµ‹æ•°æ®é›†ï¼Œæœ‰åˆ«äºä»¥å‰çš„ä»¥ç»“æœåŒ¹é…ä¸ºè¯„æµ‹æ¨¡å¼çš„æ•°æ®é›†MR-Benï¼Œæˆ‘ä»¬çš„æ•°æ®é›†åŸºäºGSM8K[1], MMLU[2], LogiQA[3], MHPP[4]ç­‰æ•°æ®é›†ç»ç”±ç»†è‡´çš„é«˜æ°´å¹³äººå·¥æ ‡æ³¨æ„å»ºè€Œæˆï¼Œæ˜¾è‘—åœ°å¢åŠ äº†éš¾åº¦åŠåŒºåˆ†åº¦ã€‚æˆ‘ä»¬ç»†è‡´åœ°åˆ†æäº†åŒ…æ‹¬claude3.5, GPT4-Turbo, Kimi, Zhipu, Yi-Large, Qwen2, DeepseekCoderv2 ç­‰å›½å†…å¤–ä¸€çº¿çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå‘ç°å¼€æºçš„æ¨¡å‹åœ¨å¤æ‚æ¨ç†çš„åœºæ™¯ä¸‹æœ‰æœ›è¿½ä¸Šé¡¶å°–çš„é—­æºæ¨¡å‹ã€‚è¯¥è¯„æµ‹æ•°æ®é›†çš„æ‰€æœ‰æ•°æ®å‡å·²å¼€æºï¼Œå¹¶ä¸”æ”¯æŒä¸€é”®è¯„æµ‹ã€‚æ¬¢è¿æ‰€æœ‰åšå¤§æ¨¡å‹è®­ç»ƒçš„å°ä¼™ä¼´å‘æˆ‘ä»¬åˆ†äº«ä½ çš„è¯„æµ‹ç»“æœï¼Œæˆ‘ä»¬ä¼šåŠæ—¶æ›´æ–°æ¦œå•ã€‚",
                            "en": "Welcome to the dataset page for the Meta-Reasoning Benchmark associated with our recent publication `Mr-Ben: A Comprehensive Meta-Reasoning Benchmark for Large Language Models`. We have provided a demo evaluate script for you to try out benchmark in mere two steps. We encourage everyone to try out our benchmark in the SOTA models and return its results to us. We would be happy to include it in the eval_results and update the evaluation tables below for you."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mr_ben_meta_reasoning_benchmark'. Error: Path opencompass/mr_ben_meta_reasoning_benchmark is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1077",
                    "name": "SALAD-Bench",
                    "version": "1.0.0",
                    "description": "SALAD-Bench, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy\nspanning three levels, and versatile functionalities.",
                    "url": "opencompass/opencompass_1077.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1077",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1077",
                        "name": "SALAD-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OpenSafetyLab/SALAD-BENCH",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.235.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50186740",
                            "name": "OpenSafetyLab",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186740-5e27e10c-3894-4bfd-b0c9-2079f251882d.png",
                            "nickname": "OpenSafetyLab"
                        },
                        "lookNum": "563",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-29 16:25:08",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-29 16:25:08",
                        "createDate": "2024-09-27 16:14:48",
                        "desc": {
                            "cn": "SALAD-Bench æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€æ”»å‡»å’Œé˜²å¾¡æ–¹æ³•çš„å®‰å…¨åŸºå‡†ã€‚SALAD-Bench çš„ç‰¹ç‚¹åœ¨äºå…¶å¹¿æ³›æ€§ï¼Œè¶…è¶Šäº†ä¼ ç»ŸåŸºå‡†ï¼Œå…·æœ‰å¤§è§„æ¨¡ã€ä¸°å¯Œçš„å¤šæ ·æ€§ã€å¤æ‚çš„ä¸‰å±‚åˆ†ç±»æ³•ä»¥åŠå¤šåŠŸèƒ½æ€§ã€‚",
                            "en": "SALAD-Bench, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy\nspanning three levels, and versatile functionalities."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/salad_bench'. Error: Path opencompass/salad_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1128",
                    "name": "Gorilla",
                    "version": "1.0.0",
                    "description": "Gorilla enables LLMs to use tools by invoking APIs. Given a natural language query, Gorilla comes up with the semantically- and syntactically- correct API to invoke. ",
                    "url": "opencompass/opencompass_1128.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1128",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1128",
                        "name": "Gorilla",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ShishirPatil/gorilla",
                        "paperLink": "https://arxiv.org/pdf/2305.15334",
                        "officialWebsiteLink": "https://gorilla.cs.berkeley.edu/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "550",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-16 11:03:02",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-16 11:03:02",
                        "createDate": "2024-10-11 15:12:19",
                        "desc": {
                            "cn": "Gorilla ä½¿å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡è°ƒç”¨ API ä½¿ç”¨å·¥å…·ã€‚é’ˆå¯¹è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼ŒGorilla èƒ½å¤Ÿç”Ÿæˆè¯­ä¹‰å’Œè¯­æ³•ä¸Šæ­£ç¡®çš„ API è°ƒç”¨ã€‚",
                            "en": "Gorilla enables LLMs to use tools by invoking APIs. Given a natural language query, Gorilla comes up with the semantically- and syntactically- correct API to invoke. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gorilla'. Error: Path opencompass/gorilla is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1080",
                    "name": "E-EVAL",
                    "version": "1.0.0",
                    "description": "E-EVAL is the first comprehensive evaluation benchmark specifically tailored for Chinese K-12 education. E-EVAL comprises 4,351 multiple-choice questions spanning primary, middle, and high school levels, covering a diverse array of subjects.\n",
                    "url": "opencompass/opencompass_1080.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1080",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1080",
                        "name": "E-EVAL",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å­¦ç§‘",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AI-EDU-LAB/E-EVAL",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.462.pdf",
                        "officialWebsiteLink": "https://eevalbenchmark.com/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50186339",
                            "name": "AI-EDU-LAB",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186339-e3a36811-6e48-440b-a243-b7d59c6d4bd7.png",
                            "nickname": "AI-EDU-LAB"
                        },
                        "lookNum": "545",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-29 16:25:01",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-29 16:25:01",
                        "createDate": "2024-09-27 18:25:46",
                        "desc": {
                            "cn": "E-EVAL æ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹ä¸­å›½ K-12 æ•™è‚²çš„ç»¼åˆè¯„ä¼°åŸºå‡†ã€‚E-EVAL åŒ…å« 4,351 é“é€‰æ‹©é¢˜ï¼Œæ¶µç›–å°å­¦ã€åˆä¸­å’Œé«˜ä¸­å„ä¸ªå¹´çº§ï¼Œæ¶‰åŠå¤šç§å­¦ç§‘ã€‚",
                            "en": "E-EVAL is the first comprehensive evaluation benchmark specifically tailored for Chinese K-12 education. E-EVAL comprises 4,351 multiple-choice questions spanning primary, middle, and high school levels, covering a diverse array of subjects.\n"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/e_eval'. Error: Path opencompass/e_eval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1141",
                    "name": "CHARM",
                    "version": "1.0.0",
                    "description": "CHARM is the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense.",
                    "url": "opencompass/opencompass_1141.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1141",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1141",
                        "name": "CHARM",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/opendatalab/CHARM",
                        "paperLink": "https://aclanthology.org/2024.acl-long.604.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "528",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-19 18:04:11",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-19 18:04:11",
                        "createDate": "2024-10-14 14:20:06",
                        "desc": {
                            "cn": "CHARM æ˜¯é¦–ä¸ªå…¨é¢æ·±å…¥è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸­æ–‡ä¸­çš„å¸¸è¯†æ¨ç†èƒ½åŠ›çš„åŸºå‡†ï¼Œæ¶µç›–äº†å…¨çƒé€šç”¨çš„å¸¸è¯†å’Œç‰¹å®šäºä¸­å›½çš„å¸¸è¯†ã€‚",
                            "en": "CHARM is the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/charm'. Error: Path opencompass/charm is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1108",
                    "name": "HotpotQA",
                    "version": "1.0.0",
                    "description": "HOTPOTQA is a dataset with 113k Wikipedia-based question-answer pairs.",
                    "url": "opencompass/opencompass_1108.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1108",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1108",
                        "name": "HotpotQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/hotpotqa/hotpot",
                        "paperLink": "https://arxiv.org/pdf/1809.09600",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "504",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-10 18:36:24",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-10 18:36:24",
                        "createDate": "2025-01-10 18:28:24",
                        "desc": {
                            "cn": "HotpotQA ç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒ…å« 113,000 ä¸ªåŸºäºç»´åŸºç™¾ç§‘çš„é—®é¢˜å’Œç­”æ¡ˆã€‚",
                            "en": "HOTPOTQA is a dataset with 113k Wikipedia-based question-answer pairs."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/hotpotqa'. Error: Path opencompass/hotpotqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1070",
                    "name": "OlympiadBench",
                    "version": "1.0.0",
                    "description": "OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring\n8,476 problems from Olympiad-level mathematics and physics competitions, including the\nChinese college entrance exam. Each problem is detailed with expert-level annotations\nfor step-by-step reasoning. ",
                    "url": "opencompass/opencompass_1070.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1070",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1070",
                        "name": "OlympiadBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å­¦ç§‘",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OpenBMB/OlympiadBench",
                        "paperLink": "https://arxiv.org/pdf/2402.14008",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "044167",
                            "name": "OpenBMB",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/044167-87da8649-04d0-47ea-90e1-a77a1d2e9585.png",
                            "nickname": "OpenBMB"
                        },
                        "lookNum": "489",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-27 16:53:44",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-27 16:53:44",
                        "createDate": "2024-09-26 17:51:57",
                        "desc": {
                            "cn": "OlympiadBench æ˜¯ä¸€ä¸ªå¥¥æ—åŒ¹å…‹çº§åˆ«çš„åŒè¯­å¤šæ¨¡æ€ç§‘å­¦åŸºå‡†ï¼ŒåŒ…å«æ¥è‡ªå¥¥æ—åŒ¹å…‹çº§æ•°å­¦å’Œç‰©ç†ç«èµ›çš„8,476é“é¢˜ç›®ï¼ŒåŒ…æ‹¬ä¸­å›½é«˜è€ƒã€‚æ¯é“é¢˜ç›®éƒ½é…æœ‰ä¸“å®¶çº§åˆ«çš„æ³¨é‡Šï¼Œæä¾›é€æ­¥æ¨ç†çš„è¯¦ç»†è¯´æ˜ã€‚",
                            "en": "OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring\n8,476 problems from Olympiad-level mathematics and physics competitions, including the\nChinese college entrance exam. Each problem is detailed with expert-level annotations\nfor step-by-step reasoning. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/olympiadbench'. Error: Path opencompass/olympiadbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1137",
                    "name": "CMB",
                    "version": "1.0.0",
                    "description": "CMB is Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework.",
                    "url": "opencompass/opencompass_1137.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1137",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {
                        "gen": "CMB_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1137",
                        "name": "CMB",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NAACL 2024",
                                "en": "NAACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/FreedomIntelligence/CMB",
                        "paperLink": "https://arxiv.org/pdf/2308.08833",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "478",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-16 11:02:56",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-16 11:02:56",
                        "createDate": "2024-10-15 16:31:07",
                        "desc": {
                            "cn": "CMB æ˜¯ä¸€ä¸ªç»¼åˆåŒ»å­¦åŸºå‡†ï¼Œä¸“ä¸ºä¸­æ–‡è€Œè®¾è®¡ï¼Œå¹¶å®Œå…¨ä¾èµ–äºæœ¬åœŸçš„ä¸­æ–‡è¯­è¨€å’Œæ–‡åŒ–æ¡†æ¶ä¸­ã€‚",
                            "en": "CMB is Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cmb'. Error: Path opencompass/cmb is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1178",
                    "name": "MathVista",
                    "version": "1.0.0",
                    "description": "MathVista is a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets.",
                    "url": "opencompass/opencompass_1178.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1178",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Multimodal",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1178",
                        "name": "MathVista",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/lupantech/MathVista",
                        "paperLink": "https://arxiv.org/pdf/2310.02255",
                        "officialWebsiteLink": "https://mathvista.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "470",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-10 18:23:44",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-10 18:23:44",
                        "createDate": "2025-01-10 18:23:16",
                        "desc": {
                            "cn": "MathVistaç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ•°å­¦èƒ½åŠ›ï¼Œç»“åˆäº†ä¸°å¯Œçš„æ•°å­¦å’Œè§†è§‰ä»»åŠ¡æŒ‘æˆ˜ã€‚å®ƒç”±6141ä¸ªç¤ºä¾‹ç»„æˆï¼Œæ¥è‡ª28ä¸ªæ¶‰åŠæ•°å­¦çš„ç°æœ‰å¤šæ¨¡æ€æ•°æ®é›†å’Œ3ä¸ªæ–°åˆ›å»ºçš„æ•°æ®é›†ã€‚",
                            "en": "MathVista is a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mathvista'. Error: Path opencompass/mathvista is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1093",
                    "name": "APPS",
                    "version": "1.0.0",
                    "description": "APPS is a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code.",
                    "url": "opencompass/opencompass_1093.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1093",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {
                        "gen": "APPS_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1093",
                        "name": "APPS",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/hendrycks/apps",
                        "paperLink": "https://arxiv.org/pdf/2105.09938",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "460",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-30 15:32:52",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-30 15:32:52",
                        "createDate": "2024-09-29 17:01:36",
                        "desc": {
                            "cn": "APPS æ˜¯ä¸€ä¸ªä»£ç ç”Ÿæˆè¯„æµ‹åŸºå‡†ï¼Œè¯¥è¯„æµ‹åŸºå‡†æµ‹é‡æ¨¡å‹æ ¹æ®ä»»æ„è‡ªç„¶è¯­è¨€è§„èŒƒç”Ÿæˆä»¤äººæ»¡æ„çš„ Python ä»£ç çš„èƒ½åŠ›ã€‚",
                            "en": "APPS is a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/apps'. Error: Path opencompass/apps is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1083",
                    "name": "GAOKAO-MM",
                    "version": "1.0.0",
                    "description": "GAOKAO-MM is a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. ",
                    "url": "opencompass/opencompass_1083.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1083",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1083",
                        "name": "GAOKAO-MM",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å­¦ç§‘",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OpenMOSS/GAOKAO-MM",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.521.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50175431",
                            "name": "OpenMOSS",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50175431-71e98faf-7e15-46ce-8c84-56eb373cf63a.png",
                            "nickname": "OpenMOSS"
                        },
                        "lookNum": "455",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-29 16:24:53",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-29 16:24:53",
                        "createDate": "2024-09-29 11:48:20",
                        "desc": {
                            "cn": "GAOKAO-MM æ˜¯ä¸€ä¸ªåŸºäºä¸­å›½é«˜è€ƒçš„å¤šæ¨¡æ€åŸºå‡†ï¼ŒåŒ…å« 8 ä¸ªç§‘ç›®å’Œ 12 ç§å›¾åƒç±»å‹ï¼Œä¾‹å¦‚å›¾è¡¨ã€å‡½æ•°å›¾ã€åœ°å›¾å’Œç…§ç‰‡ã€‚GAOKAO-MM æºè‡ªæœ¬åœŸä¸­æ–‡è¯­å¢ƒï¼Œå¹¶å¯¹æ¨¡å‹çš„èƒ½åŠ›è®¾ç½®äº†äººç±»æ°´å¹³çš„è¦æ±‚ï¼ŒåŒ…æ‹¬æ„ŸçŸ¥ã€ç†è§£ã€çŸ¥è¯†å’Œæ¨ç†ã€‚",
                            "en": "GAOKAO-MM is a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gaokao_mm'. Error: Path opencompass/gaokao_mm is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1082",
                    "name": "StudentEval",
                    "version": "1.0.0",
                    "description": "STUDENTEVAL contains 1,749 prompts written by 80 students who have only completed one introductory Python course. STUDENTEVAL contains numerous non-expert prompts describing the same problem, enabling exploration of key factors in prompt success. ",
                    "url": "opencompass/opencompass_1082.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1082",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1082",
                        "name": "StudentEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Wellesley-EASEL-lab/StudentEval",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.501.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50186257",
                            "name": "Wellesley-EASEL-lab",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186257-118d14f5-a2d6-40c7-b3c5-712031ca526e.png",
                            "nickname": "Wellesley-EASEL-lab"
                        },
                        "lookNum": "454",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-29 16:24:55",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-29 16:24:55",
                        "createDate": "2024-09-29 11:16:00",
                        "desc": {
                            "cn": "StudentEval åŒ…å« 1,749 ä¸ªç”± 80 åä»…å®Œæˆä¸€é—¨å…¥é—¨ Python è¯¾ç¨‹çš„å­¦ç”Ÿæ’°å†™çš„æç¤ºã€‚StudentEval ä¸­åŒ…å«è®¸å¤šéä¸“å®¶æç¤ºï¼Œæè¿°ç›¸åŒçš„é—®é¢˜ï¼Œä½¿å¾—æ¢ç´¢æç¤ºæˆåŠŸçš„å…³é”®å› ç´ æˆä¸ºå¯èƒ½ã€‚",
                            "en": "STUDENTEVAL contains 1,749 prompts written by 80 students who have only completed one introductory Python course. STUDENTEVAL contains numerous non-expert prompts describing the same problem, enabling exploration of key factors in prompt success. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/studenteval'. Error: Path opencompass/studenteval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1079",
                    "name": "CFLUE",
                    "version": "1.0.0",
                    "description": "CFLUE is the Chinese Financial Language Understanding Evaluation benchmark, designed to assess the capability of LLMs across various dimensions. ",
                    "url": "opencompass/opencompass_1079.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1079",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1079",
                        "name": "CFLUE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/aliyun/cflue",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.337.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50186956",
                            "name": "Alibaba_Cloud",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186956-08003a88-f404-4b02-a282-c0c377547acd.png",
                            "nickname": "Alibaba_Cloud"
                        },
                        "lookNum": "453",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-29 16:25:03",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-29 16:25:03",
                        "createDate": "2024-09-27 17:54:49",
                        "desc": {
                            "cn": "CFLUE æ˜¯ä¸­å›½é‡‘èè¯­è¨€ç†è§£è¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ä¸ªç»´åº¦ä¸Šçš„èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼ŒCFLUE æä¾›äº†é’ˆå¯¹çŸ¥è¯†è¯„ä¼°å’Œåº”ç”¨è¯„ä¼°é‡èº«å®šåˆ¶çš„æ•°æ®é›†ã€‚åœ¨çŸ¥è¯†è¯„ä¼°æ–¹é¢ï¼Œå®ƒåŒ…å«è¶…è¿‡ 38,000 é“é€‰æ‹©é¢˜åŠç›¸å…³çš„è§£å†³æ–¹æ¡ˆè§£é‡Šã€‚",
                            "en": "CFLUE is the Chinese Financial Language Understanding Evaluation benchmark, designed to assess the capability of LLMs across various dimensions. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cflue'. Error: Path opencompass/cflue is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1135",
                    "name": "GPQA",
                    "version": "1.0.0",
                    "description": "GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry.",
                    "url": "opencompass/opencompass_1135.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1135",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1135",
                        "name": "GPQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/idavidrein/gpqa",
                        "paperLink": "https://arxiv.org/pdf/2311.12022",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "452",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-14 20:26:40",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-14 20:26:40",
                        "createDate": "2024-10-12 16:00:16",
                        "desc": {
                            "cn": "GPQA åŒ…å« 448 é“ç”±ç”Ÿç‰©å­¦ã€ç‰©ç†å­¦å’ŒåŒ–å­¦é¢†åŸŸä¸“å®¶æ’°å†™çš„å¤šé¡¹é€‰æ‹©é¢˜ã€‚",
                            "en": "GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gpqa'. Error: Path opencompass/gpqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1081",
                    "name": "NaturalCodeBench",
                    "version": "1.0.0",
                    "description": "NaturalCodeBench is challenging code benchmark designed to mirror the complexity and variety of scenarios in real coding tasks. NCB comprises 402 high-quality problems in Python and Java, meticulously selected from natural user queries from online coding services, covering 6 different domains.",
                    "url": "opencompass/opencompass_1081.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1081",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1081",
                        "name": "NaturalCodeBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/THUDM/NaturalCodeBench",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.471.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "043910",
                            "name": "THUDM",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/043910-ad5eb16e-0c3f-4e1b-b1d6-e9dc8ab355a7.png",
                            "nickname": "æ™ºè°±.AI"
                        },
                        "lookNum": "449",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-29 16:24:58",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-29 16:24:58",
                        "createDate": "2024-09-29 10:05:45",
                        "desc": {
                            "cn": "NaturalCodeBench æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»£ç åŸºå‡†ï¼Œæ—¨åœ¨åæ˜ çœŸå®ç¼–ç ä»»åŠ¡ä¸­çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚NaturalCodeBench åŒ…å« 402 ä¸ªé«˜è´¨é‡çš„ Python å’Œ Java é—®é¢˜ï¼Œè¿™äº›é—®é¢˜æ˜¯ä»åœ¨çº¿ç¼–ç æœåŠ¡çš„è‡ªç„¶ç”¨æˆ·æŸ¥è¯¢ä¸­ç²¾å¿ƒæŒ‘é€‰çš„ï¼Œæ¶µç›–äº† 6 ä¸ªä¸åŒçš„é¢†åŸŸã€‚",
                            "en": "NaturalCodeBench is challenging code benchmark designed to mirror the complexity and variety of scenarios in real coding tasks. NCB comprises 402 high-quality problems in Python and Java, meticulously selected from natural user queries from online coding services, covering 6 different domains."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/naturalcodebench'. Error: Path opencompass/naturalcodebench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1124",
                    "name": "RealToxicityPrompts",
                    "version": "1.0.0",
                    "description": "RealToxicityPrompts is a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely used toxicity classiï¬er. ",
                    "url": "opencompass/opencompass_1124.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1124",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1124",
                        "name": "RealToxicityPrompts",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/allenai/real-toxicity-prompts",
                        "paperLink": "https://aclanthology.org/2020.findings-emnlp.301.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "448",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-11 20:24:44",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-11 20:24:44",
                        "createDate": "2024-10-11 13:38:08",
                        "desc": {
                            "cn": "RealToxicityPrompts æ˜¯ä¸€ä¸ªåŒ…å« 100,000 ä¸ªè‡ªç„¶å‡ºç°çš„ã€å¥å­çº§æç¤ºçš„æ•°æ®é›†ï¼Œè¿™äº›æç¤ºæ¥è‡ªäºå¤§é‡çš„è‹±è¯­ç½‘ç»œæ–‡æœ¬ï¼Œå¹¶é…æœ‰æ¥è‡ªå¹¿æ³›ä½¿ç”¨çš„æ¯’æ€§åˆ†ç±»å™¨çš„æ¯’æ€§è¯„åˆ†ã€‚",
                            "en": "RealToxicityPrompts is a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely used toxicity classiï¬er. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/realtoxicityprompts'. Error: Path opencompass/realtoxicityprompts is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1276",
                    "name": "MMLU-Pro",
                    "version": "1.0.0",
                    "description": "MMLU-Pro is the extension of MMLU, integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options.",
                    "url": "opencompass/opencompass_1276.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1276",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1276",
                        "name": "MMLU-Pro",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å­¦ç§‘",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/TIGER-AI-Lab/MMLU-Pro",
                        "paperLink": "https://arxiv.org/abs/2406.01574",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "447",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 14:44:23",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 14:44:23",
                        "createDate": "2024-12-24 19:48:37",
                        "desc": {
                            "cn": "MMLU-Proæ˜¯MMLUçš„æ‰©å±•ç‰ˆæœ¬ï¼Œæ¶µç›–äº†æ›´å…·æŒ‘æˆ˜æ€§ã€ä»¥æ¨ç†ä¸ºé‡ç‚¹çš„é—®é¢˜ï¼Œå¹¶å°†é€‰æ‹©é›†ä»4ä¸ªé€‰é¡¹æ‰©å±•åˆ°10ä¸ªé€‰é¡¹ã€‚",
                            "en": "MMLU-Pro is the extension of MMLU, integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmlu_pro'. Error: No data found in /home/budadmin/.cache/opencompass/./data/mmlu_pro for split 'test'. Please check if the split exists and contains data files.\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1134",
                    "name": "TheoremQA",
                    "version": "1.0.0",
                    "description": "TheoremQA is the first theorem-driven question-answering dataset designed to evaluate AI modelsâ€™ capabilities to apply theorems to solve challenging science problems. It is curated by domain experts containing 800 high-quality questions covering 350 theorems from Math, Physics, EE&CS, and Finance.",
                    "url": "opencompass/opencompass_1134.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1134",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {
                        "gen": "TheoremQA_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1134",
                        "name": "TheoremQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/TIGER-AI-Lab/TheoremQA",
                        "paperLink": "https://arxiv.org/pdf/2305.12524",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "442",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-14 20:15:11",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-14 20:15:11",
                        "createDate": "2024-10-12 13:48:08",
                        "desc": {
                            "cn": "TheoremQA æ˜¯ç¬¬ä¸€ä¸ªåŸºäºå®šç†çš„é—®é¢˜å›ç­”æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼° AI æ¨¡å‹åº”ç”¨å®šç†è§£å†³å¤æ‚ç§‘å­¦é—®é¢˜çš„èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†ç”±é¢†åŸŸä¸“å®¶ç²¾å¿ƒç­–åˆ’ï¼ŒåŒ…å« 800 ä¸ªé«˜è´¨é‡é—®é¢˜ï¼Œæ¶µç›–æ¥è‡ªæ•°å­¦ã€ç‰©ç†ã€ç”µæ°”ä¸è®¡ç®—æœºç§‘å­¦ä»¥åŠé‡‘èçš„ 350 ä¸ªå®šç†ã€‚",
                            "en": "TheoremQA is the first theorem-driven question-answering dataset designed to evaluate AI modelsâ€™ capabilities to apply theorems to solve challenging science problems. It is curated by domain experts containing 800 high-quality questions covering 350 theorems from Math, Physics, EE&CS, and Finance."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/theoremqa'. Error: Path opencompass/theoremqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1100",
                    "name": "ScienceQA",
                    "version": "1.0.0",
                    "description": "SCIENCEQA is a new benchmark that consists of âˆ¼21k multimodal multiple choice questions with diverse science topics and annotations of their answers with corresponding lectures and explanations.",
                    "url": "opencompass/opencompass_1100.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1100",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1100",
                        "name": "ScienceQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://scienceqa.github.io/",
                        "paperLink": "https://lupantech.github.io/papers/neurips22_scienceqa.pdf",
                        "officialWebsiteLink": "https://scienceqa.github.io/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "437",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-09 20:03:37",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-09 20:03:37",
                        "createDate": "2024-10-09 14:29:43",
                        "desc": {
                            "cn": "SCIENCEQA åŒ…å«çº¦ 21,000 é“å¤šæ¨¡æ€é€‰æ‹©é¢˜ï¼Œæ¶µç›–å¤šç§ç§‘å­¦ä¸»é¢˜ï¼Œå¹¶é™„æœ‰ç›¸åº”çš„è®²åº§å’Œè§£é‡Šçš„ç­”æ¡ˆæ³¨é‡Šã€‚",
                            "en": "SCIENCEQA is a new benchmark that consists of âˆ¼21k multimodal multiple choice questions with diverse science topics and annotations of their answers with corresponding lectures and explanations."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/scienceqa'. Error: Path opencompass/scienceqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1088",
                    "name": "UHGEval",
                    "version": "1.0.0",
                    "description": "UHGEval(Unconstrained Hallucination Generation Evaluation) benchmark contains hallucinations generated by LLMs with minimal restrictions.",
                    "url": "opencompass/opencompass_1088.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1088",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1088",
                        "name": "UHGEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/IAAR-Shanghai/UHGEval",
                        "paperLink": "https://arxiv.org/pdf/2311.15296",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "432",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-10 10:34:42",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-10 10:34:42",
                        "createDate": "2024-10-09 20:11:08",
                        "desc": {
                            "cn": "UHGEval åŸºå‡†ï¼ŒåŒ…å«ç”±é™åˆ¶æ¡ä»¶æœ€å°çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆçš„å¹»è§‰ã€‚",
                            "en": "UHGEval(Unconstrained Hallucination Generation Evaluation) benchmark contains hallucinations generated by LLMs with minimal restrictions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/uhgeval'. Error: Path opencompass/uhgeval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1164",
                    "name": "TaskBench",
                    "version": "1.0.0",
                    "description": "TaskBench aims to evaluate the capability of LLMs in task automation, containing 28,271 samples spanning 3 critical stages: task decomposition, tool invocation, and parameter prediction.",
                    "url": "opencompass/opencompass_1164.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1164",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1164",
                        "name": "TaskBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/microsoft/JARVIS/tree/main/taskbench",
                        "paperLink": "https://arxiv.org/pdf/2311.18760",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "429",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 16:32:37",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 16:32:37",
                        "createDate": "2024-10-18 15:47:56",
                        "desc": {
                            "cn": "TaskBenchæ—¨åœ¨è¯„ä¼°LLMåœ¨ä»»åŠ¡è‡ªåŠ¨åŒ–æ–¹é¢çš„èƒ½åŠ›ï¼ŒåŒ…å«é¢å‘ä»»åŠ¡åˆ†è§£ã€å·¥å…·è°ƒç”¨å’Œå‚æ•°é¢„æµ‹ä¸‰ä¸ªå…³é”®é˜¶æ®µçš„28271ä¸ªæ ·æœ¬ã€‚",
                            "en": "TaskBench aims to evaluate the capability of LLMs in task automation, containing 28,271 samples spanning 3 critical stages: task decomposition, tool invocation, and parameter prediction."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/taskbench'. Error: Path opencompass/taskbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1086",
                    "name": "Belebele",
                    "version": "1.0.0",
                    "description": "BELEBELE, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in\nhigh-, medium-, and low-resource languages.",
                    "url": "opencompass/opencompass_1086.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1086",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1086",
                        "name": "Belebele",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/facebookresearch/belebele",
                        "paperLink": "https://arxiv.org/pdf/2308.16884",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "426",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-30 15:33:02",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-30 15:33:02",
                        "createDate": "2024-09-29 13:42:49",
                        "desc": {
                            "cn": "BELEBELE æ˜¯ä¸€ä¸ªå¤šé¡¹é€‰æ‹©æœºå™¨é˜…è¯»ç†è§£ï¼ˆMRCï¼‰æ•°æ®é›†ï¼Œæ¶µç›– 122 ç§è¯­è¨€å˜ä½“ã€‚è¯¥æ•°æ®é›†æ˜¾è‘—æ‰©å±•äº†è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰åŸºå‡†çš„è¯­è¨€è¦†ç›–èŒƒå›´ï¼Œä½¿å¾—å¯ä»¥åœ¨é«˜ã€ä¸­ã€ä½èµ„æºè¯­è¨€ä¸­è¯„ä¼°æ–‡æœ¬æ¨¡å‹ã€‚",
                            "en": "BELEBELE, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in\nhigh-, medium-, and low-resource languages."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/belebele'. Error: Path opencompass/belebele is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1078",
                    "name": "DebugBench",
                    "version": "1.0.0",
                    "description": "DebugBench is an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. ",
                    "url": "opencompass/opencompass_1078.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1078",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1078",
                        "name": "DebugBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/thunlp/DebugBench",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.247.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50188779",
                            "name": "THUNLP",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50188779-1d9e7f11-f6dd-4c37-a464-f75de27aa50a.png",
                            "nickname": "THUNLP"
                        },
                        "lookNum": "425",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-29 16:25:05",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-29 16:25:05",
                        "createDate": "2024-09-27 17:44:25",
                        "desc": {
                            "cn": "DebugBench æ˜¯ä¸€ä¸ªåŒ…å« 4,253 ä¸ªå®ä¾‹çš„ LLM è°ƒè¯•åŸºå‡†ã€‚å®ƒæ¶µç›–äº† C++ã€Java å’Œ Python ä¸­çš„å››ä¸ªä¸»è¦é”™è¯¯ç±»åˆ«å’Œ 18 ç§æ¬¡è¦ç±»å‹ã€‚",
                            "en": "DebugBench is an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/debugbench'. Error: Path opencompass/debugbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_568",
                    "name": "CriticBench",
                    "version": "1.0.0",
                    "description": "CriticBench, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. CriticBench encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity.",
                    "url": "opencompass/opencompass_568.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_568",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "568",
                        "name": "CriticBench",
                        "emoji": "ğŸ˜‚",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "critique",
                                "en": "critique"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/open-compass/CriticBench",
                        "paperLink": "https://arxiv.org/abs/2402.13764",
                        "officialWebsiteLink": "https://open-compass.github.io/CriticBench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "40025311",
                            "name": "Tian-Lan",
                            "avatar": null,
                            "nickname": "Tian-Lan"
                        },
                        "lookNum": "425",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-02-23 19:41:43",
                        "createDate": "2024-02-23 18:01:59",
                        "desc": {
                            "cn": "CriticBenchæ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œæ—¨åœ¨å…¨é¢å¯é åœ°è¯„ä¼°LLMçš„å››ä¸ªå…³é”®æ‰¹åˆ¤èƒ½åŠ›ç»´åº¦ã€‚CriticBenchåŒ…æ‹¬ä¹é¡¹ä¸åŒçš„ä»»åŠ¡ï¼Œæ¯é¡¹ä»»åŠ¡éƒ½è¯„ä¼°LLMåœ¨ä¸åŒè´¨é‡ç²’åº¦æ°´å¹³ä¸Šå¯¹å“åº”è¿›è¡Œæ‰¹è¯„çš„èƒ½åŠ›ã€‚",
                            "en": "CriticBench, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. CriticBench encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/criticbench'. Error: Path opencompass/criticbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1148",
                    "name": "AbsPyramid",
                    "version": "1.0.0",
                    "description": "ABSPYRAMID is a unified entailment graph of 221K textual descriptions of abstraction knowledge. ABSPYRAMID collects abstract knowledge for three components\nof diverse events to comprehensively evaluate the abstraction ability of language\nmodels in the open domain.",
                    "url": "opencompass/opencompass_1148.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1148",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1148",
                        "name": "AbsPyramid",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NAACL 2024",
                                "en": "NAACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/HKUST-KnowComp/AbsPyramid",
                        "paperLink": "https://aclanthology.org/2024.findings-naacl.252.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50109576",
                            "name": "HKUST-KnowComp",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50109576-bd7795e6-da4b-40f0-a4e6-979ddb3230c2.png",
                            "nickname": "OpenXLab-dJloo3kUv"
                        },
                        "lookNum": "415",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-27 17:22:48",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-27 17:22:48",
                        "createDate": "2024-10-15 14:04:22",
                        "desc": {
                            "cn": "ABSPYRAMID æ˜¯åŒ…å« 221,000 æ¡æ–‡æœ¬æè¿°çš„æŠ½è±¡çŸ¥è¯†,æ”¶é›†äº†å¤šç§äº‹ä»¶çš„ä¸‰ä¸ªç»„æˆéƒ¨åˆ†çš„æŠ½è±¡çŸ¥è¯†ï¼Œä»¥å…¨é¢è¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨å¼€æ”¾åŸŸä¸­çš„æŠ½è±¡èƒ½åŠ›ã€‚",
                            "en": "ABSPYRAMID is a unified entailment graph of 221K textual descriptions of abstraction knowledge. ABSPYRAMID collects abstract knowledge for three components\nof diverse events to comprehensively evaluate the abstraction ability of language\nmodels in the open domain."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/abspyramid'. Error: Path opencompass/abspyramid is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1538",
                    "name": "SuperGPQA",
                    "version": "1.0.0",
                    "description": "SuperGPQA, a comprehensive benchmark designed to evaluate the knowledge and reasoning abilities of Large Language Models (LLMs) across 285 graduate-level disciplines. SuperGPQA features at least 50 questions per discipline, covering a broad spectrum of graduate-level topics.",
                    "url": "opencompass/opencompass_1538.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1538",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {
                        "gen": "SuperGPQA_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1538",
                        "name": "SuperGPQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å­¦ç§‘",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SuperGPQA/SuperGPQA/",
                        "paperLink": "https://arxiv.org/abs/2502.14739",
                        "officialWebsiteLink": "https://supergpqa.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "412",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-02-26 18:52:28",
                        "supportOnlineEval": false,
                        "updateDate": "2025-02-26 18:52:28",
                        "createDate": "2025-02-26 09:51:42",
                        "desc": {
                            "cn": "SuperGPQAï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ 285 ä¸ªç ”ç©¶ç”Ÿå­¦ç§‘é¢†åŸŸçš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›çš„å…¨é¢åŸºå‡†ã€‚SuperGPQA æ¯ä¸ªå­¦ç§‘è‡³å°‘åŒ…å« 50 ä¸ªé—®é¢˜ï¼Œæ¶µç›–å¹¿æ³›çš„ç¡•å£«ç ”ç©¶ç”Ÿå­¦ç§‘ä¸»é¢˜ã€‚",
                            "en": "SuperGPQA, a comprehensive benchmark designed to evaluate the knowledge and reasoning abilities of Large Language Models (LLMs) across 285 graduate-level disciplines. SuperGPQA features at least 50 questions per discipline, covering a broad spectrum of graduate-level topics."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/supergpqa'. Error: Path opencompass/supergpqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1069",
                    "name": "AIR-Bench",
                    "version": "1.0.0",
                    "description": "AIR-Bench is the first benchmark designed to evaluate the\nability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format.",
                    "url": "opencompass/opencompass_1069.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1069",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1069",
                        "name": "AIR-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OFA-Sys/AIR-Bench",
                        "paperLink": "https://arxiv.org/pdf/2402.07729",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50186884",
                            "name": "OFA-Sys",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186884-c56d3538-0c00-4d32-86ed-c28d0817e429.jfif",
                            "nickname": "OFA-Sys"
                        },
                        "lookNum": "401",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-27 16:53:47",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-27 16:53:47",
                        "createDate": "2024-09-26 16:07:49",
                        "desc": {
                            "cn": "AIR-Bench æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨è¯„ä¼° LALMs ç†è§£å„ç§éŸ³é¢‘ä¿¡å·ï¼ˆåŒ…æ‹¬äººç±»è¯­è¨€ã€è‡ªç„¶å£°éŸ³å’ŒéŸ³ä¹ï¼‰èƒ½åŠ›çš„åŸºå‡†ï¼Œå¹¶è¿›ä¸€æ­¥è¯„ä¼°å…¶ä»¥æ–‡æœ¬å½¢å¼ä¸äººç±»äº’åŠ¨çš„èƒ½åŠ›ã€‚AIR-Bench åŒ…æ‹¬ä¸¤ä¸ªç»´åº¦ï¼šåŸºç¡€åŸºå‡†å’ŒèŠå¤©åŸºå‡†ã€‚å‰è€…ç”±19ä¸ªä»»åŠ¡ç»„æˆï¼ŒåŒ…å«çº¦19,000ä¸ªå•é€‰é¢˜ï¼Œæ—¨åœ¨æ£€æŸ¥LALMsçš„åŸºæœ¬å•ä»»åŠ¡èƒ½åŠ›ã€‚åè€…åŒ…å«2,000ä¸ªå¼€æ”¾å¼é—®ç­”æ•°æ®å®ä¾‹ï¼Œç›´æ¥è¯„ä¼°æ¨¡å‹å¯¹å¤æ‚éŸ³é¢‘çš„ç†è§£åŠå…¶éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ã€‚",
                            "en": "AIR-Bench is the first benchmark designed to evaluate the\nability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/air_bench'. Error: Path opencompass/air_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1087",
                    "name": "Reveal",
                    "version": "1.0.0",
                    "description": "REVEAL(ReasoningVerification Evaluation) is a new dataset to benchmark automatic verifiers of complex Chain-ofThought reasoning in open-domain question answering settings. ",
                    "url": "opencompass/opencompass_1087.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1087",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1087",
                        "name": "Reveal",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://reveal-dataset.github.io/",
                        "paperLink": "https://arxiv.org/pdf/2402.00559",
                        "officialWebsiteLink": "https://reveal-dataset.github.io/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "400",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-30 15:32:59",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-30 15:32:59",
                        "createDate": "2024-09-29 14:26:40",
                        "desc": {
                            "cn": "Reveal æ˜¯ä¸€ä¸ªç”¨äºåŸºå‡†æµ‹è¯•å¼€æ”¾åŸŸé—®ç­”ç¯å¢ƒä¸­å¤æ‚é“¾å¼æ¨ç†è‡ªåŠ¨éªŒè¯å™¨çš„æ–°æ•°æ®é›†ã€‚Reveal åŒ…å«å…³äºè¯­è¨€æ¨¡å‹ç­”æ¡ˆä¸­æ¯ä¸ªæ¨ç†æ­¥éª¤çš„ç›¸å…³æ€§ã€è¯æ®æ®µè½çš„å½’å› å’Œé€»è¾‘æ­£ç¡®æ€§çš„å…¨é¢æ ‡ç­¾ï¼Œæ¶µç›–å¤šç§æ•°æ®é›†å’Œæœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ã€‚",
                            "en": "REVEAL(ReasoningVerification Evaluation) is a new dataset to benchmark automatic verifiers of complex Chain-ofThought reasoning in open-domain question answering settings. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/reveal'. Error: Path opencompass/reveal is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1129",
                    "name": "WikiSQL",
                    "version": "1.0.0",
                    "description": "WikiSQL is a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia that is an order of magnitude larger than comparable datasets.",
                    "url": "opencompass/opencompass_1129.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1129",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1129",
                        "name": "WikiSQL",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/salesforce/WikiSQL",
                        "paperLink": "https://arxiv.org/pdf/1709.00103v7",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "393",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-16 11:03:06",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-16 11:03:06",
                        "createDate": "2024-10-11 15:27:39",
                        "desc": {
                            "cn": "WikiSQL æ˜¯ä¸€ä¸ªåŒ…å« 80,654 ä¸ªæ‰‹åŠ¨æ ‡æ³¨ç¤ºä¾‹çš„é—®é¢˜å’Œ SQL æŸ¥è¯¢çš„æ•°æ®é›†ï¼Œåˆ†å¸ƒåœ¨æ¥è‡ªç»´åŸºç™¾ç§‘çš„ 24,241 ä¸ªè¡¨æ ¼ä¸­ã€‚",
                            "en": "WikiSQL is a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia that is an order of magnitude larger than comparable datasets."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/wikisql'. Error: Path opencompass/wikisql is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1351",
                    "name": "AgentHarm",
                    "version": "1.0.0",
                    "description": "AgentHarm tests the robustness of LLMs to jailbreak attacks. It includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment.",
                    "url": "opencompass/opencompass_1351.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1351",
                    "sample_count": 1000,
                    "traits": [
                        "Safety",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1351",
                        "name": "AgentHarm",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/UKGovernmentBEIS/inspect_evals/tree/main/src/inspect_evals/agentharm",
                        "paperLink": "https://arxiv.org/abs/2410.09024",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "392",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:28",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:28",
                        "createDate": "2025-01-09 16:22:26",
                        "desc": {
                            "cn": "AgentHarmç”¨äºè¯„ä¼°LLMæ™ºèƒ½ä½“å¯¹è¶Šç‹±æ”»å‡»çš„é²æ£’æ€§ï¼ŒåŒ…æ‹¬110å¥—æ¶æ„æ™ºèƒ½ä½“ä»»åŠ¡ï¼ˆå…¶ä¸­æœ‰440ä¸ªå¼ºåŒ–ä»»åŠ¡ï¼‰ï¼Œæ¶µç›–æ¬ºè¯ˆã€ç½‘ç»œçŠ¯ç½ªå’Œéªšæ‰°ç­‰11ä¸ªå±å®³ç±»åˆ«ã€‚",
                            "en": "AgentHarm tests the robustness of LLMs to jailbreak attacks. It includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/agentharm'. Error: Path opencompass/agentharm is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1076",
                    "name": "PCA-Bench",
                    "version": "1.0.0",
                    "description": "PCA-Bench is a multimodal decisionmaking benchmark for evaluating the integrated capabilities of Multimodal Large Language Models (MLLMs). Departing from previous\nbenchmarks focusing on simplistic tasks and individual model capability.",
                    "url": "opencompass/opencompass_1076.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1076",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1076",
                        "name": "PCA-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/pkunlp-icler/PCA-EVAL",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.64.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50188713",
                            "name": "PKUNLP-ICLER",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50188713-85c05c22-aa65-44f2-b06f-aa859ed819eb.png",
                            "nickname": "PKUNLP-ICLER"
                        },
                        "lookNum": "383",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-27 17:50:33",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-27 17:50:33",
                        "createDate": "2024-09-27 15:11:18",
                        "desc": {
                            "cn": "PCA-Bench æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å†³ç­–åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç»¼åˆèƒ½åŠ›ã€‚ä¸ä¹‹å‰ä¸“æ³¨äºç®€å•ä»»åŠ¡å’Œå•ä¸ªæ¨¡å‹èƒ½åŠ›çš„åŸºå‡†ä¸åŒï¼ŒPCA-Bench å¼•å…¥äº†ä¸‰ä¸ªå¤æ‚åœºæ™¯ï¼šè‡ªåŠ¨é©¾é©¶ã€å®¶åº­æœºå™¨äººå’Œå¼€æ”¾ä¸–ç•Œæ¸¸æˆã€‚",
                            "en": "PCA-Bench is a multimodal decisionmaking benchmark for evaluating the integrated capabilities of Multimodal Large Language Models (MLLMs). Departing from previous\nbenchmarks focusing on simplistic tasks and individual model capability."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/pca_bench'. Error: Path opencompass/pca_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1098",
                    "name": "GrailQA",
                    "version": "1.0.0",
                    "description": "GrailQA is a new large-scale, high-quality dataset for question answering on knowledge bases (KBQA) on Freebase with 64,331 questions annotated with both answers and corresponding logical forms in different syntax (i.e., SPARQL, S-expression, etc.). ",
                    "url": "opencompass/opencompass_1098.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1098",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1098",
                        "name": "GrailQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/dki-lab/GrailQA",
                        "paperLink": "https://arxiv.org/pdf/2011.07743",
                        "officialWebsiteLink": "https://dki-lab.github.io/GrailQA/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "383",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-09 20:03:43",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-09 20:03:43",
                        "createDate": "2024-10-09 11:14:58",
                        "desc": {
                            "cn": "GrailQA æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ï¼Œç”¨äºçŸ¥è¯†åº“é—®ç­”ï¼ŒåŒ…å« 64,331 ä¸ªé—®é¢˜ï¼Œå¹¶é™„æœ‰ç­”æ¡ˆå’Œä¸åŒè¯­æ³•çš„ç›¸åº”é€»è¾‘å½¢å¼ã€‚",
                            "en": "GrailQA is a new large-scale, high-quality dataset for question answering on knowledge bases (KBQA) on Freebase with 64,331 questions annotated with both answers and corresponding logical forms in different syntax (i.e., SPARQL, S-expression, etc.). "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/grailqa'. Error: Path opencompass/grailqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1219",
                    "name": "CS-Eval",
                    "version": "1.0.0",
                    "description": "CS-Eval is a large language model cybersecurity capability evaluation suite jointly established by Alibaba Security, Fudan University, and the University of Chinese Academy of Sciences. The dataset encompasses 11 major cybersecurity categories and 42 subcategories, offering comprehensive assessment ",
                    "url": "opencompass/opencompass_1219.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1219",
                    "sample_count": 1000,
                    "traits": [
                        "Examination",
                        "Knowledge",
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1219",
                        "name": "CS-Eval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å­¦ç§‘",
                                "en": "Examination"
                            },
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            },
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/CS-EVAL/CS-Eval",
                        "paperLink": "https://arxiv.org/pdf/2411.16239",
                        "officialWebsiteLink": "https://cs-eval.com",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "41601314",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-JdyYs9Oa3"
                        },
                        "lookNum": "383",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-11-27 16:53:44",
                        "supportOnlineEval": false,
                        "updateDate": "2024-11-27 16:53:44",
                        "createDate": "2024-11-26 10:42:49",
                        "desc": {
                            "cn": "CS-Eval æ˜¯ç”±é˜¿é‡Œå®‰å…¨ã€å¤æ—¦å¤§å­¦å’Œä¸­å›½ç§‘å­¦é™¢å¤§å­¦è”åˆå»ºç«‹çš„å¤§æ¨¡å‹ç½‘ç»œå®‰å…¨èƒ½åŠ›è¯„æµ‹é›†ã€‚æ•°æ®é›†è¦†ç›–11ä¸ªç½‘ç»œå®‰å…¨å¤§ç±»é¢†åŸŸã€42ä¸ªå­ç±»é¢†åŸŸï¼Œæä¾›çŸ¥è¯†å‹å’Œå®æˆ˜å‹çš„ç»¼åˆè¯„ä¼°ä»»åŠ¡ï¼Œæ”¯æŒç”¨æˆ·è‡ªä¸»è¯„æµ‹ï¼ŒåŒæ—¶ä¸ºå¤§æ¨¡å‹è½åœ°ç½‘ç»œå®‰å…¨æä¾›å‚è€ƒå’Œå¯å‘ã€‚\n",
                            "en": "CS-Eval is a large language model cybersecurity capability evaluation suite jointly established by Alibaba Security, Fudan University, and the University of Chinese Academy of Sciences. The dataset encompasses 11 major cybersecurity categories and 42 subcategories, offering comprehensive assessment "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cs_eval'. Error: Path opencompass/cs_eval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1084",
                    "name": "StableToolBench",
                    "version": "1.0.0",
                    "description": "StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status.",
                    "url": "opencompass/opencompass_1084.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1084",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1084",
                        "name": "StableToolBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/THUNLP-MT/StableToolBench",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.664.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50186303",
                            "name": "THUNLP-MT",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186303-a2701a3a-208f-4cc9-88b7-e09515c70865.png",
                            "nickname": "THUNLP-MT"
                        },
                        "lookNum": "377",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-29 16:24:50",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-29 16:24:50",
                        "createDate": "2024-09-29 13:09:10",
                        "desc": {
                            "cn": "StableToolBench æ˜¯ä¸€ä¸ªä» ToolBench å‘å±•è€Œæ¥çš„åŸºå‡†ï¼Œæå‡ºäº†ä¸€ä¸ªè™šæ‹Ÿ API æœåŠ¡å™¨å’Œç¨³å®šçš„è¯„ä¼°ç³»ç»Ÿã€‚è™šæ‹Ÿ API æœåŠ¡å™¨åŒ…å«ä¸€ä¸ªç¼“å­˜ç³»ç»Ÿå’Œ API æ¨¡æ‹Ÿå™¨ï¼Œè¿™äº›ç»„ä»¶ç›¸è¾…ç›¸æˆï¼Œä»¥ç¼“è§£ API çŠ¶æ€å˜åŒ–å¸¦æ¥çš„å½±å“ã€‚",
                            "en": "StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/stabletoolbench'. Error: Path opencompass/stabletoolbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1152",
                    "name": "SportQA",
                    "version": "1.0.0",
                    "description": "SportQA aims to evaluate LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenariobased reasoning tasks.",
                    "url": "opencompass/opencompass_1152.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1152",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1152",
                        "name": "SportQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NAACL 2024",
                                "en": "NAACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/haotianxia/SportQA",
                        "paperLink": "https://aclanthology.org/2024.naacl-long.283.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "372",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-27 17:23:16",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-27 17:23:16",
                        "createDate": "2024-10-15 15:51:00",
                        "desc": {
                            "cn": "SportQA ä¸“é—¨ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä½“è‚²ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚SportQA åŒ…å«è¶…è¿‡ 70,000 é“å¤šé¡¹é€‰æ‹©é¢˜ï¼Œåˆ†ä¸ºä¸‰ä¸ªä¸åŒçš„éš¾åº¦çº§åˆ«ï¼Œé’ˆå¯¹ä»åŸºæœ¬å†å²äº‹å®åˆ°å¤æ‚æƒ…å¢ƒæ¨ç†ä»»åŠ¡çš„å„ç§ä½“è‚²çŸ¥è¯†ã€‚",
                            "en": "SportQA aims to evaluate LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenariobased reasoning tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/sportqa'. Error: Path opencompass/sportqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1287",
                    "name": "MedBench",
                    "version": "1.0.0",
                    "description": "MedBench is committed to building a scientific, fair and rigorous Chinese medical model evaluation system and open platform. Based on authoritative standards, we constantly update and maintain high-quality datasets, and comprehensively quantify capabilities of models in various medical dimensions.",
                    "url": "opencompass/opencompass_1287.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1287",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1287",
                        "name": "MedBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Medical",
                                "en": "Medical"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/open-compass/opencompass/tree/main/opencompass/datasets/medbench/",
                        "paperLink": "https://www.sciopen.com/article/10.26599/BDMA.2024.9020044",
                        "officialWebsiteLink": "https://medbench.opencompass.org.cn/home",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "368",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-27 14:45:32",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-27 14:45:32",
                        "createDate": "2024-12-27 14:01:19",
                        "desc": {
                            "cn": "MedBenchè‡´åŠ›äºæ‰“é€ ä¸€ä¸ªç§‘å­¦ã€å…¬å¹³ä¸”ä¸¥è°¨çš„ä¸­æ–‡åŒ»ç–—å¤§æ¨¡å‹è¯„æµ‹ä½“ç³»åŠå¼€æ”¾å¹³å°ã€‚æˆ‘ä»¬åŸºäºåŒ»å­¦æƒå¨æ ‡å‡†ï¼Œä¸æ–­æ›´æ–°ç»´æŠ¤é«˜è´¨é‡çš„åŒ»å­¦æ•°æ®é›†ï¼Œå…¨æ–¹ä½å¤šç»´åº¦é‡åŒ–æ¨¡å‹åœ¨å„ä¸ªåŒ»å­¦ç»´åº¦çš„èƒ½åŠ›ã€‚",
                            "en": "MedBench is committed to building a scientific, fair and rigorous Chinese medical model evaluation system and open platform. Based on authoritative standards, we constantly update and maintain high-quality datasets, and comprehensively quantify capabilities of models in various medical dimensions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medbench'. Error: Path opencompass/medbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1125",
                    "name": "Mind2Web",
                    "version": "1.0.0",
                    "description": "MIND2WEB is the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website.",
                    "url": "opencompass/opencompass_1125.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1125",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1125",
                        "name": "Mind2Web",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OSU-NLP-Group/Mind2Web",
                        "paperLink": "https://arxiv.org/pdf/2306.06070",
                        "officialWebsiteLink": "https://osu-nlp-group.github.io/Mind2Web/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "361",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-11 20:24:41",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-11 20:24:41",
                        "createDate": "2024-10-11 13:51:20",
                        "desc": {
                            "cn": "MIND2WEB æ˜¯é¦–ä¸ªç”¨äºå¼€å‘å’Œè¯„ä¼°é€šç”¨ç½‘é¡µä»£ç†çš„æ•°æ®é›†ï¼Œèƒ½å¤Ÿæ ¹æ®è¯­è¨€æŒ‡ä»¤åœ¨ä»»ä½•ç½‘ç«™ä¸Šå®Œæˆå¤æ‚ä»»åŠ¡ã€‚",
                            "en": "MIND2WEB is the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mind2web'. Error: Path opencompass/mind2web is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1328",
                    "name": "GTA",
                    "version": "1.0.0",
                    "description": "GTA is meant for LLM's tool-use evaluations under real-world scenarios, including 229 real-world tasks and executable tool chains.",
                    "url": "opencompass/opencompass_1328.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1328",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1328",
                        "name": "GTA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/open-compass/GTA",
                        "paperLink": "https://arxiv.org/abs/2407.08713",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "355",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 16:32:41",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 16:32:41",
                        "createDate": "2025-01-03 11:35:50",
                        "desc": {
                            "cn": "GTAç”¨äºè¯„ä¼°LLMè°ƒç”¨å·¥å…·è§£å†³å®é™…é—®é¢˜çš„èƒ½åŠ›ï¼Œç”±229ä¸ªçœŸå®ä»»åŠ¡å’Œå¯æ‰§è¡Œå·¥å…·é“¾ç»„æˆã€‚",
                            "en": "GTA is meant for LLM's tool-use evaluations under real-world scenarios, including 229 real-world tasks and executable tool chains."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gta'. Error: Path opencompass/gta is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1142",
                    "name": "MIRAGE",
                    "version": "1.0.0",
                    "description": "MIRAGE is a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. ",
                    "url": "opencompass/opencompass_1142.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1142",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1142",
                        "name": "MIRAGE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://teddy-xionggz.github.io/benchmark-medical-rag/",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.372.pdf",
                        "officialWebsiteLink": "https://teddy-xionggz.github.io/benchmark-medical-rag/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "351",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-19 18:04:07",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-19 18:04:07",
                        "createDate": "2024-10-14 18:05:57",
                        "desc": {
                            "cn": "MIRAGE æ˜¯é¦–ä¸ªæ­¤ç±»åŸºå‡†ï¼ŒåŒ…å«æ¥è‡ªäº”ä¸ªåŒ»å­¦é—®ç­”æ•°æ®é›†çš„ 7,663 ä¸ªé—®é¢˜ã€‚",
                            "en": "MIRAGE is a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mirage'. Error: Path opencompass/mirage is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1245",
                    "name": "KOR-Bench",
                    "version": "1.0.0",
                    "description": "Knowledge-Orthogonal Reasoning Benchmark (KOR-Bench) encompasses five task categories: Operation, Logic, Cipher, Puzzle, and Counterfactual. KOR-Bench emphasizes the effectiveness of models in applying new rule descriptions to solve novel rule-driven questions. ",
                    "url": "opencompass/opencompass_1245.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1245",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1245",
                        "name": "KOR-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/KOR-Bench/KOR-Bench",
                        "paperLink": "https://arxiv.org/abs/2410.06526",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "332",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 11:12:43",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 11:12:43",
                        "createDate": "2025-01-10 19:42:40",
                        "desc": {
                            "cn": "KOR-Benchç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬äº”ä¸ªä»»åŠ¡ç±»åˆ«ï¼šæ“ä½œã€é€»è¾‘ã€å¯†ç ã€æ‹¼å›¾å’Œåäº‹å®ã€‚",
                            "en": "Knowledge-Orthogonal Reasoning Benchmark (KOR-Bench) encompasses five task categories: Operation, Logic, Cipher, Puzzle, and Counterfactual. KOR-Bench emphasizes the effectiveness of models in applying new rule descriptions to solve novel rule-driven questions. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/kor_bench'. Error: Path opencompass/kor_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1359",
                    "name": "SEED-Bench",
                    "version": "1.0.0",
                    "description": "SEED-Bench aims at the evaluation of generative comprehension in MLLMs, consisting of 19K multiple choice questions, which spans 12 evaluation dimensions including the comprehension of both the image and video modality. ",
                    "url": "opencompass/opencompass_1359.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1359",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1359",
                        "name": "SEED-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AILab-CVC/SEED-Bench",
                        "paperLink": "https://arxiv.org/abs/2307.16125",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "323",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:55",
                        "createDate": "2025-01-09 21:34:54",
                        "desc": {
                            "cn": "SEED-Benchç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„ç†è§£èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¯¹å›¾åƒå’Œè§†é¢‘çš„ç†è§£ï¼Œç”±è·¨è¶Š12ä¸ªè¯„ä¼°ç»´åº¦çš„19Ké“å¤šé¡¹é€‰æ‹©é¢˜ç»„æˆã€‚",
                            "en": "SEED-Bench aims at the evaluation of generative comprehension in MLLMs, consisting of 19K multiple choice questions, which spans 12 evaluation dimensions including the comprehension of both the image and video modality. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/seed_bench'. Error: Path opencompass/seed_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1248",
                    "name": "MMMU",
                    "version": "1.0.0",
                    "description": "MMMU is a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks.",
                    "url": "opencompass/opencompass_1248.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1248",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1248",
                        "name": "MMMU",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MMMU-Benchmark/MMMU",
                        "paperLink": "https://arxiv.org/abs/2311.16502",
                        "officialWebsiteLink": "https://mmmu-benchmark.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "319",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-22 18:09:32",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-22 18:09:32",
                        "createDate": "2024-12-20 19:45:07",
                        "desc": {
                            "cn": "MMMUç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨å¤æ‚å¤šå­¦ç§‘ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬ä»å¤§å­¦è€ƒè¯•å’Œæ•™ç§‘ä¹¦ä¸­ç²¾å¿ƒæ”¶é›†çš„11.5Kå¤šæ¨¡æ€é—®é¢˜ï¼Œæ¶µç›–å…­ä¸ªæ ¸å¿ƒå­¦ç§‘ï¼šè‰ºæœ¯ä¸è®¾è®¡ã€å•†ä¸šã€ç§‘å­¦ã€å¥åº·ä¸åŒ»å­¦ã€äººæ–‡ä¸ç¤¾ä¼šç§‘å­¦ä»¥åŠæŠ€æœ¯ä¸å·¥ç¨‹ã€‚",
                            "en": "MMMU is a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmmu'. Error: Path opencompass/mmmu is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1115",
                    "name": "MathQA",
                    "version": "1.0.0",
                    "description": "MathQA is a new large-scale, diverse dataset of 37k English multiple-choice math word problems covering multiple math domain categories by modeling operation programs\ncorresponding to word problems in the AQuA dataset.",
                    "url": "opencompass/opencompass_1115.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1115",
                    "sample_count": 1000,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1115",
                        "name": "MathQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://math-qa.github.io/math-QA/",
                        "paperLink": "https://arxiv.org/pdf/1905.13319v1",
                        "officialWebsiteLink": "https://math-qa.github.io/math-QA/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "317",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-11 19:59:27",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-11 19:59:27",
                        "createDate": "2024-10-10 14:14:12",
                        "desc": {
                            "cn": "MathQA æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼ŒåŒ…å« 37,000 é“è‹±è¯­å¤šé¡¹é€‰æ‹©æ•°å­¦æ–‡å­—é—®é¢˜ï¼Œæ¶µç›–å¤šä¸ªæ•°å­¦é¢†åŸŸç±»åˆ«ã€‚",
                            "en": "MathQA is a new large-scale, diverse dataset of 37k English multiple-choice math word problems covering multiple math domain categories by modeling operation programs\ncorresponding to word problems in the AQuA dataset."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mathqa'. Error: Path opencompass/mathqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1244",
                    "name": "Omni-MATH",
                    "version": "1.0.0",
                    "description": "Omni-MATH focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels",
                    "url": "opencompass/opencompass_1244.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1244",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1244",
                        "name": "Omni-MATH",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/KbsdJames/Omni-MATH",
                        "paperLink": "https://arxiv.org/abs/2410.07985",
                        "officialWebsiteLink": "https://omni-math.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "313",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-30 16:29:59",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-30 16:29:59",
                        "createDate": "2024-12-30 16:20:58",
                        "desc": {
                            "cn": "Omni-MATHç”¨äºè¯„ä¼°LLMåœ¨å¥¥æ—åŒ¹å…‹æ°´å¹³ä¸Šçš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬4428é“ç«èµ›çº§é—®é¢˜ï¼Œå¹¶å¸¦æœ‰ä¸¥æ ¼çš„äººå·¥æ³¨é‡Šã€‚è¿™äº›é—®é¢˜è¢«ç²¾å¿ƒåˆ†ç±»ä¸ºè¶…è¿‡33ä¸ªå­é¢†åŸŸï¼Œæ¶µç›–10å¤šä¸ªä¸åŒçš„éš¾åº¦çº§åˆ«ã€‚",
                            "en": "Omni-MATH focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/omni_math'. Error: Path opencompass/omni_math is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1091",
                    "name": "RoleLLM",
                    "version": "1.0.0",
                    "description": "RoleLLM is a role-playing framework of data construction and evaluation (RoleBench), as well as solutions for both closed-source and open-source models (RoleGPT, RoleLLaMA, RoleGLM). We also propose Context-Instruct for long-text knowledge extraction and role-specific knowledge injection.",
                    "url": "opencompass/opencompass_1091.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1091",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1091",
                        "name": "RoleLLM",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/InteractiveNLP-Team/RoleLLM-public",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.878.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50187516",
                            "name": "InteractiveNLP-Team",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50187516-87c7afab-16ed-48c4-8147-1298485c7231.png",
                            "nickname": "InteractiveNLP-Team"
                        },
                        "lookNum": "310",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-30 15:54:49",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-30 15:54:49",
                        "createDate": "2024-09-29 16:37:55",
                        "desc": {
                            "cn": "RoleLLM æ˜¯ä¸€ä¸ªè§’è‰²æ‰®æ¼”çš„æ•°æ®æ„å»ºå’Œè¯„ä¼°æ¡†æ¶ï¼ŒåŒæ—¶æä¾›é—­æºå’Œå¼€æºæ¨¡å‹çš„è§£å†³æ–¹æ¡ˆï¼ˆRoleGPTã€RoleLLaMAã€RoleGLMï¼‰ã€‚",
                            "en": "RoleLLM is a role-playing framework of data construction and evaluation (RoleBench), as well as solutions for both closed-source and open-source models (RoleGPT, RoleLLaMA, RoleGLM). We also propose Context-Instruct for long-text knowledge extraction and role-specific knowledge injection."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/rolellm'. Error: Path opencompass/rolellm is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1413",
                    "name": "LiveCodeBench",
                    "version": "1.0.0",
                    "description": "LiveCodeBench evaluates LLMs' coding abilities. It continuously collects new problems over time from contests across LeetCode, AtCoder, and CodeForces. Notably, it also focuses on a broader range of code related capabilities besides code generation.",
                    "url": "opencompass/opencompass_1413.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1413",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1413",
                        "name": "LiveCodeBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/LiveCodeBench/LiveCodeBench",
                        "paperLink": "https://arxiv.org/abs/2403.07974",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "309",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:47:04",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:47:04",
                        "createDate": "2025-01-17 20:40:50",
                        "desc": {
                            "cn": "LiveCodeBenchç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„ä»£ç èƒ½åŠ›ï¼ŒåŒ…å«æ¥è‡ªLeetCodeã€AtCoderå’ŒCodeForcesçš„åŠ¨æ€æ›´æ–°çš„é—®é¢˜ï¼Œå¹¶åœ¨ä»£ç ç”Ÿæˆèƒ½åŠ›çš„åŸºç¡€ä¸Šå°†æ›´å¹¿æ³›çš„ç›¸å…³èƒ½åŠ›çº³å…¥è€ƒé‡ã€‚",
                            "en": "LiveCodeBench evaluates LLMs' coding abilities. It continuously collects new problems over time from contests across LeetCode, AtCoder, and CodeForces. Notably, it also focuses on a broader range of code related capabilities besides code generation."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/livecodebench'. Error: Path opencompass/livecodebench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1109",
                    "name": "WinoGrande",
                    "version": "1.0.0",
                    "description": "WINOGRANDE is a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset.",
                    "url": "opencompass/opencompass_1109.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1109",
                    "sample_count": 1767,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {
                        "gen": "winogrande_gen",
                        "ppl": "winogrande_ppl",
                        "ll": "winogrande_ll"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 100,
                        "estimated_output_tokens": 50
                    },
                    "original_data": {
                        "id": "1109",
                        "name": "WinoGrande",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/allenai/winogrande",
                        "paperLink": "https://arxiv.org/pdf/1907.10641",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "305",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-09 20:03:23",
                        "supportOnlineEval": true,
                        "updateDate": "2024-10-09 20:03:23",
                        "createDate": "2024-10-09 18:41:18",
                        "desc": {
                            "cn": "WINOGRANDE åŒ…å« 44,000 ä¸ªé—®é¢˜ï¼Œå—åˆ° WSC è®¾è®¡çš„å¯å‘ï¼Œä½†è¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥æé«˜æ•°æ®é›†çš„è§„æ¨¡å’Œéš¾åº¦ã€‚",
                            "en": "WINOGRANDE is a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "{\"qID\": \"3087LXLJ6OUC8YHY6A67DCJQIOA0FY-1\", \"sentence\": \"Eric was disappointed to see that Ryan had formica countertops, because _ thought formica was tacky.\", \"option1\": \"Eric\", \"option2\": \"Ryan\"}",
                                    "difficulty": "easy",
                                    "task_type": "qa, comprehension"
                                },
                                {
                                    "question": "{\"qID\": \"3TFJJUELSHNX771VAX8KWW3TOWSC24-2\", \"sentence\": \"Rachel was the victim of an assault by Elena. _ had was taken to the jail by the police.\", \"option1\": \"Rachel\", \"option2\": \"Elena\"}",
                                    "difficulty": "easy",
                                    "task_type": "comprehension, reasoning"
                                },
                                {
                                    "question": "{\"qID\": \"3DQYSJDTYNPSZANDEBB848L0FTSXEF-1\", \"sentence\": \"The cute little dog was the one Craig gave Donald after _ found out he was allergic to dogs.\", \"option1\": \"Craig\", \"option2\": \"Donald\"}",
                                    "difficulty": "moderate",
                                    "task_type": "qa"
                                }
                            ],
                            "total_questions": 1767,
                            "question_format": "comprehension, contextual_comprehension, fill_in_the_blank, qa, reasoning",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Evaluates critical thinking, comprehension, reading_comprehension skills",
                                "Covers multiple difficulty levels: Beginner, Intermediate",
                                "Diverse task types: comprehension, reasoning, fill_in_the_blank"
                            ],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": "analysis/opencompass_1109_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 5,
                        "successful": 5,
                        "failed": 0
                    }
                },
                {
                    "id": "opencompass_1133",
                    "name": "Spider",
                    "version": "1.0.0",
                    "description": "Spider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students. The goal of the Spider challenge is to develop natural language interfaces to cross-domain databases. ",
                    "url": "opencompass/opencompass_1133.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1133",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1133",
                        "name": "Spider",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/taoyds/spider/tree/master/evaluation_examples",
                        "paperLink": "https://arxiv.org/pdf/1809.08887v5",
                        "officialWebsiteLink": "https://yale-lily.github.io/spider",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "305",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-16 11:03:12",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-16 11:03:12",
                        "createDate": "2024-10-12 13:37:46",
                        "desc": {
                            "cn": "Spider æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤æ‚ä¸”è·¨é¢†åŸŸçš„è¯­ä¹‰è§£æå’Œæ–‡æœ¬åˆ° SQL æ•°æ®é›†ã€‚Spider æŒ‘æˆ˜çš„ç›®æ ‡æ˜¯å¼€å‘è‡ªç„¶è¯­è¨€æ¥å£ä»¥è®¿é—®è·¨é¢†åŸŸæ•°æ®åº“ã€‚è¯¥æ•°æ®é›†åŒ…å« 10,181 ä¸ªé—®é¢˜å’Œ 5,693 ä¸ªç‹¬ç‰¹çš„å¤æ‚ SQL æŸ¥è¯¢ï¼Œæ¶µç›– 200 ä¸ªåŒ…å«å¤šä¸ªè¡¨çš„æ•°æ®åº“ï¼Œæ¶‰åŠ 138 ä¸ªä¸åŒçš„é¢†åŸŸã€‚",
                            "en": "Spider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students. The goal of the Spider challenge is to develop natural language interfaces to cross-domain databases. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/spider'. Error: Path opencompass/spider is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1207",
                    "name": "MMBench-Video",
                    "version": "1.0.0",
                    "description": "MMBench-Video is a comprehensive video understanding evaluation benchmark that covers long videos, multiple shots, and evaluates the temporal understanding ability of MLLMs. Contains over 600 videos, 16 categories, and manually annotated Q&A pairs.",
                    "url": "opencompass/opencompass_1207.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1207",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1207",
                        "name": "MMBench-Video",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 3,
                        "githubLink": "https://github.com/open-compass/VLMEvalKit",
                        "paperLink": "https://arxiv.org/abs/2406.14515",
                        "officialWebsiteLink": "https://mmbench-video.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "305",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 17:04:03",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 17:04:03",
                        "createDate": "2025-01-21 14:14:14",
                        "desc": {
                            "cn": "MMBench-Videoæ˜¯å…¨é¢è§†é¢‘ç†è§£è¯„æµ‹åŸºå‡†ï¼Œè¦†ç›–é•¿è§†é¢‘ã€å¤šé•œå¤´ï¼Œè¯„ä¼°MLLMsæ—¶åºç†è§£èƒ½åŠ›ã€‚åŒ…å«16ç±»å…±600+è§†é¢‘ä»¥åŠäººå·¥æ ‡æ³¨é—®ç­”å¯¹ã€‚",
                            "en": "MMBench-Video is a comprehensive video understanding evaluation benchmark that covers long videos, multiple shots, and evaluates the temporal understanding ability of MLLMs. Contains over 600 videos, 16 categories, and manually annotated Q&A pairs."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmbench_video'. Error: Path opencompass/mmbench_video is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1772",
                    "name": "S1-Bench",
                    "version": "1.0.0",
                    "description": "S1-Bench is a novel benchmark designed to evaluate the performance of LRMs on simple tasks that are more aligned with intuitive System 1 thinking, rather than deliberate System 2 reasoning. S1-Bench offers a set of simple, diverse, and naturally clear questions across multiple domains and languages.",
                    "url": "opencompass/opencompass_1772.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1772",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Understanding",
                        "Instruct"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1772",
                        "name": "S1-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            },
                            {
                                "cn": "æŒ‡ä»¤è·Ÿéš",
                                "en": "Instruct"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "ç³»ç»Ÿ1",
                                "en": "ç³»ç»Ÿ1"
                            },
                            {
                                "cn": "å¿«æ€è€ƒ",
                                "en": "å¿«æ€è€ƒ"
                            },
                            {
                                "cn": "LRM",
                                "en": "LRM"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/WYRipple/S1_Bench",
                        "paperLink": "https://arxiv.org/abs/2504.10368",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "86305501",
                            "name": "WYRipple",
                            "avatar": "https://thirdwx.qlogo.cn/mmopen/vi_32/skbupBhJ87rXicbT3SlhqHwQCLK486tTT6yJribPhbj5sMzbyU6F17SBErBljjNXtRqGcCd8ZBt0MzIPsEFz0hdWNg7JRf5EL1X5FDkvqC3xU/132",
                            "nickname": "WYRipple"
                        },
                        "lookNum": "303",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-25 17:00:35",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-25 17:00:35",
                        "createDate": "2025-04-24 17:48:23",
                        "desc": {
                            "cn": "S1-Benchæ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§æ¨¡å‹åœ¨ç®€å•ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè¿™äº›ä»»åŠ¡æ›´å€¾å‘äºç›´è§‚çš„ç³»ç»Ÿ1æ€ç»´ï¼Œè€Œéæ·±æ€ç†Ÿè™‘çš„ç³»ç»Ÿ2æ¨ç†ã€‚å°½ç®¡å¤§æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­é€šè¿‡æ˜ç¡®çš„æ€ç»´é“¾å–å¾—äº†æ˜¾è‘—çªç ´ï¼Œä½†å®ƒä»¬å¯¹æ·±åº¦åˆ†ææ€ç»´çš„ä¾èµ–å¯èƒ½é™åˆ¶äº†å…¶ç³»ç»Ÿ1æ€ç»´èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç›®å‰ç¼ºä¹è¯„ä¼°å¤§æ¨¡å‹åœ¨éœ€è¦æ­¤ç±»èƒ½åŠ›çš„ä»»åŠ¡ä¸­è¡¨ç°çš„åŸºå‡†ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼ŒS1-Benchæä¾›äº†ä¸€ç»„ç®€å•ã€å¤šæ ·ä¸”è‡ªç„¶æ¸…æ™°çš„é—®é¢˜ï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸå’Œè¯­è¨€ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å¤§æ¨¡å‹åœ¨æ­¤ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚",
                            "en": "S1-Bench is a novel benchmark designed to evaluate the performance of LRMs on simple tasks that are more aligned with intuitive System 1 thinking, rather than deliberate System 2 reasoning. S1-Bench offers a set of simple, diverse, and naturally clear questions across multiple domains and languages."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/s1_bench'. Error: Path opencompass/s1_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1599",
                    "name": "GAIA",
                    "version": "1.0.0",
                    "description": "GAIA is a benchmark which aims at evaluating next-generation LLMs (LLMs with augmented capabilities due to added tooling, efficient prompting, access to search, etc), mading of more than 450 non-trivial question with an unambiguous answer, requiring different levels of tooling and autonomy to solve.",
                    "url": "opencompass/opencompass_1599.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1599",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1599",
                        "name": "GAIA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2311.12983",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "302",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 11:17:15",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 11:17:15",
                        "createDate": "2025-03-06 11:15:47",
                        "desc": {
                            "cn": "GAIA æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°ä¸‹ä¸€ä»£LLMsï¼ˆç”±äºå¢åŠ äº†å·¥å…·ã€é«˜æ•ˆçš„æç¤ºã€è®¿é—®æœç´¢ç­‰åŠŸèƒ½è€Œå…·æœ‰å¢å¼ºèƒ½åŠ›çš„LLMsï¼‰çš„åŸºå‡†ï¼Œç”±è¶…è¿‡ 450 ä¸ªéå¹³å‡¡é—®é¢˜ç»„æˆï¼Œè¿™äº›é—®é¢˜æœ‰æ˜ç¡®çš„ç­”æ¡ˆï¼Œéœ€è¦ä¸åŒå±‚æ¬¡çš„å·¥å…·å’Œè‡ªä¸»æ€§æ¥è§£å†³ã€‚",
                            "en": "GAIA is a benchmark which aims at evaluating next-generation LLMs (LLMs with augmented capabilities due to added tooling, efficient prompting, access to search, etc), mading of more than 450 non-trivial question with an unambiguous answer, requiring different levels of tooling and autonomy to solve."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gaia'. Error: Path opencompass/gaia is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1266",
                    "name": "BABILong",
                    "version": "1.0.0",
                    "description": "BABILong is designed to test language models' ability to reason across facts distributed in extremely long documents. It contains a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. ",
                    "url": "opencompass/opencompass_1266.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1266",
                    "sample_count": 1000,
                    "traits": [
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1266",
                        "name": "BABILong",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "é•¿æ–‡æœ¬",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/booydar/babilong",
                        "paperLink": "https://arxiv.org/abs/2406.10149",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "302",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 14:14:54",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 14:14:54",
                        "createDate": "2024-12-25 14:14:32",
                        "desc": {
                            "cn": "BABILongæ—¨åœ¨æµ‹è¯•è¯­è¨€æ¨¡å‹å¯¹åˆ†å¸ƒåœ¨æé•¿æ–‡æ¡£ä¸­çš„äº‹å®è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ï¼Œæ¶µç›–äº‹å®é“¾æ¥ã€ç®€å•å½’çº³ã€æ¨å¯¼ã€è®¡æ•°å’Œå¤„ç†åˆ—è¡¨/é›†åˆç­‰20ç§å„ç±»æ¨ç†ä»»åŠ¡ã€‚",
                            "en": "BABILong is designed to test language models' ability to reason across facts distributed in extremely long documents. It contains a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/babilong'. Error: No data found in /home/budadmin/.cache/opencompass/./data/babilong/data/ for split 'test'. Please check if the split exists and contains data files.\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1123",
                    "name": "Crows-Pairs",
                    "version": "1.0.0",
                    "description": "CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping.",
                    "url": "opencompass/opencompass_1123.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1123",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1123",
                        "name": "Crows-Pairs",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/nyu-mll/crows-pairs",
                        "paperLink": "https://arxiv.org/pdf/2010.00133v1",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "271",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-11 20:24:47",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-11 20:24:47",
                        "createDate": "2024-10-11 13:33:41",
                        "desc": {
                            "cn": "CrowS-Pairs åŒ…å« 1508 ä¸ªç¤ºä¾‹ï¼Œæ¶µç›–ä¸ä¹ç§åè§ç±»å‹ç›¸å…³çš„åˆ»æ¿å°è±¡ï¼Œä¾‹å¦‚ç§æ—ã€å®—æ•™å’Œå¹´é¾„ã€‚åœ¨ CrowS-Pairs ä¸­ï¼Œæ¨¡å‹ä¼šæ¥æ”¶åˆ°ä¸¤å¥è¯ï¼šä¸€å¥æ˜¯æ›´å…·åˆ»æ¿å°è±¡çš„ï¼Œå¦ä¸€å¥åˆ™æ˜¯è¾ƒå°‘åˆ»æ¿å°è±¡çš„ã€‚",
                            "en": "CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/crows_pairs'. Error: Path opencompass/crows_pairs is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1542",
                    "name": "StructFlowBench",
                    "version": "1.0.0",
                    "description": "StructFlowBench is a structurally annotated multi-turn benchmark that leverages a structure-driven generation paradigm to enhance the simulation of complex dialogue scenarios.",
                    "url": "opencompass/opencompass_1542.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1542",
                    "sample_count": 1000,
                    "traits": [
                        "Instruct"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1542",
                        "name": "StructFlowBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æŒ‡ä»¤è·Ÿéš",
                                "en": "Instruct"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MLGroupJLU/StructFlowBench",
                        "paperLink": "https://arxiv.org/abs/2502.14494",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "252",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-02-25 19:09:02",
                        "supportOnlineEval": false,
                        "updateDate": "2025-02-25 19:09:02",
                        "createDate": "2025-02-25 12:46:25",
                        "desc": {
                            "cn": "StructFlowBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«155æ¡æ•°æ®çš„ç»“æ„åŒ–æ ‡æ³¨å¤šè½®åŸºå‡†ï¼Œå®ƒåˆ©ç”¨ç»“æ„é©±åŠ¨ç”ŸæˆèŒƒå¼æ¥å¢å¼ºå¤æ‚å¯¹è¯åœºæ™¯çš„æ¨¡æ‹Ÿã€‚",
                            "en": "StructFlowBench is a structurally annotated multi-turn benchmark that leverages a structure-driven generation paradigm to enhance the simulation of complex dialogue scenarios."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/structflowbench'. Error: Path opencompass/structflowbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1701",
                    "name": "WritingBench",
                    "version": "1.0.0",
                    "description": "WritingBench: A Comprehensive Benchmark for Generative Writing",
                    "url": "opencompass/opencompass_1701.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1701",
                    "sample_count": 1000,
                    "traits": [
                        "Long-Context",
                        "Creation"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1701",
                        "name": "WritingBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "é•¿æ–‡æœ¬",
                                "en": "Long-Context"
                            },
                            {
                                "cn": "åˆ›ä½œ",
                                "en": "Creation"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/X-PLUG/WritingBench",
                        "paperLink": "https://arxiv.org/pdf/2503.05244",
                        "officialWebsiteLink": "https://modelscope.cn/studios/iic/DeepWriting",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "52302232",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-qSFSXYARf"
                        },
                        "lookNum": "246",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-03 19:45:43",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-03 19:45:43",
                        "createDate": "2025-04-02 15:22:21",
                        "desc": {
                            "cn": "WritingBench: A Comprehensive Benchmark for Generative Writing",
                            "en": "WritingBench: A Comprehensive Benchmark for Generative Writing"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/writingbench'. Error: Path opencompass/writingbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1132",
                    "name": "TabFact",
                    "version": "1.0.0",
                    "description": "TabFac consists of 117,854 manually annotated statements with regard to 16,573 Wikipedia tables, their relations are classified as ENTAILED and REFUTED. ",
                    "url": "opencompass/opencompass_1132.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1132",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1132",
                        "name": "TabFact",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/wenhuchen/Table-Fact-Checking",
                        "paperLink": "https://arxiv.org/pdf/1909.02164v5",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "245",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-16 11:03:09",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-16 11:03:09",
                        "createDate": "2024-10-12 11:35:24",
                        "desc": {
                            "cn": "TabFac åŒ…å« 117,854 æ¡æ‰‹åŠ¨æ ‡æ³¨çš„è¯­å¥ï¼Œæ¶‰åŠ 16,573 ä¸ªç»´åŸºç™¾ç§‘è¡¨æ ¼ï¼Œæ˜¯ç¬¬ä¸€ä¸ªè¯„ä¼°ç»“æ„åŒ–æ•°æ®ä¸Šè¯­è¨€æ¨ç†çš„æ•°æ®é›†ï¼Œæ¶‰åŠåœ¨ç¬¦å·å’Œè¯­è¨€ä¸¤ä¸ªæ–¹é¢çš„æ··åˆæ¨ç†èƒ½åŠ›ã€‚",
                            "en": "TabFac consists of 117,854 manually annotated statements with regard to 16,573 Wikipedia tables, their relations are classified as ENTAILED and REFUTED. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/tabfact'. Error: Path opencompass/tabfact is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1835",
                    "name": "AudioJailbreak",
                    "version": "1.0.0",
                    "description": "LAMs face jailbreak risks. AJailBench, our new benchmark, reveals leading LAMs lack robustness. Subtle audio perturbations significantly degrade their safety. We release AJailBench for research.",
                    "url": "opencompass/opencompass_1835.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1835",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Language",
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1835",
                        "name": "AudioJailbreak",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            },
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "audio",
                                "en": "audio"
                            },
                            {
                                "cn": "jailbreak",
                                "en": "jailbreak"
                            },
                            {
                                "cn": "LAM",
                                "en": "LAM"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/mbzuai-nlp/AudioJailbreak",
                        "paperLink": "https://arxiv.org/abs/2505.15406",
                        "officialWebsiteLink": "https://huggingface.co/datasets/MBZUAI/AudioJailbreak",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "53309115",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-2Fm1XxTnD"
                        },
                        "lookNum": "239",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-05-27 12:51:28",
                        "supportOnlineEval": false,
                        "updateDate": "2025-05-27 12:51:28",
                        "createDate": "2025-05-24 12:28:39",
                        "desc": {
                            "cn": "LAM é¢ä¸´è¶Šç‹±é£é™©ã€‚æˆ‘ä»¬æ–°çš„åŸºå‡†æµ‹è¯• AJailBench æ­ç¤ºï¼Œé¢†å…ˆçš„ LAM ç¼ºä¹ç¨³å¥æ€§ã€‚ç»†å¾®çš„éŸ³é¢‘å¹²æ‰°ä¼šæ˜¾è‘—é™ä½å…¶å®‰å…¨æ€§ã€‚æˆ‘ä»¬å‘å¸ƒ AJailBench è¿›è¡Œç ”ç©¶ã€‚",
                            "en": "LAMs face jailbreak risks. AJailBench, our new benchmark, reveals leading LAMs lack robustness. Subtle audio perturbations significantly degrade their safety. We release AJailBench for research."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/audiojailbreak'. Error: Path opencompass/audiojailbreak is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1836",
                    "name": "PashtoOCR",
                    "version": "1.0.0",
                    "description": "PsOCR is a large-scale synthetic dataset for Optical Character Recognition in low-resource Pashto language.",
                    "url": "opencompass/opencompass_1836.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1836",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding",
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1836",
                        "name": "PashtoOCR",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            },
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "OCR",
                                "en": "OCR"
                            },
                            {
                                "cn": "NLP",
                                "en": "NLP"
                            },
                            {
                                "cn": "Computer Vision",
                                "en": "Computer Vision"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/zirak-ai/PashtoOCR",
                        "paperLink": "https://arxiv.org/abs/2505.10055",
                        "officialWebsiteLink": "https://zirak.ai/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "53006610",
                            "name": "ijazulhaq",
                            "avatar": null,
                            "nickname": "ijazulhaq"
                        },
                        "lookNum": "225",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-05-27 12:52:04",
                        "supportOnlineEval": false,
                        "updateDate": "2025-05-27 12:52:04",
                        "createDate": "2025-05-26 09:43:14",
                        "desc": {
                            "cn": "PsOCR is a large-scale synthetic dataset for Optical Character Recognition in low-resource Pashto language.",
                            "en": "PsOCR is a large-scale synthetic dataset for Optical Character Recognition in low-resource Pashto language."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/pashtoocr'. Error: Path opencompass/pashtoocr is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1335",
                    "name": "LTMbenchmark",
                    "version": "1.0.0",
                    "description": "LTMbenchmark assess the long-term memory, continual learning, and information integration capabilities of the agents via dynamic conversational tasks. ",
                    "url": "opencompass/opencompass_1335.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1335",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1335",
                        "name": "LTMbenchmark",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/GoodAI/goodai-ltm-benchmark",
                        "paperLink": "https://arxiv.org/abs/2409.20222",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "225",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:05:39",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:05:39",
                        "createDate": "2025-01-03 16:10:04",
                        "desc": {
                            "cn": "LTMbenchmarké€šè¿‡åŠ¨æ€å¯¹è¯ä»»åŠ¡è¯„ä¼°æ™ºèƒ½ä½“çš„é•¿æœŸè®°å¿†ã€æŒç»­å­¦ä¹ å’Œä¿¡æ¯é›†æˆèƒ½åŠ›ã€‚",
                            "en": "LTMbenchmark assess the long-term memory, continual learning, and information integration capabilities of the agents via dynamic conversational tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ltmbenchmark'. Error: Path opencompass/ltmbenchmark is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1246",
                    "name": "LiveBench",
                    "version": "1.0.0",
                    "description": "LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-free versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. ",
                    "url": "opencompass/opencompass_1246.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1246",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1246",
                        "name": "LiveBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/livebench/livebench",
                        "paperLink": "https://arxiv.org/abs/2406.19314",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "224",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-22 18:09:35",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-22 18:09:35",
                        "createDate": "2024-12-20 19:35:51",
                        "desc": {
                            "cn": "LiveBenchæ˜¯ä¸€ä¸ªLLMåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–æ•°å­¦ã€ç¼–ç ã€æ¨ç†ã€è¯­è¨€ã€æŒ‡ä»¤éµå¾ªå’Œæ•°æ®åˆ†æï¼ŒåŒ…å«åŸºäºæœ€è¿‘å‘å¸ƒçš„æ•°å­¦ç«èµ›ã€arXiv è®ºæ–‡ã€æ–°é—»æ–‡ç« å’Œæ•°æ®é›†çš„é—®é¢˜ï¼ŒåŠç»å…¸åŸºå‡†æµ‹è¯•ï¼ˆå¦‚ Big-Bench Hardã€AMPS å’Œ IFEvalï¼‰çš„æ›´éš¾ã€æ— æ±¡æŸ“çš„ä»»åŠ¡ã€‚",
                            "en": "LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-free versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/livebench'. Error: Path opencompass/livebench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1252",
                    "name": "RE-Bench",
                    "version": "1.0.0",
                    "description": "RE-Bench (Research Engineering Benchmark, v1) consists of 7 challenging, open-ended ML research engineering environments and data from 71 8-hour attempts by 61 distinct human experts.",
                    "url": "opencompass/opencompass_1252.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1252",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Code",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1252",
                        "name": "RE-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/METR/ai-rd-tasks/tree/main",
                        "paperLink": "https://arxiv.org/abs/2411.15114",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "223",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-30 16:30:41",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-30 16:30:41",
                        "createDate": "2024-12-30 16:24:14",
                        "desc": {
                            "cn": "RE-Benchç”¨äºè¯„ä¼°AIæ™ºèƒ½ä½“ç ”å‘çš„è‡ªåŠ¨åŒ–èƒ½åŠ›ï¼Œå®ƒç”±61ä½äººç±»ä¸“å®¶71æ¬¡åœ¨7ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¼€æ”¾å¼MLç ”ç©¶å·¥ç¨‹ç¯å¢ƒä¸­çš„8å°æ—¶å°è¯•çš„æ•°æ®ç»„æˆã€‚",
                            "en": "RE-Bench (Research Engineering Benchmark, v1) consists of 7 challenging, open-ended ML research engineering environments and data from 71 8-hour attempts by 61 distinct human experts."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/re_bench'. Error: Path opencompass/re_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1322",
                    "name": "InfiBench",
                    "version": "1.0.0",
                    "description": "InfiBench is a large-scale freeform question-answering (QA) benchmark for code to our knowledge, comprising 234 carefully selected high-quality Stack Overflow questions that span across 15 programming languages.",
                    "url": "opencompass/opencompass_1322.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1322",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1322",
                        "name": "InfiBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/infi-coder/infibench-evaluation-harness/",
                        "paperLink": "https://arxiv.org/abs/2404.07940",
                        "officialWebsiteLink": "https://infi-coder.github.io/infibench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "219",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:05:42",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:05:42",
                        "createDate": "2024-12-31 15:59:24",
                        "desc": {
                            "cn": "InfiBenchç”¨äºè¯„æµ‹LLMå›ç­”ä»£ç ç›¸å…³é—®é¢˜çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¶µç›–15ç§ç¼–ç¨‹è¯­è¨€çš„234ä¸ªç²¾å¿ƒæŒ‘é€‰çš„é«˜è´¨é‡Stack Overflowé—®é¢˜ã€‚",
                            "en": "InfiBench is a large-scale freeform question-answering (QA) benchmark for code to our knowledge, comprising 234 carefully selected high-quality Stack Overflow questions that span across 15 programming languages."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/infibench'. Error: Path opencompass/infibench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1641",
                    "name": "MedAgents-Bench",
                    "version": "1.0.0",
                    "description": "MedAgentsBench features 862 challenging medical questions from seven datasets, focusing on cases where models struggle. It emphasizes multi-step clinical reasoning and addresses limitations in existing benchmarks by eliminating simple questions and standardizing evaluation protocols.",
                    "url": "opencompass/opencompass_1641.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1641",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning",
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1641",
                        "name": "MedAgents-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Medical",
                                "en": "Medical"
                            },
                            {
                                "cn": "Agent",
                                "en": "Agent"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/gersteinlab/medagents-benchmark",
                        "paperLink": "https://arxiv.org/abs/2503.07459",
                        "officialWebsiteLink": "https://github.com/gersteinlab/medagents-benchmark",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "26204315",
                            "name": null,
                            "avatar": null,
                            "nickname": "super-dainiu"
                        },
                        "lookNum": "215",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-17 13:59:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-17 13:59:33",
                        "createDate": "2025-03-17 12:44:07",
                        "desc": {
                            "cn": "MedAgentsBenchæ˜¯ä¸€ä¸ªä¸“æ³¨äºå¤æ‚åŒ»å­¦æ¨ç†çš„åŸºå‡†æµ‹è¯•ï¼Œä»ä¸ƒä¸ªåŒ»å­¦æ•°æ®é›†ä¸­ç²¾é€‰äº†862ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜ã€‚è¿™äº›æ•°æ®é›†åŒ…æ‹¬MedQAã€PubMedQAã€MedMCQAã€MedBulletsã€MedExQAã€MedXpertQAå’ŒMMLU/MMLU-Proï¼Œæ¶µç›–äº†ä»åŒ»å­¦æ‰§ç…§è€ƒè¯•åˆ°ç ”ç©¶æ–‡çŒ®çš„å¤šç§åŒ»å­¦é—®é¢˜ã€‚\nè¯¥åŸºå‡†é€‰æ‹©å°‘äº50%æ¨¡å‹èƒ½æ­£ç¡®å›ç­”çš„é—®é¢˜ï¼Œç¡®ä¿åŒ»å­¦çŸ¥è¯†é¢†åŸŸå…¨é¢è¦†ç›–ï¼Œå¹¶ä¼˜å…ˆé€‰æ‹©éœ€è¦å¤šæ­¥ä¸´åºŠæ¨ç†çš„é—®é¢˜ã€‚è¿™è§£å†³äº†ç°æœ‰è¯„ä¼°ä¸­ç®€å•é—®é¢˜æ™®éå­˜åœ¨ã€è¯„ä¼°åè®®ä¸ä¸€è‡´ï¼Œä»¥åŠç¼ºä¹æ€§èƒ½-æˆæœ¬-æ—¶é—´åˆ†æçš„å±€é™ã€‚",
                            "en": "MedAgentsBench features 862 challenging medical questions from seven datasets, focusing on cases where models struggle. It emphasizes multi-step clinical reasoning and addresses limitations in existing benchmarks by eliminating simple questions and standardizing evaluation protocols."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medagents_bench'. Error: Path opencompass/medagents_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2217",
                    "name": "SmartBench",
                    "version": "1.0.0",
                    "description": "SmartBench is the first benchmark specifically designed to evaluate the capabilities of on-device large language models in smartphone scenarios.",
                    "url": "opencompass/opencompass_2217.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2217",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2217",
                        "name": "SmartBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": " Capabilities of on-device LLMs",
                                "en": " Capabilities of on-device LLMs"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/vivo-ai-lab/SmartBench",
                        "paperLink": "https://arxiv.org/abs/2503.06029",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "92500767",
                            "name": null,
                            "avatar": null,
                            "nickname": "æµ©å±¿ğŸ’–æ¥ "
                        },
                        "lookNum": "215",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-10-15 16:26:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-10-15 16:26:55",
                        "createDate": "2025-10-10 17:50:51",
                        "desc": {
                            "cn": "SmartBench æ˜¯é¦–ä¸ªé¢å‘æ™ºèƒ½æ‰‹æœºç»ˆç«¯å¤§æ¨¡å‹èƒ½åŠ›è¯„ä¼°çš„åŸºå‡†ï¼ŒåŸºäºæ‰‹æœºå‚å•†æä¾›çš„åŠŸèƒ½å°†å…¶åˆ’åˆ†ä¸ºäº”ç±»å…±20é¡¹ä»»åŠ¡ï¼Œæ¶µç›–æ–‡æœ¬æ‘˜è¦ã€é—®ç­”ã€ä¿¡æ¯æŠ½å–ã€å†…å®¹åˆ›ä½œå’Œé€šçŸ¥ç®¡ç†ç­‰åœºæ™¯ã€‚å®ƒæä¾›é«˜è´¨é‡æ•°æ®é›†ä¸å®šåˆ¶åŒ–è¯„ä¼°æ ‡å‡†ï¼Œæ—¨åœ¨æ¨åŠ¨ç»ˆç«¯å¤§æ¨¡å‹åœ¨ç§»åŠ¨åº”ç”¨ä¸­çš„æ ‡å‡†åŒ–è¯„ä¼°ä¸å‘å±•ã€‚",
                            "en": "SmartBench is the first benchmark specifically designed to evaluate the capabilities of on-device large language models in smartphone scenarios."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/smartbench'. Error: Path opencompass/smartbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1116",
                    "name": "AQUA-RAT",
                    "version": "1.0.0",
                    "description": "AQUA-RAT contains the algebraic word problems. The dataset consists of about 100,000 algebraic word problems with natural language rationales.",
                    "url": "opencompass/opencompass_1116.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1116",
                    "sample_count": 1000,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1116",
                        "name": "AQUA-RAT",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/google-deepmind/AQuA",
                        "paperLink": "https://arxiv.org/pdf/1705.04146",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "214",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-11 19:59:29",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-11 19:59:29",
                        "createDate": "2024-10-10 14:20:23",
                        "desc": {
                            "cn": "AQUA-RAT åŒ…å«ä»£æ•°æ–‡å­—é—®é¢˜ã€‚è¯¥æ•°æ®é›†ç”±çº¦ 100,000 é“å¸¦æœ‰è‡ªç„¶è¯­è¨€æ¨ç†çš„ä»£æ•°æ–‡å­—é—®é¢˜ç»„æˆã€‚",
                            "en": "AQUA-RAT contains the algebraic word problems. The dataset consists of about 100,000 algebraic word problems with natural language rationales."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/aqua_rat'. Error: Path opencompass/aqua_rat is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1562",
                    "name": "BenchMAX",
                    "version": "1.0.0",
                    "description": "BenchMAX is a comprehensive, high-quality, and multiway parallel multilingual benchmark comprising 10 tasks designed to assess crucial capabilities across 17 diverse language.",
                    "url": "opencompass/opencompass_1562.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1562",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1562",
                        "name": "BenchMAX",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/CONE-MT/BenchMAX",
                        "paperLink": "https://arxiv.org/abs/2502.07346",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "213",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-03 10:56:52",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-03 10:56:52",
                        "createDate": "2025-02-27 11:05:13",
                        "desc": {
                            "cn": "BenchMAX æ˜¯ä¸€ä¸ªå…¨é¢ã€é«˜è´¨é‡çš„å¤šå‘å¹¶è¡Œå¤šè¯­è¨€åŸºå‡†ï¼ŒåŒ…å« 10 ä¸ªä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼° 17 ç§ä¸åŒè¯­è¨€çš„å…³é”®èƒ½åŠ›ã€‚",
                            "en": "BenchMAX is a comprehensive, high-quality, and multiway parallel multilingual benchmark comprising 10 tasks designed to assess crucial capabilities across 17 diverse language."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/benchmax'. Error: Path opencompass/benchmax is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1576",
                    "name": "PhysReason",
                    "version": "1.0.0",
                    "description": "PhysReason is a comprehensive physics-based reasoning benchmark consisting of 1,200 physics problems spanning multiple domains, with a focus on both knowledge-based (25%) and reasoning-based (75%) questions.",
                    "url": "opencompass/opencompass_1576.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1576",
                    "sample_count": 1000,
                    "traits": [
                        "Examination",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1576",
                        "name": "PhysReason",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å­¦ç§‘",
                                "en": "Examination"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2502.12054",
                        "officialWebsiteLink": "https://dxzxy12138.github.io/PhysReason/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "212",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-04 16:59:45",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-04 16:59:45",
                        "createDate": "2025-03-03 16:02:34",
                        "desc": {
                            "cn": "PhysReason æ˜¯ä¸€ä¸ªåŒ…å« 1,200 ä¸ªç‰©ç†é—®é¢˜çš„ç»¼åˆç‰©ç†æ¨ç†åŸºå‡†ï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸï¼Œé‡ç‚¹å…³æ³¨åŸºäºçŸ¥è¯†ï¼ˆ25%ï¼‰å’Œæ¨ç†ï¼ˆ75%ï¼‰çš„é—®é¢˜ã€‚",
                            "en": "PhysReason is a comprehensive physics-based reasoning benchmark consisting of 1,200 physics problems spanning multiple domains, with a focus on both knowledge-based (25%) and reasoning-based (75%) questions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/physreason'. Error: Path opencompass/physreason is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1283",
                    "name": "P-MMEval",
                    "version": "1.0.0",
                    "description": "P-MMEval is a comprehensive multilingual multitask benchmark, covering effective fundamental and capability-specialized datasets.",
                    "url": "opencompass/opencompass_1283.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1283",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1283",
                        "name": "P-MMEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2411.09116",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "43209637",
                            "name": "Qwen",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/43209637-2eaaac15-94a9-4382-9047-16ae007f33f6.png",
                            "nickname": "Qwen"
                        },
                        "lookNum": "210",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 19:59:39",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 19:59:39",
                        "createDate": "2024-12-25 19:57:03",
                        "desc": {
                            "cn": "P-MMEvalæ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šè¯­è¨€å¤šä»»åŠ¡åŸºå‡†ï¼Œæ¶µç›–äº†é«˜æ•ˆçš„åŸºç¡€å’Œä¸“é¡¹èƒ½åŠ›æ•°æ®é›†ã€‚",
                            "en": "P-MMEval is a comprehensive multilingual multitask benchmark, covering effective fundamental and capability-specialized datasets."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/p_mmeval'. Error: Path opencompass/p_mmeval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1072",
                    "name": "xCodeEval",
                    "version": "1.0.0",
                    "description": "xCodeEval is the largest executable multilingual multitask benchmark to date consisting of 25M document-level coding examples (16.5B tokens) from about 7.5K unique problems covering up to 11 programming languages with execution-level parallelism.",
                    "url": "opencompass/opencompass_1072.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1072",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1072",
                        "name": "xCodeEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ntunlp/xCodeEval",
                        "paperLink": "https://arxiv.org/pdf/2303.03004",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50187454",
                            "name": "NTU-NLP",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50187454-16b303e1-3675-4b47-b9ec-83f20eef9b46.png",
                            "nickname": "NTU-NLP"
                        },
                        "lookNum": "210",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-27 16:53:40",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-27 16:53:40",
                        "createDate": "2024-09-27 12:40:47",
                        "desc": {
                            "cn": "xCodeEval æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å¯æ‰§è¡Œå¤šè¯­è¨€å¤šä»»åŠ¡åŸºå‡†ï¼ŒåŒ…å« 2500 ä¸‡ä¸ªæ–‡æ¡£çº§ç¼–ç ç¤ºä¾‹ï¼ˆ165 äº¿ä¸ªæ ‡è®°ï¼‰ï¼Œæ¥è‡ªçº¦ 7500 ä¸ªç‹¬ç‰¹é—®é¢˜ï¼Œæ¶µç›–å¤šè¾¾ 11 ç§ç¼–ç¨‹è¯­è¨€ã€‚å®ƒåŒ…æ‹¬ 7 ä¸ªä»»åŠ¡ï¼Œæ¶‰åŠä»£ç ç†è§£ã€ç”Ÿæˆã€ç¿»è¯‘å’Œæ£€ç´¢ã€‚",
                            "en": "xCodeEval is the largest executable multilingual multitask benchmark to date consisting of 25M document-level coding examples (16.5B tokens) from about 7.5K unique problems covering up to 11 programming languages with execution-level parallelism."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/xcodeeval'. Error: Path opencompass/xcodeeval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1251",
                    "name": "PlanBench",
                    "version": "1.0.0",
                    "description": "PlanBench is an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. ",
                    "url": "opencompass/opencompass_1251.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1251",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1251",
                        "name": "PlanBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/karthikv792/LLMs-Planning",
                        "paperLink": "https://arxiv.org/abs/2206.10498",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "207",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-30 16:29:45",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-30 16:29:45",
                        "createDate": "2024-12-30 16:23:47",
                        "desc": {
                            "cn": "PlanBenchç”¨äºè¯„ä¼°LLMçš„è§„åˆ’èƒ½åŠ›ï¼ŒåŸºäºè‡ªåŠ¨åŒ–è§„åˆ’ç¤¾åŒºï¼ˆå°¤å…¶æ˜¯åœ¨å›½é™…è§„åˆ’ç«èµ›ï¼‰ä¸­æ¶‰åŠçš„å„ç§é¢†åŸŸæ¥æµ‹è¯•å¤§æ¨¡å‹åœ¨è§„åˆ’æˆ–æ¨ç†è¡ŒåŠ¨å’Œå˜æ›´æ–¹é¢çš„èƒ½åŠ›ã€‚",
                            "en": "PlanBench is an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/planbench'. Error: Path opencompass/planbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1748",
                    "name": "VisualPuzzles",
                    "version": "1.0.0",
                    "description": "VisualPuzzles is a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of 1168 diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. ",
                    "url": "opencompass/opencompass_1748.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1748",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1748",
                        "name": "VisualPuzzles",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/neulab/VisualPuzzles",
                        "paperLink": "https://arxiv.org/abs/2504.10342",
                        "officialWebsiteLink": "https://neulab.github.io/VisualPuzzles/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "53009543",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-oErCdNaBY"
                        },
                        "lookNum": "204",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-18 17:11:40",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-18 17:11:40",
                        "createDate": "2025-04-17 21:40:00",
                        "desc": {
                            "cn": "LLM èƒ½è€ƒå…¬åŠ¡å‘˜å—ï¼Ÿæˆ‘ä»¬åšäº†ä¸ªæµ‹è¯•â€¦\n\nè¿‘å¹´æ¥ï¼Œå¤§æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›çªé£çŒ›è¿›ï¼Œä¼¼ä¹â€œè¶Šæ¥è¶Šèªæ˜â€äº†ã€‚ä½†æœ‰ä¸€ä¸ªå…³é”®é—®é¢˜ä»ç„¶æ‘†åœ¨çœ¼å‰ï¼š\nğŸ¤” å®ƒä»¬çœŸçš„ä¼šâ€œæ¨ç†â€å—ï¼Ÿ\n\nğŸš€ æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåä¸º VisualPuzzles ğŸ§© çš„å…¨æ–°æ•°æ®é›†ï¼Œä¸“é—¨ç”¨æ¥å›ç­”è¿™ä¸ªé—®é¢˜ï¼š\nè„±ç¦»ä¸“ä¸šçŸ¥è¯†çš„æ”¯æŒï¼Œå¤§æ¨¡å‹èƒ½é é€»è¾‘æœ¬èº«è§£é¢˜å—ï¼Ÿ\næˆ‘ä»¬ä»å¤šä¸ªæ¥æºç²¾å¿ƒæŒ‘é€‰æˆ–æ”¹ç¼–äº† 1168 é“å›¾æ–‡é€»è¾‘é¢˜ï¼Œå…¶ä¸­ä¸€ä¸ªé‡è¦æ¥æºä¾¿æ˜¯ä¸­å›½å›½å®¶å…¬åŠ¡å‘˜è€ƒè¯•è¡Œæµ‹ä¸­çš„é€»è¾‘æ¨ç†é¢˜ï¼ˆæ²¡é”™ï¼ŒçœŸÂ·è€ƒå…¬éš¾åº¦ï¼‰ğŸ¯",
                            "en": "VisualPuzzles is a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of 1168 diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/visualpuzzles'. Error: Path opencompass/visualpuzzles is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1017",
                    "name": "Yue_Benchmark",
                    "version": "1.0.0",
                    "description": "The benchmarks introduced for evaluating large language models (LLMs) on Cantonese include Yue-TruthfulQA, Yue-GSM8K, Yue-ARC-C, Yue-MMLU, and Yue-TRANS. Each of these benchmarks focuses on different aspects of language understanding and generation in Cantonese, offering a comprehensive means of assessing the capabilities of LLMs in handling this language.",
                    "url": "opencompass/opencompass_1017.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1017",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1017",
                        "name": "Yue_Benchmark",
                        "emoji": "ğŸ²",
                        "dimensions": [
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "factual generation",
                                "en": "factual generation"
                            },
                            {
                                "cn": "complex reasoning",
                                "en": "complex reasoning"
                            },
                            {
                                "cn": "general knowledge",
                                "en": "general knowledge"
                            },
                            {
                                "cn": "mathematical logic",
                                "en": "mathematical logic"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/jiangjyjy/Yue-Benchmark",
                        "paperLink": "https://arxiv.org/abs/2408.16756",
                        "officialWebsiteLink": "https://huggingface.co/datasets/BillBao/Yue-Benchmark",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50157836",
                            "name": null,
                            "avatar": null,
                            "nickname": "enemy"
                        },
                        "lookNum": "204",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-30 11:30:05",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-30 11:30:05",
                        "createDate": "2024-12-28 23:41:20",
                        "desc": {
                            "cn": "Yue_Benchmarkç”¨äºè¯„ä¼°ç²¤è¯­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚è¯¥è¯„æµ‹é›†åŒ…å«ï¼šYue TruthtyQAã€Yue-GSM8Kã€Yue-ARC-Cã€Yue MMLUå’ŒYue TRANSï¼Œä¾§é‡äºç²¤è¯­è¯­è¨€ç†è§£å’Œç”Ÿæˆçš„ä¸åŒæ–¹é¢ï¼Œä¸ºè¯„ä¼°LLMç²¤è¯­èƒ½åŠ›æä¾›äº†ä¸€ç§å…¨é¢çš„æ–¹æ³•ã€‚",
                            "en": "The benchmarks introduced for evaluating large language models (LLMs) on Cantonese include Yue-TruthfulQA, Yue-GSM8K, Yue-ARC-C, Yue-MMLU, and Yue-TRANS. Each of these benchmarks focuses on different aspects of language understanding and generation in Cantonese, offering a comprehensive means of assessing the capabilities of LLMs in handling this language."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/yue_benchmark'. Error: Path opencompass/yue_benchmark is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1238",
                    "name": "LINGOLY",
                    "version": "1.0.0",
                    "description": "Aiming at evaluating LLM's advanced reasoning abilities, LingOly is composed of  olympiad-level linguistic reasoning puzzles in low-resource and extinct languages. It covers more than 90 mostly low-resource languages, and contains 1,133 problems across 6 formats and 5 levels of human difficulty.",
                    "url": "opencompass/opencompass_1238.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1238",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1238",
                        "name": "LINGOLY",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/am-bean/lingOly",
                        "paperLink": "https://arxiv.org/abs/2406.06196",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "193",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 11:46:22",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 11:46:22",
                        "createDate": "2024-12-20 14:03:59",
                        "desc": {
                            "cn": "LingOlyç”±ä½èµ„æºå’Œå·²ç­ç»è¯­è¨€çš„å¥¥æ—åŒ¹å…‹çº§åˆ«è¯­è¨€æ¨ç†è°œé¢˜ç»„æˆï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„é«˜çº§æ¨ç†èƒ½åŠ›ã€‚å…¶ä¸­æ¶µç›–äº†90å¤šç§è¯­è¨€ï¼Œå…±æœ‰1133ä¸ªæ¶‰åŠ6ç§æ ¼å¼å’Œ5ä¸ªäººå·¥éš¾åº¦çº§åˆ«çš„é—®é¢˜ã€‚",
                            "en": "Aiming at evaluating LLM's advanced reasoning abilities, LingOly is composed of  olympiad-level linguistic reasoning puzzles in low-resource and extinct languages. It covers more than 90 mostly low-resource languages, and contains 1,133 problems across 6 formats and 5 levels of human difficulty."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/lingoly'. Error: Path opencompass/lingoly is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1355",
                    "name": "HallusionBench",
                    "version": "1.0.0",
                    "description": "HallusionBench is a comprehensive benchmark designed for the evaluation of image-context reasoning, comprising 346 images paired with 1129 questions.",
                    "url": "opencompass/opencompass_1355.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1355",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1355",
                        "name": "HallusionBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/tianyi-lab/HallusionBench",
                        "paperLink": "https://arxiv.org/abs/2310.14566",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "192",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 11:27:39",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 11:27:39",
                        "createDate": "2025-01-09 20:08:24",
                        "desc": {
                            "cn": "HallusionBenchæ˜¯ä¸€ä¸ªä¸“ä¸ºè¯„ä¼°å›¾åƒä¸Šä¸‹æ–‡æ¨ç†è€Œè®¾è®¡çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬346å¼ å›¾åƒå’Œ1129ä¸ªé—®é¢˜ã€‚",
                            "en": "HallusionBench is a comprehensive benchmark designed for the evaluation of image-context reasoning, comprising 346 images paired with 1129 questions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/hallusionbench'. Error: Path opencompass/hallusionbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1643",
                    "name": "Creation-MMBench",
                    "version": "1.0.0",
                    "description": "A multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs. It features three main aspects: 1. Comprehensive Creation Benchmark for MLLM and LLM. 2. Robust Evaluation Methodology. 3. Attractive Experiment Insight. ",
                    "url": "opencompass/opencompass_1643.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1643",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Long-Context",
                        "Creation"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1643",
                        "name": "Creation-MMBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "é•¿æ–‡æœ¬",
                                "en": "Long-Context"
                            },
                            {
                                "cn": "åˆ›ä½œ",
                                "en": "Creation"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 3,
                        "githubLink": "https://github.com/open-compass/Creation-MMBench",
                        "paperLink": "",
                        "officialWebsiteLink": "https://open-compass.github.io/Creation-MMBench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "40047277",
                            "name": "fangxy-09",
                            "avatar": "https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKFASuxOhldeP1BsgbQewWr71yNBNAehdVp5Qqrxq0D6hQ0libbrYZs9n5GVtoicy4uOvtNrmebicSKQ/132",
                            "nickname": "FangXinyu-0913"
                        },
                        "lookNum": "188",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-05-06 11:00:38",
                        "supportOnlineEval": false,
                        "updateDate": "2025-05-06 11:00:38",
                        "createDate": "2025-04-30 12:29:15",
                        "desc": {
                            "cn": "ä¸“ä¸ºè¯„ä¼° å¤šæ¨¡æ€å¤§æ¨¡å‹ çš„åˆ›ä½œèƒ½åŠ›è€Œè®¾è®¡çš„å¤šæ¨¡æ€åŸºå‡†ã€‚é‡‡ç”¨ä¸¤ä¸ªä¸åŒæŒ‡æ ‡å¯¹æ¨¡å‹çš„åŸºç¡€æ„ŸçŸ¥èƒ½åŠ›å’Œæ·±å±‚æ¬¡è§†è§‰åˆ›ä½œèƒ½åŠ›è¿›è¡Œè¯„ä¼°ï¼Œé‡‡ç”¨GPT-4oä½œä¸ºè¯„åˆ¤æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚",
                            "en": "A multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs. It features three main aspects: 1. Comprehensive Creation Benchmark for MLLM and LLM. 2. Robust Evaluation Methodology. 3. Attractive Experiment Insight. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/creation_mmbench'. Error: Path opencompass/creation_mmbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1349",
                    "name": "CyberSecEval",
                    "version": "1.0.0",
                    "description": "CyberSecEval is a comprehensive benchmark developed to help bolster the cybersecurity of LLMs. It provides a thorough evaluation in two crucial security domains: the propensity to generate insecure code and the level of compliance when asked to assist in cyberattacks.",
                    "url": "opencompass/opencompass_1349.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1349",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1349",
                        "name": "CyberSecEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks",
                        "paperLink": "https://arxiv.org/abs/2312.04724",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "186",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:31",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:31",
                        "createDate": "2025-01-09 12:17:41",
                        "desc": {
                            "cn": "CyberSecEvalæ—¨åœ¨è¯„ä¼°LLMçš„å®‰å…¨æ€§ï¼Œèšç„¦äºå¤§æ¨¡å‹ç”Ÿæˆä¸å®‰å…¨ä»£ç çš„å€¾å‘ä»¥åŠå½“è¢«è¦æ±‚ååŠ©ç½‘ç»œæ”»å‡»æ—¶çš„åˆè§„æ€§æ°´å¹³ã€‚",
                            "en": "CyberSecEval is a comprehensive benchmark developed to help bolster the cybersecurity of LLMs. It provides a thorough evaluation in two crucial security domains: the propensity to generate insecure code and the level of compliance when asked to assist in cyberattacks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cyberseceval'. Error: Path opencompass/cyberseceval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1546",
                    "name": "CodeCriticBench",
                    "version": "1.0.0",
                    "description": "CodeCriticBench assesses LLMs' critiquing ability in code generation and QA tasks. Covering 10 criteria, it features a 4.3k-samples dataset with three difficulty levels and balanced distribution.",
                    "url": "opencompass/opencompass_1546.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1546",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1546",
                        "name": "CodeCriticBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/multimodal-art-projection/CodeCriticBench",
                        "paperLink": "https://arxiv.org/abs/2502.16614",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "183",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-02-26 18:41:24",
                        "supportOnlineEval": false,
                        "updateDate": "2025-02-26 18:41:24",
                        "createDate": "2025-02-26 11:00:09",
                        "desc": {
                            "cn": "CodeCriticBench ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°LLMsåœ¨ä»£ç ç”Ÿæˆå’Œä»£ç é—®ç­”ä»»åŠ¡ä¸­çš„æ‰¹è¯„èƒ½åŠ›ã€‚å…¶æ¶µç›– 10 ä¸ªä¸åŒçš„æ ‡å‡†ï¼Œæ•°æ®é›†æ ¹æ®éš¾åº¦åˆ†ä¸ºä¸‰ä¸ªç­‰çº§ï¼Œå…±åŒ…å«4.3kä¸ªæ ·æœ¬ï¼Œç¡®ä¿äº†éš¾åº¦çº§åˆ«çš„å¹³è¡¡åˆ†å¸ƒã€‚",
                            "en": "CodeCriticBench assesses LLMs' critiquing ability in code generation and QA tasks. Covering 10 criteria, it features a 4.3k-samples dataset with three difficulty levels and balanced distribution."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/codecriticbench'. Error: Path opencompass/codecriticbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1564",
                    "name": "EmbodiedBench",
                    "version": "1.0.0",
                    "description": "EmbodiedBench is a comprehensive benchmark designed to evaluate Multi-modal Large Language Models (MLLMs) as embodied agents.",
                    "url": "opencompass/opencompass_1564.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1564",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1564",
                        "name": "EmbodiedBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/EmbodiedBench/EmbodiedBench",
                        "paperLink": "https://arxiv.org/abs/2502.09560",
                        "officialWebsiteLink": "https://embodiedbench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "183",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-03 11:00:23",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-03 11:00:23",
                        "createDate": "2025-02-27 11:25:40",
                        "desc": {
                            "cn": "EmbodiedBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºå…·èº«æ™ºèƒ½ä½“çš„å…¨é¢åŸºå‡†ã€‚",
                            "en": "EmbodiedBench is a comprehensive benchmark designed to evaluate Multi-modal Large Language Models (MLLMs) as embodied agents."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/embodiedbench'. Error: Path opencompass/embodiedbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1736",
                    "name": "MMTB",
                    "version": "1.0.0",
                    "description": "Large language models (LLMs) demonstrate strong potential as agents for tool invocation due to their advanced comprehension and planning capabilities. Users increasingly rely on LLM-based agents to solve complex missions through iterative interactions. However, existing benchmarks predominantly acce",
                    "url": "opencompass/opencompass_1736.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1736",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1736",
                        "name": "MMTB",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/yupeijei1997/MMTB",
                        "paperLink": "https://arxiv.org/abs/2504.02623",
                        "officialWebsiteLink": "https://harrywgcn.github.io/mmtb-leaderboard/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "90306731",
                            "name": null,
                            "avatar": null,
                            "nickname": "yupeijei1997"
                        },
                        "lookNum": "182",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:08:39",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:08:39",
                        "createDate": "2025-04-11 15:30:25",
                        "desc": {
                            "cn": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‡­å€Ÿå…¶å…ˆè¿›çš„ç†è§£èƒ½åŠ›å’Œè§„åˆ’èƒ½åŠ›ï¼Œåœ¨ä½œä¸ºå·¥å…·è°ƒç”¨çš„æ™ºèƒ½ä½“æ–¹é¢å±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç”¨æˆ·è¶Šæ¥è¶Šä¾èµ–åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“ï¼Œé€šè¿‡è¿­ä»£äº¤äº’æ¥è§£å†³å¤æ‚ä»»åŠ¡ã€‚ ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦æ˜¯åœ¨å•ä»»åŠ¡åœºæ™¯ä¸­è¯„ä¼°æ™ºèƒ½ä½“ï¼Œæ— æ³•ä½“ç°ç°å®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº† Multi-Mission Tool Bench æµ‹è¯•ã€‚åœ¨è¿™ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¯ä¸ªæµ‹è¯•ç”¨ä¾‹éƒ½åŒ…å«å¤šä¸ªç›¸äº’å…³è”çš„ä»»åŠ¡ã€‚è¿™ç§è®¾è®¡è¦æ±‚æ™ºèƒ½ä½“èƒ½å¤ŸåŠ¨æ€é€‚åº”ä¸æ–­å˜åŒ–çš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„åŸºå‡†æµ‹è¯•æ¢ç´¢äº†åœ¨å›ºå®šä»»åŠ¡æ•°é‡ä¸‹æ‰€æœ‰å¯èƒ½çš„ä»»åŠ¡åˆ‡æ¢æ¨¡å¼ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ•°æ®ç”Ÿæˆæ¡†æ¶æ¥æ„å»ºè¿™ä¸ªåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åˆ©ç”¨åŠ¨æ€å†³ç­–æ ‘æ¥",
                            "en": "Large language models (LLMs) demonstrate strong potential as agents for tool invocation due to their advanced comprehension and planning capabilities. Users increasingly rely on LLM-based agents to solve complex missions through iterative interactions. However, existing benchmarks predominantly acce"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmtb'. Error: Path opencompass/mmtb is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1117",
                    "name": "NaturalProofs",
                    "version": "1.0.0",
                    "description": "NATURALPROOFS is a multi-domain corpus of mathematical statements and their proofs, written in natural mathematical language. NATURALPROOFS unifies broad coverage, deep coverage, and low-resource mathematical sources, allowing for evaluating both in-distribution and zero-shot generalization.",
                    "url": "opencompass/opencompass_1117.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1117",
                    "sample_count": 1000,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1117",
                        "name": "NaturalProofs",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/wellecks/naturalproofs#naturalproofs-dataset",
                        "paperLink": "https://arxiv.org/pdf/2104.01112v2",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "181",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-11 19:59:32",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-11 19:59:32",
                        "createDate": "2024-10-10 14:39:51",
                        "desc": {
                            "cn": "NATURALPROOFS æ˜¯ä¸€ä¸ªå¤šé¢†åŸŸçš„æ•°å­¦è¯­å¥åŠå…¶è¯æ˜çš„è¯­æ–™åº“ï¼Œé‡‡ç”¨è‡ªç„¶æ•°å­¦è¯­è¨€ç¼–å†™ã€‚æ•´åˆäº†å¹¿æ³›è¦†ç›–ã€æ·±å…¥è¦†ç›–å’Œä½èµ„æºæ•°å­¦æ¥æºï¼Œä¾¿äºè¯„ä¼°åˆ†å¸ƒå†…å’Œé›¶æ ·æœ¬æ³›åŒ–çš„èƒ½åŠ›ã€‚",
                            "en": "NATURALPROOFS is a multi-domain corpus of mathematical statements and their proofs, written in natural mathematical language. NATURALPROOFS unifies broad coverage, deep coverage, and low-resource mathematical sources, allowing for evaluating both in-distribution and zero-shot generalization."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/naturalproofs'. Error: Path opencompass/naturalproofs is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1334",
                    "name": "MMDU",
                    "version": "1.0.0",
                    "description": "MMDU is intended for evaluating the multi-image multi-turn dialogue capabilities. It comprises 110 high-quality multi-image multi-turn dialogues with more than 1600 questions, each accompanied by detailed long-form answers. ",
                    "url": "opencompass/opencompass_1334.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1334",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1334",
                        "name": "MMDU",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Liuziyu77/MMDU",
                        "paperLink": "https://arxiv.org/abs/2406.11833",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "179",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 16:19:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 16:19:55",
                        "createDate": "2025-01-03 14:51:38",
                        "desc": {
                            "cn": "MMDUç”¨äºè¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šå›¾åƒå¤šè½®å¯¹è¯èƒ½åŠ›ï¼ŒåŒ…å«110ä¸ªé«˜è´¨é‡çš„å¤šå›¾åƒå¤šè½®å¯¹è¯ï¼Œç”±1600å¤šä¸ªé™„æœ‰è¯¦ç»†çš„é•¿ç¯‡ç­”æ¡ˆçš„é—®é¢˜ç»„æˆã€‚",
                            "en": "MMDU is intended for evaluating the multi-image multi-turn dialogue capabilities. It comprises 110 high-quality multi-image multi-turn dialogues with more than 1600 questions, each accompanied by detailed long-form answers. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmdu'. Error: Path opencompass/mmdu is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1146",
                    "name": "BUST",
                    "version": "1.0.0",
                    "description": "BUST is a comprehensive benchmark for evaluating synthetic text detectors, focusing on their effectiveness against outputs from various Large Language Models (LLMs). ",
                    "url": "opencompass/opencompass_1146.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1146",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1146",
                        "name": "BUST",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NAACL 2024",
                                "en": "NAACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/IDSIA-NLP/BUST",
                        "paperLink": "https://aclanthology.org/2024.naacl-long.444.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50126080",
                            "name": "IDSIA-NLP",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50126080-9bda56f6-aaad-43e3-86d5-1278eb268d20.png",
                            "nickname": "OpenXLab-rR7CGPrkS"
                        },
                        "lookNum": "179",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-27 17:23:25",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-27 17:23:25",
                        "createDate": "2024-10-15 13:46:56",
                        "desc": {
                            "cn": "BUST æ˜¯ä¸€ä¸ªç»¼åˆåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°åˆæˆæ–‡æœ¬æ£€æµ‹å™¨ï¼ŒBUST ä½¿ç”¨å¤šç§æŒ‡æ ‡æ¥è¯„ä¼°æ£€æµ‹å™¨ï¼ŒåŒ…æ‹¬è¯­è¨€ç‰¹å¾ã€å¯è¯»æ€§å’Œä½œè€…æ€åº¦ã€‚",
                            "en": "BUST is a comprehensive benchmark for evaluating synthetic text detectors, focusing on their effectiveness against outputs from various Large Language Models (LLMs). "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/bust'. Error: Path opencompass/bust is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1239",
                    "name": "CVQA",
                    "version": "1.0.0",
                    "description": "CVQA is a new culturally-diverse multilingual Visual Question Answering benchmark, designed to cover a rich set of languages and cultures. It includes culturally-driven images and 10k questions from across 30 countries on 4 continents.",
                    "url": "opencompass/opencompass_1239.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1239",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1239",
                        "name": "CVQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2406.05967",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "178",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-30 16:30:03",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-30 16:30:03",
                        "createDate": "2024-12-30 16:18:03",
                        "desc": {
                            "cn": "CVQAæ˜¯ä¸€ç§æ–°çš„æ–‡åŒ–å¤šå…ƒåŒ–å¤šè¯­è¨€è§†è§‰é—®ç­”åŸºå‡†ï¼Œæ—¨åœ¨æ¶µç›–ä¸°å¯Œçš„è¯­è¨€å’Œæ–‡åŒ–ï¼ŒåŒ…æ‹¬æ¥è‡ªå››å¤§æ´²30ä¸ªå›½å®¶/åœ°åŒºçš„æ–‡åŒ–å‘å›¾åƒå’Œ10000ä¸ªé—®é¢˜ã€‚",
                            "en": "CVQA is a new culturally-diverse multilingual Visual Question Answering benchmark, designed to cover a rich set of languages and cultures. It includes culturally-driven images and 10k questions from across 30 countries on 4 continents."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cvqa'. Error: Path opencompass/cvqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1582",
                    "name": "CodeMMLU",
                    "version": "1.0.0",
                    "description": "CodeMMLU is a comprehensive benchmark designed to evaluate the capabilities of large language models (LLMs) in coding and software knowledge. It builds upon the structure of multiple-choice question answering (MCQA) to cover a wide range of programming tasks and domains.",
                    "url": "opencompass/opencompass_1582.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1582",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1582",
                        "name": "CodeMMLU",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/FSoft-AI4Code/CodeMMLU",
                        "paperLink": "https://arxiv.org/abs/2406.15877",
                        "officialWebsiteLink": "https://fsoft-ai4code.github.io/codemmlu/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "175",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:46:54",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:46:54",
                        "createDate": "2025-03-04 11:00:59",
                        "desc": {
                            "cn": "CodeMMLU æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¼–ç å’Œè½¯ä»¶çŸ¥è¯†æ–¹é¢èƒ½åŠ›çš„å…¨é¢åŸºå‡†ã€‚å®ƒåŸºäºå¤šé¡¹é€‰æ‹©é¢˜å›ç­”ï¼ˆMCQAï¼‰çš„ç»“æ„ï¼Œæ¶µç›–äº†å¹¿æ³›çš„ç¼–ç¨‹ä»»åŠ¡å’Œé¢†åŸŸï¼ŒåŒ…æ‹¬ä»£ç ç”Ÿæˆã€ç¼ºé™·æ£€æµ‹ã€è½¯ä»¶å·¥ç¨‹åŸåˆ™ç­‰ã€‚",
                            "en": "CodeMMLU is a comprehensive benchmark designed to evaluate the capabilities of large language models (LLMs) in coding and software knowledge. It builds upon the structure of multiple-choice question answering (MCQA) to cover a wide range of programming tasks and domains."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/codemmlu'. Error: Path opencompass/codemmlu is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1357",
                    "name": "MME",
                    "version": "1.0.0",
                    "description": "MME is a comprehensive MLLM evaluation benchmark. It measures both perception and cognition abilities on a total of 14 subtasks.",
                    "url": "opencompass/opencompass_1357.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1357",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1357",
                        "name": "MME",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation",
                        "paperLink": "https://arxiv.org/abs/2306.13394",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "174",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 16:03:56",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 16:03:56",
                        "createDate": "2025-01-09 20:56:27",
                        "desc": {
                            "cn": "MMEæ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€å¤§æ¨¡å‹è¯„ä¼°åŸºå‡†ï¼Œæ¶µç›–14 ä¸ªè€ƒå¯Ÿæ„ŸçŸ¥å’Œè®¤çŸ¥èƒ½åŠ›çš„å­ä»»åŠ¡ã€‚",
                            "en": "MME is a comprehensive MLLM evaluation benchmark. It measures both perception and cognition abilities on a total of 14 subtasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mme'. Error: Path opencompass/mme is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1107",
                    "name": "StrategyQA",
                    "version": "1.0.0",
                    "description": "STRATEGYQA is a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy",
                    "url": "opencompass/opencompass_1107.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1107",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {
                        "gen": "StrategyQA_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1107",
                        "name": "StrategyQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/eladsegal/strategyqa",
                        "paperLink": "https://arxiv.org/pdf/2101.02235",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "173",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-09 20:03:28",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-09 20:03:28",
                        "createDate": "2024-10-09 17:54:13",
                        "desc": {
                            "cn": "STRATEGYQA æ˜¯ä¸€ä¸ªé—®ç­”åŸºå‡†ï¼Œå…¶ä¸­æ‰€éœ€çš„æ¨ç†æ­¥éª¤åœ¨é—®é¢˜ä¸­æ˜¯éšå«çš„ï¼Œå¯ä»¥é€šè¿‡ç­–ç•¥è¿›è¡Œæ¨æ–­ã€‚",
                            "en": "STRATEGYQA is a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/strategyqa'. Error: Path opencompass/strategyqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2073",
                    "name": "J1-Bench",
                    "version": "1.0.0",
                    "description": "J1-Bench is an interactive and comprehensive legal benchmark where LLM agents engage in diverse legal scenarios, completing tasks through interactions with various participants under procedural rules.",
                    "url": "opencompass/opencompass_2073.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2073",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2073",
                        "name": "J1-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "legal agent",
                                "en": "legal agent"
                            },
                            {
                                "cn": "interactive benchmark",
                                "en": "interactive benchmark"
                            },
                            {
                                "cn": "legal benchmark",
                                "en": "legal benchmark"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/FudanDISC/J1Bench",
                        "paperLink": "https://arxiv.org/abs/2507.04037",
                        "officialWebsiteLink": "https://j1bench.github.io/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "18905101",
                            "name": "ShengbinYue",
                            "avatar": null,
                            "nickname": "yueshengbin"
                        },
                        "lookNum": "171",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-21 17:12:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-21 17:12:55",
                        "createDate": "2025-07-21 15:07:28",
                        "desc": {
                            "cn": "J1-Bench æ˜¯ä¸€ä¸ªäº¤äº’å¼çš„ç»¼åˆæ³•å¾‹åŸºå‡†ï¼Œæ³•å¾‹æ™ºèƒ½ä½“åœ¨æ­¤å‚ä¸å„ç§æ³•å¾‹æƒ…æ™¯ï¼Œæ ¹æ®ç¨‹åºè§„åˆ™é€šè¿‡ä¸ä¸åŒå‚ä¸è€…çš„äº’åŠ¨å®ŒæˆæŒ‡å®šçš„æ³•å¾‹ä»»åŠ¡ã€‚",
                            "en": "J1-Bench is an interactive and comprehensive legal benchmark where LLM agents engage in diverse legal scenarios, completing tasks through interactions with various participants under procedural rules."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/j1_bench'. Error: Path opencompass/j1_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1617",
                    "name": "IFIR",
                    "version": "1.0.0",
                    "description": "IFIR is the first comprehensive benchmark designed to evaluate instruction-following information retrieval (IR) in expert domains. IFIR includes 2,426 high-quality examples and covers eight subsets across four specialized domains: finance, law, healthcare, and science literature.",
                    "url": "opencompass/opencompass_1617.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1617",
                    "sample_count": 1000,
                    "traits": [
                        "Instruct"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1617",
                        "name": "IFIR",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æŒ‡ä»¤è·Ÿéš",
                                "en": "Instruct"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SighingSnow/IFIR",
                        "paperLink": "https://arxiv.org/abs/2503.04644",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "169",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-10 17:11:56",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-10 17:11:56",
                        "createDate": "2025-03-10 11:01:15",
                        "desc": {
                            "cn": " IFIRï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°ä¸“å®¶é¢†åŸŸæŒ‡ä»¤è·Ÿéšä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰çš„ç»¼åˆåŸºå‡†ã€‚IFIR åŒ…å« 2,426 ä¸ªé«˜è´¨é‡ç¤ºä¾‹ï¼Œæ¶µç›–å››ä¸ªä¸“ä¸šé¢†åŸŸï¼ˆé‡‘èã€æ³•å¾‹ã€åŒ»ç–—ä¿å¥å’Œç§‘å­¦æ–‡çŒ®ï¼‰çš„å…«ä¸ªå­é›†ã€‚",
                            "en": "IFIR is the first comprehensive benchmark designed to evaluate instruction-following information retrieval (IR) in expert domains. IFIR includes 2,426 high-quality examples and covers eight subsets across four specialized domains: finance, law, healthcare, and science literature."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ifir'. Error: Path opencompass/ifir is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1242",
                    "name": "AgentBoard",
                    "version": "1.0.0",
                    "description": "AgentBoard is tailored to analytical evaluation of LLM agents. It offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assessment of agents for multi-faceted analysis through interactive visualization.",
                    "url": "opencompass/opencompass_1242.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1242",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1242",
                        "name": "AgentBoard",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/hkust-nlp/AgentBoard/tree/main",
                        "paperLink": "https://arxiv.org/abs/2401.13178",
                        "officialWebsiteLink": "https://hkust-nlp.github.io/agentboard/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "164",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 11:46:29",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 11:46:29",
                        "createDate": "2024-12-20 17:19:44",
                        "desc": {
                            "cn": "AgentBoardä¸“ç”¨äºLLM Agentçš„åˆ†æè¯„ä¼°ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªç²¾ç»†æŒ‡æ ‡ç”¨äºæ•è·å¢é‡è¿›æ­¥ï¼Œä»¥åŠä¸€ä¸ªå…¨é¢çš„è¯„ä¼°å·¥å…·åŒ…ï¼Œèƒ½åŸºäºäº¤äº’å¼å¯è§†åŒ–è¯„ä¼°è¿›è¡Œå¤šæ–¹é¢åˆ†æã€‚",
                            "en": "AgentBoard is tailored to analytical evaluation of LLM agents. It offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assessment of agents for multi-faceted analysis through interactive visualization."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/agentboard'. Error: Path opencompass/agentboard is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1113",
                    "name": "SVAMP",
                    "version": "1.0.0",
                    "description": "SVAMP includes one-unknown arithmetic word problems with grade level up to 4 by applying simple variations over word problems in an existing dataset. SVAMP further highlights the brittle nature of existing models when trained on these benchmark datasets.",
                    "url": "opencompass/opencompass_1113.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1113",
                    "sample_count": 1000,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {
                        "gen": "SVAMP_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1113",
                        "name": "SVAMP",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/arkilpatel/SVAMP",
                        "paperLink": "https://arxiv.org/pdf/2103.07191v2",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "163",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-11 19:59:21",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-11 19:59:21",
                        "createDate": "2024-10-10 13:41:55",
                        "desc": {
                            "cn": "SVAMP æ˜¯ä¸€ä¸ªåŒ…å«ç®—æœ¯æ–‡å­—é—®é¢˜çš„æ•°æ®é›†ï¼Œæœ€é«˜é€‚ç”¨äºå››å¹´çº§çš„å­¦ç”Ÿï¼Œæ˜¯é€šè¿‡å¯¹ç°æœ‰æ•°æ®é›†ä¸­çš„æ–‡å­—é—®é¢˜åº”ç”¨ç®€å•å˜ä½“è€Œç”Ÿæˆã€‚",
                            "en": "SVAMP includes one-unknown arithmetic word problems with grade level up to 4 by applying simple variations over word problems in an existing dataset. SVAMP further highlights the brittle nature of existing models when trained on these benchmark datasets."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/svamp'. Error: Path opencompass/svamp is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1541",
                    "name": "VLM2-Bench",
                    "version": "1.0.0",
                    "description": "VLMÂ²-Bench is the first comprehensive benchmark that evaluates vision-language models' (VLMs) ability to visually link matching cues across multi-image sequences and videos. The benchmark consists of 9 subtasks with over 3,000 test cases.",
                    "url": "opencompass/opencompass_1541.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1541",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1541",
                        "name": "VLM2-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/vlm2-bench/VLM2-Bench",
                        "paperLink": "https://arxiv.org/abs/2502.12084",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "162",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-02-26 18:53:47",
                        "supportOnlineEval": false,
                        "updateDate": "2025-02-26 18:53:47",
                        "createDate": "2025-02-25 20:23:33",
                        "desc": {
                            "cn": "VLMÂ²-Bench æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šå›¾åƒåºåˆ—å’Œè§†é¢‘ä¸­è§†è§‰é“¾æ¥åŒ¹é…çº¿ç´¢èƒ½åŠ›çš„åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…æ‹¬ 9 ä¸ªå­ä»»åŠ¡ï¼Œè¶…è¿‡ 3000 ä¸ªæµ‹è¯•æ¡ˆä¾‹ï¼Œæ—¨åœ¨è¯„ä¼°äººç±»æ—¥å¸¸ä½¿ç”¨çš„æ ¹æœ¬è§†è§‰é“¾æ¥èƒ½åŠ›ã€‚",
                            "en": "VLMÂ²-Bench is the first comprehensive benchmark that evaluates vision-language models' (VLMs) ability to visually link matching cues across multi-image sequences and videos. The benchmark consists of 9 subtasks with over 3,000 test cases."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/vlm2_bench'. Error: Path opencompass/vlm2_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1523",
                    "name": "MMIE",
                    "version": "1.0.0",
                    "description": " MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs).",
                    "url": "opencompass/opencompass_1523.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1523",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1523",
                        "name": "MMIE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Lillianwei-h/MMIE",
                        "paperLink": "https://arxiv.org/abs/2410.10139",
                        "officialWebsiteLink": "https://mmie-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "161",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:58:15",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:58:15",
                        "createDate": "2025-02-21 11:23:50",
                        "desc": {
                            "cn": " MMIEï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çŸ¥è¯†å¯†é›†å‹åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸­çš„äº¤é”™å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚",
                            "en": " MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs)."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmie'. Error: Path opencompass/mmie is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1103",
                    "name": "QASC",
                    "version": "1.0.0",
                    "description": "QASC is a question-answering dataset with a focus on sentence composition. It consists of 9,980 8-way multiple-choice questions about grade school science (8,134 train, 926 dev, 920 test), and comes with a corpus of 17M sentences.",
                    "url": "opencompass/opencompass_1103.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1103",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1103",
                        "name": "QASC",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/allenai/qasc",
                        "paperLink": "https://arxiv.org/pdf/1910.11473",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "160",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-09 20:03:32",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-09 20:03:32",
                        "createDate": "2024-10-09 16:45:32",
                        "desc": {
                            "cn": "QASC æ˜¯ä¸€ä¸ªä¸“æ³¨äºå¥å­ç»„åˆçš„é—®ç­”æ•°æ®é›†ã€‚å®ƒåŒ…å« 9,980 é“å°å­¦ç§‘å­¦çš„å¤šé¡¹é€‰æ‹©é¢˜ï¼ˆ8,134 é“ç”¨äºè®­ç»ƒï¼Œ926 é“ç”¨äºå¼€å‘ï¼Œ920 é“ç”¨äºæµ‹è¯•ï¼‰ï¼Œå¹¶é…æœ‰ä¸€ä¸ªåŒ…å« 1,700 ä¸‡ä¸ªå¥å­çš„è¯­æ–™åº“ã€‚",
                            "en": "QASC is a question-answering dataset with a focus on sentence composition. It consists of 9,980 8-way multiple-choice questions about grade school science (8,134 train, 926 dev, 920 test), and comes with a corpus of 17M sentences."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/qasc'. Error: Path opencompass/qasc is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1337",
                    "name": "JailTrickBench",
                    "version": "1.0.0",
                    "description": "JailTrickBench can evaluate the impact of various attack settings on LLM performance, including 8 key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives.",
                    "url": "opencompass/opencompass_1337.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1337",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1337",
                        "name": "JailTrickBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/usail-hkust/JailTrickBench",
                        "paperLink": "https://arxiv.org/abs/2406.09324",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "158",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:05:47",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:05:47",
                        "createDate": "2025-01-03 16:33:01",
                        "desc": {
                            "cn": "JailTrickBenchç”¨äºè¯„ä¼°LLMåº”å¯¹å„ç§è¶Šç‹±æ”»å‡»çš„èƒ½åŠ›ï¼Œæ¶µç›–ä»ç›®æ ‡çº§å’Œæ”»å‡»çº§2ä¸ªè§’åº¦å®æ–½è¶Šç‹±æ”»å‡»çš„8ä¸ªå…³é”®å› ç´ ã€‚",
                            "en": "JailTrickBench can evaluate the impact of various attack settings on LLM performance, including 8 key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/jailtrickbench'. Error: Path opencompass/jailtrickbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1572",
                    "name": "LOKI",
                    "version": "1.0.0",
                    "description": "LOKI, a multimodal synthetic data detection benchmark, designed specifically to comprehensively assess the capabilities of LMMs in detecting synthetic data.",
                    "url": "opencompass/opencompass_1572.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1572",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1572",
                        "name": "LOKI",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/opendatalab/LOKI",
                        "paperLink": "https://arxiv.org/abs/2410.09732",
                        "officialWebsiteLink": "https://opendatalab.github.io/LOKI/#fingding",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "157",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-18 15:43:49",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-18 15:43:49",
                        "createDate": "2025-03-18 15:43:15",
                        "desc": {
                            "cn": "LOKIæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åˆæˆæ•°æ®æ£€æµ‹åŸºå‡†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºå…¨é¢è¯„ä¼° LMMs åœ¨æ£€æµ‹åˆæˆæ•°æ®æ–¹é¢çš„èƒ½åŠ›ã€‚",
                            "en": "LOKI, a multimodal synthetic data detection benchmark, designed specifically to comprehensively assess the capabilities of LMMs in detecting synthetic data."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/loki'. Error: Path opencompass/loki is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1250",
                    "name": "Collie",
                    "version": "1.0.0",
                    "description": "COLLIE is a grammar-based framework that allows the specification of rich, compositional constraints with diverse generation levels (word, sentence, paragraph, passage) and modeling challenges (e.g.,language understanding, logical reasoning, counting, semantic planning).",
                    "url": "opencompass/opencompass_1250.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1250",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1250",
                        "name": "Collie",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/princeton-nlp/Collie",
                        "paperLink": "https://arxiv.org/abs/2307.08689",
                        "officialWebsiteLink": "https://collie-benchmark.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "157",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-30 16:29:49",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-30 16:29:49",
                        "createDate": "2024-12-30 16:23:20",
                        "desc": {
                            "cn": "COLLIEç”¨äºè¯„ä¼°å¤§æ¨¡å‹åœ¨çº¦æŸæ€§æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¯æŒ‡å®šå…·æœ‰ä¸åŒç”Ÿæˆçº§åˆ«ï¼ˆå•è¯ã€å¥å­ã€æ®µè½ã€æ®µè½ï¼‰å’Œå»ºæ¨¡æŒ‘æˆ˜ï¼ˆä¾‹å¦‚ï¼Œè¯­è¨€ç†è§£ã€é€»è¾‘æ¨ç†ã€è®¡æ•°ã€è¯­ä¹‰è§„åˆ’ï¼‰çš„ä¸°å¯Œç»„åˆã€‚",
                            "en": "COLLIE is a grammar-based framework that allows the specification of rich, compositional constraints with diverse generation levels (word, sentence, paragraph, passage) and modeling challenges (e.g.,language understanding, logical reasoning, counting, semantic planning)."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/collie'. Error: Path opencompass/collie is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1543",
                    "name": "KITAB-Bench",
                    "version": "1.0.0",
                    "description": "KITAB-Bench is a Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding,  and spans 36 sub-domains with over 8,809 samples, carefully curated to rigorously evaluate essential skills required for Arabic OCR and document analysis.",
                    "url": "opencompass/opencompass_1543.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1543",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1543",
                        "name": "KITAB-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/mbzuai-oryx/KITAB-Bench",
                        "paperLink": "https://arxiv.org/abs/2502.14949",
                        "officialWebsiteLink": "https://mbzuai-oryx.github.io/KITAB-Bench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "156",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-05 13:36:30",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-05 13:36:30",
                        "createDate": "2025-03-05 13:36:03",
                        "desc": {
                            "cn": "KITAB-Benchæ˜¯ä¸€ä¸ªå…¨é¢å¤šé¢†åŸŸé˜¿æ‹‰ä¼¯æ–‡ OCR å’Œæ–‡æ¡£ç†è§£åŸºå‡†ï¼ŒåŒ…å« 36 ä¸ªå­é¢†åŸŸï¼Œè¶…è¿‡ 8,809 ä¸ªæ ·æœ¬ï¼Œç»è¿‡ç²¾å¿ƒæŒ‘é€‰ï¼Œä»¥ä¸¥æ ¼è¯„ä¼°é˜¿æ‹‰ä¼¯ OCR å’Œæ–‡æ¡£åˆ†ææ‰€éœ€çš„åŸºæœ¬æŠ€èƒ½ã€‚",
                            "en": "KITAB-Bench is a Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding,  and spans 36 sub-domains with over 8,809 samples, carefully curated to rigorously evaluate essential skills required for Arabic OCR and document analysis."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/kitab_bench'. Error: Path opencompass/kitab_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1332",
                    "name": "UniBench",
                    "version": "1.0.0",
                    "description": "UniBench is meant for evaluating VLMs' reasoning abilities. It is a unified implementation of 50+ VLM benchmarks spanning a comprehensive range of carefully categorized capabilities from object recognition to spatial awareness, counting, and much more. ",
                    "url": "opencompass/opencompass_1332.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1332",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1332",
                        "name": "UniBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/facebookresearch/unibench",
                        "paperLink": "https://arxiv.org/abs/2408.04810",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "156",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-08 11:56:31",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-08 11:56:31",
                        "createDate": "2025-01-03 14:22:05",
                        "desc": {
                            "cn": "UniBenchæ—¨åœ¨è¯„ä¼°VLMçš„æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬50ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¯¹è±¡è¯†åˆ«ã€ç©ºé—´æ„ŸçŸ¥ã€è®¡æ•°ç­‰ä»»åŠ¡ã€‚",
                            "en": "UniBench is meant for evaluating VLMs' reasoning abilities. It is a unified implementation of 50+ VLM benchmarks spanning a comprehensive range of carefully categorized capabilities from object recognition to spatial awareness, counting, and much more. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/unibench'. Error: Path opencompass/unibench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1333",
                    "name": "RedCode",
                    "version": "1.0.0",
                    "description": "RedCode provides comprehensive and practical evaluations on the safety of code agents, including 4,050 risky test cases covering 25 types of critical vulnerabilities spanning 8 domains and 160 prompts aiming to generate harmful code or software.",
                    "url": "opencompass/opencompass_1333.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1333",
                    "sample_count": 1000,
                    "traits": [
                        "Safety",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1333",
                        "name": "RedCode",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AI-secure/RedCode",
                        "paperLink": "https://arxiv.org/abs/2411.07781",
                        "officialWebsiteLink": "https://redcode-agent.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "155",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 11:12:36",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 11:12:36",
                        "createDate": "2025-01-10 19:41:07",
                        "desc": {
                            "cn": "RedCodeæ—¨åœ¨ä¸ºLLMä»£ç æ™ºèƒ½ä½“çš„å®‰å…¨æ€§æä¾›å…¨é¢å®ç”¨çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬æ¥è‡ª8ä¸ªé¢†åŸŸ25ç§å…³é”®æ¼æ´çš„4050ä¸ªé£é™©æµ‹è¯•ç”¨ä¾‹ï¼Œä»¥åŠ160ä¸ªç”Ÿæˆæœ‰å®³ä»£ç çš„æç¤ºã€‚",
                            "en": "RedCode provides comprehensive and practical evaluations on the safety of code agents, including 4,050 risky test cases covering 25 types of critical vulnerabilities spanning 8 domains and 160 prompts aiming to generate harmful code or software."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/redcode'. Error: Path opencompass/redcode is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1367",
                    "name": "A-OKVQA",
                    "version": "1.0.0",
                    "description": "A-OKVQA assesses commonsense reasoning abilities. It is a crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer.",
                    "url": "opencompass/opencompass_1367.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1367",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1367",
                        "name": "A-OKVQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/allenai/aokvqa",
                        "paperLink": "https://arxiv.org/abs/2206.01718",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "155",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:39",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:39",
                        "createDate": "2025-01-10 12:28:01",
                        "desc": {
                            "cn": "A-OKVQAç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„å¸¸è¯†åŠæ¨ç†èƒ½åŠ›ï¼Œç”±25Kä¸ªä¸åŒçš„é—®é¢˜ç»„æˆï¼Œéœ€è¦å¯¹å›¾åƒä¸­æè¿°çš„åœºæ™¯è¿›è¡ŒæŸç§å½¢å¼çš„å¸¸è¯†æ€§æ¨ç†æ¥å›ç­”ã€‚",
                            "en": "A-OKVQA assesses commonsense reasoning abilities. It is a crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/a_okvqa'. Error: Path opencompass/a_okvqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1275",
                    "name": "MMLongBench-Doc",
                    "version": "1.0.0",
                    "description": "MMLongBench-Doc is a long-context multi-modal benchmark comprising 1,062 expert-annotated questions.",
                    "url": "opencompass/opencompass_1275.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1275",
                    "sample_count": 1000,
                    "traits": [
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1275",
                        "name": "MMLongBench-Doc",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "é•¿æ–‡æœ¬",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/mayubo2333/MMLongBench-Doc",
                        "paperLink": "https://arxiv.org/abs/2407.01523",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "154",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:14:42",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:14:42",
                        "createDate": "2025-01-07 14:12:34",
                        "desc": {
                            "cn": "MMLONGBENCH-DOCæ˜¯ä¸€ä¸ªé•¿ä¸Šä¸‹æ–‡çš„å¤šæ¨¡æ€åŸºå‡†ï¼Œç”±1062ä¸ªä¸“å®¶æ³¨é‡Šçš„é—®é¢˜ç»„æˆã€‚",
                            "en": "MMLongBench-Doc is a long-context multi-modal benchmark comprising 1,062 expert-annotated questions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmlongbench_doc'. Error: Path opencompass/mmlongbench_doc is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1548",
                    "name": "MedHallu",
                    "version": "1.0.0",
                    "description": "MedHallu is a comprehensive benchmark dataset designed to evaluate the ability of large language models to detect hallucinations in medical question-answering tasks. ",
                    "url": "opencompass/opencompass_1548.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1548",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1548",
                        "name": "MedHallu",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Medical",
                                "en": "Medical"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MedHallu/MedHalu",
                        "paperLink": "https://arxiv.org/abs/2502.14302",
                        "officialWebsiteLink": "https://medhallu.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "153",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-02-26 19:00:05",
                        "supportOnlineEval": false,
                        "updateDate": "2025-02-26 19:00:05",
                        "createDate": "2025-02-26 13:33:02",
                        "desc": {
                            "cn": "MedHallu æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦é—®é¢˜è§£ç­”ä»»åŠ¡ä¸­æ£€æµ‹å¹»è§‰çš„èƒ½åŠ›ã€‚",
                            "en": "MedHallu is a comprehensive benchmark dataset designed to evaluate the ability of large language models to detect hallucinations in medical question-answering tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medhallu'. Error: Path opencompass/medhallu is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1331",
                    "name": "MedSafetyBench",
                    "version": "1.0.0",
                    "description": "MedSafetyBench is designed to measure the medical safety of LLMs. It includes 1,800 medical safety demonstrations, where each safety demonstration consists of a harmful medical request and a corresponding safe response. ",
                    "url": "opencompass/opencompass_1331.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1331",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1331",
                        "name": "MedSafetyBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AI4LIFE-GROUP/med-safety-bench",
                        "paperLink": "https://arxiv.org/abs/2403.03744",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "150",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 16:32:34",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 16:32:34",
                        "createDate": "2025-01-03 14:14:58",
                        "desc": {
                            "cn": "MedSafetyBenchç”¨äºè¯„ä¼°LLMåœ¨åŒ»ç–—å®‰å…¨ä¸Šçš„è¡¨ç°ï¼ŒåŒ…å«1800ä¸ªç”±æœ‰å®³è¯·æ±‚å’Œå®‰å…¨å“åº”ç»„æˆçš„åŒ»ç–—å®‰å…¨åœºæ™¯ã€‚",
                            "en": "MedSafetyBench is designed to measure the medical safety of LLMs. It includes 1,800 medical safety demonstrations, where each safety demonstration consists of a harmful medical request and a corresponding safe response. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medsafetybench'. Error: Path opencompass/medsafetybench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1705",
                    "name": "OlymMATH",
                    "version": "1.0.0",
                    "description": "A benchmark of 200 Olympiad math problems across algebra, geometry, number theory, and combinatorics. Available in English and Chinese, it features two difficulty levels: EASY (AIME-level) to test standard reasoning, and HARD to challenge advanced models.",
                    "url": "opencompass/opencompass_1705.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1705",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Reasoning",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1705",
                        "name": "OlymMATH",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/RUCAIBox/OlymMATH",
                        "paperLink": "https://arxiv.org/abs/2503.21380",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "52300476",
                            "name": "CoderBak",
                            "avatar": null,
                            "nickname": "CoderBak"
                        },
                        "lookNum": "150",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-03 19:46:03",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-03 19:46:03",
                        "createDate": "2025-04-02 23:55:49",
                        "desc": {
                            "cn": "ä¸€ä¸ªåŒ…å« 200 é“å¥¥æ—åŒ¹å…‹æ•°å­¦é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–ä»£æ•°ã€å‡ ä½•ã€æ•°è®ºå’Œç»„åˆã€‚æˆ‘ä»¬æä¾›è‹±æ–‡å’Œä¸­æ–‡ç‰ˆæœ¬ï¼Œå¹¶æä¾›ä¸¤ä¸ªéš¾åº¦ç­‰çº§ï¼šEASYï¼ˆAIMEæ°´å¹³ï¼‰ç”¨äºæµ‹è¯•æ ‡å‡†æ¨ç†èƒ½åŠ›ï¼Œä»¥åŠ HARD ç”¨äºæŒ‘æˆ˜é«˜çº§æ¨¡å‹ã€‚",
                            "en": "A benchmark of 200 Olympiad math problems across algebra, geometry, number theory, and combinatorics. Available in English and Chinese, it features two difficulty levels: EASY (AIME-level) to test standard reasoning, and HARD to challenge advanced models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/olymmath'. Error: Path opencompass/olymmath is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1451",
                    "name": "MME-RealWorld",
                    "version": "1.0.0",
                    "description": "MME-RealWorld evaluates MLLMs' real-world recognition, featuring 13,366 high-resolution images averaging 2,000 Ã— 1,500 pixels. ",
                    "url": "opencompass/opencompass_1451.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1451",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1451",
                        "name": "MME-RealWorld",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/yfzhang114/MME-RealWorld",
                        "paperLink": "https://arxiv.org/abs/2408.13257",
                        "officialWebsiteLink": "https://mme-realworld.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "150",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-05 15:26:47",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-05 15:26:47",
                        "createDate": "2025-01-27 12:05:30",
                        "desc": {
                            "cn": "MME-RealWorldç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹å¯¹çœŸå®åœºæ™¯çš„ç†è§£èƒ½åŠ›ï¼ŒåŒ…å«13366ä¸ªå¹³å‡2000*1500åƒç´ çš„é«˜åˆ†è¾¨ç‡å›¾åƒã€‚",
                            "en": "MME-RealWorld evaluates MLLMs' real-world recognition, featuring 13,366 high-resolution images averaging 2,000 Ã— 1,500 pixels. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mme_realworld'. Error: Path opencompass/mme_realworld is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1270",
                    "name": "Spider2-V",
                    "version": "1.0.0",
                    "description": "Spider2-V is the first multimodal agent benchmark focusing on professional data science and engineering workflows, featuring 494 real-world tasks in authentic computer environments and incorporating 20 enterprise-level professional applications.",
                    "url": "opencompass/opencompass_1270.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1270",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1270",
                        "name": "Spider2-V",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/xlang-ai/Spider2-V",
                        "paperLink": "https://arxiv.org/abs/2407.10956",
                        "officialWebsiteLink": "https://spider2-v.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "150",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:14:51",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:14:51",
                        "createDate": "2025-01-07 14:07:32",
                        "desc": {
                            "cn": "Spider2-Væ˜¯ç¬¬ä¸€ä¸ªä¸“æ³¨äºä¸“ä¸šæ•°æ®ç§‘å­¦å’Œå·¥ç¨‹å·¥ä½œæµç¨‹çš„å¤šæ¨¡æ€ä»£ç†åŸºå‡†æµ‹è¯•ï¼Œæ•´åˆäº†20 ä¸ªä¼ä¸šçº§ä¸“ä¸šåº”ç”¨ç¨‹åºï¼ŒåŒ…å«æ¥è‡ªçœŸå®è®¡ç®—æœºç¯å¢ƒçš„494 ä¸ªçœŸå®ä»»åŠ¡ã€‚",
                            "en": "Spider2-V is the first multimodal agent benchmark focusing on professional data science and engineering workflows, featuring 494 real-world tasks in authentic computer environments and incorporating 20 enterprise-level professional applications."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/spider2_v'. Error: Path opencompass/spider2_v is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1272",
                    "name": "GSM1k",
                    "version": "1.0.0",
                    "description": "GSM1k is meant for evaluating LLM's math reasoning ability. It mirrors the style and complexity of the established GSM8k benchmark while consider the problem of data-leaking and overfitting",
                    "url": "opencompass/opencompass_1272.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1272",
                    "sample_count": 1000,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1272",
                        "name": "GSM1k",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/scaleapi/gsm1k_eval",
                        "paperLink": "https://arxiv.org/abs/2405.00332",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "149",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 14:36:23",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 14:36:23",
                        "createDate": "2024-12-24 19:16:04",
                        "desc": {
                            "cn": "GSM1kå¯ç”¨äºè¯„ä¼°LLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼›å®ƒä¸GSM8kä¿æŒäº†é£æ ¼å’Œå¤æ‚æ€§çš„ä¸€è‡´ï¼ŒåŒæ—¶è€ƒè™‘äº†æ•°æ®æ³„éœ²å’Œè¿‡æ‹Ÿåˆçš„é—®é¢˜ã€‚",
                            "en": "GSM1k is meant for evaluating LLM's math reasoning ability. It mirrors the style and complexity of the established GSM8k benchmark while consider the problem of data-leaking and overfitting"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gsm1k'. Error: Path opencompass/gsm1k is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1532",
                    "name": "ZeroBench",
                    "version": "1.0.0",
                    "description": "ZeroBench is a challenging visual reasoning benchmark for LMMs. It consists of a main set of 100 high-quality, manually curated questions covering numerous domains, reasoning types and image type. Questions have been designed and calibrated to be beyond the capabilities of current frontier models.\n",
                    "url": "opencompass/opencompass_1532.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1532",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1532",
                        "name": "ZeroBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/jonathan-roberts1/zerobench",
                        "paperLink": "https://arxiv.org/abs/2502.09696",
                        "officialWebsiteLink": "https://zerobench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "147",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-02-25 19:54:07",
                        "supportOnlineEval": false,
                        "updateDate": "2025-02-25 19:54:07",
                        "createDate": "2025-02-24 13:40:00",
                        "desc": {
                            "cn": "ZeroBench æ˜¯é’ˆå¯¹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰æ¨ç†åŸºå‡†ã€‚å®ƒç”±ä¸€ç»„ä¸»è¦çš„ 100 ä¸ªé«˜è´¨é‡äººå·¥é—®é¢˜ç»„æˆï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸã€æ¨ç†ç±»å‹å’Œå›¾åƒç±»å‹ã€‚ZeroBench ä¸­çš„é—®é¢˜ç»è¿‡è®¾è®¡å’Œæ ¡å‡†ï¼Œå·²ç»è¶…å‡ºäº†å½“å‰å‰æ²¿æ¨¡å‹çš„èƒ½åŠ›èŒƒå›´ã€‚",
                            "en": "ZeroBench is a challenging visual reasoning benchmark for LMMs. It consists of a main set of 100 high-quality, manually curated questions covering numerous domains, reasoning types and image type. Questions have been designed and calibrated to be beyond the capabilities of current frontier models.\n"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/zerobench'. Error: Path opencompass/zerobench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1365",
                    "name": "BLINK",
                    "version": "1.0.0",
                    "description": "BLINK focuses on MLLMs' core visual perception abilities. It contains 3,807 multiple-choice questions spanning 14 classic computer vision tasks. ",
                    "url": "opencompass/opencompass_1365.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1365",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1365",
                        "name": "BLINK",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/zeyofu/BLINK_Benchmark",
                        "paperLink": "https://arxiv.org/abs/2404.12390",
                        "officialWebsiteLink": "https://zeyofu.github.io/blink/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "146",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:42",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:42",
                        "createDate": "2025-01-10 12:06:09",
                        "desc": {
                            "cn": "BLINKç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼ŒåŒ…å«æ¥è‡ª14ä¸ªç»å…¸è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„3807é“å¤šé¡¹é€‰æ‹©é¢˜ã€‚",
                            "en": "BLINK focuses on MLLMs' core visual perception abilities. It contains 3,807 multiple-choice questions spanning 14 classic computer vision tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/blink'. Error: Path opencompass/blink is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1326",
                    "name": "MedJourney",
                    "version": "1.0.0",
                    "description": "MedJourney offers a comprehensive assessment of LLMs' effectiveness in real-world clinical settings. It includes multiple tasks from 4 stages of a typical patient's hospital visit journey and comprises 12 datasets.",
                    "url": "opencompass/opencompass_1326.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1326",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1326",
                        "name": "MedJourney",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Medical-AI-Learning/MedJourney",
                        "paperLink": "https://openreview.net/pdf?id=XXaIoJyYs7",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "145",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 16:32:46",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 16:32:46",
                        "createDate": "2025-01-02 20:04:00",
                        "desc": {
                            "cn": "MedJourneyç”¨äºè¯„ä¼° LLM åœ¨çœŸå®ä¸´åºŠç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ï¼Œå…¶ä¸­åŒ…å«å¤šä¸ªä»»åŠ¡ï¼Œæ¶µç›–æ¥è‡ªæ‚£è€…å°±è¯Šå…¸å‹æµç¨‹çš„4ä¸ªé˜¶æ®µçš„12ä¸ªæ•°æ®é›†ã€‚",
                            "en": "MedJourney offers a comprehensive assessment of LLMs' effectiveness in real-world clinical settings. It includes multiple tasks from 4 stages of a typical patient's hospital visit journey and comprises 12 datasets."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medjourney'. Error: Path opencompass/medjourney is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1268",
                    "name": "CTIBench",
                    "version": "1.0.0",
                    "description": "CTIBench is a benchmark designed to assess LLMs' performance in CTI (Cyber threat intelligence) applications. It includes multiple datasets focused on evaluating knowledge acquired by LLMs in the cyber-threat landscape. ",
                    "url": "opencompass/opencompass_1268.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1268",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1268",
                        "name": "CTIBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/xashru/cti-bench",
                        "paperLink": "https://arxiv.org/abs/2406.07599",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "145",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 14:15:02",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 14:15:02",
                        "createDate": "2024-12-24 15:01:43",
                        "desc": {
                            "cn": "CTIBenchæ—¨åœ¨è¯„ä¼°LLMåœ¨CTIï¼ˆç½‘ç»œå®‰å…¨æƒ…æŠ¥ï¼‰åœºæ™¯ä¸‹çš„èƒ½åŠ›ï¼ŒåŒ…å«å¤šä¸ªæ•°æ®é›†ï¼Œä¸“æ³¨äºè¯„æµ‹LLMåœ¨ç½‘ç»œå¨èƒç¯å¢ƒä¸­è·å¾—çš„çŸ¥è¯†ã€‚",
                            "en": "CTIBench is a benchmark designed to assess LLMs' performance in CTI (Cyber threat intelligence) applications. It includes multiple datasets focused on evaluating knowledge acquired by LLMs in the cyber-threat landscape. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ctibench'. Error: Path opencompass/ctibench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1149",
                    "name": "InstruSum",
                    "version": "1.0.0",
                    "description": "InstruSum evaluates the task of instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for desired summary characteristics.\n",
                    "url": "opencompass/opencompass_1149.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1149",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1149",
                        "name": "InstruSum",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NAACL 2024",
                                "en": "NAACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/yale-nlp/InstruSum",
                        "paperLink": "https://aclanthology.org/2024.findings-naacl.280.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50107925",
                            "name": "yale-nlp",
                            "avatar": null,
                            "nickname": "OpenXLab-kJ37KHAvs"
                        },
                        "lookNum": "144",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-27 17:22:45",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-27 17:22:45",
                        "createDate": "2024-10-15 14:35:05",
                        "desc": {
                            "cn": "InstruSum æ˜¯è¯„æµ‹æŒ‡ä»¤å¯æ§çš„æ–‡æœ¬æ‘˜è¦ä»»åŠ¡ï¼Œæ¨¡å‹è¾“å…¥åŒ…æ‹¬æºæ–‡ç« å’Œå¯¹æ‰€éœ€æ‘˜è¦ç‰¹å¾çš„è‡ªç„¶è¯­è¨€è¦æ±‚ã€‚",
                            "en": "InstruSum evaluates the task of instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for desired summary characteristics.\n"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/instrusum'. Error: Path opencompass/instrusum is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1273",
                    "name": "MolPuzzle",
                    "version": "1.0.0",
                    "description": "MolPuzzle is a benchmark for evaluating MLM's reasoning ability, comprising 234 instances of structure elucidation, which feature over 18,000 QA samples.",
                    "url": "opencompass/opencompass_1273.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1273",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1273",
                        "name": "MolPuzzle",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/KehanGuo2/MolPuzzle",
                        "paperLink": "https://openreview.net/pdf?id=t1mAXb4Cop",
                        "officialWebsiteLink": "https://kehanguo2.github.io/Molpuzzle.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "144",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:14:38",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:14:38",
                        "createDate": "2025-01-07 14:07:05",
                        "desc": {
                            "cn": "MolPuzzleç”¨äºè€ƒå¯ŸMLMçš„æ¨ç†èƒ½åŠ›ï¼ŒåŒ…å«234ä¸ªç»“æ„è§£æå®ä¾‹ä»¥åŠè¶…è¿‡18000ä¸ªQAæ ·æœ¬ã€‚",
                            "en": "MolPuzzle is a benchmark for evaluating MLM's reasoning ability, comprising 234 instances of structure elucidation, which feature over 18,000 QA samples."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/molpuzzle'. Error: Path opencompass/molpuzzle is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1323",
                    "name": "SciFIBench",
                    "version": "1.0.0",
                    "description": "SciFIBench is a scientific figure interpretation benchmark for LMMs, consisting of 2000 questions split between two tasks across 8 categories. ",
                    "url": "opencompass/opencompass_1323.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1323",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1323",
                        "name": "SciFIBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/jonathan-roberts1/SciFIBench",
                        "paperLink": "https://arxiv.org/abs/2405.08807",
                        "officialWebsiteLink": "https://scifibench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "143",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 11:12:46",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 11:12:46",
                        "createDate": "2025-01-13 11:57:33",
                        "desc": {
                            "cn": " SciFIBenchç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„ç§‘å­¦å›¾è¡¨è§£é‡Šèƒ½åŠ›ï¼Œç”±2000ä¸ªé—®é¢˜ç»„æˆï¼Œæ¶µç›–8ä¸ªç±»åˆ«çš„2ç§ä»»åŠ¡ã€‚",
                            "en": "SciFIBench is a scientific figure interpretation benchmark for LMMs, consisting of 2000 questions split between two tasks across 8 categories. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/scifibench'. Error: Path opencompass/scifibench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1000",
                    "name": "Q-Bench",
                    "version": "1.0.0",
                    "description": "Q-Bench/Q-Bench+ is a benchmark for general-purpose foundation models on low-level vision.",
                    "url": "opencompass/opencompass_1000.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1000",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1000",
                        "name": "Q-Bench",
                        "emoji": "ğŸ²",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "yes-or-no",
                                "en": "yes-or-no"
                            },
                            {
                                "cn": "what",
                                "en": "what"
                            },
                            {
                                "cn": "how",
                                "en": "how"
                            },
                            {
                                "cn": "distortion",
                                "en": "distortion"
                            },
                            {
                                "cn": "others",
                                "en": "others"
                            },
                            {
                                "cn": "in-context distortion",
                                "en": "in-context distortion"
                            },
                            {
                                "cn": "in-context others",
                                "en": "in-context others"
                            },
                            {
                                "cn": "overall",
                                "en": "overall"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Q-Future/Q-Bench",
                        "paperLink": "https://arxiv.org/abs/2309.14181",
                        "officialWebsiteLink": "https://q-future.github.io/Q-Bench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50151145",
                            "name": null,
                            "avatar": null,
                            "nickname": "Orange"
                        },
                        "lookNum": "143",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-16 20:27:46",
                        "createDate": "2024-08-13 09:33:41",
                        "desc": {
                            "cn": "Q-Bench/Q-Bench+æ˜¯ä¸€ä¸ªé¢å‘å¤šæ¨¡æ€å¤§æ¨¡å‹åº•å±‚è§†è§‰ç†è§£çš„æ•°æ®é›†ã€‚æ­¤æ•°æ®é›†ä»åº•å±‚è§†è§‰çš„æ„ŸçŸ¥ã€æè¿°ã€è¯„ä»·èƒ½åŠ›å‡ºå‘æ¥å¯¹å¤šæ¨¡æ€å¤§æ¨¡å‹è¿›è¡Œå®Œæ•´çš„æµ‹è¯•ï¼Œæµ‹è¯•çš„å¯¹è±¡æ—¢åŒ…æ‹¬å•å¼ å›¾åƒä¹ŸåŒ…æ‹¬å›¾åƒå¯¹ã€‚",
                            "en": "Q-Bench/Q-Bench+ is a benchmark for general-purpose foundation models on low-level vision."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/q_bench'. Error: Path opencompass/q_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1330",
                    "name": "SG-Bench",
                    "version": "1.0.0",
                    "description": "SG-Bench assess LLM safety across various tasks and prompt types. It integrates both generative and discriminative evaluation tasks and includes extended data to examine the impact of prompt engineering and jailbreak. ",
                    "url": "opencompass/opencompass_1330.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1330",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1330",
                        "name": "SG-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MurrayTom/SG-Bench",
                        "paperLink": "https://arxiv.org/abs/2410.21965",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "142",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-08 11:56:40",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-08 11:56:40",
                        "createDate": "2025-01-03 14:07:00",
                        "desc": {
                            "cn": "SG-Benchç”¨äºè¯„ä¼°LLMåœ¨ä¸åŒä»»åŠ¡å’Œæç¤ºä¸‹çš„å®‰å…¨æ€§ï¼Œæ•´åˆäº†ç”Ÿæˆæ€§å’Œåˆ¤åˆ«æ€§è¯„ä¼°ä»»åŠ¡ï¼Œå¹¶åŒ…å«æ‰©å±•æ•°æ®ä»¥åº¦é‡æç¤ºå·¥ç¨‹å’Œè¶Šç‹±å¯¹å®‰å…¨æ€§çš„å½±å“ã€‚",
                            "en": "SG-Bench assess LLM safety across various tasks and prompt types. It integrates both generative and discriminative evaluation tasks and includes extended data to examine the impact of prompt engineering and jailbreak. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/sg_bench'. Error: Path opencompass/sg_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1336",
                    "name": "CompBench",
                    "version": "1.0.0",
                    "description": "CompBench is designed to evaluate the comparative reasoning capability of multimodal large language models, including a collection of around 40K image pairs and visually oriented questions covering 8 dimensions",
                    "url": "opencompass/opencompass_1336.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1336",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1336",
                        "name": "CompBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/RaptorMai/CompBench",
                        "paperLink": "https://arxiv.org/abs/2407.16837",
                        "officialWebsiteLink": "https://compbench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "142",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:05:36",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:05:36",
                        "createDate": "2025-01-03 16:21:25",
                        "desc": {
                            "cn": "CompBenchæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ¯”è¾ƒæ¨ç†èƒ½åŠ›ï¼ŒåŒ…å«çº¦4ä¸‡ä¸ªå›¾åƒå¯¹åŠ8ä¸ªç»´åº¦çš„è§†è§‰é…å¯¹é—®é¢˜ã€‚",
                            "en": "CompBench is designed to evaluate the comparative reasoning capability of multimodal large language models, including a collection of around 40K image pairs and visually oriented questions covering 8 dimensions"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/compbench'. Error: Path opencompass/compbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1241",
                    "name": "MedCalc-Bench",
                    "version": "1.0.0",
                    "description": " MedCalc-Bench focuses on evaluating the medical calculation capability of LLMs. It contains an evaluation set of over 1000 manually reviewed instances from 55 different medical calculation tasks.",
                    "url": "opencompass/opencompass_1241.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1241",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1241",
                        "name": "MedCalc-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ncbi-nlp/MedCalc-Bench",
                        "paperLink": "https://arxiv.org/abs/2406.12036",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "140",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 11:46:27",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 11:46:27",
                        "createDate": "2024-12-20 17:03:18",
                        "desc": {
                            "cn": "MedCalc-Benchä¸“æ³¨äºè¯„ä¼°LLMçš„åŒ»å­¦è®¡ç®—èƒ½åŠ›ï¼ŒåŒ…å«æ¥è‡ª55ä¸ªä¸åŒåŒ»å­¦è®¡ç®—ä»»åŠ¡çš„1000ä¸ªç»è¿‡äººå·¥å®¡æŸ¥çš„å®ä¾‹ã€‚",
                            "en": " MedCalc-Bench focuses on evaluating the medical calculation capability of LLMs. It contains an evaluation set of over 1000 manually reviewed instances from 55 different medical calculation tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medcalc_bench'. Error: Path opencompass/medcalc_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1565",
                    "name": "MME-CoT",
                    "version": "1.0.0",
                    "description": "MME-CoT is a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes.",
                    "url": "opencompass/opencompass_1565.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1565",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1565",
                        "name": "MME-CoT",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/CaraJ7/MME-CoT",
                        "paperLink": "https://arxiv.org/abs/2502.09621",
                        "officialWebsiteLink": "https://mmecot.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "139",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-03 11:01:40",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-03 11:01:40",
                        "createDate": "2025-02-27 11:38:00",
                        "desc": {
                            "cn": "MME-CoTï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼° LMMs CoT æ¨ç†æ€§èƒ½çš„åŸºå‡†ï¼Œæ¶µç›–å…­ä¸ªé¢†åŸŸï¼šæ•°å­¦ã€ç§‘å­¦ã€OCRã€é€»è¾‘ã€æ—¶ç©ºå’Œä¸€èˆ¬åœºæ™¯ã€‚",
                            "en": "MME-CoT is a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mme_cot'. Error: Path opencompass/mme_cot is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1324",
                    "name": "FLUB",
                    "version": "1.0.0",
                    "description": "FLUB evaluates the reasoning and understanding abilities of LLMs. It includes three tasks with increasing difficulty, consisting of the tricky, humorous, and misleading texts collected from the real internet environment.",
                    "url": "opencompass/opencompass_1324.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1324",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1324",
                        "name": "FLUB",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/THUKElab/FLUB",
                        "paperLink": "https://arxiv.org/abs/2402.11100",
                        "officialWebsiteLink": "https://thukelab.github.io/FLUB/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "138",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:05:50",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:05:50",
                        "createDate": "2025-01-02 19:42:41",
                        "desc": {
                            "cn": "FLUBç”¨äºè¯„ä¼°LLMçš„æ¨ç†å’Œç†è§£èƒ½åŠ›ï¼Œå…¶ä¸­åŒ…å«3ä¸ªéš¾åº¦é€’è¿›çš„ä»»åŠ¡ï¼Œç”±ä»çœŸå®äº’è”ç½‘ç¯å¢ƒä¸­æ”¶é›†çš„ç‹¡çŒ¾ã€å¹½é»˜å’Œè¯¯å¯¼æ€§çš„æ–‡æœ¬æ„æˆã€‚",
                            "en": "FLUB evaluates the reasoning and understanding abilities of LLMs. It includes three tasks with increasing difficulty, consisting of the tricky, humorous, and misleading texts collected from the real internet environment."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/flub'. Error: Path opencompass/flub is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1102",
                    "name": "MS_MARCO",
                    "version": "1.0.0",
                    "description": "MS MARCO comprises of 1,010,916 anonymized questionsâ€”sampled from Bingâ€™s search query logsâ€”each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages.",
                    "url": "opencompass/opencompass_1102.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1102",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1102",
                        "name": "MS_MARCO",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/microsoft/MSMARCO-Question-Answering",
                        "paperLink": "https://arxiv.org/pdf/1611.09268",
                        "officialWebsiteLink": "https://microsoft.github.io/msmarco/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "138",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-09 20:03:35",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-09 20:03:35",
                        "createDate": "2024-10-09 16:38:58",
                        "desc": {
                            "cn": "MS MARCO æ•°æ®é›†åŒ…å« 1,010,916 ä¸ªæ¥è‡ª Bing çš„æœç´¢æŸ¥è¯¢æ—¥å¿—çš„åŒ¿åé—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½æœ‰ä¸€ä¸ªäººå·¥ç”Ÿæˆçš„ç­”æ¡ˆå’Œ 182,669 ä¸ªå®Œå…¨ç”±äººé‡å†™çš„ç”Ÿæˆç­”æ¡ˆã€‚æ­¤å¤–ï¼Œè¯¥æ•°æ®é›†è¿˜åŒ…å«ä» 3,563,535 ä¸ªç”± Bing æ£€ç´¢çš„ç½‘é¡µæ–‡æ¡£ä¸­æå–çš„ 8,841,823 ä¸ªæ®µè½ï¼Œè¿™äº›æ®µè½æä¾›äº†ç­–åˆ’è‡ªç„¶è¯­è¨€ç­”æ¡ˆæ‰€éœ€çš„ä¿¡æ¯ã€‚",
                            "en": "MS MARCO comprises of 1,010,916 anonymized questionsâ€”sampled from Bingâ€™s search query logsâ€”each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ms_marco'. Error: Path opencompass/ms_marco is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1269",
                    "name": "ConvBench",
                    "version": "1.0.0",
                    "description": "ConvBench is a novel multi-turn conversation evaluation benchmark tailored for Large Vision-Language Models (LVLMs). It comprises 577 meticulously curated multi-turn conversations encompassing 215 tasks reflective of real-world demands.",
                    "url": "opencompass/opencompass_1269.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1269",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1269",
                        "name": "ConvBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/shirlyliu64/ConvBench",
                        "paperLink": "https://arxiv.org/abs/2403.20194",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "135",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:14:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:14:55",
                        "createDate": "2025-01-07 14:14:09",
                        "desc": {
                            "cn": "ConvBenchæ˜¯ä¸“ç”¨äºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ ï¼ˆLVLMï¼‰çš„æ–°å‹å¤šè½®å¯¹è¯è¯„ä¼°åŸºå‡†ï¼Œç”±åŸºäº215 ä¸ªåæ˜ å®é™…éœ€æ±‚çš„ä»»åŠ¡çš„577ä¸ªå¤šè½®æ¬¡å¯¹è¯ç»„æˆã€‚ ",
                            "en": "ConvBench is a novel multi-turn conversation evaluation benchmark tailored for Large Vision-Language Models (LVLMs). It comprises 577 meticulously curated multi-turn conversations encompassing 215 tasks reflective of real-world demands."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/convbench'. Error: Path opencompass/convbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1566",
                    "name": "MM-IQ",
                    "version": "1.0.0",
                    "description": "MM-IQ is a comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms.",
                    "url": "opencompass/opencompass_1566.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1566",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1566",
                        "name": "MM-IQ",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AceCHQ/MMIQ",
                        "paperLink": "https://arxiv.org/abs/2502.00698",
                        "officialWebsiteLink": "https://acechq.github.io/MMIQ-benchmark/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "134",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-03 11:03:22",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-03 11:03:22",
                        "createDate": "2025-02-27 13:43:06",
                        "desc": {
                            "cn": "MM-IQï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 2,710 ä¸ªç²¾å¿ƒæŒ‘é€‰çš„æµ‹è¯•é¡¹ç›®çš„ç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–äº† 8 ç§ä¸åŒçš„æ¨ç†èŒƒå¼ã€‚",
                            "en": "MM-IQ is a comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mm_iq'. Error: Path opencompass/mm_iq is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1534",
                    "name": "CHASE-Code",
                    "version": "1.0.0",
                    "description": "CHASE is a unified framework to synthetically generate challenging problems using LLMs without human involvement",
                    "url": "opencompass/opencompass_1534.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1534",
                    "sample_count": 1000,
                    "traits": [
                        "Code",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1534",
                        "name": "CHASE-Code",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            },
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/McGill-NLP/CHASE",
                        "paperLink": "https://arxiv.org/pdf/2502.14678",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "134",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 17:02:59",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 17:02:59",
                        "createDate": "2025-02-24 14:12:19",
                        "desc": {
                            "cn": "CHASEæ˜¯ä¸€ä¸ªæ— éœ€äººå·¥å‚ä¸çš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºåˆæˆç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜",
                            "en": "CHASE is a unified framework to synthetically generate challenging problems using LLMs without human involvement"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/chase_code'. Error: Path opencompass/chase_code is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1518",
                    "name": "miniCTX",
                    "version": "1.0.0",
                    "description": "A context-rich benchmark for evaluating neural theorem proving in realistic scenarios, providing premises, full context, multi-source benchmark, and temporal splits, enabling evaluation of a model's ability to work with context that evolves over time.",
                    "url": "opencompass/opencompass_1518.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1518",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1518",
                        "name": "miniCTX",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Strong Reasoning",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/cmu-l3/minictx-eval",
                        "paperLink": "https://www.arxiv.org/pdf/2408.03350",
                        "officialWebsiteLink": "https://cmu-l3.github.io/minictx/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "133",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:53:10",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:53:10",
                        "createDate": "2025-02-20 15:38:03",
                        "desc": {
                            "cn": "ä¸€ä¸ªç”¨äºè¯„ä¼°ç°å®åœºæ™¯ä¸­ç¥ç»å®šç†è¯æ˜çš„ä¸°å¯Œä¸Šä¸‹æ–‡åŸºå‡†ã€‚é€šè¿‡æä¾›å‰æã€å®Œæ•´ä¸Šä¸‹æ–‡ã€å¤šæºåŸºå‡†å’Œæ—¶åºåˆ†å‰²ï¼Œçªå‡ºå…¶è¯„ä¼°æ¨¡å‹å¤„ç†éšæ—¶é—´æ¼”å˜çš„ä¸Šä¸‹æ–‡èƒ½åŠ›ã€‚",
                            "en": "A context-rich benchmark for evaluating neural theorem proving in realistic scenarios, providing premises, full context, multi-source benchmark, and temporal splits, enabling evaluation of a model's ability to work with context that evolves over time."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/minictx'. Error: Path opencompass/minictx is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1618",
                    "name": "FedMABench",
                    "version": "1.0.0",
                    "description": "FedMABench is an open-source benchmark for federated training and evaluation of mobile agents, specifically designed for heterogeneous scenarios.",
                    "url": "opencompass/opencompass_1618.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1618",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1618",
                        "name": "FedMABench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/wwh0411/FedMABench",
                        "paperLink": "https://arxiv.org/abs/2503.05143",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "132",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-10 17:12:07",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-10 17:12:07",
                        "createDate": "2025-03-10 17:06:15",
                        "desc": {
                            "cn": "FedMABench æ˜¯ä¸€ä¸ªå¼€æºçš„è”é‚¦è®­ç»ƒå’Œè¯„ä¼°ç§»åŠ¨ä»£ç†çš„åŸºå‡†ï¼Œç‰¹åˆ«ä¸ºå¼‚æ„åœºæ™¯è®¾è®¡ã€‚",
                            "en": "FedMABench is an open-source benchmark for federated training and evaluation of mobile agents, specifically designed for heterogeneous scenarios."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/fedmabench'. Error: Path opencompass/fedmabench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1325",
                    "name": "LLM-Uncertainty-Bench",
                    "version": "1.0.0",
                    "description": "LLM-Uncertainty-Bench is a new benchmarking approach for LLMs that integrates uncertainty quantification. It spans 5 representative natural language processing tasks, each has a dataset with 10,000 instances.",
                    "url": "opencompass/opencompass_1325.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1325",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1325",
                        "name": "LLM-Uncertainty-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/smartyfh/LLM-Uncertainty-Bench",
                        "paperLink": "https://arxiv.org/abs/2401.12794",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "130",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 14:21:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 14:21:33",
                        "createDate": "2025-01-02 19:52:18",
                        "desc": {
                            "cn": "LLM-Uncertainty-Benchå°†ä¸ç¡®å®šæ€§çº³å…¥LLMè¯„ä¼°ï¼ŒåŒ…å«5ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½æœ‰åŒ…å«10000ä¸ªå®ä¾‹çš„æ•°æ®é›†æ”¯æ’‘ã€‚",
                            "en": "LLM-Uncertainty-Bench is a new benchmarking approach for LLMs that integrates uncertainty quantification. It spans 5 representative natural language processing tasks, each has a dataset with 10,000 instances."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/llm_uncertainty_bench'. Error: Path opencompass/llm_uncertainty_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1099",
                    "name": "MKQA",
                    "version": "1.0.0",
                    "description": "MKQA is an open-domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). ",
                    "url": "opencompass/opencompass_1099.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1099",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1099",
                        "name": "MKQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/apple/ml-mkqa",
                        "paperLink": "https://arxiv.org/pdf/2007.15207",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "130",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-09 20:03:40",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-09 20:03:40",
                        "createDate": "2024-10-09 14:12:42",
                        "desc": {
                            "cn": "MKQA æ˜¯ä¸€ä¸ªå¼€æ”¾åŸŸé—®ç­”è¯„ä¼°é›†ï¼ŒåŒ…å« 10,000 å¯¹é—®é¢˜å’Œç­”æ¡ˆï¼Œæ¶µç›– 26 ç§ç±»å‹å¤šæ ·çš„è¯­è¨€ï¼ˆæ€»è®¡ 260,000 å¯¹é—®é¢˜å’Œç­”æ¡ˆï¼‰ã€‚",
                            "en": "MKQA is an open-domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mkqa'. Error: Path opencompass/mkqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1114",
                    "name": "ASDiv",
                    "version": "1.0.0",
                    "description": "ASDiv is a new MWP corpus that contains diverse lexicon patterns with wide problem type coverage. Each problem provides consistent equations and answers. It is further annotated with the corresponding problem type and grade level.",
                    "url": "opencompass/opencompass_1114.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1114",
                    "sample_count": 1000,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1114",
                        "name": "ASDiv",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/chaochun/nlu-asdiv-dataset/tree/master",
                        "paperLink": "https://aclanthology.org/2020.acl-main.92.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "130",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-11 19:59:24",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-11 19:59:24",
                        "createDate": "2024-10-10 14:06:57",
                        "desc": {
                            "cn": "ASDiv æ˜¯ä¸€ä¸ªæ–°çš„æ•°å­¦æ–‡å­—é—®é¢˜ï¼ˆMWPï¼‰è¯­æ–™åº“ï¼ŒåŒ…å«å¤šæ ·çš„è¯æ±‡æ¨¡å¼ï¼Œè¦†ç›–å¹¿æ³›çš„é—®é¢˜ç±»å‹ã€‚æ¯ä¸ªé—®é¢˜æä¾›å¯¹åº”çš„æ–¹ç¨‹å’Œç­”æ¡ˆã€‚å®ƒè¿›ä¸€æ­¥æ ‡æ³¨äº†ç›¸åº”çš„é—®é¢˜ç±»å‹å’Œå¹´çº§æ°´å¹³ï¼Œå¯ç”¨äºæµ‹è¯•ç³»ç»Ÿçš„èƒ½åŠ›ï¼Œå¹¶æŒ‡æ˜é—®é¢˜çš„éš¾åº¦ç­‰çº§ã€‚",
                            "en": "ASDiv is a new MWP corpus that contains diverse lexicon patterns with wide problem type coverage. Each problem provides consistent equations and answers. It is further annotated with the corresponding problem type and grade level."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/asdiv'. Error: Path opencompass/asdiv is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1606",
                    "name": "MCiteBench",
                    "version": "1.0.0",
                    "description": "MCiteBench is a benchmark to evaluate multimodal citation text generation in MLLMs. It consists of 3,000 samples from 1,749 academic papers, featuring 2,000 Explanation tasks and 1,000 Locating tasks, with balanced evidence across text, figures, tables, and mixed modalities.",
                    "url": "opencompass/opencompass_1606.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1606",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1606",
                        "name": "MCiteBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/caiyuhu/MCiteBench",
                        "paperLink": "https://arxiv.org/abs/2503.02589",
                        "officialWebsiteLink": "https://caiyuhu.github.io/MCiteBench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "129",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-20 14:11:57",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-20 14:11:57",
                        "createDate": "2025-03-20 14:11:46",
                        "desc": {
                            "cn": "MCiteBench æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹ä¸­å¤šæ¨¡æ€å¼•ç”¨æ–‡æœ¬ç”Ÿæˆçš„åŸºå‡†ï¼Œç”± 1,749 ç¯‡å­¦æœ¯è®ºæ–‡ä¸­çš„ 3,000 ä¸ªæ ·æœ¬ç»„æˆï¼ŒåŒ…æ‹¬ 2,000 ä¸ªè§£é‡Šä»»åŠ¡å’Œ 1,000 ä¸ªå®šä½ä»»åŠ¡ï¼Œåœ¨æ–‡æœ¬ã€å›¾è¡¨ã€è¡¨æ ¼å’Œæ··åˆæ¨¡æ€ä¸­å¹³è¡¡è¯æ®ã€‚",
                            "en": "MCiteBench is a benchmark to evaluate multimodal citation text generation in MLLMs. It consists of 3,000 samples from 1,749 academic papers, featuring 2,000 Explanation tasks and 1,000 Locating tasks, with balanced evidence across text, figures, tables, and mixed modalities."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mcitebench'. Error: Path opencompass/mcitebench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1891",
                    "name": "MedXpertQA",
                    "version": "1.0.0",
                    "description": "We introduce MedXpertQA, a highly challenging and comprehensive medical benchmark to evaluate expert-level medical knowledge and advanced reasoning. \nIt has been accepted by ICML 2025 and selected by Google DeepMind as the benchmark for MedGemma.",
                    "url": "opencompass/opencompass_1891.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1891",
                    "sample_count": 1000,
                    "traits": [
                        "Examination",
                        "Reasoning",
                        "Knowledge"
                    ],
                    "eval_type": {
                        "gen": "MedXpertQA_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1891",
                        "name": "MedXpertQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å­¦ç§‘",
                                "en": "Examination"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Medical",
                                "en": "Medical"
                            },
                            {
                                "cn": "ICML 2025",
                                "en": "ICML 2025"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/TsinghuaC3I/MedXpertQA",
                        "paperLink": "https://arxiv.org/abs/2501.18362",
                        "officialWebsiteLink": "https://medxpertqa.github.io",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "85505819",
                            "name": "pjlab-opencompass",
                            "avatar": null,
                            "nickname": "pjlab-opencompass"
                        },
                        "lookNum": "129",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 21:34:53",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 21:34:53",
                        "createDate": "2025-06-04 21:02:16",
                        "desc": {
                            "cn": "MedXpertQAæ˜¯ç”±æ¸…åå¤§å­¦å’Œä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤æ„å»ºçš„å…¨é¢ä¸”å…·æœ‰é«˜åº¦æŒ‘æˆ˜æ€§çš„åŒ»å­¦åŸºå‡†ï¼Œç”¨äºè¯„ä¼°ä¸“å®¶çº§çš„åŒ»å­¦çŸ¥è¯†å’Œé«˜çº§æ¨ç†èƒ½åŠ›ã€‚è®ºæ–‡å·²è¢«ICML 2025æ¥æ”¶ï¼Œå¹¶è¢«Google DeepMindä½¿ç”¨ä½œä¸ºMedGemmaçš„è¯„ä¼°åŸºå‡†ã€‚MedXpertQA å…±åŒ…å« 4,460 é“é¢˜ç›®ï¼Œæ¶µç›– 17 ä¸ªåŒ»å­¦ä¸“ç§‘å’Œ 11 ä¸ªèº«ä½“ç³»ç»Ÿã€‚è¯¥åŸºå‡†åŒ…å«ä¸¤ä¸ªå­é›†ï¼šç”¨äºæ–‡æœ¬åŒ»å­¦èƒ½åŠ›è¯„ä¼°çš„ Text å­é›†ï¼Œä»¥åŠç”¨äºå¤šæ¨¡æ€åŒ»å­¦èƒ½åŠ›è¯„ä¼°çš„ MM å­é›†ã€‚MM å­é›†é¦–æ¬¡å¼•å…¥äº†å¸¦æœ‰å¤šæ ·åŒ–å›¾åƒå’Œä¸°å¯Œä¸´åºŠä¿¡æ¯ï¼ˆå¦‚ç—…å†å’Œæ£€æŸ¥ç»“æœï¼‰çš„ä¸“å®¶çº§è€ƒè¯•é¢˜ï¼ŒåŒºåˆ«äºä¼ ç»Ÿå¤šæ¨¡æ€åŒ»å­¦åŸºå‡†ä¸­åŸºäºå›¾åƒæè¿°ç”Ÿæˆçš„ç®€å•é—®ç­”å¯¹ã€‚",
                            "en": "We introduce MedXpertQA, a highly challenging and comprehensive medical benchmark to evaluate expert-level medical knowledge and advanced reasoning. \nIt has been accepted by ICML 2025 and selected by Google DeepMind as the benchmark for MedGemma."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medxpertqa'. Error: Path opencompass/medxpertqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1607",
                    "name": "ToolRet",
                    "version": "1.0.0",
                    "description": "ToolRet is a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets.",
                    "url": "opencompass/opencompass_1607.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1607",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1607",
                        "name": "ToolRet",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/mangopy/tool-retrieval-benchmark",
                        "paperLink": "https://arxiv.org/abs/2503.01763",
                        "officialWebsiteLink": "https://mangopy.github.io/tool-retrieval-benchmark/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "128",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:43:53",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:43:53",
                        "createDate": "2025-03-06 14:40:00",
                        "desc": {
                            "cn": "ToolRetæ˜¯ä¸€ä¸ªåŒ…å« 7.6k ä¸ªä¸åŒæ£€ç´¢ä»»åŠ¡çš„å¼‚æ„å·¥å…·æ£€ç´¢åŸºå‡†ï¼Œä»¥åŠä»ç°æœ‰æ•°æ®é›†ä¸­æ”¶é›†çš„ 43k ä¸ªå·¥å…·è¯­æ–™åº“ã€‚",
                            "en": "ToolRet is a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/toolret'. Error: Path opencompass/toolret is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1361",
                    "name": "RealworldQA",
                    "version": "1.0.0",
                    "description": "RealWorldQA is a benchmark designed for real-world understanding, including 765 images, each accompanied by a question and a verifiable answer.",
                    "url": "opencompass/opencompass_1361.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1361",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1361",
                        "name": "RealworldQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "128",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 17:25:35",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 17:25:35",
                        "createDate": "2025-01-09 22:10:13",
                        "desc": {
                            "cn": "RealWorldQAç”¨äºè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œä¸­çš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼ŒåŒ…å«765å¼ å›¾åƒï¼Œæ¯å¼ å›¾åƒéƒ½é…æœ‰ä¸€ä¸ªé—®é¢˜å’Œæ˜“äºéªŒè¯çš„ç­”æ¡ˆã€‚",
                            "en": "RealWorldQA is a benchmark designed for real-world understanding, including 765 images, each accompanied by a question and a verifiable answer."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/realworldqa'. Error: Path opencompass/realworldqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1635",
                    "name": "KnowLogic",
                    "version": "1.0.0",
                    "description": "KnowLogic is a knowledge-driven synthetic benchmark designed to evaluate the reasoning abilities of large language models (LLMs). It includes 5400 bilingual (Chinese and English) questions across various domains, covering different aspects of commonsense knowledge and logical reasoning.",
                    "url": "opencompass/opencompass_1635.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1635",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1635",
                        "name": "KnowLogic",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/pokerwf/KnowLogic",
                        "paperLink": "https://arxiv.org/abs/2503.06218",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "128",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-13 15:31:21",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-13 15:31:21",
                        "createDate": "2025-03-13 14:36:03",
                        "desc": {
                            "cn": "KnowLogic æ˜¯ä¸€ä¸ªä»¥çŸ¥è¯†é©±åŠ¨çš„åˆæˆåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ˆLLMsï¼‰ã€‚å®ƒåŒ…å«æ¶µç›–å„ä¸ªé¢†åŸŸã€æ¶µç›–å¸¸è¯†çŸ¥è¯†å’Œé€»è¾‘æ¨ç†ä¸åŒæ–¹é¢çš„ 5400 ä¸ªä¸­è‹±åŒè¯­é—®é¢˜ã€‚",
                            "en": "KnowLogic is a knowledge-driven synthetic benchmark designed to evaluate the reasoning abilities of large language models (LLMs). It includes 5400 bilingual (Chinese and English) questions across various domains, covering different aspects of commonsense knowledge and logical reasoning."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/knowlogic'. Error: Path opencompass/knowlogic is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1376",
                    "name": "GenAI-Bench",
                    "version": "1.0.0",
                    "description": "GenAI-Bench is a benchmark designed to benchmark MLLMsâ€™s ability in judging the quality of AI generative contents, containing over 40,000 human ratings to evaluate the performance of MLLMs on aligning with human preferences.",
                    "url": "opencompass/opencompass_1376.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1376",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1376",
                        "name": "GenAI-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/TIGER-AI-Lab/GenAI-Bench",
                        "paperLink": "https://arxiv.org/abs/2406.13743",
                        "officialWebsiteLink": "https://linzhiqiu.github.io/papers/genai_bench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "127",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:33",
                        "createDate": "2025-01-10 17:02:53",
                        "desc": {
                            "cn": "GenAI-Benchç”¨äºè¡¡é‡MLLMåˆ¤æ–­AIç”Ÿæˆå†…å®¹è´¨é‡çš„èƒ½åŠ›ï¼ŒåŒ…å«è¶…è¿‡40000ä¸ªäººå·¥è¯„åˆ†ç”¨ä»¥è¯„ä¼°å¤§æ¨¡å‹ä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚",
                            "en": "GenAI-Bench is a benchmark designed to benchmark MLLMsâ€™s ability in judging the quality of AI generative contents, containing over 40,000 human ratings to evaluate the performance of MLLMs on aligning with human preferences."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/genai_bench'. Error: Path opencompass/genai_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1453",
                    "name": "MMIU",
                    "version": "1.0.0",
                    "description": "MMIU test MLLMs' multi-image understanding capabilities, encompassesing 7 types of multi-image relationships, 52 tasks, 77K images, and 11K meticulously curated multiple-choice questions.",
                    "url": "opencompass/opencompass_1453.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1453",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1453",
                        "name": "MMIU",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OpenGVLab/MMIU",
                        "paperLink": "https://arxiv.org/abs/2408.02718",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "127",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-02-26 18:55:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-02-26 18:55:33",
                        "createDate": "2025-01-27 14:55:19",
                        "desc": {
                            "cn": "MMIUç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„å¤šå›¾ç†è§£èƒ½åŠ›ï¼ŒåŒ…å«7ç§ç±»å‹çš„å¤šå›¾åƒå…³ç³»ã€52ä¸ªä»»åŠ¡ã€77Kå›¾åƒå’Œ11Kç²¾å¿ƒç­–åˆ’çš„å¤šé€‰é¢˜ã€‚",
                            "en": "MMIU test MLLMs' multi-image understanding capabilities, encompassesing 7 types of multi-image relationships, 52 tasks, 77K images, and 11K meticulously curated multiple-choice questions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmiu'. Error: Path opencompass/mmiu is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1524",
                    "name": "RM-Bench",
                    "version": "1.0.0",
                    "description": "RM-Bench, a benchmark dataset for evaluating reward models of language modeling.",
                    "url": "opencompass/opencompass_1524.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1524",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1524",
                        "name": "RM-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/THU-KEG/RM-Bench",
                        "paperLink": "https://arxiv.org/abs/2410.16184",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "126",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 17:00:31",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 17:00:31",
                        "createDate": "2025-02-21 11:34:57",
                        "desc": {
                            "cn": "RM-Benchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹å¥–åŠ±æ¨¡å‹çš„åŸºå‡†æ•°æ®é›†",
                            "en": "RM-Bench, a benchmark dataset for evaluating reward models of language modeling."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/rm_bench'. Error: Path opencompass/rm_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1648",
                    "name": "DME",
                    "version": "1.0.0",
                    "description": "VLB provides a robust and comprehensive assessment for LVLMs with reduced data contamination and flexible complexity. Based on LlavaBench and MMvet, we have curated two more challenging versions of the datasets: LlavaBench_hard and MMvet_hard. These are the hardest multimodal combinations.",
                    "url": "opencompass/opencompass_1648.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1648",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1648",
                        "name": "DME",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/yangyue5114/DME",
                        "paperLink": "https://arxiv.org/abs/2410.08695",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "125",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-18 17:39:57",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-18 17:39:57",
                        "createDate": "2025-03-18 17:37:18",
                        "desc": {
                            "cn": "VLB ä¸º LVLMs æä¾›äº†ä¸€ç§ç¨³å¥ä¸”å…¨é¢çš„è¯„ä¼°ï¼Œé™ä½äº†æ•°æ®æ±¡æŸ“å¹¶å…·æœ‰çµæ´»çš„å¤æ‚æ€§ã€‚åŸºäº LlavaBench å’Œ MMvetï¼Œæˆ‘ä»¬ç²¾å¿ƒåˆ¶ä½œäº†ä¸¤ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ç‰ˆæœ¬ï¼šLlavaBench_hard å’Œ MMvet_hardã€‚è¿™äº›æ˜¯æˆ‘ä»¬åŠ¨æ€ç­–ç•¥ä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„å¤šæ¨¡æ€ç»„åˆï¼ˆV1+L4ï¼‰ã€‚",
                            "en": "VLB provides a robust and comprehensive assessment for LVLMs with reduced data contamination and flexible complexity. Based on LlavaBench and MMvet, we have curated two more challenging versions of the datasets: LlavaBench_hard and MMvet_hard. These are the hardest multimodal combinations."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/dme'. Error: Path opencompass/dme is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1605",
                    "name": "Deepfake-Eval-2024",
                    "version": "1.0.0",
                    "description": "Deepfake-Eval-2024 is an in-the-wild deepfake dataset. Deepfake-Eval-2024 contains 44 hours of videos, 56.5 hours of audio, and 1,975 images, encompassing contemporary manipulation technologies, diverse media content, 88 different website sources, and 52 different languages. ",
                    "url": "opencompass/opencompass_1605.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1605",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1605",
                        "name": "Deepfake-Eval-2024",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/nuriachandra/Deepfake-Eval-2024",
                        "paperLink": "https://arxiv.org/abs/2503.02857",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "124",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:45:17",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:45:17",
                        "createDate": "2025-03-06 14:04:04",
                        "desc": {
                            "cn": "Deepfake-Eval-2024 æ˜¯ä¸€ä¸ªçœŸå®åœºæ™¯çš„æ·±åº¦ä¼ªé€ æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«44å°æ—¶çš„è§†é¢‘ã€56.5å°æ—¶çš„éŸ³é¢‘å’Œ1,975å¼ å›¾åƒï¼Œæ¶µç›–å½“ä»£ç¯¡æ”¹æŠ€æœ¯ã€å¤šæ ·åŒ–çš„åª’ä½“å†…å®¹ã€æ¥è‡ª88ä¸ªä¸åŒç½‘ç«™æ¥æºçš„ç´ æä»¥åŠ52ç§ä¸åŒè¯­è¨€ã€‚",
                            "en": "Deepfake-Eval-2024 is an in-the-wild deepfake dataset. Deepfake-Eval-2024 contains 44 hours of videos, 56.5 hours of audio, and 1,975 images, encompassing contemporary manipulation technologies, diverse media content, 88 different website sources, and 52 different languages. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/deepfake_eval_2024'. Error: Path opencompass/deepfake_eval_2024 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1537",
                    "name": "MVL-SIB",
                    "version": "1.0.0",
                    "description": "MVL-SIB is a multilingual dataset that provides image-sentence pairs spanning 205 languages and 7 topical categories (entertainment, geography, health, politics, science, sports, travel). It was constructed by extending the SIB-200 benchmark.",
                    "url": "opencompass/opencompass_1537.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1537",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding",
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1537",
                        "name": "MVL-SIB",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            },
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2502.12852",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "123",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-03 11:48:04",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-03 11:48:04",
                        "createDate": "2025-02-24 18:19:14",
                        "desc": {
                            "cn": "MVL-SIB æ˜¯ä¸€ä¸ªå¤šè¯­è¨€æ•°æ®é›†ï¼Œæä¾›äº†æ¶µç›– 205 ç§è¯­è¨€å’Œ 7 ä¸ªä¸»é¢˜ç±»åˆ«çš„å›¾åƒ-å¥å­å¯¹ï¼ˆ entertainment ï¼Œ geography ï¼Œ health ï¼Œ politics ï¼Œ science ï¼Œ sports ï¼Œ travel ï¼‰ã€‚å®ƒé€šè¿‡æ‰©å±• SIB-200 åŸºå‡†æ„å»ºè€Œæˆã€‚",
                            "en": "MVL-SIB is a multilingual dataset that provides image-sentence pairs spanning 205 languages and 7 topical categories (entertainment, geography, health, politics, science, sports, travel). It was constructed by extending the SIB-200 benchmark."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mvl_sib'. Error: Path opencompass/mvl_sib is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1319",
                    "name": "ActionAtlas",
                    "version": "1.0.0",
                    "description": "ActionAtlas is a multiple-choice video question answering benchmark, including 934 videos showcasing 580 unique actions across 56 sports, with a total of 1896 actions within choices.",
                    "url": "opencompass/opencompass_1319.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1319",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1319",
                        "name": "ActionAtlas",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/mrsalehi/action-atlas",
                        "paperLink": "https://arxiv.org/abs/2410.05774",
                        "officialWebsiteLink": "https://mrsalehi.github.io/action-atlas/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "123",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 15:07:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 15:07:55",
                        "createDate": "2024-12-31 15:19:23",
                        "desc": {
                            "cn": "ActionAtlasæ˜¯ä¸€ä¸ªå¤šé¡¹é€‰æ‹©è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬934ä¸ªè§†é¢‘ï¼Œå±•ç¤ºäº†56é¡¹è¿åŠ¨ä¸­çš„580ä¸ªç‹¬ç‰¹åŠ¨ä½œï¼Œé€‰é¡¹å…±åŒ…å«1896ä¸ªåŠ¨ä½œã€‚",
                            "en": "ActionAtlas is a multiple-choice video question answering benchmark, including 934 videos showcasing 580 unique actions across 56 sports, with a total of 1896 actions within choices."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/actionatlas'. Error: Path opencompass/actionatlas is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1581",
                    "name": "HoloBench",
                    "version": "1.0.0",
                    "description": "HoloBench is a benchmark designed to evaluate the ability of long-context language models (LCLMs) to perform holistic reasoning over extended text contexts. ",
                    "url": "opencompass/opencompass_1581.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1581",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning",
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1581",
                        "name": "HoloBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "é•¿æ–‡æœ¬",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/megagonlabs/holobench",
                        "paperLink": "https://arxiv.org/abs/2410.11996",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "123",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:46:42",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:46:42",
                        "createDate": "2025-03-04 10:53:51",
                        "desc": {
                            "cn": "HoloBench æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹ï¼ˆLCLMsï¼‰åœ¨æ‰©å±•æ–‡æœ¬ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œæ•´ä½“æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚",
                            "en": "HoloBench is a benchmark designed to evaluate the ability of long-context language models (LCLMs) to perform holistic reasoning over extended text contexts. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/holobench'. Error: Path opencompass/holobench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1671",
                    "name": "Forensics-bench",
                    "version": "1.0.0",
                    "description": "A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models",
                    "url": "opencompass/opencompass_1671.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1671",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1671",
                        "name": "Forensics-bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Forgery Detection",
                                "en": "Forgery Detection"
                            },
                            {
                                "cn": "Large Vision Language Models",
                                "en": "Large Vision Language Models"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Forensics-Bench/Forensics-Bench",
                        "paperLink": "https://arxiv.org/pdf/2503.15024",
                        "officialWebsiteLink": "https://forensics-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "5001578",
                            "name": null,
                            "avatar": null,
                            "nickname": "åŠ²åŠ²"
                        },
                        "lookNum": "123",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-22 12:30:46",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-22 12:30:46",
                        "createDate": "2025-03-24 22:19:51",
                        "desc": {
                            "cn": "A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models",
                            "en": "A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/forensics_bench'. Error: Path opencompass/forensics_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1843",
                    "name": "EWMBench",
                    "version": "1.0.0",
                    "description": "EWMBench is a benchmark for evaluating Embodied World Models, covering aspects such as scene consistency, motion correctness, and semantic alignment. It includes a diverse dataset and multi-dimensional metrics tailored for embodied manipulation tasks.",
                    "url": "opencompass/opencompass_1843.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1843",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Agent",
                        "Instruct"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1843",
                        "name": "EWMBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            },
                            {
                                "cn": "æŒ‡ä»¤è·Ÿéš",
                                "en": "Instruct"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "worldmodel",
                                "en": "worldmodel"
                            },
                            {
                                "cn": "embodied AI",
                                "en": "embodied AI"
                            },
                            {
                                "cn": "manipulate",
                                "en": "manipulate"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AgibotTech/EWMBench",
                        "paperLink": "https://arxiv.org/abs/2505.09694",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "80003662",
                            "name": null,
                            "avatar": null,
                            "nickname": "NoomiHu"
                        },
                        "lookNum": "122",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-05-27 12:53:24",
                        "supportOnlineEval": false,
                        "updateDate": "2025-05-27 12:53:24",
                        "createDate": "2025-05-27 10:49:31",
                        "desc": {
                            "cn": "EWMBench æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å…·èº«ä¸–ç•Œæ¨¡å‹çš„åŸºå‡†ï¼Œæ¶µç›–â€œåœºæ™¯ä¸€è‡´æ€§â€ã€â€œåŠ¨ä½œæ­£ç¡®æ€§â€å’Œâ€œè¯­ä¹‰å¯¹é½â€ç­‰æ–¹é¢ã€‚å®ƒåŒ…å«å¤šæ ·åŒ–çš„æ•°æ®é›†å’Œé¢å‘å…·èº«ä»»åŠ¡çš„å¤šç»´è¯„ä¼°æŒ‡æ ‡ï¼Œèƒ½å¤Ÿæ­ç¤ºç°æœ‰æ¨¡å‹çš„å±€é™ï¼Œå¹¶ä¸ºç”Ÿæˆå…·ç‰©ç†åŸºç¡€ã€ä»»åŠ¡å¯¼å‘çš„è§†é¢‘æä¾›è¯„ä»·æ ‡å‡†ã€‚\n",
                            "en": "EWMBench is a benchmark for evaluating Embodied World Models, covering aspects such as scene consistency, motion correctness, and semantic alignment. It includes a diverse dataset and multi-dimensional metrics tailored for embodied manipulation tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ewmbench'. Error: Path opencompass/ewmbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1329",
                    "name": "OlympicArena",
                    "version": "1.0.0",
                    "description": "OlympicArena evaluates cognitive reasoning abilities. It includes 11,163 bilingual problems spanning seven fields and 62 international Olympic competitions. ",
                    "url": "opencompass/opencompass_1329.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1329",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1329",
                        "name": "OlympicArena",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/GAIR-NLP/OlympicArena",
                        "paperLink": "https://arxiv.org/abs/2406.12753",
                        "officialWebsiteLink": "https://gair-nlp.github.io/OlympicArena/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "121",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 16:32:39",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 16:32:39",
                        "createDate": "2025-01-03 11:50:50",
                        "desc": {
                            "cn": "OlympicArenaç”¨äºè¯„ä¼°å¤§æ¨¡å‹çš„è®¤çŸ¥æ¨ç†èƒ½åŠ›ï¼ŒåŒ…å«æ¥è‡ª7ä¸ªé¢†åŸŸã€62é¡¹å›½é™…å¥¥æ—åŒ¹å…‹æ¯”èµ›çš„11163ä¸ªåŒè¯­é—®é¢˜ã€‚",
                            "en": "OlympicArena evaluates cognitive reasoning abilities. It includes 11,163 bilingual problems spanning seven fields and 62 international Olympic competitions. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/olympicarena'. Error: Path opencompass/olympicarena is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1853",
                    "name": "MiniLongBench",
                    "version": "1.0.0",
                    "description": "MiniLongBench is a low-cost benchmark for evaluating the Long Context Understanding (LCU) capabilities of LLMs, featuring a compact yet diverse test set of only 237 samples spanning 6 major task categories and 21 distinct tasks.",
                    "url": "opencompass/opencompass_1853.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1853",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding",
                        "Language",
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1853",
                        "name": "MiniLongBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            },
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            },
                            {
                                "cn": "é•¿æ–‡æœ¬",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MilkThink-Lab/MiniLongBench",
                        "paperLink": "https://arxiv.org/abs/2505.19959",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "43307103",
                            "name": null,
                            "avatar": null,
                            "nickname": "linggm3"
                        },
                        "lookNum": "121",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-05-30 10:20:05",
                        "supportOnlineEval": false,
                        "updateDate": "2025-05-30 10:20:05",
                        "createDate": "2025-05-27 21:12:49",
                        "desc": {
                            "cn": "MiniLongBench is a low-cost benchmark for evaluating the Long Context Understanding (LCU) capabilities of LLMs, featuring a compact yet diverse test set of only 237 samples spanning 6 major task categories and 21 distinct tasks.",
                            "en": "MiniLongBench is a low-cost benchmark for evaluating the Long Context Understanding (LCU) capabilities of LLMs, featuring a compact yet diverse test set of only 237 samples spanning 6 major task categories and 21 distinct tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/minilongbench'. Error: Path opencompass/minilongbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1500",
                    "name": "CRPE",
                    "version": "1.0.0",
                    "description": "CRPE is a benchmark designed to quantitatively evaluate the object recognition and relation comprehension ability of models. It consists of four splits, and the evaluation is formulated as single-choice questions. ",
                    "url": "opencompass/opencompass_1500.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1500",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1500",
                        "name": "CRPE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OpenGVLab/all-seeing",
                        "paperLink": "https://arxiv.org/abs/2402.19474",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "121",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:46:25",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:46:25",
                        "createDate": "2025-02-13 19:32:36",
                        "desc": {
                            "cn": "CRPEç”¨äºå®šé‡è¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„å¯¹è±¡è¯†åˆ«å’Œå…³ç³»ç†è§£èƒ½åŠ›ï¼Œåˆ†ä¸ºå››éƒ¨åˆ†ï¼Œä»¥å•é€‰é¢˜å½¢å¼å‘ˆç°ã€‚",
                            "en": "CRPE is a benchmark designed to quantitatively evaluate the object recognition and relation comprehension ability of models. It consists of four splits, and the evaluation is formulated as single-choice questions. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/crpe'. Error: Path opencompass/crpe is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1327",
                    "name": "EHRNoteQA",
                    "version": "1.0.0",
                    "description": "EHRNoteQA evaluates LLM's ability to assist clinical decision-making based on electronic health records, comprising 962 different QA pairs each linked to distinct patients' discharge summaries. ",
                    "url": "opencompass/opencompass_1327.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1327",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1327",
                        "name": "EHRNoteQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ji-youn-kim/EHRNoteQA",
                        "paperLink": "https://arxiv.org/abs/2402.16040",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "120",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 16:32:43",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 16:32:43",
                        "createDate": "2025-01-02 20:15:09",
                        "desc": {
                            "cn": "EHRNoteQAç”¨äºè¯„ä¼°LLMåŸºäºç”µå­å¥åº·è®°å½•è¾…åŠ©ä¸´åºŠå†³ç­–çš„èƒ½åŠ›ï¼Œç”±962ä¸ªé—®ç­”å¯¹ç»„æˆï¼Œæ¯ä¸ªé—®ç­”å¯¹éƒ½ä¸ä¸åŒæ‚£è€…çš„å‡ºé™¢æ€»ç»“ç›¸å…³è”ã€‚",
                            "en": "EHRNoteQA evaluates LLM's ability to assist clinical decision-making based on electronic health records, comprising 962 different QA pairs each linked to distinct patients' discharge summaries. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ehrnoteqa'. Error: Path opencompass/ehrnoteqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1855",
                    "name": "TransBench",
                    "version": "1.0.0",
                    "description": "TransBench is the first industry-oriented comprehensive multilingual translation evaluation system designed for industrial applications. It quantifies translation model performance across diverse industries and linguistic environments through meticulously curated datasets aligned with standards.",
                    "url": "opencompass/opencompass_1855.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1855",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1855",
                        "name": "TransBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AIDC-AI/TransBench",
                        "paperLink": "https://arxiv.org/pdf/2505.14244",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "120",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:20",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:20",
                        "createDate": "2025-06-04 11:17:40",
                        "desc": {
                            "cn": "TransBench is the first industry-oriented comprehensive multilingual translation evaluation system designed for industrial applications. It quantifies translation model performance across diverse industries and linguistic environments through meticulously curated datasets aligned with standards.",
                            "en": "TransBench is the first industry-oriented comprehensive multilingual translation evaluation system designed for industrial applications. It quantifies translation model performance across diverse industries and linguistic environments through meticulously curated datasets aligned with standards."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/transbench'. Error: Path opencompass/transbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1554",
                    "name": "JL1-CD",
                    "version": "1.0.0",
                    "description": "JL1-CD is a large-scale, sub-meter, all-inclusive open-source dataset for remote sensing image change detection (CD). It contains 5,000 pairs of 512Ã—512 pixel satellite images with a resolution of 0.5 to 0.75 meters, covering various types of surface changes in multiple regions of China. ",
                    "url": "opencompass/opencompass_1554.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1554",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1554",
                        "name": "JL1-CD",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/circleLZY/MTKD-CD",
                        "paperLink": "https://arxiv.org/abs/2502.13407",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "119",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:46:12",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:46:12",
                        "createDate": "2025-03-05 18:54:32",
                        "desc": {
                            "cn": "JL1-CD æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€äºšç±³çº§ã€å…¨åŒ…å«çš„å¼€æºé¥æ„Ÿå½±åƒå˜åŒ–æ£€æµ‹ï¼ˆCDï¼‰æ•°æ®é›†ã€‚å®ƒåŒ…å« 5000 å¯¹ 512Ã—512 åƒç´ çš„å«æ˜Ÿå½±åƒï¼Œåˆ†è¾¨ç‡ä¸º 0.5 è‡³ 0.75 ç±³ï¼Œè¦†ç›–ä¸­å›½å¤šä¸ªåœ°åŒºçš„å„ç§åœ°è¡¨å˜åŒ–ã€‚",
                            "en": "JL1-CD is a large-scale, sub-meter, all-inclusive open-source dataset for remote sensing image change detection (CD). It contains 5,000 pairs of 512Ã—512 pixel satellite images with a resolution of 0.5 to 0.75 meters, covering various types of surface changes in multiple regions of China. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/jl1_cd'. Error: Path opencompass/jl1_cd is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1120",
                    "name": "ProofNet",
                    "version": "1.0.0",
                    "description": "ProofNet is a benchmark for autoformalization and formal proving of undergraduate-level mathematics. The ProofNet benchmarks consists of 371 examples, each consisting of a formal theorem statement in Lean 3, a natural language theorem statement, and a natural language proof. ",
                    "url": "opencompass/opencompass_1120.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1120",
                    "sample_count": 1000,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1120",
                        "name": "ProofNet",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/zhangir-azerbayev/ProofNet",
                        "paperLink": "https://arxiv.org/pdf/2302.12433",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "118",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-10 20:21:42",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-10 20:21:42",
                        "createDate": "2024-10-10 14:54:13",
                        "desc": {
                            "cn": "ProofNet æ˜¯ä¸€ä¸ªç”¨äºæœ¬ç§‘æ•°å­¦çš„è‡ªåŠ¨å½¢å¼åŒ–å’Œå½¢å¼è¯æ˜çš„åŸºå‡†ã€‚åŒ…å« 371 ä¸ªç¤ºä¾‹ï¼Œæ¯ä¸ªç¤ºä¾‹åŒ…æ‹¬ä¸€ä¸ª Lean 3 ä¸­çš„å½¢å¼å®šç†é™ˆè¿°ã€ä¸€ä¸ªè‡ªç„¶è¯­è¨€å®šç†é™ˆè¿°å’Œä¸€ä¸ªè‡ªç„¶è¯­è¨€è¯æ˜ã€‚è¿™äº›é—®é¢˜ä¸»è¦æ¥è‡ªæµè¡Œçš„æœ¬ç§‘çº¯æ•°å­¦æ•™æï¼Œæ¶µç›–å®åˆ†æã€å¤åˆ†æã€çº¿æ€§ä»£æ•°ã€æŠ½è±¡ä»£æ•°å’Œæ‹“æ‰‘ç­‰ä¸»é¢˜ã€‚",
                            "en": "ProofNet is a benchmark for autoformalization and formal proving of undergraduate-level mathematics. The ProofNet benchmarks consists of 371 examples, each consisting of a formal theorem statement in Lean 3, a natural language theorem statement, and a natural language proof. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/proofnet'. Error: Path opencompass/proofnet is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1145",
                    "name": "FREB-TQA",
                    "version": "1.0.0",
                    "description": "FREB-TQA is a Fine-grained Robustness Evaluation Benchmark for Table Question Answering. ",
                    "url": "opencompass/opencompass_1145.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1145",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1145",
                        "name": "FREB-TQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NAACL 2024",
                                "en": "NAACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/boschresearch/FREB-TQA",
                        "paperLink": "https://aclanthology.org/2024.naacl-long.137.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50109778",
                            "name": "Bosch_Research",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50109778-dd360e6a-5366-4ab9-8d82-260b4e376600.png",
                            "nickname": "OpenXLab-XxvqMQwcb"
                        },
                        "lookNum": "118",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-27 17:23:19",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-27 17:23:19",
                        "createDate": "2024-10-15 13:24:34",
                        "desc": {
                            "cn": "FREB-TQA æ˜¯ä¸€ä¸ªç»†ç²’åº¦çš„ç¨³å¥æ€§è¯„ä¼°åŸºå‡†ï¼Œä¸“æ³¨äºè¡¨æ ¼é—®ç­”ï¼ˆTQAï¼‰ã€‚",
                            "en": "FREB-TQA is a Fine-grained Robustness Evaluation Benchmark for Table Question Answering. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/freb_tqa'. Error: Path opencompass/freb_tqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1712",
                    "name": "ToolHop",
                    "version": "1.0.0",
                    "description": "ToolHop is a dataset specifically designed for rigorous evaluation of multi-hop tool use, which ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach.",
                    "url": "opencompass/opencompass_1712.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1712",
                    "sample_count": 1000,
                    "traits": [
                        "Agent",
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1712",
                        "name": "ToolHop",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            },
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://huggingface.co/datasets/bytedance-research/ToolHop",
                        "paperLink": "https://arxiv.org/abs/2501.02506",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "62407087",
                            "name": null,
                            "avatar": null,
                            "nickname": "å°¼æ‘©"
                        },
                        "lookNum": "118",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-07 15:15:34",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-07 15:15:34",
                        "createDate": "2025-04-05 23:37:16",
                        "desc": {
                            "cn": "ToolHopæ˜¯ä¸€ä¸ªé€šè¿‡æŸ¥è¯¢é©±åŠ¨æ„å»ºçš„æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºè¯„æµ‹å¤§æ¨¡å‹çš„å¤šè·³å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œå…·å¤‡å¤šæ ·åŒ–çš„æŸ¥è¯¢ã€æœ‰æ„ä¹‰çš„ç›¸äº’ä¾èµ–å…³ç³»ã€æœ¬åœ°å¯æ‰§è¡Œçš„å·¥å…·ã€è¯¦ç»†çš„åé¦ˆå’Œå¯éªŒè¯çš„ç­”æ¡ˆäº”å¤§ç‰¹å¾ã€‚",
                            "en": "ToolHop is a dataset specifically designed for rigorous evaluation of multi-hop tool use, which ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/toolhop'. Error: Path opencompass/toolhop is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1279",
                    "name": "WhodunitBench",
                    "version": "1.0.0",
                    "description": "WhodunitBench is used to evaluate large multimodal agent under complex tasks and dynamic scenarios.",
                    "url": "opencompass/opencompass_1279.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1279",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1279",
                        "name": "WhodunitBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/jun0wanan/WhodunitBench-Murder_Mystery_Games",
                        "paperLink": "https://openreview.net/pdf?id=qmvtDIfbmS",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "117",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 18:10:25",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 18:10:25",
                        "createDate": "2024-12-24 20:24:49",
                        "desc": {
                            "cn": "WhodunitBenchç”¨äºè¯„ä¼°å¤§å‹å¤šæ¨¡å¼ä»£ç†åœ¨å¤æ‚ä»»åŠ¡åœºæ™¯ä¸‹çš„åŠ¨æ€è¯„ä¼°ã€‚",
                            "en": "WhodunitBench is used to evaluate large multimodal agent under complex tasks and dynamic scenarios."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/whodunitbench'. Error: Path opencompass/whodunitbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1579",
                    "name": "PosterSum",
                    "version": "1.0.0",
                    "description": "The PosterSum dataset is a multimodal benchmark designed for the summarization of scientific posters into research paper abstracts. The dataset consists of 16,305 research posters collected from major machine learning conferences, including ICLR, ICML, and NeurIPS, spanning the years 2022-2024. ",
                    "url": "opencompass/opencompass_1579.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1579",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1579",
                        "name": "PosterSum",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/saxenarohit/postersum",
                        "paperLink": "https://arxiv.org/abs/2502.17540",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "116",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:46:01",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:46:01",
                        "createDate": "2025-03-05 18:56:15",
                        "desc": {
                            "cn": "PosterSum æ•°æ®é›†æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨å°†ç§‘å­¦æµ·æŠ¥æ€»ç»“æˆç ”ç©¶è®ºæ–‡æ‘˜è¦ã€‚è¯¥æ•°æ®é›†åŒ…å«ä»2022-2024å¹´çš„ä¸»è¦æœºå™¨å­¦ä¹ ä¼šè®®ï¼ˆåŒ…æ‹¬ ICLRã€ICML å’Œ NeurIPSï¼‰æ”¶é›†çš„ 16,305 ç¯‡ç ”ç©¶æµ·æŠ¥ã€‚",
                            "en": "The PosterSum dataset is a multimodal benchmark designed for the summarization of scientific posters into research paper abstracts. The dataset consists of 16,305 research posters collected from major machine learning conferences, including ICLR, ICML, and NeurIPS, spanning the years 2022-2024. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/postersum'. Error: Path opencompass/postersum is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1625",
                    "name": "ubuntu_osworld",
                    "version": "1.0.0",
                    "description": "OSWorld is a first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across operating systems. ",
                    "url": "opencompass/opencompass_1625.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1625",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1625",
                        "name": "ubuntu_osworld",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/xlang-ai/OSWorld",
                        "paperLink": "https://arxiv.org/abs/2404.07972",
                        "officialWebsiteLink": "https://os-world.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "116",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-12 11:07:38",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-12 11:07:38",
                        "createDate": "2025-03-11 16:09:49",
                        "desc": {
                            "cn": "OSWorld æ˜¯ä¸€ä¸ªé¦–åˆ›çš„ã€å¯æ‰©å±•çš„ã€çœŸå®è®¡ç®—æœºç¯å¢ƒï¼Œç”¨äºå¤šæ¨¡æ€æ™ºèƒ½ä½“ï¼Œæ”¯æŒæ“ä½œç³»ç»Ÿè·¨å¹³å°çš„ä»»åŠ¡è®¾ç½®ã€åŸºäºæ‰§è¡Œçš„è¯„ä¼°å’Œäº¤äº’å¼å­¦ä¹ ã€‚",
                            "en": "OSWorld is a first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across operating systems. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ubuntu_osworld'. Error: Path opencompass/ubuntu_osworld is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1243",
                    "name": "EmbodiedAgentInterface",
                    "version": "1.0.0",
                    "description": "Embodied Agent Interface supports the formalization of various types of tasks and input-output specifications of LLM-based modules, offering a comprehensive assessment of LLMs' performance for different subtasks and pinpointing the strengths and weaknesses in LLM-powered embodied AI systems.",
                    "url": "opencompass/opencompass_1243.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1243",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1243",
                        "name": "EmbodiedAgentInterface",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Embodied Decision Making",
                                "en": "Embodied Decision Making"
                            }
                        ],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/embodied-agent-interface/embodied-agent-interface",
                        "paperLink": "https://arxiv.org/abs/2410.07166",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "116",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 11:46:31",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 11:46:31",
                        "createDate": "2024-12-20 17:40:13",
                        "desc": {
                            "cn": "Embodied Agent Interfaceæ”¯æŒå„ç§ç±»å‹çš„ä»»åŠ¡å’ŒåŸºäºLLMæ¨¡å—çš„è¾“å…¥è¾“å‡ºè§„èŒƒçš„å½¢å¼åŒ–ï¼Œå¯¹LLMåœ¨ä¸åŒå­ä»»åŠ¡ä¸­çš„æ€§èƒ½è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒæŒ‡å‡ºäº†åŸºäºLLMçš„å…·èº«AIç³»ç»Ÿçš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚",
                            "en": "Embodied Agent Interface supports the formalization of various types of tasks and input-output specifications of LLM-based modules, offering a comprehensive assessment of LLMs' performance for different subtasks and pinpointing the strengths and weaknesses in LLM-powered embodied AI systems."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/embodiedagentinterface'. Error: Path opencompass/embodiedagentinterface is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1908",
                    "name": "AudioTrust",
                    "version": "1.0.0",
                    "description": "AudioTrust is a comprehensive trust evaluation framework for Audio Large Language Models (ALLMs) that effectively reveals potential risks in six dimensions: fairness, hallucination, security, privacy, robustness, and authentication. It aggregates over 4,420 real-world audio/text data samples, coveri",
                    "url": "opencompass/opencompass_1908.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1908",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1908",
                        "name": "AudioTrust",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/JusperLee/AudioTrust",
                        "paperLink": "https://arxiv.org/abs/2505.16211",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "21000833",
                            "name": null,
                            "avatar": null,
                            "nickname": "JusperLee"
                        },
                        "lookNum": "115",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-09 11:29:41",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-09 11:29:41",
                        "createDate": "2025-06-06 17:31:02",
                        "desc": {
                            "cn": "AudioTrusté’ˆå¯¹Audio Large Language Modelsï¼ˆALLMsï¼‰çš„å…¨æ–¹ä½å¯ä¿¡è¯„ä¼°æ¡†æ¶ï¼Œæœ‰æ•ˆæ­ç¤ºéŸ³é¢‘å¤§æ¨¡å‹åœ¨å…¬å¹³æ€§ã€å¹»è§‰ã€å®‰å…¨ã€éšç§ã€é²æ£’æ€§å’Œèº«ä»½éªŒè¯å…­å¤§ç»´åº¦çš„æ½œåœ¨é£é™©ã€‚æ±‡é›†4,420+æ¡çœŸå®åœºæ™¯éŸ³é¢‘/æ–‡æœ¬æ•°æ®ï¼Œè¦†ç›–æ—¥å¸¸å¯¹è¯ã€ç´§æ€¥å‘¼å«ã€è¯­éŸ³åŠ©æ‰‹ç­‰18ç§å®éªŒè®¾ç½®ï¼Œè®¾è®¡9é¡¹éŸ³é¢‘ç‰¹å®šè¯„æµ‹æŒ‡æ ‡ï¼Œæ„å»ºè‡ªåŠ¨åŒ–è¯„ä¼°æµæ°´çº¿ã€‚ä¸»è¦å‘ç°ï¼šé—­æºæ¨¡å‹åœ¨é²æ£’æ€§å’Œå®‰å…¨é˜²æŠ¤ä¸Šè¡¨ç°æ›´ä½³ï¼Œå¼€æºæ¨¡å‹å¯¹éšç§å’Œå…¬å¹³æ€§ä»å­˜ç›²åŒºï¼›å¤šæ•°ALLMså¯¹æ€§åˆ«ã€å£éŸ³ã€å¹´é¾„ç­‰æ•æ„Ÿå±æ€§å­˜åœ¨ç³»ç»Ÿæ€§åè§ã€‚æœŸå¾…ç ”ç©¶è€…åŸºäºAudioTrustç»§ç»­ä¼˜åŒ–éŸ³é¢‘å¤§æ¨¡å‹ï¼Œå…±åŒæ¨åŠ¨æ›´å®‰å…¨ã€å¯ä¿¡çš„AIéŸ³é¢‘ç”Ÿæ€å‘å±•ï¼",
                            "en": "AudioTrust is a comprehensive trust evaluation framework for Audio Large Language Models (ALLMs) that effectively reveals potential risks in six dimensions: fairness, hallucination, security, privacy, robustness, and authentication. It aggregates over 4,420 real-world audio/text data samples, coveri"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/audiotrust'. Error: Path opencompass/audiotrust is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1587",
                    "name": "MR-GSM8K",
                    "version": "1.0.0",
                    "description": "MR-GSM8K is a challenging benchmark designed to evaluate the meta-reasoning capabilities of state-of-the-art Large Language Models (LLMs). It goes beyond traditional evaluation metrics by focusing on the reasoning process rather than just the final answer.",
                    "url": "opencompass/opencompass_1587.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1587",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1587",
                        "name": "MR-GSM8K",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/dvlab-research/MR-GSM8K",
                        "paperLink": "https://arxiv.org/abs/2312.17080",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "114",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:47:12",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:47:12",
                        "createDate": "2025-03-04 14:13:29",
                        "desc": {
                            "cn": "MR-GSM8K æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°æœ€å…ˆè¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…ƒæ¨ç†èƒ½åŠ›çš„æŒ‘æˆ˜æ€§åŸºå‡†ã€‚å®ƒè¶…è¶Šäº†ä¼ ç»Ÿçš„è¯„ä¼°æŒ‡æ ‡ï¼Œä¸“æ³¨äºæ¨ç†è¿‡ç¨‹è€Œéä»…ä»…å…³æ³¨æœ€ç»ˆç­”æ¡ˆï¼Œä»è€Œå¯¹æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›è¿›è¡Œæ›´ç»†è‡´çš„è¯„ä¼°ã€‚",
                            "en": "MR-GSM8K is a challenging benchmark designed to evaluate the meta-reasoning capabilities of state-of-the-art Large Language Models (LLMs). It goes beyond traditional evaluation metrics by focusing on the reasoning process rather than just the final answer."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mr_gsm8k'. Error: Path opencompass/mr_gsm8k is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1609",
                    "name": "MASK",
                    "version": "1.0.0",
                    "description": "The MASK evaluation provides a rigorous benchmark for evaluating honesty in large language models by measuring whether models remain truthful when incentivized to lie. The public set contains 1,028 high-quality human-labeled examples across six distinct archetypes.",
                    "url": "opencompass/opencompass_1609.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1609",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1609",
                        "name": "MASK",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/centerforaisafety/mask",
                        "paperLink": "https://arxiv.org/abs/2503.03750",
                        "officialWebsiteLink": "https://www.mask-benchmark.ai/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "114",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:43:36",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:43:36",
                        "createDate": "2025-03-06 16:39:16",
                        "desc": {
                            "cn": "MASK è¯„ä¼°æä¾›äº†ä¸€ä¸ªä¸¥æ ¼çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è¯šå®åº¦ï¼Œé€šè¿‡æµ‹é‡æ¨¡å‹åœ¨å—åˆ°è¯±ä½¿è¯´è°çš„æ¿€åŠ±æ—¶æ˜¯å¦ä¿æŒçœŸå®æ€§ã€‚å…¬å…±é›†åŒ…å« 1,028 ä¸ªé«˜è´¨é‡çš„äººæ ‡æ³¨ç¤ºä¾‹ï¼Œæ¶µç›–å…­ä¸ªä¸åŒçš„åŸå‹ã€‚",
                            "en": "The MASK evaluation provides a rigorous benchmark for evaluating honesty in large language models by measuring whether models remain truthful when incentivized to lie. The public set contains 1,028 high-quality human-labeled examples across six distinct archetypes."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mask'. Error: Path opencompass/mask is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1574",
                    "name": "Text2World",
                    "version": "1.0.0",
                    "description": "Text2World, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. ",
                    "url": "opencompass/opencompass_1574.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1574",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1574",
                        "name": "Text2World",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Aaron617/text2world",
                        "paperLink": "https://arxiv.org/abs/2502.13092",
                        "officialWebsiteLink": "https://text-to-world.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "113",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-04 16:58:03",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-04 16:58:03",
                        "createDate": "2025-03-03 14:38:08",
                        "desc": {
                            "cn": "Text2World åŸºäºè§„åˆ’é¢†åŸŸå®šä¹‰è¯­è¨€ (PDDL)ï¼Œæ‹¥æœ‰æ•°ç™¾ä¸ªä¸åŒçš„é¢†åŸŸï¼Œå¹¶é‡‡ç”¨å¤šæ ‡å‡†ã€åŸºäºæ‰§è¡Œçš„è¡¡é‡æ ‡å‡†æ¥è¿›è¡Œæ›´ç¨³å¥çš„è¯„ä¼°ã€‚",
                            "en": "Text2World, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/text2world'. Error: Path opencompass/text2world is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1358",
                    "name": "Video-MME",
                    "version": "1.0.0",
                    "description": "Video-MME is an evaluation benchmark of multi-modal LLMs in video analysis, including 900 videos in various duration with a total of 254 hours which spans 6 primary visual domains with 30 subfields.",
                    "url": "opencompass/opencompass_1358.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1358",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1358",
                        "name": "Video-MME",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/BradyFU/Video-MME",
                        "paperLink": "https://github.com/BradyFU/Video-MME",
                        "officialWebsiteLink": "https://video-mme.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "113",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:57",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:57",
                        "createDate": "2025-01-09 21:09:31",
                        "desc": {
                            "cn": "Video-MMEç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„è§†é¢‘åˆ†æèƒ½åŠ›ï¼ŒåŒ…å«900ä¸ªä¸åŒé•¿åº¦çš„è§†é¢‘ï¼Œæ¥è‡ª6ä¸ªä¸»è¦è§†è§‰é¢†åŸŸå’Œ30ä¸ªå­é¢†åŸŸï¼Œæ€»æ—¶é•¿è¾¾254å°æ—¶ã€‚",
                            "en": "Video-MME is an evaluation benchmark of multi-modal LLMs in video analysis, including 900 videos in various duration with a total of 254 hours which spans 6 primary visual domains with 30 subfields."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/video_mme'. Error: Path opencompass/video_mme is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1364",
                    "name": "MMT-Bench",
                    "version": "1.0.0",
                    "description": "MMT-Bench is designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning. It comprises 31,325 multi-choice visual questions, covering 32 core meta-tasks and 162 subtasks in multimodal understanding.",
                    "url": "opencompass/opencompass_1364.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1364",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1364",
                        "name": "MMT-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OpenGVLab/MMT-Bench",
                        "paperLink": "https://arxiv.org/abs/2404.16006",
                        "officialWebsiteLink": "https://mmt-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "113",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:44",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:44",
                        "createDate": "2025-01-10 11:56:21",
                        "desc": {
                            "cn": "MMT-Benchè€ƒå¯Ÿå¤šæ¨¡æ€å¤§æ¨¡å‹çš„è§†è§‰è¯†åˆ«ã€å®šä½ã€æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ï¼ŒåŒ…æ‹¬31325ä¸ªå¤šé€‰è§†è§‰é—®é¢˜ï¼Œæ¶µç›–äº†32ä¸ªæ ¸å¿ƒå…ƒä»»åŠ¡å’Œ162ä¸ªå¤šæ¨¡æ€ç†è§£å­ä»»åŠ¡ã€‚",
                            "en": "MMT-Bench is designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning. It comprises 31,325 multi-choice visual questions, covering 32 core meta-tasks and 162 subtasks in multimodal understanding."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmt_bench'. Error: Path opencompass/mmt_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1730",
                    "name": "PaperBench",
                    "version": "1.0.0",
                    "description": "PaperBench is a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research.",
                    "url": "opencompass/opencompass_1730.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1730",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1730",
                        "name": "PaperBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/openai/preparedness",
                        "paperLink": "https://arxiv.org/abs/2504.01848",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "113",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:12:27",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:12:27",
                        "createDate": "2025-04-11 14:10:05",
                        "desc": {
                            "cn": "PaperBenchï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°AIä»£ç†å¤åˆ¶æœ€æ–°AIç ”ç©¶èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚",
                            "en": "PaperBench is a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/paperbench'. Error: Path opencompass/paperbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1580",
                    "name": "A-Bench",
                    "version": "1.0.0",
                    "description": " A-Bench is a benchmark designed to diagnose whether LMMs are masters at evaluating AIGIs. 2,864 AIGIs from 16 text-to-image models are sampled, each paired with question-answers annotated by human experts, and tested across 18 leading LMMs.",
                    "url": "opencompass/opencompass_1580.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1580",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1580",
                        "name": "A-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Q-Future/A-Bench",
                        "paperLink": "https://arxiv.org/abs/2406.03070",
                        "officialWebsiteLink": "https://a-bench-sjtu.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "112",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:46:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:46:33",
                        "createDate": "2025-03-04 10:43:10",
                        "desc": {
                            "cn": "A-Benchæ˜¯ä¸€ä¸ªæ—¨åœ¨è¯Šæ–­ LMMs æ˜¯å¦æ“…é•¿è¯„ä¼° AIGIs çš„åŸºå‡†ï¼Œä» 16 ä¸ªæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­é‡‡æ ·äº† 2,864 ä¸ª AIGIsï¼Œæ¯ä¸ªéƒ½ä¸ç”±äººç±»ä¸“å®¶æ ‡æ³¨çš„é—®é¢˜-ç­”æ¡ˆé…å¯¹ã€‚",
                            "en": " A-Bench is a benchmark designed to diagnose whether LMMs are masters at evaluating AIGIs. 2,864 AIGIs from 16 text-to-image models are sampled, each paired with question-answers annotated by human experts, and tested across 18 leading LMMs."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/a_bench'. Error: Path opencompass/a_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1278",
                    "name": "ChronoMagic-Bench",
                    "version": "1.0.0",
                    "description": "ChronoMagic-Bench can evaluate the temporal and metamorphic capabilities of the T2V (text-to-video) models in time-lapse video generation, introducing 1,649 prompts and real-world videos as references.",
                    "url": "opencompass/opencompass_1278.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1278",
                    "sample_count": 1000,
                    "traits": [
                        "Creation"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1278",
                        "name": "ChronoMagic-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "åˆ›ä½œ",
                                "en": "Creation"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/PKU-YuanGroup/ChronoMagic-Bench",
                        "paperLink": "https://arxiv.org/abs/2406.18522",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "112",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:14:44",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:14:44",
                        "createDate": "2025-01-07 14:13:34",
                        "desc": {
                            "cn": "ChronoMagic-Benchç”¨æ¥è¯„ä¼° T2V ï¼ˆæ–‡æœ¬åˆ°è§†é¢‘ ï¼‰æ¨¡å‹åœ¨å»¶æ—¶è§†é¢‘ç”Ÿæˆä¸­çš„æ—¶é—´å’Œå˜å½¢èƒ½åŠ›ï¼Œå¼•å…¥äº†1649ä¸ªæç¤ºå’ŒçœŸå®ä¸–ç•Œçš„è§†é¢‘ä½œä¸ºå‚è€ƒã€‚",
                            "en": "ChronoMagic-Bench can evaluate the temporal and metamorphic capabilities of the T2V (text-to-video) models in time-lapse video generation, introducing 1,649 prompts and real-world videos as references."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/chronomagic_bench'. Error: Path opencompass/chronomagic_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1280",
                    "name": "AMBROSIA",
                    "version": "1.0.0",
                    "description": "AMBROSIA is a new benchmark for recognizing and interpreting ambiguous requests in text-to-SQL. It contains questions showcasing three different types of ambiguity (scope ambiguity, attachment ambiguity, and vagueness), their interpretations, and corresponding SQL queries.",
                    "url": "opencompass/opencompass_1280.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1280",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1280",
                        "name": "AMBROSIA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/saparina/ambrosia",
                        "paperLink": "https://arxiv.org/abs/2406.19073",
                        "officialWebsiteLink": "https://ambrosia-benchmark.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "110",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 14:31:36",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 14:31:36",
                        "createDate": "2024-12-24 20:31:47",
                        "desc": {
                            "cn": "AMBROSIAæ˜¯è¯†åˆ«å’Œè§£é‡Štext-to-SQLä¸­æ­§ä¹‰è¯·æ±‚çš„æ–°åŸºå‡†ï¼Œå…¶ä¸­åŒ…å«ä¸‰ç§ä¸åŒç±»å‹çš„æ­§ä¹‰ï¼ˆèŒƒå›´æ­§ä¹‰ã€é™„ä»¶æ­§ä¹‰å’Œæ¨¡ç³Šæ€§ï¼‰é—®é¢˜ã€å®ƒä»¬çš„è§£é‡Šå’Œç›¸åº”çš„SQLæŸ¥è¯¢ã€‚",
                            "en": "AMBROSIA is a new benchmark for recognizing and interpreting ambiguous requests in text-to-SQL. It contains questions showcasing three different types of ambiguity (scope ambiguity, attachment ambiguity, and vagueness), their interpretations, and corresponding SQL queries."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ambrosia'. Error: Path opencompass/ambrosia is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1608",
                    "name": "SwiLTra-Bench",
                    "version": "1.0.0",
                    "description": "SwiLTra-Bench is a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems.",
                    "url": "opencompass/opencompass_1608.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1608",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1608",
                        "name": "SwiLTra-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2503.01372",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "109",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:43:44",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:43:44",
                        "createDate": "2025-03-06 15:15:16",
                        "desc": {
                            "cn": "SwiLTra-Benchæ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡ 18 ä¸‡å¯¹å¯¹é½çš„ç‘å£«æ³•å¾‹ç¿»è¯‘è¯­æ–™åº“çš„å…¨é¢å¤šè¯­è¨€åŸºå‡†ï¼ŒåŒ…æ‹¬æ‰€æœ‰ç‘å£«è¯­è¨€ä»¥åŠè‹±è¯­çš„æ³•å¾‹ã€æ‘˜è¦å’Œæ–°é—»ç¨¿ï¼Œæ—¨åœ¨è¯„ä¼°åŸºäºLLMçš„ç¿»è¯‘ç³»ç»Ÿã€‚",
                            "en": "SwiLTra-Bench is a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/swiltra_bench'. Error: Path opencompass/swiltra_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1557",
                    "name": "AIRBench-2024",
                    "version": "1.0.0",
                    "description": "AIR-Bench 2024, the first policy-aligned AI safety benchmark, structures 8 government regulations and 16 corporate policies into four security tiers, with 5,694 diverse prompts spanning these categories.",
                    "url": "opencompass/opencompass_1557.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1557",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1557",
                        "name": "AIRBench-2024",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/stanford-crfm/air-bench-2024",
                        "paperLink": "https://arxiv.org/abs/2407.17436",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "108",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:50:05",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:50:05",
                        "createDate": "2025-02-26 17:30:51",
                        "desc": {
                            "cn": "AIR-Bench 2024æ˜¯é¦–ä¸ªä¸æ–°å…´æ”¿åºœæ³•è§„å’Œä¼ä¸šæ”¿ç­–ç›¸ä¸€è‡´çš„ AI å®‰å…¨åŸºå‡†ï¼Œ å°† 8 é¡¹æ”¿åºœæ³•è§„å’Œ 16 é¡¹ä¼ä¸šæ”¿ç­–åˆ†è§£ä¸ºå››çº§å®‰å…¨åˆ†ç±»ï¼Œæ¶µç›–äº†è¿™äº›ç±»åˆ«çš„ 5,694 ä¸ªå¤šæ ·åŒ–çš„æç¤ºã€‚",
                            "en": "AIR-Bench 2024, the first policy-aligned AI safety benchmark, structures 8 government regulations and 16 corporate policies into four security tiers, with 5,694 diverse prompts spanning these categories."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/airbench_2024'. Error: Path opencompass/airbench_2024 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1360",
                    "name": "LLaVA-Bench",
                    "version": "1.0.0",
                    "description": "LLaVA-Bench evaluates the model's ability in more challenging tasks, including a diverse set of 24 images with 60 questions in total, including indoor and outdoor scenes, memes, paintings, sketches, etc.",
                    "url": "opencompass/opencompass_1360.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1360",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1360",
                        "name": "LLaVA-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/haotian-liu/LLaVA/blob/main/docs/LLaVA_Bench.md",
                        "paperLink": "",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "108",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:49",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:49",
                        "createDate": "2025-01-09 21:47:19",
                        "desc": {
                            "cn": "LLaVA-Benchç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹åº”å¯¹å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå†…å«24 å¼ å›¾åƒåŠ60ä¸ªé—®é¢˜ï¼ŒåŒ…æ‹¬å®¤å†…å’Œå®¤å¤–åœºæ™¯ã€æ¨¡å› ã€ç»˜ç”»ã€ç´ æç­‰ã€‚",
                            "en": "LLaVA-Bench evaluates the model's ability in more challenging tasks, including a diverse set of 24 images with 60 questions in total, including indoor and outdoor scenes, memes, paintings, sketches, etc."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/llava_bench'. Error: Path opencompass/llava_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1636",
                    "name": "UrbanVideo-Bench",
                    "version": "1.0.0",
                    "description": "The benchmark is designed to evaluate whether video-large language models (Video-LLMs) can naturally process continuous first-person visual observations like humans, enabling recall, perception, reasoning, and navigation.",
                    "url": "opencompass/opencompass_1636.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1636",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1636",
                        "name": "UrbanVideo-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/EmbodiedCity/UrbanVideo-Bench.code",
                        "paperLink": "https://arxiv.org/abs/2503.06157",
                        "officialWebsiteLink": "https://embodiedcity.github.io/UrbanVideo-Bench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "108",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-13 15:31:18",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-13 15:31:18",
                        "createDate": "2025-03-13 14:54:33",
                        "desc": {
                            "cn": "UrbanVideo-Benchæ—¨åœ¨è¯„ä¼°è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰æ˜¯å¦èƒ½å¤Ÿåƒäººç±»ä¸€æ ·è‡ªç„¶åœ°å¤„ç†è¿ç»­çš„ç¬¬ä¸€äººç§°è§†è§‰è§‚å¯Ÿï¼Œå®ç°å›å¿†ã€æ„ŸçŸ¥ã€æ¨ç†å’Œå¯¼èˆªã€‚",
                            "en": "The benchmark is designed to evaluate whether video-large language models (Video-LLMs) can naturally process continuous first-person visual observations like humans, enabling recall, perception, reasoning, and navigation."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/urbanvideo_bench'. Error: Path opencompass/urbanvideo_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1680",
                    "name": "BigOBench",
                    "version": "1.0.0",
                    "description": "BigO(Bench)æ˜¯ä¸€ä¸ªåŒ…å«çº¦ 300 ä¸ªéœ€è¦ç”¨ Python è§£å†³çš„ä»£ç é—®é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥åŠ 3,105 ä¸ªç¼–ç é—®é¢˜å’Œ 1,190,250 ä¸ªè§£å†³æ–¹æ¡ˆç”¨äºè®­ç»ƒï¼Œä»¥è¯„ä¼°LLMsèƒ½å¦æ‰¾åˆ°ä»£ç è§£å†³æ–¹æ¡ˆçš„æ—¶é—´-ç©ºé—´å¤æ‚åº¦ï¼Œæˆ–è€…ç”Ÿæˆç¬¦åˆæ—¶é—´-ç©ºé—´å¤æ‚åº¦è¦æ±‚çš„ä»£ç è§£å†³æ–¹æ¡ˆã€‚",
                    "url": "opencompass/opencompass_1680.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1680",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1680",
                        "name": "BigOBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/facebookresearch/bigobench",
                        "paperLink": "https://arxiv.org/abs/2503.15242",
                        "officialWebsiteLink": "https://facebookresearch.github.io/BigOBench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "108",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-26 15:56:21",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-26 15:56:21",
                        "createDate": "2025-03-26 14:44:38",
                        "desc": {
                            "cn": "BigO(Bench)æ˜¯ä¸€ä¸ªåŒ…å«çº¦ 300 ä¸ªéœ€è¦ç”¨ Python è§£å†³çš„ä»£ç é—®é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥åŠ 3,105 ä¸ªç¼–ç é—®é¢˜å’Œ 1,190,250 ä¸ªè§£å†³æ–¹æ¡ˆç”¨äºè®­ç»ƒï¼Œä»¥è¯„ä¼°LLMsèƒ½å¦æ‰¾åˆ°ä»£ç è§£å†³æ–¹æ¡ˆçš„æ—¶é—´-ç©ºé—´å¤æ‚åº¦ï¼Œæˆ–è€…ç”Ÿæˆç¬¦åˆæ—¶é—´-ç©ºé—´å¤æ‚åº¦è¦æ±‚çš„ä»£ç è§£å†³æ–¹æ¡ˆã€‚",
                            "en": "BigO(Bench)æ˜¯ä¸€ä¸ªåŒ…å«çº¦ 300 ä¸ªéœ€è¦ç”¨ Python è§£å†³çš„ä»£ç é—®é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥åŠ 3,105 ä¸ªç¼–ç é—®é¢˜å’Œ 1,190,250 ä¸ªè§£å†³æ–¹æ¡ˆç”¨äºè®­ç»ƒï¼Œä»¥è¯„ä¼°LLMsèƒ½å¦æ‰¾åˆ°ä»£ç è§£å†³æ–¹æ¡ˆçš„æ—¶é—´-ç©ºé—´å¤æ‚åº¦ï¼Œæˆ–è€…ç”Ÿæˆç¬¦åˆæ—¶é—´-ç©ºé—´å¤æ‚åº¦è¦æ±‚çš„ä»£ç è§£å†³æ–¹æ¡ˆã€‚"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/bigobench'. Error: Path opencompass/bigobench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1583",
                    "name": "JudgeBench",
                    "version": "1.0.0",
                    "description": "JudgeBench is a benchmark aimed at evaluating LLM-based judges for objective correctness on challenging response pairs.",
                    "url": "opencompass/opencompass_1583.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1583",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1583",
                        "name": "JudgeBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ScalerLab/JudgeBench",
                        "paperLink": "https://arxiv.org/abs/2410.12784",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "107",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:48:25",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:48:25",
                        "createDate": "2025-03-04 11:06:42",
                        "desc": {
                            "cn": "JudgeBench æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°åŸºäºLLMçš„è£åˆ¤åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å“åº”å¯¹ä¸Šçš„å®¢è§‚æ­£ç¡®æ€§çš„åŸºå‡†ã€‚",
                            "en": "JudgeBench is a benchmark aimed at evaluating LLM-based judges for objective correctness on challenging response pairs."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/judgebench'. Error: Path opencompass/judgebench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1321",
                    "name": "ShoppingMMLU",
                    "version": "1.0.0",
                    "description": "Shopping MMLU is a diverse multi-task online shopping benchmark derived from real-world Amazon data. It consists of 57 tasks covering 4 major shopping skills: concept understanding, knowledge reasoning, user behavior alignment, and multi-linguality.",
                    "url": "opencompass/opencompass_1321.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1321",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1321",
                        "name": "ShoppingMMLU",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/KL4805/ShoppingMMLU",
                        "paperLink": "https://arxiv.org/abs/2410.20745",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "106",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 14:21:36",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 14:21:36",
                        "createDate": "2024-12-31 15:44:55",
                        "desc": {
                            "cn": "Shopping MMLUæ˜¯ä¸€ä¸ªåŸºäºçœŸå®äºšé©¬é€Šæ•°æ®çš„å¤šæ ·åŒ–å¤šä»»åŠ¡åœ¨çº¿è´­ç‰©åŸºå‡†æµ‹è¯•ï¼Œç”±57é¡¹ä»»åŠ¡ç»„æˆï¼Œæ¶µç›–æ¦‚å¿µç†è§£ã€çŸ¥è¯†æ¨ç†ã€ç”¨æˆ·è¡Œä¸ºå¯¹é½å’Œå¤šè¯­è¨€4å¤§è´­ç‰©åœºæ™¯æŠ€èƒ½ã€‚",
                            "en": "Shopping MMLU is a diverse multi-task online shopping benchmark derived from real-world Amazon data. It consists of 57 tasks covering 4 major shopping skills: concept understanding, knowledge reasoning, user behavior alignment, and multi-linguality."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/shoppingmmlu'. Error: Path opencompass/shoppingmmlu is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1267",
                    "name": "SpreadsheetBench",
                    "version": "1.0.0",
                    "description": "SpreadsheetBench is a challenging spreadsheet manipulation benchmark built from 912 real questions gathered from online Excel forums.",
                    "url": "opencompass/opencompass_1267.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1267",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1267",
                        "name": "SpreadsheetBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/RUCKBReasoning/SpreadsheetBench",
                        "paperLink": "https://arxiv.org/abs/2406.14991",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "106",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 14:11:06",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 14:11:06",
                        "createDate": "2024-12-24 14:50:13",
                        "desc": {
                            "cn": "SpreadsheetBenchæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç”µå­è¡¨æ ¼æ“ä½œåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«912ä¸ªæ¥è‡ªåœ¨çº¿Excelè®ºå›çš„çœŸå®é—®é¢˜",
                            "en": "SpreadsheetBench is a challenging spreadsheet manipulation benchmark built from 912 real questions gathered from online Excel forums."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/spreadsheetbench'. Error: Path opencompass/spreadsheetbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1634",
                    "name": "VisualSimpleQA",
                    "version": "1.0.0",
                    "description": "VisualSimpleQA is a multimodal fact-seeking benchmark with two key features. ",
                    "url": "opencompass/opencompass_1634.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1634",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1634",
                        "name": "VisualSimpleQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2503.06492",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "105",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-13 15:31:25",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-13 15:31:25",
                        "createDate": "2025-03-13 14:29:04",
                        "desc": {
                            "cn": "VisualSimpleQA æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€äº‹å®å¯»æ±‚åŸºå‡†ï¼Œå…·æœ‰ä¸¤ä¸ªå…³é”®ç‰¹æ€§ã€‚é¦–å…ˆï¼Œå®ƒä½¿è§†è§‰å’Œè¯­è¨€æ¨¡æ€ä¸­ LVLMs çš„è¯„ä¼°æ›´åŠ ç®€åŒ–å’Œè§£è€¦ã€‚å…¶æ¬¡ï¼Œå®ƒçº³å…¥äº†æ˜ç¡®çš„éš¾åº¦æ ‡å‡†ï¼Œä»¥æŒ‡å¯¼äººå·¥æ ‡æ³¨å¹¶ä¿ƒè¿›æå–å…·æœ‰æŒ‘æˆ˜æ€§çš„å­é›†ï¼Œå³ VisualSimpleQA-hardã€‚",
                            "en": "VisualSimpleQA is a multimodal fact-seeking benchmark with two key features. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/visualsimpleqa'. Error: Path opencompass/visualsimpleqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1654",
                    "name": "V-STaR",
                    "version": "1.0.0",
                    "description": "V-STaR is a spatio-temporal reasoning benchmark for Video-LLMs, evaluating Video-LLMâ€™s spatio-temporal reasoning ability in answering questions explicitly in the context of â€œwhenâ€, â€œwhereâ€, and â€œwhatâ€.",
                    "url": "opencompass/opencompass_1654.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1654",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1654",
                        "name": "V-STaR",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/V-STaR-Bench/V-STaR",
                        "paperLink": "https://arxiv.org/abs/2503.11495",
                        "officialWebsiteLink": "https://v-star-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "105",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-20 14:27:09",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-20 14:27:09",
                        "createDate": "2025-03-20 13:44:36",
                        "desc": {
                            "cn": "V-STaR æ˜¯ä¸€ä¸ªé’ˆå¯¹ Video-LLMsçš„ç©ºé—´æ—¶é—´æ¨ç†åŸºå‡†ï¼Œè¯„ä¼° Video-LLMåœ¨â€œä½•æ—¶â€ã€â€œä½•åœ°â€å’Œâ€œä½•ç‰©â€çš„ä¸Šä¸‹æ–‡ä¸­æ˜ç¡®å›ç­”é—®é¢˜çš„ç©ºé—´æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚",
                            "en": "V-STaR is a spatio-temporal reasoning benchmark for Video-LLMs, evaluating Video-LLMâ€™s spatio-temporal reasoning ability in answering questions explicitly in the context of â€œwhenâ€, â€œwhereâ€, and â€œwhatâ€."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/v_star'. Error: Path opencompass/v_star is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1717",
                    "name": "ViLBench",
                    "version": "1.0.0",
                    "description": "ViLBench is a benchmark designed to evaluate vision-language models. It features 600 examples from 5 datasets, selected based on the criterion that process reward models offer greater improvements over output reward models in guiding generations.",
                    "url": "opencompass/opencompass_1717.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1717",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1717",
                        "name": "ViLBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "vision-language",
                                "en": "vision-language"
                            },
                            {
                                "cn": "math",
                                "en": "math"
                            },
                            {
                                "cn": "reasoning",
                                "en": "reasoning"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2503.20271",
                        "officialWebsiteLink": "https://ucsc-vlaa.github.io/ViLBench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "62408676",
                            "name": null,
                            "avatar": null,
                            "nickname": "ImKe"
                        },
                        "lookNum": "105",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-08 10:14:31",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-08 10:14:31",
                        "createDate": "2025-04-08 02:53:29",
                        "desc": {
                            "cn": "ViLBench æ˜¯ä¸€é¡¹æ—¨åœ¨è¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ•°æ®é›†ï¼Œå…¶å¼ºè°ƒå¯¹æ¨¡å‹è¿›è¡Œç»†ç²’åº¦çš„é€æ­¥æ¨ç†èƒ½åŠ›æµ‹è¯•ã€‚è¯¥åŸºå‡†å…±åŒ…å«600ä¸ªç»è¿‡ä¸¥æ ¼ç­›é€‰çš„æ ·æœ¬ï¼Œæ¥æºäºäº”ä¸ªä¸åŒçš„è§†è§‰-è¯­è¨€æ•°æ®é›†ï¼Œç­›é€‰æ ‡å‡†æ˜¯åœ¨æ¨¡å‹ç­”æ¡ˆé€‰æ‹©è¿‡ç¨‹ä¸­ï¼Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆprocess-reward modelï¼‰ç›¸è¾ƒäºè¾“å‡ºå¥–åŠ±æ¨¡å‹ï¼ˆoutput-reward modelï¼‰å…·æœ‰æ›´æ˜¾è‘—çš„æ€§èƒ½æå‡æ•ˆæœã€‚",
                            "en": "ViLBench is a benchmark designed to evaluate vision-language models. It features 600 examples from 5 datasets, selected based on the criterion that process reward models offer greater improvements over output reward models in guiding generations."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/vilbench'. Error: Path opencompass/vilbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1318",
                    "name": "WikiContradict",
                    "version": "1.0.0",
                    "description": "WikiContradict is a benchmark consisting of 253 high-quality, human-annotated instances designed to assess LLM performance when augmented with retrieved passages containing real-world knowledge conflicts.",
                    "url": "opencompass/opencompass_1318.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1318",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1318",
                        "name": "WikiContradict",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2406.13805",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "104",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 14:21:41",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 14:21:41",
                        "createDate": "2024-12-31 14:05:23",
                        "desc": {
                            "cn": "WikiContradictæ—¨åœ¨è¯„ä¼°LLMé‡åˆ°åŒ…å«çœŸå®ä¸–ç•ŒçŸ¥è¯†å†²çªçš„æ®µè½æ£€ç´¢å¢å¼ºæ—¶çš„æ€§èƒ½ï¼Œç”±253ä¸ªé«˜è´¨é‡çš„äººå·¥æ³¨é‡Šå®ä¾‹ç»„æˆã€‚",
                            "en": "WikiContradict is a benchmark consisting of 253 high-quality, human-annotated instances designed to assess LLM performance when augmented with retrieved passages containing real-world knowledge conflicts."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/wikicontradict'. Error: Path opencompass/wikicontradict is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1147",
                    "name": "M3T",
                    "version": "1.0.0",
                    "description": "M3T is a novel benchmark dataset tailored to evaluate NMT systems on the comprehensive task of translating semi-structured documents. This dataset aims to bridge the evaluation gap in document-level NMT systems, acknowledging the challenges posed by rich text layouts in real-world applications.",
                    "url": "opencompass/opencompass_1147.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1147",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1147",
                        "name": "M3T",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NAACL 2024",
                                "en": "NAACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/amazon-science/m3t-multi-modal-translation-bench",
                        "paperLink": "https://aclanthology.org/2024.naacl-short.41.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "5010775",
                            "name": "amazon-science",
                            "avatar": null,
                            "nickname": "OpenXLab-hzYw4sVeC"
                        },
                        "lookNum": "104",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-27 17:23:28",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-27 17:23:28",
                        "createDate": "2024-10-15 13:57:09",
                        "desc": {
                            "cn": "M3T æ˜¯æ—¨åœ¨è¯„ä¼°ç¥ç»æœºå™¨ç¿»è¯‘ï¼ˆNMTï¼‰ç³»ç»Ÿåœ¨ç¿»è¯‘åŠç»“æ„åŒ–æ–‡æ¡£çš„ç»¼åˆä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚",
                            "en": "M3T is a novel benchmark dataset tailored to evaluate NMT systems on the comprehensive task of translating semi-structured documents. This dataset aims to bridge the evaluation gap in document-level NMT systems, acknowledging the challenges posed by rich text layouts in real-world applications."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/m3t'. Error: Path opencompass/m3t is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1971",
                    "name": "DyCodeEval",
                    "version": "1.0.0",
                    "description": "DyCodeEval introduces methods to generate dynamic evaluation dataset and metric. Using multi-agent cooperation to rewrite benchmarks at evaluation time, it generates semantically equivalent, diverse, and non-deterministic problemsâ€”reducing data contamination and enabling more trustworthy evaluation.",
                    "url": "opencompass/opencompass_1971.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1971",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Reasoning",
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1971",
                        "name": "DyCodeEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Code",
                                "en": "Code"
                            },
                            {
                                "cn": "Dynamic Benchmarking",
                                "en": "Dynamic Benchmarking"
                            },
                            {
                                "cn": "ICML 2025",
                                "en": "ICML 2025"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SeekingDream/DyCodeEval",
                        "paperLink": "https://arxiv.org/pdf/2503.04149",
                        "officialWebsiteLink": "https://codekaleidoscope.github.io/dycodeeval.html",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "53004543",
                            "name": null,
                            "avatar": null,
                            "nickname": "CM"
                        },
                        "lookNum": "104",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-24 10:13:26",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-24 10:13:26",
                        "createDate": "2025-06-24 05:07:16",
                        "desc": {
                            "cn": "DyCodeEval æå‡ºäº†ä¸€ç§åŠ¨æ€ç”Ÿæˆè¯„æµ‹æ•°æ®é›†å’Œè¯„æµ‹æŒ‡æ ‡çš„æ–¹æ³•ã€‚é€šè¿‡å¤šæ™ºèƒ½ä½“åä½œï¼Œåœ¨è¯„æµ‹æ—¶å¯¹åŸºå‡†é¢˜ç›®è¿›è¡Œé‡å†™ï¼Œç”Ÿæˆè¯­ä¹‰ç­‰ä»·ã€å¤šæ ·åŒ–ä¸”éç¡®å®šæ€§çš„é—®é¢˜ï¼Œä»è€Œå‡å°‘æ•°æ®æ±¡æŸ“ï¼Œå®ç°æ›´å¯ä¿¡çš„è¯„æµ‹ã€‚\n",
                            "en": "DyCodeEval introduces methods to generate dynamic evaluation dataset and metric. Using multi-agent cooperation to rewrite benchmarks at evaluation time, it generates semantically equivalent, diverse, and non-deterministic problemsâ€”reducing data contamination and enabling more trustworthy evaluation."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/dycodeeval'. Error: Path opencompass/dycodeeval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1277",
                    "name": "VideoGUI",
                    "version": "1.0.0",
                    "description": "VideoGUI is designed to evaluate GUI assistants on visual-centric GUI tasks. Sourced from high-quality web instructional videos, it focuses on tasks involving professional and novel software and complex activities (e.g., video editing). ",
                    "url": "opencompass/opencompass_1277.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1277",
                    "sample_count": 1000,
                    "traits": [
                        "Creation"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1277",
                        "name": "VideoGUI",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "åˆ›ä½œ",
                                "en": "Creation"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/showlab/videogui",
                        "paperLink": "https://arxiv.org/abs/2406.10227",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "104",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:14:48",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:14:48",
                        "createDate": "2025-01-07 14:13:51",
                        "desc": {
                            "cn": "VideoGUIæ—¨åœ¨è¯„ä¼°ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„GUIä»»åŠ¡ä¸Šçš„GUIåŠ©æ‰‹ï¼Œæ¥è‡ªé«˜è´¨é‡çš„ç½‘ç»œæ•™å­¦è§†é¢‘ï¼Œä¾§é‡äºæ¶‰åŠä¸“ä¸šå’Œæ–°é¢–è½¯ä»¶å’Œå¤æ‚æ´»åŠ¨ï¼ˆä¾‹å¦‚è§†é¢‘ç¼–è¾‘ï¼‰çš„ä»»åŠ¡ã€‚",
                            "en": "VideoGUI is designed to evaluate GUI assistants on visual-centric GUI tasks. Sourced from high-quality web instructional videos, it focuses on tasks involving professional and novel software and complex activities (e.g., video editing). "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/videogui'. Error: Path opencompass/videogui is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1586",
                    "name": "MRAG-Bench",
                    "version": "1.0.0",
                    "description": "MRAG-Bench consists of 16,130 images and 1,353 human-annotated multiple-choice questions across 9 distinct scenarios, providing a robust and systematic evaluation of Large Vision Language Model (LVLM)â€™s vision-centric multimodal retrieval-augmented generation (RAG) abilities.",
                    "url": "opencompass/opencompass_1586.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1586",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1586",
                        "name": "MRAG-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/mragbench/MRAG-Bench",
                        "paperLink": "https://arxiv.org/abs/2410.08182",
                        "officialWebsiteLink": "https://mragbench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "102",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:47:20",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:47:20",
                        "createDate": "2025-03-04 13:56:04",
                        "desc": {
                            "cn": "MRAG-Bench åŒ…å« 16,130 å¼ å›¾ç‰‡å’Œ 1,353 ä¸ªè·¨è¶Š 9 ä¸ªä¸åŒåœºæ™¯çš„äººæ ‡æ³¨å¤šé€‰é¢˜ï¼Œä¸ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰çš„è§†è§‰ä¸­å¿ƒå¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰èƒ½åŠ›æä¾›äº†ç¨³å¥å’Œç³»ç»Ÿçš„è¯„ä¼°ã€‚",
                            "en": "MRAG-Bench consists of 16,130 images and 1,353 human-annotated multiple-choice questions across 9 distinct scenarios, providing a robust and systematic evaluation of Large Vision Language Model (LVLM)â€™s vision-centric multimodal retrieval-augmented generation (RAG) abilities."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mrag_bench'. Error: Path opencompass/mrag_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1356",
                    "name": "MM-Vet",
                    "version": "1.0.0",
                    "description": "MM-Vet evaluates the capabilities to deal with complicated multimodal tasks, defining 6 core VL capabilities and examining the 16 integrations of interest derived from the capability combination.",
                    "url": "opencompass/opencompass_1356.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1356",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1356",
                        "name": "MM-Vet",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/yuweihao/MM-Vet",
                        "paperLink": "https://arxiv.org/abs/2308.02490",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "102",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:24:00",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:24:00",
                        "createDate": "2025-01-09 20:41:34",
                        "desc": {
                            "cn": "MM-Vetç”¨äºè¯„ä¼°å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡èƒ½åŠ›ï¼Œæ¶µç›–äº†6ä¸ªæ ¸å¿ƒè§†è§‰è¯­è¨€åŠŸèƒ½çš„16ç§åŠŸèƒ½ç»„åˆã€‚",
                            "en": "MM-Vet evaluates the capabilities to deal with complicated multimodal tasks, defining 6 core VL capabilities and examining the 16 integrations of interest derived from the capability combination."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mm_vet'. Error: Path opencompass/mm_vet is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1363",
                    "name": "SEED-Bench-2",
                    "version": "1.0.0",
                    "description": "SEED-Bench-2 assesses both text and image generation of MLLMs. It spans 27 evaluation dimensions, featuring 24K multiple-choice questions with precise human annotations.",
                    "url": "opencompass/opencompass_1363.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1363",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1363",
                        "name": "SEED-Bench-2",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AILab-CVC/SEED-Bench",
                        "paperLink": "https://arxiv.org/abs/2311.17092",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "102",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:52",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:52",
                        "createDate": "2025-01-10 11:31:59",
                        "desc": {
                            "cn": "SEED-Bench-2ç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ–‡æœ¬å’Œå›¾åƒç”Ÿæˆèƒ½åŠ›ï¼ŒåŒ…æ‹¬è·¨è¶Š27ä¸ªç»´åº¦çš„24Ké“å¤šé€‰é¢˜åŠå‡†ç¡®çš„äººå·¥æ³¨é‡Šã€‚",
                            "en": "SEED-Bench-2 assesses both text and image generation of MLLMs. It spans 27 evaluation dimensions, featuring 24K multiple-choice questions with precise human annotations."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/seed_bench_2'. Error: Path opencompass/seed_bench_2 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1640",
                    "name": "EMMA",
                    "version": "1.0.0",
                    "description": "EMMA is composed of 2,788 problems, of which 1,796 are newly constructed, across four domains. Within each subject, we further provide fine-grained labels for each question based on the specific skills it measures.",
                    "url": "opencompass/opencompass_1640.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1640",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1640",
                        "name": "EMMA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/hychaochao/EMMA",
                        "paperLink": "https://www.arxiv.org/abs/2501.05444",
                        "officialWebsiteLink": "https://emma-benchmark.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "102",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-17 10:46:29",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-17 10:46:29",
                        "createDate": "2025-03-17 10:44:03",
                        "desc": {
                            "cn": "EMMA ç”± 2,788 ä¸ªé—®é¢˜ç»„æˆï¼Œå…¶ä¸­ 1,796 ä¸ªæ˜¯æ–°æ„å»ºçš„ï¼Œæ¶µç›–å››ä¸ªé¢†åŸŸã€‚åœ¨æ¯ä¸ªä¸»é¢˜ä¸­ï¼Œæˆ‘ä»¬æ ¹æ®æ‰€æµ‹é‡çš„å…·ä½“æŠ€èƒ½ä¸ºæ¯ä¸ªé—®é¢˜æä¾›ç»†ç²’åº¦æ ‡ç­¾ã€‚",
                            "en": "EMMA is composed of 2,788 problems, of which 1,796 are newly constructed, across four domains. Within each subject, we further provide fine-grained labels for each question based on the specific skills it measures."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/emma'. Error: Path opencompass/emma is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1274",
                    "name": "IMDL-BenCo",
                    "version": "1.0.0",
                    "description": "IMDL-BenCo offers a comprehensive IMDL benchmark and modular codebase.",
                    "url": "opencompass/opencompass_1274.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1274",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1274",
                        "name": "IMDL-BenCo",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/scu-zjz/IMDLBenCo",
                        "paperLink": "https://arxiv.org/abs/2406.10580",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "102",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-30 16:30:17",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-30 16:30:17",
                        "createDate": "2024-12-30 16:26:51",
                        "desc": {
                            "cn": "IMDL-BenCoæä¾›äº†å…¨é¢çš„IMDLåŸºå‡†æµ‹è¯•å’Œæ¨¡å—åŒ–ä»£ç åº“ã€‚",
                            "en": "IMDL-BenCo offers a comprehensive IMDL benchmark and modular codebase."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/imdl_benco'. Error: Path opencompass/imdl_benco is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1320",
                    "name": "IaC-Eval",
                    "version": "1.0.0",
                    "description": "IaC-Eval is meant for quantitatively evaluating the capabilities of LLMs in cloud IaC code generation, containing 458 questions ranging from simple to difficult across various cloud services.",
                    "url": "opencompass/opencompass_1320.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1320",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1320",
                        "name": "IaC-Eval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/autoiac-project/iac-eval",
                        "paperLink": "https://openreview.net/pdf?id=7TCK0aBL1C",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "101",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 14:21:38",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 14:21:38",
                        "createDate": "2024-12-31 15:35:20",
                        "desc": {
                            "cn": "IaC-Evalç”¨äºå®šé‡è¯„ä¼°LLMåœ¨äº‘IaCä»£ç ç”Ÿæˆä¸­çš„åŠŸèƒ½ï¼Œå…¶ä¸­åŒ…å«458ä¸ªä»æ˜“åˆ°éš¾çš„é—®é¢˜ï¼Œæ¶µç›–äº†å„ç§äº‘æœåŠ¡ã€‚",
                            "en": "IaC-Eval is meant for quantitatively evaluating the capabilities of LLMs in cloud IaC code generation, containing 458 questions ranging from simple to difficult across various cloud services."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/iac_eval'. Error: Path opencompass/iac_eval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1362",
                    "name": "POPE",
                    "version": "1.0.0",
                    "description": "POPE is an improved evaluation method for LVLMs' object hallucination by proposing a polling-based query method. It offers a more stable and flexible solution.",
                    "url": "opencompass/opencompass_1362.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1362",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1362",
                        "name": "POPE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AoiDragon/POPE",
                        "paperLink": "https://arxiv.org/abs/2305.10355",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "101",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:47",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:47",
                        "createDate": "2025-01-10 10:53:21",
                        "desc": {
                            "cn": "POPEç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„ç‰©ä½“å¹»è§‰ï¼ŒåŸºäºè½®è¯¢çš„æŸ¥è¯¢æ–¹æ³•è®¾è®¡ï¼Œæä¾›äº†ä¸€ç§æ›´ç¨³å®šã€æ›´çµæ´»çš„è¯„ä¼°æ–¹æ¡ˆã€‚",
                            "en": "POPE is an improved evaluation method for LVLMs' object hallucination by proposing a polling-based query method. It offers a more stable and flexible solution."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/pope'. Error: Path opencompass/pope is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1751",
                    "name": "MultiLoKo",
                    "version": "1.0.0",
                    "description": "MultiLoKo is a multilingual knowledge benchmark, covering 30 languages plus English.",
                    "url": "opencompass/opencompass_1751.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1751",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1751",
                        "name": "MultiLoKo",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/facebookresearch/multiloko/",
                        "paperLink": "https://arxiv.org/abs/2504.10356",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "101",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-21 12:34:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-21 12:34:33",
                        "createDate": "2025-04-21 11:55:37",
                        "desc": {
                            "cn": "MultiLoKoæ˜¯ä¸€ä¸ªå¤šè¯­è¨€çŸ¥è¯†åŸºå‡†ï¼Œæ¶µç›–30ç§è¯­è¨€åŠè‹±è¯­ã€‚",
                            "en": "MultiLoKo is a multilingual knowledge benchmark, covering 30 languages plus English."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/multiloko'. Error: Path opencompass/multiloko is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1539",
                    "name": "MM-RLHF",
                    "version": "1.0.0",
                    "description": " MM-RLHF, a comprehensive project for aligning Multimodal Large Language Models (MLLMs) with human preferences. The dataset and algorithms enable consistent performance improvements across 10 dimensions and 27 benchmarks for open-source MLLMs.",
                    "url": "opencompass/opencompass_1539.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1539",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1539",
                        "name": "MM-RLHF",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            },
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Kwai-YuanQi/MM-RLHF",
                        "paperLink": "https://arxiv.org/abs/2406.08487",
                        "officialWebsiteLink": "https://mm-rlhf.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "100",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:50:29",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:50:29",
                        "createDate": "2025-02-24 19:28:08",
                        "desc": {
                            "cn": "MM-RLHFï¼Œè¿™æ˜¯ä¸€ä¸ªå°†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸äººç±»åå¥½å¯¹é½çš„å…¨é¢é¡¹ç›®ï¼Œä½¿å¼€æºå¤šè¯­è¨€æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨ 10 ä¸ªç»´åº¦å’Œ 27 ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°æŒç»­çš„æ€§èƒ½æå‡ã€‚",
                            "en": " MM-RLHF, a comprehensive project for aligning Multimodal Large Language Models (MLLMs) with human preferences. The dataset and algorithms enable consistent performance improvements across 10 dimensions and 27 benchmarks for open-source MLLMs."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mm_rlhf'. Error: Path opencompass/mm_rlhf is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1317",
                    "name": "RepLiQA",
                    "version": "1.0.0",
                    "description": "RepLiQA is suited for question-answering and topic retrieval tasks, including collection of five splits of test sets. Accurate answers can only be generated if a model can find relevant content within the provided document.",
                    "url": "opencompass/opencompass_1317.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1317",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1317",
                        "name": "RepLiQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ServiceNow/repliqa",
                        "paperLink": "https://arxiv.org/abs/2406.11811",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "100",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 14:21:43",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 14:21:43",
                        "createDate": "2024-12-31 13:56:35",
                        "desc": {
                            "cn": "RepLiQAé€‚ç”¨äºé—®ç­”å’Œä¸»é¢˜æ£€ç´¢ä»»åŠ¡ï¼Œé›†åˆäº†æ˜¯5ä¸ªæµ‹è¯•é›†ï¼›åªæœ‰å½“æ¨¡å‹å¯ä»¥åœ¨æä¾›çš„æ–‡æ¡£ä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹æ—¶ï¼Œæ‰èƒ½ç”Ÿæˆå‡†ç¡®çš„ç­”æ¡ˆã€‚",
                            "en": "RepLiQA is suited for question-answering and topic retrieval tasks, including collection of five splits of test sets. Accurate answers can only be generated if a model can find relevant content within the provided document."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/repliqa'. Error: Path opencompass/repliqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1633",
                    "name": "ProJudge",
                    "version": "1.0.0",
                    "description": "ProJudge is a comprehensive, multi-modal, multi-discipline, and multi-difficulty benchmark specifically designed for evaluating abilities of MLLM-based process judges.It comprises 2,400 test cases and 50,118 step-level labels, spanning four scientific disciplines with diverse difficulty levels and",
                    "url": "opencompass/opencompass_1633.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1633",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1633",
                        "name": "ProJudge",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/jiaxin-ai/ProJudge",
                        "paperLink": "https://arxiv.org/abs/2503.06553",
                        "officialWebsiteLink": "https://projudge.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "100",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-13 15:31:28",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-13 15:31:28",
                        "createDate": "2025-03-13 14:14:35",
                        "desc": {
                            "cn": "ProJudge æ˜¯ä¸€ä¸ªé’ˆå¯¹åŸºäº MLLM çš„è¿‡ç¨‹è£åˆ¤èƒ½åŠ›çš„å…¨é¢ã€å¤šæ¨¡æ€ã€å¤šå­¦ç§‘å’Œå¤šéš¾åº¦çš„åŸºå‡†ã€‚å®ƒåŒ…å« 2,400 ä¸ªæµ‹è¯•æ¡ˆä¾‹å’Œ 50,118 ä¸ªæ­¥éª¤çº§æ ‡ç­¾ï¼Œæ¶µç›–å››ä¸ªç§‘å­¦å­¦ç§‘ï¼Œéš¾åº¦çº§åˆ«å’Œå†…å®¹å¤šæ ·åŒ–ã€‚",
                            "en": "ProJudge is a comprehensive, multi-modal, multi-discipline, and multi-difficulty benchmark specifically designed for evaluating abilities of MLLM-based process judges.It comprises 2,400 test cases and 50,118 step-level labels, spanning four scientific disciplines with diverse difficulty levels and"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/projudge'. Error: Path opencompass/projudge is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1624",
                    "name": "CodeElo",
                    "version": "1.0.0",
                    "description": "Researchers introduce CodeElo, a standardized competition-level code generation benchmark that effectively addresses all these challenges for the first time. CodeElo benchmark is mainly based on the official CodeForces platform and tries to align with the platform as much as possible. ",
                    "url": "opencompass/opencompass_1624.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1624",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1624",
                        "name": "CodeElo",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/QwenLM/CodeElo",
                        "paperLink": "https://arxiv.org/abs/2501.01257",
                        "officialWebsiteLink": "https://codeelo-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "99",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-12 11:07:44",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-12 11:07:44",
                        "createDate": "2025-03-11 15:53:47",
                        "desc": {
                            "cn": "CodeEloï¼Œè¿™æ˜¯ä¸€ä¸ªæ ‡å‡†åŒ–çš„ç«èµ›çº§ä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ï¼Œæœ‰æ•ˆè§£å†³äº†æ‰€æœ‰è¿™äº›æŒ‘æˆ˜ã€‚CodeElo åŸºå‡†æµ‹è¯•ä¸»è¦åŸºäºå®˜æ–¹ CodeForces å¹³å°ï¼Œå¹¶å°½å¯èƒ½ä¸è¯¥å¹³å°ä¿æŒä¸€è‡´ã€‚",
                            "en": "Researchers introduce CodeElo, a standardized competition-level code generation benchmark that effectively addresses all these challenges for the first time. CodeElo benchmark is mainly based on the official CodeForces platform and tries to align with the platform as much as possible. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/codeelo'. Error: Path opencompass/codeelo is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1905",
                    "name": "MVPBench",
                    "version": "1.0.0",
                    "description": "MVPBench, a curated benchmark designed to rigorously evaluate visual physical reasoning through the lens of visual CoT. Each example features interleaved multi-image inputs and demands not only the correct final answer but also a coherent, step-by-step reasoning path grounded in evolving visual cues",
                    "url": "opencompass/opencompass_1905.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1905",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1905",
                        "name": "MVPBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/CSU-JPG/MVPBench",
                        "paperLink": "https://arxiv.org/pdf/2505.24182",
                        "officialWebsiteLink": "https://csu-jpg.github.io/MVPBench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "53302848",
                            "name": null,
                            "avatar": null,
                            "nickname": "LoneR"
                        },
                        "lookNum": "99",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-06 14:45:42",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-06 14:45:42",
                        "createDate": "2025-06-06 12:35:56",
                        "desc": {
                            "cn": "MVPBenchä¸“æ³¨äºè§†è§‰ç‰©ç†æ¨ç†ä¸­çš„è§†è§‰é“¾å¼æ€ç»´ï¼ˆCoTï¼‰èƒ½åŠ›è¯„ä¼°ã€‚è¯¥åŸºå‡†æ¶µç›–çœŸå®å›¾åƒã€å¤šæ­¥é€»è¾‘ä¸å¤šæ¡å¯è¡Œæ€ç»´è·¯å¾„ï¼Œæ¯ä¸ªæ ·ä¾‹å‡é…æœ‰å›¾åƒè¯æ®ï¼Œè¦æ±‚æ¨¡å‹åœ¨å‰¥ç¦»æ–‡æœ¬æç¤ºä¾èµ–çš„å‰æä¸‹ï¼Œä¸ä»…å¾—å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œè¿˜éœ€æ­£ç¡®å®Œæˆæ¯ä¸€ä¸ªä¸­é—´æ¨ç†æ­¥éª¤ï¼Œæ¨¡æ‹Ÿäººç±»çš„é€æ­¥å›¾åƒæ¨ç†è¿‡ç¨‹ã€‚",
                            "en": "MVPBench, a curated benchmark designed to rigorously evaluate visual physical reasoning through the lens of visual CoT. Each example features interleaved multi-image inputs and demands not only the correct final answer but also a coherent, step-by-step reasoning path grounded in evolving visual cues"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mvpbench'. Error: Path opencompass/mvpbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1658",
                    "name": "MiLiC-Eval",
                    "version": "1.0.0",
                    "description": "MiLiC-Eval is an NLP evaluation suite for Minority Languages in China, covering Tibetan (bo), Uyghur (ug), Kazakh (kk, in the Kazakh Arabic script), and Mongolian (mn, in the traditional Mongolian script).",
                    "url": "opencompass/opencompass_1658.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1658",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1658",
                        "name": "MiLiC-Eval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/luciusssss/MiLiC-Eval",
                        "paperLink": "https://arxiv.org/abs/2503.01150",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "99",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-20 14:26:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-20 14:26:55",
                        "createDate": "2025-03-20 14:21:29",
                        "desc": {
                            "cn": "MiLiC-Eval æ˜¯é’ˆå¯¹ä¸­å›½å°‘æ•°æ°‘æ—è¯­è¨€çš„ NLP è¯„ä¼°å¥—ä»¶ï¼Œæ¶µç›–è—è¯­ï¼ˆboï¼‰ã€ç»´å¾å°”è¯­ï¼ˆugï¼‰ã€å“ˆè¨å…‹è¯­ï¼ˆkkï¼Œä½¿ç”¨å“ˆè¨å…‹é˜¿æ‹‰ä¼¯æ–‡è„šæœ¬ï¼‰å’Œè’™å¤è¯­ï¼ˆmnï¼Œä½¿ç”¨ä¼ ç»Ÿè’™å¤æ–‡è„šæœ¬ï¼‰ã€‚",
                            "en": "MiLiC-Eval is an NLP evaluation suite for Minority Languages in China, covering Tibetan (bo), Uyghur (ug), Kazakh (kk, in the Kazakh Arabic script), and Mongolian (mn, in the traditional Mongolian script)."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/milic_eval'. Error: Path opencompass/milic_eval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1670",
                    "name": "IndicMMLU-Pro",
                    "version": "1.0.0",
                    "description": "IndicMMLU-Pro provides a standardized evaluation framework to push the research boundaries in Indic language AI, facilitating the development of more accurate, efficient, and culturally sensitive models.",
                    "url": "opencompass/opencompass_1670.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1670",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1670",
                        "name": "IndicMMLU-Pro",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2501.15747",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "99",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-24 18:48:16",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-24 18:48:16",
                        "createDate": "2025-03-24 18:46:36",
                        "desc": {
                            "cn": "IndicMMLU-Pro æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥æ¨åŠ¨å°åº¦è¯­ç³»è¯­è¨€ AI çš„ç ”ç©¶è¾¹ç•Œï¼Œä¿ƒè¿›æ›´å‡†ç¡®ã€é«˜æ•ˆå’Œå…·æœ‰æ–‡åŒ–æ•æ„Ÿæ€§çš„æ¨¡å‹çš„å‘å±•ã€‚",
                            "en": "IndicMMLU-Pro provides a standardized evaluation framework to push the research boundaries in Indic language AI, facilitating the development of more accurate, efficient, and culturally sensitive models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/indicmmlu_pro'. Error: Path opencompass/indicmmlu_pro is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1984",
                    "name": "SWE-bench-Live",
                    "version": "1.0.0",
                    "description": "SWE-bench-Live is a live-updatable benchmark designed for evaluating large language models (LLMs) and agents on real-world software issue resolution tasks.",
                    "url": "opencompass/opencompass_1984.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1984",
                    "sample_count": 1000,
                    "traits": [
                        "Code",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1984",
                        "name": "SWE-bench-Live",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SWE-bench-Live",
                        "paperLink": "https://arxiv.org/abs/2505.23419",
                        "officialWebsiteLink": "https://huggingface.co/SWE-bench-Live",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "99",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-27 18:12:47",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-27 18:12:47",
                        "createDate": "2025-06-26 14:18:04",
                        "desc": {
                            "cn": "SWE-bench-Live æ˜¯ä¸€ä¸ªé¢å‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ™ºèƒ½ä½“çš„å®æ—¶å¯æ›´æ–°è¯„æµ‹åŸºå‡†ï¼Œä¸“æ³¨äºçœŸå®ä¸–ç•Œè½¯ä»¶å¼€å‘ä¸­çš„é—®é¢˜ä¿®å¤ä»»åŠ¡ã€‚ è¯¥åŸºå‡†ä» 2024 å¹´ä»¥æ¥çš„ GitHub æ´»è·ƒä»“åº“ä¸­è‡ªåŠ¨æ”¶é›†äº† 1,319 ä¸ªé—®é¢˜ä¿®å¤ä»»åŠ¡ï¼Œæ¶µç›– 93 ä¸ªé¡¹ç›®ï¼Œå¹¶ä¸ºæ¯ä¸ªä»»åŠ¡æä¾›å¯å¤ç°çš„ Docker æ‰§è¡Œç¯å¢ƒã€‚ ",
                            "en": "SWE-bench-Live is a live-updatable benchmark designed for evaluating large language models (LLMs) and agents on real-world software issue resolution tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/swe_bench_live'. Error: Path opencompass/swe_bench_live is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1584",
                    "name": "MMSearch",
                    "version": "1.0.0",
                    "description": "Logo MMSearch is a multimodal search benchmark crafted to evaluate the potential of LMMs to function as a multimodal AI search engine. This benchmark encompasses a meticulously collected dataset of 300 queries spanning 14 subfields. ",
                    "url": "opencompass/opencompass_1584.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1584",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1584",
                        "name": "MMSearch",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/CaraJ7/MMSearch",
                        "paperLink": "https://arxiv.org/abs/2409.12959",
                        "officialWebsiteLink": "https://mmsearch.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "98",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:47:36",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:47:36",
                        "createDate": "2025-03-04 11:25:58",
                        "desc": {
                            "cn": "MMSearch æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æœç´¢åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLMMsï¼‰ä½œä¸ºå¤šæ¨¡æ€ AI æœç´¢å¼•æ“çš„æ½œåŠ›ã€‚è¯¥åŸºå‡†åŒ…å«äº†ä¸€ä¸ªç²¾å¿ƒæ”¶é›†çš„åŒ…å« 300 ä¸ªæŸ¥è¯¢çš„æ•°æ®é›†ï¼Œæ¶µç›– 14 ä¸ªå­é¢†åŸŸã€‚",
                            "en": "Logo MMSearch is a multimodal search benchmark crafted to evaluate the potential of LMMs to function as a multimodal AI search engine. This benchmark encompasses a meticulously collected dataset of 300 queries spanning 14 subfields. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmsearch'. Error: Path opencompass/mmsearch is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1510",
                    "name": "LongVideoBench",
                    "version": "1.0.0",
                    "description": "LongVideoBench tests LMMs' understanding of long videos. It's a question-answering benchmark with video-language interleaved inputs up to an hour long and comprises 3,763 web-collected videos with subtitles across diverse themes,",
                    "url": "opencompass/opencompass_1510.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1510",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1510",
                        "name": "LongVideoBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/longvideobench/LongVideoBench",
                        "paperLink": "https://arxiv.org/abs/2407.15754",
                        "officialWebsiteLink": "https://longvideobench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "97",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:51:19",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:51:19",
                        "createDate": "2025-02-17 15:50:45",
                        "desc": {
                            "cn": "LongVideoBenchç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„é•¿è§†é¢‘ç†è§£èƒ½åŠ›ï¼Œæ˜¯åŸºäºäº¤é”™é•¿è§†é¢‘è¯­æ–™æ„å»ºçš„é—®ç­”é›†ï¼ŒåŒ…å«3763ä¸ªä¸åŒä¸»é¢˜çš„å¸¦å­—å¹•çš„è§†é¢‘ã€‚",
                            "en": "LongVideoBench tests LMMs' understanding of long videos. It's a question-answering benchmark with video-language interleaved inputs up to an hour long and comprises 3,763 web-collected videos with subtitles across diverse themes,"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/longvideobench'. Error: Path opencompass/longvideobench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1553",
                    "name": "OmniAlign-V",
                    "version": "1.0.0",
                    "description": "OmniAlign-V datasets mainly focus on improving the alignment of Multi-modal Large Language Models(MLLMs) with human preference. It contains 205k high-quality Image-Quetion-Answer pairs with open-ended, creative quetions and long, knowledge-rich, comprehensive answers.",
                    "url": "opencompass/opencompass_1553.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1553",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1553",
                        "name": "OmniAlign-V",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/PhoenixZ810/OmniAlign-V",
                        "paperLink": "https://arxiv.org/abs/2502.18411",
                        "officialWebsiteLink": "https://phoenixz810.github.io/OmniAlign-V",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "96",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:49:29",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:49:29",
                        "createDate": "2025-02-26 14:41:09",
                        "desc": {
                            "cn": "OmniAlign-V æ•°æ®é›†ä¸»è¦å…³æ³¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸äººç±»åå¥½çš„å¯¹é½ã€‚å®ƒåŒ…å« 205k ä¸ªé«˜è´¨é‡çš„å›¾åƒ-é—®ç­”å¯¹ï¼ŒåŒ…å«å¼€æ”¾å¼ã€åˆ›æ„æ€§é—®é¢˜ä»¥åŠé•¿ç¯‡ã€çŸ¥è¯†ä¸°å¯Œã€å†…å®¹å…¨é¢çš„ç­”æ¡ˆã€‚",
                            "en": "OmniAlign-V datasets mainly focus on improving the alignment of Multi-modal Large Language Models(MLLMs) with human preference. It contains 205k high-quality Image-Quetion-Answer pairs with open-ended, creative quetions and long, knowledge-rich, comprehensive answers."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/omnialign_v'. Error: Path opencompass/omnialign_v is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1645",
                    "name": "MastermindEval",
                    "version": "1.0.0",
                    "description": "Evaluating Reasoning Capabilities of LLMs Using the Mastermind Board Game.",
                    "url": "opencompass/opencompass_1645.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1645",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1645",
                        "name": "MastermindEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/flairNLP/mastermind",
                        "paperLink": "https://arxiv.org/abs/2503.05891",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "96",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-18 16:17:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-18 16:17:33",
                        "createDate": "2025-03-18 16:08:48",
                        "desc": {
                            "cn": "MastermindEvalä½¿ç”¨çŒœè°œæ¸¸æˆæ£‹ç›˜è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚\n",
                            "en": "Evaluating Reasoning Capabilities of LLMs Using the Mastermind Board Game."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mastermindeval'. Error: Path opencompass/mastermindeval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1396",
                    "name": "AV-Odyssey-Bench",
                    "version": "1.0.0",
                    "description": "AV-Odyssey Bench. This benchmark encompasses 26 different tasks and 4,555 carefully crafted problems, each incorporating text, visual, and audio components. All data are newly collected and annotated by humans, not from any existing audio-visual dataset.",
                    "url": "opencompass/opencompass_1396.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1396",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1396",
                        "name": "AV-Odyssey-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "audio-visual",
                                "en": "audio-visual"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AV-Odyssey/AV-Odyssey",
                        "paperLink": "https://arxiv.org/pdf/2412.02611",
                        "officialWebsiteLink": "https://huggingface.co/datasets/AV-Odyssey/AV_Odyssey_Bench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "40068663",
                            "name": null,
                            "avatar": null,
                            "nickname": "è±ªà¼™à¾‡"
                        },
                        "lookNum": "96",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-15 18:34:14",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-15 18:34:14",
                        "createDate": "2025-01-15 17:26:48",
                        "desc": {
                            "cn": "AV-Odyssey Bench. This benchmark encompasses 26 different tasks and 4,555 carefully crafted problems, each incorporating text, visual, and audio components. All data are newly collected and annotated by humans, not from any existing audio-visual dataset.",
                            "en": "AV-Odyssey Bench. This benchmark encompasses 26 different tasks and 4,555 carefully crafted problems, each incorporating text, visual, and audio components. All data are newly collected and annotated by humans, not from any existing audio-visual dataset."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/av_odyssey_bench'. Error: Path opencompass/av_odyssey_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1571",
                    "name": "BRIGHT",
                    "version": "1.0.0",
                    "description": "BRIGHT is the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. ",
                    "url": "opencompass/opencompass_1571.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1571",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1571",
                        "name": "BRIGHT",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/xlang-ai/BRIGHT",
                        "paperLink": "https://arxiv.org/abs/2407.12883",
                        "officialWebsiteLink": "https://brightbenchmark.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "95",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:50:14",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:50:14",
                        "createDate": "2025-02-27 18:29:17",
                        "desc": {
                            "cn": "BRIGHT æ˜¯ç¬¬ä¸€ä¸ªéœ€è¦å¤§é‡æ¨ç†æ¥æ£€ç´¢ç›¸å…³æ–‡æ¡£çš„æ–‡æœ¬æ£€ç´¢åŸºå‡†ã€‚",
                            "en": "BRIGHT is the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/bright'. Error: Path opencompass/bright is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1679",
                    "name": "ContextualJudgeBench",
                    "version": "1.0.0",
                    "description": "ContextualJudgeBench is a pairwise benchmark with 2,000 samples for evaluating LLM-as-judge models in two contextual settings: Contextual QA and summarization. We propose a pairwise evaluation hierarchy and generate splits for our proposed hierarchy.",
                    "url": "opencompass/opencompass_1679.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1679",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1679",
                        "name": "ContextualJudgeBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SalesforceAIResearch/ContextualJudgeBench",
                        "paperLink": "https://arxiv.org/abs/2503.15620",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "95",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-26 15:56:25",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-26 15:56:25",
                        "createDate": "2025-03-26 14:40:00",
                        "desc": {
                            "cn": "ContextualJudgeBench æ˜¯ä¸€ä¸ªåŒ…å« 2,000 ä¸ªæ ·æœ¬çš„æˆå¯¹åŸºå‡†ï¼Œç”¨äºè¯„ä¼°åœ¨ä¸¤ä¸ªä¸Šä¸‹æ–‡ç¯å¢ƒä¸‹çš„LLM-as-judge æ¨¡å‹ï¼šä¸Šä¸‹æ–‡é—®ç­”å’Œæ‘˜è¦ã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªæˆå¯¹è¯„ä¼°å±‚æ¬¡ç»“æ„ï¼Œå¹¶ä¸ºæˆ‘ä»¬çš„å±‚æ¬¡ç»“æ„ç”Ÿæˆåˆ†å‰²ã€‚",
                            "en": "ContextualJudgeBench is a pairwise benchmark with 2,000 samples for evaluating LLM-as-judge models in two contextual settings: Contextual QA and summarization. We propose a pairwise evaluation hierarchy and generate splits for our proposed hierarchy."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/contextualjudgebench'. Error: Path opencompass/contextualjudgebench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1547",
                    "name": "MMIR",
                    "version": "1.0.0",
                    "description": "A benchmark for evaluating Multimodal Large Language Models (MLLMs) on detecting and reasoning about inconsistencies in layout-rich multimodal content. MMIR features 534 challenging samples across five reasoning-heavy inconsistency categories.",
                    "url": "opencompass/opencompass_1547.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1547",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1547",
                        "name": "MMIR",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/eric-ai-lab/MMIR",
                        "paperLink": "https://arxiv.org/abs/2502.16033",
                        "officialWebsiteLink": "https://jackie-2000.github.io/mmir.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "94",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:48:36",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:48:36",
                        "createDate": "2025-02-26 11:11:06",
                        "desc": {
                            "cn": "ç”¨äºè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹ï¼ˆMLLMï¼‰æ£€æµ‹å’Œæ¨ç†å¸ƒå±€ä¸°å¯Œçš„å¤šæ¨¡æ€å†…å®¹ä¸­çš„ä¸ä¸€è‡´æ€§çš„åŸºå‡†ã€‚MMIR åŒ…å« 534 ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬ï¼Œæ¶‰åŠäº”ä¸ªæ¨ç†èƒ½åŠ›è¾ƒå¼ºçš„ä¸ä¸€è‡´ç±»åˆ«ã€‚",
                            "en": "A benchmark for evaluating Multimodal Large Language Models (MLLMs) on detecting and reasoning about inconsistencies in layout-rich multimodal content. MMIR features 534 challenging samples across five reasoning-heavy inconsistency categories."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmir'. Error: Path opencompass/mmir is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1585",
                    "name": "MMAD",
                    "version": "1.0.0",
                    "description": "MMAD is the first-ever full-spectrum MLLMs benchmark in industrial Anomaly Detection. \nResearchers defined seven key subtasks of MLLMs in industrial inspection and designed a novel pipeline to generate the MMAD dataset with 39,672 questions for 8,366 industrial images. ",
                    "url": "opencompass/opencompass_1585.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1585",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1585",
                        "name": "MMAD",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/jam-cc/MMAD",
                        "paperLink": "https://arxiv.org/abs/2410.09453",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "93",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:47:27",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:47:27",
                        "createDate": "2025-03-04 13:48:12",
                        "desc": {
                            "cn": "MMADæ˜¯ç¬¬ä¸€ä¸ªå·¥ä¸šå¼‚å¸¸æ£€æµ‹é¢†åŸŸçš„å…¨è°± MLLMs åŸºå‡†ï¼Œç ”ç©¶äººå‘˜å®šä¹‰äº†å·¥ä¸šæ£€æµ‹ä¸­ MLLMs çš„ä¸ƒä¸ªå…³é”®å­ä»»åŠ¡ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ–°é¢–çš„æµç¨‹æ¥ç”ŸæˆåŒ…å« 39,672 ä¸ªé—®é¢˜ä»¥åŠ 8,366 ä¸ªå·¥ä¸šå›¾åƒçš„ MMAD æ•°æ®é›†ã€‚",
                            "en": "MMAD is the first-ever full-spectrum MLLMs benchmark in industrial Anomaly Detection. \nResearchers defined seven key subtasks of MLLMs in industrial inspection and designed a novel pipeline to generate the MMAD dataset with 39,672 questions for 8,366 industrial images. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmad'. Error: Path opencompass/mmad is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1621",
                    "name": "ProcessBench",
                    "version": "1.0.0",
                    "description": "ProcessBench can measure the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. ",
                    "url": "opencompass/opencompass_1621.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1621",
                    "sample_count": 1000,
                    "traits": [
                        "Examination",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1621",
                        "name": "ProcessBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å­¦ç§‘",
                                "en": "Examination"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/QwenLM/ProcessBench",
                        "paperLink": "https://arxiv.org/abs/2412.06559",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "93",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-12 11:08:00",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-12 11:08:00",
                        "createDate": "2025-03-11 15:28:49",
                        "desc": {
                            "cn": "ProcessBenchï¼Œç”¨äºè¡¡é‡è¯†åˆ«æ•°å­¦æ¨ç†ä¸­é”™è¯¯æ­¥éª¤çš„èƒ½åŠ›ã€‚å®ƒåŒ…å« 3,400 ä¸ªæµ‹è¯•æ¡ˆä¾‹ï¼Œä¸»è¦å…³æ³¨ç«èµ›å’Œå¥¥æ—åŒ¹å…‹çº§åˆ«çš„æ•°å­¦é—®é¢˜ã€‚",
                            "en": "ProcessBench can measure the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/processbench'. Error: Path opencompass/processbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1395",
                    "name": "SEED-Bench-2-Plus",
                    "version": "1.0.0",
                    "description": "SEED-Bench-2-Plus, a benchmark specifically designed for evaluating text-rich visual comprehension of MLLMs. The benchmark comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad categories: Charts, Maps, and Webs.",
                    "url": "opencompass/opencompass_1395.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1395",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1395",
                        "name": "SEED-Bench-2-Plus",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "text-rich image",
                                "en": "text-rich image"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AILab-CVC/SEED-Bench",
                        "paperLink": "https://arxiv.org/pdf/2404.16790",
                        "officialWebsiteLink": "https://huggingface.co/datasets/AILab-CVC/SEED-Bench-2-plus",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "40068663",
                            "name": null,
                            "avatar": null,
                            "nickname": "è±ªà¼™à¾‡"
                        },
                        "lookNum": "93",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-15 18:33:41",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-15 18:33:41",
                        "createDate": "2025-01-15 17:20:44",
                        "desc": {
                            "cn": "SEED-Bench-2-Plus, a benchmark specifically designed for evaluating text-rich visual comprehension of MLLMs. The benchmark comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad categories: Charts, Maps, and Webs.",
                            "en": "SEED-Bench-2-Plus, a benchmark specifically designed for evaluating text-rich visual comprehension of MLLMs. The benchmark comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad categories: Charts, Maps, and Webs."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/seed_bench_2_plus'. Error: Path opencompass/seed_bench_2_plus is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1513",
                    "name": "CG-Bench",
                    "version": "1.0.0",
                    "description": "CG-Bench is meant for evaluating MLLMs' long video understanding, including 12,129 QA pairs from 1219 videos in 3 major question types: perception, reasoning, and hallucination.",
                    "url": "opencompass/opencompass_1513.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1513",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1513",
                        "name": "CG-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/CG-Bench/CG-Bench",
                        "paperLink": "https://arxiv.org/abs/2412.12075v1",
                        "officialWebsiteLink": "https://cg-bench.github.io/leaderboard/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "92",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:51:42",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:51:42",
                        "createDate": "2025-02-17 16:29:23",
                        "desc": {
                            "cn": "CG-Benchç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„é•¿è§†é¢‘ç†è§£èƒ½åŠ›ï¼ŒåŸºäº1219ä¸ªè§†é¢‘è®¾è®¡äº†12129ä¸ªæ¶µç›–æ„ŸçŸ¥ã€æ¨ç†å’Œå¹»è§‰ä¸‰ç§é—®é¢˜ç±»å‹çš„QAå¯¹ã€‚",
                            "en": "CG-Bench is meant for evaluating MLLMs' long video understanding, including 12,129 QA pairs from 1219 videos in 3 major question types: perception, reasoning, and hallucination."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cg_bench'. Error: Path opencompass/cg_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1555",
                    "name": "WildBench",
                    "version": "1.0.0",
                    "description": "Weintroduce WildBench, an automated evaluation framework designed to bench-mark large language models (LLMs) using challenging, real-world user queries, which consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs.",
                    "url": "opencompass/opencompass_1555.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1555",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1555",
                        "name": "WildBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/allenai/WildBench",
                        "paperLink": "https://arxiv.org/abs/2406.04770",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "90",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:49:39",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:49:39",
                        "createDate": "2025-02-26 17:05:44",
                        "desc": {
                            "cn": "WildBenchæ¨å‡ºè‡ªåŠ¨è¯„ä¼°æ¡†æ¶å’Œæ•°æ®é›†ï¼ŒåŸºäºçœŸå®ç”¨æˆ·éš¾é¢˜è¯„æµ‹å¤§è¯­è¨€æ¨¡å‹ï¼ŒåŒ…å«ä»é€¾ç™¾ä¸‡äººæœºå¯¹è¯æ—¥å¿—ä¸­ç²¾é€‰çš„1,024ä¸ªä»»åŠ¡æ ·æœ¬ã€‚",
                            "en": "Weintroduce WildBench, an automated evaluation framework designed to bench-mark large language models (LLMs) using challenging, real-world user queries, which consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/wildbench'. Error: Path opencompass/wildbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1578",
                    "name": "MMKE-Bench",
                    "version": "1.0.0",
                    "description": " MMKE-Bench is a benchmark designed to evaluate the ability of LMMs to edit visual knowledge in real-world scenarios. It includes 2,940 pieces of knowledge and 8,363 images across 33 broad categories, with automatically generated, human-verified evaluation questions.",
                    "url": "opencompass/opencompass_1578.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1578",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1578",
                        "name": "MMKE-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MMKE-Bench-ICLR/MMKE-Bench",
                        "paperLink": "https://arxiv.org/abs/2502.19870",
                        "officialWebsiteLink": "https://mmke-bench-iclr.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "90",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-04 17:02:34",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-04 17:02:34",
                        "createDate": "2025-03-03 16:27:20",
                        "desc": {
                            "cn": "MMKE-Benchæ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼° LMM åœ¨ç°å®åœºæ™¯ä¸­ç¼–è¾‘è§†è§‰çŸ¥è¯†èƒ½åŠ›çš„åŸºå‡†ï¼ŒåŒ…æ‹¬ 33 ä¸ªå¹¿æ³›ç±»åˆ«ä¸­çš„ 2,940 æ¡çŸ¥è¯†å’Œ 8,363 å¼ å›¾åƒï¼Œä»¥åŠè‡ªåŠ¨ç”Ÿæˆå¹¶ç”±äººå·¥éªŒè¯çš„è¯„ä¼°é—®é¢˜ã€‚",
                            "en": " MMKE-Bench is a benchmark designed to evaluate the ability of LMMs to edit visual knowledge in real-world scenarios. It includes 2,940 pieces of knowledge and 8,363 images across 33 broad categories, with automatically generated, human-verified evaluation questions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmke_bench'. Error: Path opencompass/mmke_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1842",
                    "name": "HARDMath2",
                    "version": "1.0.0",
                    "description": "HARDMath2 is a benchmark for applied mathematics created by students in a graduate class at Harvard University, featuring 211 original problems covering core topics such as boundary-layer analysis, WKB methods, asymptotic solutions of nonlinear partial differential equations, and the asymptotics.",
                    "url": "opencompass/opencompass_1842.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1842",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning",
                        "Code",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1842",
                        "name": "HARDMath2",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            },
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Strong Reasoning",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/pdf/2505.11774",
                        "officialWebsiteLink": "https://huggingface.co/datasets/JVRoggeveen/HARDMath2",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "90",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 14:19:57",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 14:19:57",
                        "createDate": "2025-06-04 14:17:14",
                        "desc": {
                            "cn": "HARDMath2æ˜¯ç”±å“ˆä½›å¤§å­¦ç ”ç©¶ç”Ÿè¯¾ç¨‹çš„å­¦ç”Ÿåˆ›å»ºçš„ä¸€é¡¹åº”ç”¨æ•°å­¦åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«211é“åŸåˆ›é—®é¢˜ï¼Œæ¶µç›–è¾¹ç•Œå±‚åˆ†æã€WKBæ–¹æ³•ã€éçº¿æ€§åå¾®åˆ†æ–¹ç¨‹çš„æ¸è¿‘è§£ä»¥åŠæŒ¯è¡ç§¯åˆ†çš„æ¸è¿‘æ€§ç­‰æ ¸å¿ƒä¸»é¢˜ã€‚è¯¥åŸºå‡†é€šè¿‡ä¸€ç§åˆ›æ–°çš„åä½œæ–¹å¼æ„å»ºï¼Œå­¦ç”Ÿä¸ä»…è®¾è®¡å¹¶æ”¹è¿›ç¬¦åˆè¯¾ç¨‹å¤§çº²çš„é«˜éš¾åº¦é—®é¢˜ï¼Œè¿˜å¯¹è§£å†³æ–¹æ¡ˆè¿›è¡ŒåŒè¡ŒéªŒè¯ï¼ŒåŒæ—¶æµ‹è¯•ä¸åŒæ¨¡å‹çš„è¡¨ç°ã€‚æœ€ç»ˆï¼ŒLLMç”Ÿæˆçš„è§£ç­”ä¼šä¸å­¦ç”Ÿçš„ç­”æ¡ˆä»¥åŠæ•°å€¼çœŸå€¼è¿›è¡Œè‡ªåŠ¨å¯¹æ¯”ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„å‡†ç¡®æ€§å’Œèƒ½åŠ›ã€‚",
                            "en": "HARDMath2 is a benchmark for applied mathematics created by students in a graduate class at Harvard University, featuring 211 original problems covering core topics such as boundary-layer analysis, WKB methods, asymptotic solutions of nonlinear partial differential equations, and the asymptotics."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/hardmath2'. Error: Path opencompass/hardmath2 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1959",
                    "name": "PersonaLens",
                    "version": "1.0.0",
                    "description": "PersonaLens  a large-scale benchmark specifically designed to evaluate personalization in task-oriented dialogues. The benchmark features 1,500 in-depth user profiles, each integrating real demographic data, detailed cross-domain preferences, and rich interaction histories.",
                    "url": "opencompass/opencompass_1959.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1959",
                    "sample_count": 1000,
                    "traits": [
                        "Language",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1959",
                        "name": "PersonaLens",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "AI Assistant",
                                "en": "AI Assistant"
                            },
                            {
                                "cn": "personalization",
                                "en": "personalization"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/amazon-science/PersonaLens",
                        "paperLink": "https://arxiv.org/abs/2506.09902",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "82606592",
                            "name": "zsquaredz",
                            "avatar": null,
                            "nickname": "zsquaredz"
                        },
                        "lookNum": "89",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-20 15:03:36",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-20 15:03:36",
                        "createDate": "2025-06-19 18:58:32",
                        "desc": {
                            "cn": "PersonaLensæ˜¯ä¸€ä¸ªä¸“ä¸ºä»»åŠ¡å¯¼å‘å‹å¯¹è¯è®¾è®¡çš„ã€å¤§è§„æ¨¡çš„ä¸ªæ€§åŒ–èƒ½åŠ›è¯„æµ‹åŸºå‡†ã€‚å®ƒåŒ…å«1,500ä¸ªæ·±åº¦ç”¨æˆ·ç”»åƒï¼Œæ¯ä¸ªç”»åƒéƒ½é›†æˆäº†çœŸå®çš„äººå£ç»Ÿè®¡ä¿¡æ¯ã€è¯¦å°½çš„ä¸ªäººåå¥½åŠå†å²äº’åŠ¨è®°å½•ã€‚è¿™äº›ç”»åƒä¸è¦†ç›–20ä¸ªé¢†åŸŸçš„111é¡¹çœŸå®ä¸–ç•Œä»»åŠ¡ç›¸ç»“åˆï¼Œå¹¶è¾…ä»¥åŠ¨æ€çš„â€œæƒ…æ™¯ä¸Šä¸‹æ–‡â€æ¥æ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚ä¸ºäº†å®ç°è‡ªåŠ¨åŒ–ã€å¯æ‰©å±•çš„è¯„ä¼°ï¼Œè¯¥åŸºå‡†å¼•å…¥äº†ä¸¤ä¸ªLLMé©±åŠ¨çš„æ™ºèƒ½ä½“ï¼šä¸€ä¸ªâ€œç”¨æˆ·æ™ºèƒ½ä½“â€è´Ÿè´£æ¨¡æ‹ŸçœŸäººä¸AIè¿›è¡Œå¯¹è¯ï¼Œå¦ä¸€ä¸ªâ€œè¯„åˆ¤æ™ºèƒ½ä½“â€åˆ™å¯¹ä¸ªæ€§åŒ–æ°´å¹³ã€ä»»åŠ¡æˆåŠŸç‡å’Œå¯¹è¯è´¨é‡è¿›è¡Œç³»ç»Ÿæ€§æ‰“åˆ†ã€‚PersonaLensæ—¨åœ¨ä¸ºç ”ç©¶ç¤¾åŒºæä¾›ä¸€ä¸ªå¼ºå¤§å¯é çš„å·¥å…·ï¼Œå…±åŒæ¨åŠ¨ä¸‹ä¸€ä»£æ›´æ‡‚ä½ ã€æ›´æ™ºèƒ½çš„AIåŠ©æ‰‹çš„ç ”å‘ã€‚",
                            "en": "PersonaLens  a large-scale benchmark specifically designed to evaluate personalization in task-oriented dialogues. The benchmark features 1,500 in-depth user profiles, each integrating real demographic data, detailed cross-domain preferences, and rich interaction histories."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/personalens'. Error: Path opencompass/personalens is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1735",
                    "name": "WorldScore",
                    "version": "1.0.0",
                    "description": "WorldScore benchmark is the first unified benchmark for world generation.",
                    "url": "opencompass/opencompass_1735.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1735",
                    "sample_count": 1000,
                    "traits": [
                        "Creation",
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1735",
                        "name": "WorldScore",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "åˆ›ä½œ",
                                "en": "Creation"
                            },
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "è§†é¢‘ç”Ÿæˆ",
                                "en": "è§†é¢‘ç”Ÿæˆ"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/haoyi-duan/WorldScore",
                        "paperLink": "https://arxiv.org/abs/2504.00983",
                        "officialWebsiteLink": "https://haoyi-duan.github.io/WorldScore/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "89",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:14:14",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:14:14",
                        "createDate": "2025-04-11 15:23:05",
                        "desc": {
                            "cn": "WorldScoreåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºä¸–ç•Œç”Ÿæˆçš„ç»Ÿä¸€åŸºå‡†æµ‹è¯•ã€‚",
                            "en": "WorldScore benchmark is the first unified benchmark for world generation."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/worldscore'. Error: Path opencompass/worldscore is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1999",
                    "name": "MORSE-500",
                    "version": "1.0.0",
                    "description": "MORSE-500 introduces 500 programmatically generated videos testing 6 reasoning types: abstract, physical, planning, spatial, temporal, mathematical. Its controllable generation (via Manim, Matplotlib, generative models etc.) enables scalable difficulty, designed to evolve as SOTA models improve.",
                    "url": "opencompass/opencompass_1999.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1999",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1999",
                        "name": "MORSE-500",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/morse-benchmark/morse-500",
                        "paperLink": "https://arxiv.org/abs/2506.05523",
                        "officialWebsiteLink": "https://morse-500.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "72009045",
                            "name": null,
                            "avatar": null,
                            "nickname": "Clarence"
                        },
                        "lookNum": "89",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-30 10:06:27",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-30 10:06:27",
                        "createDate": "2025-06-30 05:39:25",
                        "desc": {
                            "cn": "å½“å‰å¤šæ¨¡æ€æ¨ç†åŸºå‡†å­˜åœ¨ä¸‰å¤§ä¸è¶³ï¼šä¾èµ–é™æ€å›¾åƒã€åé‡æ•°å­¦è§£é¢˜ã€æ˜“é¥±å’Œã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºMORSE-500è§†é¢‘åŸºå‡†ï¼šåŒ…å«500ä¸ªè„šæœ¬åŒ–è§†é¢‘ç‰‡æ®µï¼Œæ¶µç›–æŠ½è±¡ã€ç‰©ç†ã€è§„åˆ’ã€ç©ºé—´ã€æ—¶é—´ã€æ•°å­¦å…­ç±»æ¨ç†é—®é¢˜ã€‚å…¶æ ¸å¿ƒåœ¨äºç¨‹åºåŒ–ç”Ÿæˆï¼ˆä½¿ç”¨Manimã€Matplotlibç­‰ï¼‰ï¼Œå¯ç²¾ç¡®æ§åˆ¶è§†è§‰å¤æ‚åº¦ã€å¹²æ‰°ç‰©å¯†åº¦å’Œæ—¶åºåŠ¨æ€ï¼Œä»è€Œç³»ç»ŸåŒ–æå‡éš¾åº¦ã€‚ä¸æ˜“è¿‡æ—¶çš„é™æ€åŸºå‡†ä¸åŒï¼ŒMORSE-500å…·å¤‡å¯æŒç»­æ¼”è¿›èƒ½åŠ›ï¼Œå…¶å¯æ§ç”Ÿæˆæµç¨‹èƒ½æ— é™åˆ›å»ºæ–°æŒ‘æˆ˜å®ä¾‹ã€‚åœ¨é¡¶å°–æ¨¡å‹ï¼ˆGemini 2.5 Proã€OpenAI o3ç­‰ï¼‰ä¸Šçš„æµ‹è¯•æ­ç¤ºäº†æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œå°¤å…¶åœ¨æŠ½è±¡å’Œè§„åˆ’ä»»åŠ¡ä¸Šã€‚æˆ‘ä»¬å¼€æºæ•°æ®é›†ã€ç”Ÿæˆè„šæœ¬åŠè¯„ä¼°å·¥å…·ï¼Œä»¥ä¿ƒè¿›é€æ˜ã€å¯å¤ç°çš„å‰æ²¿ç ”ç©¶ã€‚",
                            "en": "MORSE-500 introduces 500 programmatically generated videos testing 6 reasoning types: abstract, physical, planning, spatial, temporal, mathematical. Its controllable generation (via Manim, Matplotlib, generative models etc.) enables scalable difficulty, designed to evolve as SOTA models improve."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/morse_500'. Error: Path opencompass/morse_500 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1502",
                    "name": "MTVQA",
                    "version": "1.0.0",
                    "description": "MTVQA is designed to evaluate LMMs' multilingual text understanding, featuring high-quality human expert annotations across 9 diverse languages. ",
                    "url": "opencompass/opencompass_1502.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1502",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1502",
                        "name": "MTVQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/bytedance/MTVQA",
                        "paperLink": "https://arxiv.org/abs/2405.11985",
                        "officialWebsiteLink": "https://bytedance.github.io/MTVQA/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "89",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:50:52",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:50:52",
                        "createDate": "2025-02-14 20:41:37",
                        "desc": {
                            "cn": "MTVQAç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹ç†è§£å¤šè¯­è¨€æ–‡æœ¬çš„èƒ½åŠ›ï¼ŒåŒ…å«æ¥è‡ª9ç§è¯­è¨€çš„ç”±äººç±»ä¸“å®¶æ³¨é‡Šçš„é«˜è´¨é‡æ•°æ®ã€‚",
                            "en": "MTVQA is designed to evaluate LMMs' multilingual text understanding, featuring high-quality human expert annotations across 9 diverse languages. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mtvqa'. Error: Path opencompass/mtvqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1778",
                    "name": "AgentRewardBench",
                    "version": "1.0.0",
                    "description": "AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. ",
                    "url": "opencompass/opencompass_1778.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1778",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1778",
                        "name": "AgentRewardBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://agent-reward-bench.github.io/",
                        "paperLink": "https://arxiv.org/abs/2504.08942",
                        "officialWebsiteLink": "https://agent-reward-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "88",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-25 17:00:44",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-25 17:00:44",
                        "createDate": "2025-04-25 16:26:45",
                        "desc": {
                            "cn": "AgentRewardBench æ˜¯é¦–ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„åˆ¤è€…è¯„ä¼°ç½‘ç»œä»£ç†æœ‰æ•ˆæ€§çš„åŸºå‡†æµ‹è¯•ã€‚AgentRewardBench åŒ…å«æ¥è‡ª 5 ä¸ªåŸºå‡†æµ‹è¯•å’Œ 4 ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹çš„ 1302 æ¡è½¨è¿¹ã€‚",
                            "en": "AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/agentrewardbench'. Error: Path opencompass/agentrewardbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1533",
                    "name": "NutritionQA",
                    "version": "1.0.0",
                    "description": "CoSyn-400K dataset contains 9 categories of synthetc text-rich images with 2.7M instruction-tuning data",
                    "url": "opencompass/opencompass_1533.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1533",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1533",
                        "name": "NutritionQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/allenai/pixmo-docs",
                        "paperLink": "https://arxiv.org/abs/2502.14846",
                        "officialWebsiteLink": "https://yueyang1996.github.io/cosyn/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "88",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 17:00:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 17:00:55",
                        "createDate": "2025-02-24 13:51:34",
                        "desc": {
                            "cn": "é€šè¿‡ä»£ç å¼•å¯¼çš„åˆæˆå¤šæ¨¡æ€æ•°æ®ç”Ÿæˆæ‰©å±•æ–‡æœ¬ä¸°å¯Œå›¾åƒç†è§£ï¼ŒCoSyn-400K æ•°æ®é›†åŒ…å« 9 ç±»åˆæˆæ–‡æœ¬ä¸°å¯Œå›¾åƒï¼Œä»¥åŠ 270 ä¸‡æ¡æŒ‡ä»¤å¾®è°ƒæ•°æ®ã€‚",
                            "en": "CoSyn-400K dataset contains 9 categories of synthetc text-rich images with 2.7M instruction-tuning data"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/nutritionqa'. Error: Path opencompass/nutritionqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1982",
                    "name": "WebUI-Bench",
                    "version": "1.0.0",
                    "description": "WebUIBench is a comprehensive benchmark designed to evaluate Multimodal Large Language Models (MLLMs) in Web UI-to-code generation tasks across four key capabilities: UI perception, HTML programming, UI-code understanding, and end-to-end transformation.",
                    "url": "opencompass/opencompass_1982.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1982",
                    "sample_count": 1000,
                    "traits": [
                        "Code",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1982",
                        "name": "WebUI-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MAIL-Tele-AI/WebUIBench",
                        "paperLink": "https://arxiv.org/abs/2506.07818",
                        "officialWebsiteLink": "https://huggingface.co/datasets/Tele-AI-MAIL/WebUIBench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "87",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-01 10:18:37",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-01 10:18:37",
                        "createDate": "2025-07-01 10:18:13",
                        "desc": {
                            "cn": "WebUIBench æ˜¯ä¸€ä¸ªé¢å‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç»¼åˆæ€§è¯„æµ‹åŸºå‡†ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°æ¨¡å‹åœ¨ Web UI åˆ°ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„å››ä¸ªå…³é”®èƒ½åŠ›ï¼šç•Œé¢æ„ŸçŸ¥ã€HTML ç¼–ç¨‹ã€ç•Œé¢-ä»£ç ç†è§£ä»¥åŠæ•´ä½“è½¬æ¢èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«æ¥è‡ª 700 å¤šä¸ªçœŸå®ç½‘ç«™çš„ 21,000 ä¸ªé«˜è´¨é‡é—®ç­”å¯¹ï¼Œæ”¯æŒå¯¹æ¨¡å‹åœ¨å„é˜¶æ®µçš„ç»†ç²’åº¦èƒ½åŠ›åˆ†æã€‚",
                            "en": "WebUIBench is a comprehensive benchmark designed to evaluate Multimodal Large Language Models (MLLMs) in Web UI-to-code generation tasks across four key capabilities: UI perception, HTML programming, UI-code understanding, and end-to-end transformation."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/webui_bench'. Error: Path opencompass/webui_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1604",
                    "name": "EgoNormia",
                    "version": "1.0.0",
                    "description": "EgoNormia is a challenging QA benchmark that tests VLMs' ability to reason over norms in context. The datset consists of 1,853 physically grounded egocentric interaction clips from Ego4D and corresponding five-way multiple-choice questions tasks for each.",
                    "url": "opencompass/opencompass_1604.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1604",
                    "sample_count": 1000,
                    "traits": [
                        "Examination",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1604",
                        "name": "EgoNormia",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å­¦ç§‘",
                                "en": "Examination"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Open-Social-World/EgoNormia",
                        "paperLink": "https://arxiv.org/abs/2502.20490",
                        "officialWebsiteLink": "https://opensocial.world/leaderboard",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "86",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:45:43",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:45:43",
                        "createDate": "2025-03-06 13:51:13",
                        "desc": {
                            "cn": "EgoNormia æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®ç­”åŸºå‡†ï¼Œç”¨äºæµ‹è¯• VLMs åœ¨ä¸Šä¸‹æ–‡ä¸­æ¨ç†è§„èŒƒçš„èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ª Ego4D çš„ 1,853 ä¸ªç‰©ç†åŸºç¡€åŒ–çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„äº¤äº’å‰ªè¾‘ï¼Œä»¥åŠæ¯ä¸ªå‰ªè¾‘å¯¹åº”çš„äº”é€‰ä¸€å¤šé¡¹é€‰æ‹©é¢˜ä»»åŠ¡ã€‚",
                            "en": "EgoNormia is a challenging QA benchmark that tests VLMs' ability to reason over norms in context. The datset consists of 1,853 physically grounded egocentric interaction clips from Ego4D and corresponding five-way multiple-choice questions tasks for each."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/egonormia'. Error: Path opencompass/egonormia is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1632",
                    "name": "ProBench",
                    "version": "1.0.0",
                    "description": "ProBench is a benchmark that contains open-ended multimodal queries that require intensive expert-level knowledge to solve. ",
                    "url": "opencompass/opencompass_1632.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1632",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1632",
                        "name": "ProBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Yan98/ProBench_eval",
                        "paperLink": "https://arxiv.org/abs/2503.06885",
                        "officialWebsiteLink": "https://yan98.github.io/ProBench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "86",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-13 15:31:13",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-13 15:31:13",
                        "createDate": "2025-03-13 14:00:05",
                        "desc": {
                            "cn": "ProBenchæ˜¯ä¸€ä¸ªåŒ…å«éœ€è¦å¤§é‡ä¸“å®¶çº§çŸ¥è¯†æ¥è§£å†³çš„å¼€æ”¾å¼å¤šæ¨¡æ€æŸ¥è¯¢çš„åŸºå‡†ã€‚ProBench åŒ…å« 10 ä¸ªä»»åŠ¡é¢†åŸŸå’Œ 56 ä¸ªå­é¢†åŸŸï¼Œæ”¯æŒ 17 ç§è¯­è¨€ï¼Œå¹¶æ”¯æŒæœ€å¤š 13 è½®å¯¹è¯ã€‚",
                            "en": "ProBench is a benchmark that contains open-ended multimodal queries that require intensive expert-level knowledge to solve. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/probench'. Error: Path opencompass/probench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1512",
                    "name": "MLVU",
                    "version": "1.0.0",
                    "description": "MLVU test MLLMs' understanding of multi-task long videos, encompassing diverse tasks based on various long videos. ",
                    "url": "opencompass/opencompass_1512.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1512",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1512",
                        "name": "MLVU",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/JUNJIE99/MLVU",
                        "paperLink": "https://arxiv.org/abs/2406.04264",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "86",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:51:32",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:51:32",
                        "createDate": "2025-02-17 16:07:19",
                        "desc": {
                            "cn": "MLVUç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„é•¿è§†é¢‘ç†è§£èƒ½åŠ›ï¼ŒåŒ…å«é¢å‘å„ç§ç±»å‹é•¿è§†é¢‘çš„å¤šæ ·åŒ–çš„è¯„ä¼°ä»»åŠ¡ã€‚",
                            "en": "MLVU test MLLMs' understanding of multi-task long videos, encompassing diverse tasks based on various long videos. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mlvu'. Error: Path opencompass/mlvu is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1929",
                    "name": "MTCMB",
                    "version": "1.0.0",
                    "description": "We introduce MTCMB-a Multi-Task Benchmark for Evaluating LLMs on TCM Knowledge, Reasoning, and Safety. Developed in collaboration with certified TCM experts, MTCMB comprises 12 sub-datasets spanning five major categories: knowledge QA, language understanding, diagnostic reasoning, formula generation",
                    "url": "opencompass/opencompass_1929.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1929",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning",
                        "Knowledge",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1929",
                        "name": "MTCMB",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Chinese Medicine",
                                "en": "Chinese Medicine"
                            },
                            {
                                "cn": "Benchmark",
                                "en": "Benchmark"
                            },
                            {
                                "cn": "TCM",
                                "en": "TCM"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Wayyuanyuan/MTCMB",
                        "paperLink": "https://doi.org/10.48550/arXiv.2506.01252",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "17001784",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-g5il9RCGl"
                        },
                        "lookNum": "85",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-16 09:49:31",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-16 09:49:31",
                        "createDate": "2025-06-14 09:16:46",
                        "desc": {
                            "cn": "å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸­åŒ»é¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¢å¤šï¼Œåˆ°åº•å¤§è¯­è¨€æ¨¡å‹åœ¨è¿™ä¸€å¤è€çš„å­¦ç§‘è¡¨ç°å¦‚ä½•ï¼Ÿè™½ç„¶æœ‰ä¸€äº›é›¶ç¢çš„è¯„æµ‹æ•°æ®é›†ï¼Œä½†å¤§éƒ½ä»¥å•é€‰é¢˜ã€ç—…æ¡ˆåˆ†æã€ç²—ç³™çš„åŒ»æ‚£å¯¹è¯ä¸ºä¸»ï¼Œå¾ˆéš¾å…¨é¢è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸­åŒ»é¢†åŸŸçš„å®é™…èƒ½åŠ›ã€‚è¿‘æ—¥ï¼Œä¸­å±±å¤§å­¦è”åˆæ¹–å—ä¸­åŒ»è¯å¤§å­¦ç­‰å›¢é˜Ÿæ¨å‡ºå…¨çƒé¦–ä¸ªä¸­åŒ»å¤šä»»åŠ¡è¯„æµ‹åŸºå‡†ï¼ˆBenchmarkï¼‰ â€”â€” MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine",
                            "en": "We introduce MTCMB-a Multi-Task Benchmark for Evaluating LLMs on TCM Knowledge, Reasoning, and Safety. Developed in collaboration with certified TCM experts, MTCMB comprises 12 sub-datasets spanning five major categories: knowledge QA, language understanding, diagnostic reasoning, formula generation"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mtcmb'. Error: Path opencompass/mtcmb is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1829",
                    "name": "MMS-VPR",
                    "version": "1.0.0",
                    "description": "MMS-VPR is a large-scale multimodal dataset for street-level place recognition in pedestrian areas. It contains 78,575 images and 2,512 videos from 207 locations in Chengdu, China, with rich metadata and spatial graph structure.",
                    "url": "opencompass/opencompass_1829.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1829",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding",
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1829",
                        "name": "MMS-VPR",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            },
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Visual Place Recognition",
                                "en": "Visual Place Recognition"
                            },
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "Dataset and Benchmark",
                                "en": "Dataset and Benchmark"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/yiasun/MMS-VPRlib",
                        "paperLink": "https://arxiv.org/abs/2505.12254",
                        "officialWebsiteLink": "https://huggingface.co/datasets/Yiwei-Ou/MMS-VPR",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "53005345",
                            "name": "Yiwei-Ou",
                            "avatar": null,
                            "nickname": "Yiwei-Ou"
                        },
                        "lookNum": "84",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-05-27 12:50:29",
                        "supportOnlineEval": false,
                        "updateDate": "2025-05-27 12:50:29",
                        "createDate": "2025-05-23 20:07:40",
                        "desc": {
                            "cn": "MMS-VPR æ˜¯ä¸€ä¸ªé¢å‘å¤æ‚åŸå¸‚æ­¥è¡Œè¡—åŒºçš„ å¤šæ¨¡æ€è¡—æ™¯è§†è§‰åœ°ç‚¹è¯†åˆ«å¤§è§„æ¨¡æ•°æ®é›†ï¼Œé‡‡é›†è‡ªæˆéƒ½çº¦ 70,800 å¹³æ–¹ç±³çš„å¼€æ”¾å¼å•†ä¸šè¡—åŒºï¼Œè¦†ç›– 207 ä¸ªåœ°ç‚¹ï¼ŒåŒ…å« 78,575 å¼ å›¾åƒå’Œ 2,512 æ®µè§†é¢‘ï¼Œæ¯æ¡æ•°æ®å‡å¸¦æœ‰ GPS åæ ‡ã€æ—¶é—´æˆ³å’Œæ–‡æœ¬å…ƒä¿¡æ¯ã€‚ç›¸è¾ƒä»¥è½¦è½½è§†è§’å’Œè¥¿æ–¹åŸå¸‚ä¸ºä¸»çš„ä¼ ç»Ÿæ•°æ®é›†ï¼ŒMMS-VPR æ›´è´´è¿‘çœŸå®ã€å¯†é›†ã€å¤šç”¨é€”çš„è¡—é“ç©ºé—´ï¼Œå…·å¤‡ä¸°å¯Œçš„è§†è§’ã€æ—¶æ®µå’Œæ¨¡æ€å¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ•°æ®é›†æ„å»ºäº†åŒ…å« 81 ä¸ªèŠ‚ç‚¹ã€125 æ¡è¾¹çš„ç©ºé—´å›¾ç»“æ„ï¼Œæ”¯æŒç»“æ„æ„ŸçŸ¥çš„åœ°ç‚¹è¯†åˆ«æ–¹æ³•ã€‚æ•°æ®é›†è¿˜å®šä¹‰äº†ä¸¤ä¸ªå­é›†ï¼ˆEdges å’ŒPointsï¼‰ï¼Œæ”¯æŒç²¾ç»†åŒ–å’Œå›¾ç»“æ„è¯„ä¼°ä»»åŠ¡ï¼ŒåŠ©åŠ›å¤šæ¨¡æ€ä¸åœ°ç†ç©ºé—´ç†è§£çš„äº¤å‰ç ”ç©¶ã€‚",
                            "en": "MMS-VPR is a large-scale multimodal dataset for street-level place recognition in pedestrian areas. It contains 78,575 images and 2,512 videos from 207 locations in Chengdu, China, with rich metadata and spatial graph structure."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mms_vpr'. Error: Path opencompass/mms_vpr is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1839",
                    "name": "tiny_qa_benchmark_pp",
                    "version": "1.0.0",
                    "description": "TinyQA is a benchmark suite designed to evaluate the reasoning abilities of large language models (LLMs). It focuses on assessing LLMs through natural language question-answer pairs, covering various types of reasoning tasks such as causal, logical, and commonsense reasoning. ",
                    "url": "opencompass/opencompass_1839.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1839",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1839",
                        "name": "tiny_qa_benchmark_pp",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Strong Reasoning",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/vincentkoc/tiny_qa_benchmark_pp",
                        "paperLink": "https://arxiv.org/pdf/2505.12058",
                        "officialWebsiteLink": "https://huggingface.co/datasets/vincentkoc/tiny_qa_benchmark_pp",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "82",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:30",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:30",
                        "createDate": "2025-06-04 11:15:55",
                        "desc": {
                            "cn": "TinyQAæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ã€‚è¯¥åŸºå‡†ä¸“æ³¨äºé€šè¿‡è‡ªç„¶è¯­è¨€é—®é¢˜å’Œç­”æ¡ˆå¯¹æ¥è¡¡é‡LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œæ¶µç›–äº†å¤šç§ç±»å‹çš„æ¨ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬å› æœæ¨ç†ã€é€»è¾‘æ¨ç†å’Œå¸¸è¯†æ¨ç†ã€‚TinyQAæä¾›äº†å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œæ—¨åœ¨æŒ‘æˆ˜LLMsçš„æ¨ç†æ·±åº¦å’Œå¹¿åº¦ã€‚é€šè¿‡ä¸¥æ ¼çš„è¯„ä¼°ï¼ŒTinyQAèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£LLMsåœ¨å¤„ç†å¤æ‚è¯­è¨€ä»»åŠ¡æ—¶çš„è¡¨ç°ï¼Œå¹¶ä¸ºæ”¹è¿›æ¨¡å‹æä¾›æ–¹å‘ã€‚",
                            "en": "TinyQA is a benchmark suite designed to evaluate the reasoning abilities of large language models (LLMs). It focuses on assessing LLMs through natural language question-answer pairs, covering various types of reasoning tasks such as causal, logical, and commonsense reasoning. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/tiny_qa_benchmark_pp'. Error: Path opencompass/tiny_qa_benchmark_pp is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1845",
                    "name": "STARK_10k",
                    "version": "1.0.0",
                    "description": "STARK is a comprehensive benchmark designed to systematically evaluate large language models (LLMs) and large reasoning models (LRMs) on spatiotemporal reasoning tasks, particularly for applications in cyber-physical systems (CPS) such as robotics, autonomous vehicles, and smart city infrastructure.",
                    "url": "opencompass/opencompass_1845.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1845",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1845",
                        "name": "STARK_10k",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/nesl/STARK_Benchmark/",
                        "paperLink": "https://arxiv.org/pdf/2505.11618",
                        "officialWebsiteLink": "https://huggingface.co/datasets/prquan/STARK_10k",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "82",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:41",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:41",
                        "createDate": "2025-06-04 11:15:01",
                        "desc": {
                            "cn": "STARK æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨æ—¶ç©ºæ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨ç½‘ç»œç‰©ç†ç³»ç»Ÿï¼ˆCPSï¼‰ä¸­çš„åº”ç”¨ï¼Œå¦‚æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½åŸå¸‚åŸºç¡€è®¾æ–½ã€‚è¯¥åŸºå‡†åŒ…å« 26 ç§ä¸åŒçš„æ—¶ç©ºä»»åŠ¡ï¼Œæ¶µç›–çŠ¶æ€ä¼°è®¡ã€æ—¶ç©ºå…³ç³»æ¨ç†å’Œä¸–ç•ŒçŸ¥è¯†æ„ŸçŸ¥æ¨ç†ä¸‰ä¸ªå±‚æ¬¡ã€‚",
                            "en": "STARK is a comprehensive benchmark designed to systematically evaluate large language models (LLMs) and large reasoning models (LRMs) on spatiotemporal reasoning tasks, particularly for applications in cyber-physical systems (CPS) such as robotics, autonomous vehicles, and smart city infrastructure."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/stark_10k'. Error: Path opencompass/stark_10k is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1656",
                    "name": "RFUAV",
                    "version": "1.0.0",
                    "description": "RFUAV offers a comprehensive benchmark dataset for Radio-Frequency (RF)-based drone detection and identification.",
                    "url": "opencompass/opencompass_1656.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1656",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1656",
                        "name": "RFUAV",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/kitoweeknd/RFUAV/",
                        "paperLink": "https://arxiv.org/pdf/2503.09033",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "82",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-20 14:27:02",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-20 14:27:02",
                        "createDate": "2025-03-20 14:05:07",
                        "desc": {
                            "cn": "RFUAV æä¾›äº†ä¸€ä¸ªåŸºäºå°„é¢‘ï¼ˆRFï¼‰çš„æ— äººæœºæ£€æµ‹å’Œè¯†åˆ«çš„å…¨é¢åŸºå‡†æ•°æ®é›†ã€‚",
                            "en": "RFUAV offers a comprehensive benchmark dataset for Radio-Frequency (RF)-based drone detection and identification."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/rfuav'. Error: Path opencompass/rfuav is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1972",
                    "name": "CPRet",
                    "version": "1.0.0",
                    "description": "Programming contests are long used to evaluate algorithmic thinking and coding skills, and have recently become benchmarks for assessing large language models (LLMs). However, the rapid expansion of problem sets has led to a surge in duplicate or highly similar problems, compromising fairness ...",
                    "url": "opencompass/opencompass_1972.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1972",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1972",
                        "name": "CPRet",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Problem Similarity",
                                "en": "Problem Similarity"
                            },
                            {
                                "cn": "Competitive Programming",
                                "en": "Competitive Programming"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/coldchair/CPRet",
                        "paperLink": "https://arxiv.org/abs/2505.12925",
                        "officialWebsiteLink": "https://huggingface.co/collections/coldchair16/cpret-682451276f05c5988fcbdf34",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "38705824",
                            "name": null,
                            "avatar": null,
                            "nickname": "coldchair"
                        },
                        "lookNum": "82",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-24 15:44:30",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-24 15:44:30",
                        "createDate": "2025-06-24 14:41:53",
                        "desc": {
                            "cn": "ç¼–ç¨‹ç«èµ›é•¿æœŸç”¨äºè¯„ä¼°ç®—æ³•ä¸ç¼–ç¨‹èƒ½åŠ›ï¼Œè¿‘å¹´æ¥ä¹Ÿè¢«ç”¨äºå¤§è¯­è¨€æ¨¡å‹çš„è¯„æµ‹ã€‚ä½†éšç€é¢˜åº“æ‰©å±•ï¼Œé‡å¤æˆ–ç›¸ä¼¼é¢˜æ¿€å¢ï¼Œå½±å“ç«èµ›å…¬å¹³æ€§ä¸æ¨¡å‹è¯„æµ‹æ•ˆæœã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºâ€œç›¸ä¼¼é¢˜ç›®æ£€ç´¢â€ä»»åŠ¡ï¼Œå¹¶æ„å»ºç»Ÿä¸€æ£€ç´¢åŸºå‡†æ•°æ®é›† CPRetï¼Œæ¶µç›–é¢˜ç›®ä¸ä»£ç çš„å››ç±»æ£€ç´¢ä»»åŠ¡ï¼ŒåŒ…å«è‡ªåŠ¨çˆ¬å–å’Œäººå·¥æ ‡æ³¨çš„æ•°æ®æ ·æœ¬ã€‚åŒæ—¶ï¼Œè®¾è®¡å¹¶è®­ç»ƒäº†ä¸¤ç§æ£€ç´¢æ¨¡å‹ CPRetriever-Code ä¸ CPRetriever-Probï¼Œæ˜¾è‘—æå‡æ£€ç´¢æ•ˆæœã€‚å®éªŒè¿˜å‘ç°ç›¸ä¼¼é¢˜ä¼šæé«˜æ¨¡å‹å¾—åˆ†ã€å‡å°æ¨¡å‹å·®å¼‚ï¼Œå¼ºè°ƒäº†è¯„æµ‹ä¸­å¼•å…¥â€œç›¸ä¼¼æ€§æ„ŸçŸ¥â€çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†å¼€æºæ£€ç´¢å¹³å°ï¼Œæ”¯æŒé‡å¤é¢˜æ£€æµ‹ä¸ç›¸ä¼¼é¢˜æ¨èã€‚é¡¹ç›®åœ°å€ï¼šhttps://github.com/coldchair/",
                            "en": "Programming contests are long used to evaluate algorithmic thinking and coding skills, and have recently become benchmarks for assessing large language models (LLMs). However, the rapid expansion of problem sets has led to a surge in duplicate or highly similar problems, compromising fairness ..."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cpret'. Error: Path opencompass/cpret is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1987",
                    "name": "DeepResearchBench",
                    "version": "1.0.0",
                    "description": "DeepResearch Bench is a comprehensive benchmark designed to evaluate large language model (LLM) agents on complex research tasks. ",
                    "url": "opencompass/opencompass_1987.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1987",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1987",
                        "name": "DeepResearchBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Ayanami0730/deep_research_bench",
                        "paperLink": "https://arxiv.org/abs/2506.11763",
                        "officialWebsiteLink": "https://deepresearch-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "81",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-30 11:37:54",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-30 11:37:54",
                        "createDate": "2025-06-30 11:37:46",
                        "desc": {
                            "cn": "DeepResearch Bench æ˜¯ä¸€ä¸ªé¢å‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™ºèƒ½ä½“çš„ç»¼åˆæ€§è¯„æµ‹åŸºå‡†ï¼Œä¸“ä¸ºè¯„ä¼°å…¶åœ¨å¤æ‚ç ”ç©¶ä»»åŠ¡ä¸­çš„è¡¨ç°è€Œè®¾è®¡ã€‚ è¯¥åŸºå‡†åŒ…å« 100 ä¸ªç”± 22 ä¸ªé¢†åŸŸçš„ä¸“å®¶ç²¾å¿ƒè®¾è®¡çš„åšå£«çº§ç ”ç©¶ä»»åŠ¡ï¼Œæ¶µç›–å¤šé¢†åŸŸçš„æ·±åº¦ç ”ç©¶éœ€æ±‚ã€‚",
                            "en": "DeepResearch Bench is a comprehensive benchmark designed to evaluate large language model (LLM) agents on complex research tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/deepresearchbench'. Error: Path opencompass/deepresearchbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1752",
                    "name": "LLM-SRBench",
                    "version": "1.0.0",
                    "description": "LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization.",
                    "url": "opencompass/opencompass_1752.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1752",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1752",
                        "name": "LLM-SRBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Strong Reasoning",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/deep-symbolic-mathematics/llm-srbench",
                        "paperLink": "https://arxiv.org/abs/2504.10415",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "80",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-21 12:34:37",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-21 12:34:37",
                        "createDate": "2025-04-21 12:02:24",
                        "desc": {
                            "cn": "LLM-SRBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«239ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å››ä¸ªç§‘å­¦é¢†åŸŸï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°åŸºäºLLMsçš„ç§‘å­¦æ–¹ç¨‹å¼å‘ç°æ–¹æ³•ï¼ŒåŒæ—¶é˜²æ­¢ç®€å•è®°å¿†ã€‚",
                            "en": "LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/llm_srbench'. Error: Path opencompass/llm_srbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1622",
                    "name": "CORAL",
                    "version": "1.0.0",
                    "description": "Researchers present a large-scale conversational RAG benchmark named CORAL and propose a unified framework for standardizing and evaluating various conversational RAG baselines.",
                    "url": "opencompass/opencompass_1622.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1622",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1622",
                        "name": "CORAL",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "RAG",
                                "en": "RAG"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Ariya12138/CORAL",
                        "paperLink": "https://arxiv.org/abs/2410.23090",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "79",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-12 11:07:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-12 11:07:55",
                        "createDate": "2025-03-11 15:39:20",
                        "desc": {
                            "cn": "CORAL æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡å¯¹è¯ RAG åŸºå‡†ï¼ŒåŒ…å«ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºæ ‡å‡†åŒ–å’Œè¯„ä¼°å„ç§å¯¹è¯ RAG åŸºçº¿ã€‚",
                            "en": "Researchers present a large-scale conversational RAG benchmark named CORAL and propose a unified framework for standardizing and evaluating various conversational RAG baselines."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/coral'. Error: Path opencompass/coral is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1733",
                    "name": "FEABench",
                    "version": "1.0.0",
                    "description": "FEABench is a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA). ",
                    "url": "opencompass/opencompass_1733.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1733",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1733",
                        "name": "FEABench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "FEAåŸºå‡†æµ‹è¯•",
                                "en": "FEAåŸºå‡†æµ‹è¯•"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/google/feabench/tree/main",
                        "paperLink": "https://arxiv.org/abs/2504.06260",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "79",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:13:05",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:13:05",
                        "createDate": "2025-04-11 14:42:38",
                        "desc": {
                            "cn": "FEABenchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å’ŒLLMä»£ç†ä½¿ç”¨æœ‰é™å…ƒåˆ†æï¼ˆFEAï¼‰æ¨¡æ‹Ÿå’Œè§£å†³ç‰©ç†ã€æ•°å­¦åŠå·¥ç¨‹é—®é¢˜èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚",
                            "en": "FEABench is a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA). "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/feabench'. Error: Path opencompass/feabench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1744",
                    "name": "StyleRec",
                    "version": "1.0.0",
                    "description": "A Benchmark Dataset for Prompt Recovery in Writing Style Transformation.",
                    "url": "opencompass/opencompass_1744.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1744",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding",
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1744",
                        "name": "StyleRec",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            },
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "NLP",
                                "en": "NLP"
                            },
                            {
                                "cn": "Writing Style Transformation",
                                "en": "Writing Style Transformation"
                            },
                            {
                                "cn": "Prompt Recovery",
                                "en": "Prompt Recovery"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/promptrecovery501/StyleRec",
                        "paperLink": "https://arxiv.org/abs/2504.04373",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "18903058",
                            "name": "qianertongre",
                            "avatar": null,
                            "nickname": "qianertongre"
                        },
                        "lookNum": "79",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-16 11:32:28",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-16 11:32:28",
                        "createDate": "2025-04-15 13:45:08",
                        "desc": {
                            "cn": "åŸºäºå†™ä½œé£æ ¼è½¬æ¢çš„æç¤ºè¯æ¢å¤çš„è¯„æµ‹é›†",
                            "en": "A Benchmark Dataset for Prompt Recovery in Writing Style Transformation."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/stylerec'. Error: Path opencompass/stylerec is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1552",
                    "name": "CEB",
                    "version": "1.0.0",
                    "description": "CEB evaluates LLM bias compositionally, featuring 11k samples characterized across bias types, social groups, and tasks.",
                    "url": "opencompass/opencompass_1552.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1552",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1552",
                        "name": "CEB",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SongW-SW/CEB",
                        "paperLink": "https://arxiv.org/abs/2407.02408",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "78",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:49:21",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:49:21",
                        "createDate": "2025-02-26 14:20:34",
                        "desc": {
                            "cn": "CEBæ˜¯ä¸€ä¸ªç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹åå·®çš„ç»„æˆè¯„ä¼°åŸºå‡†ï¼Œå¼•å…¥äº†åŒ…å« 11,004 ä¸ªæ ·æœ¬çš„ç»„æˆè¯„ä¼°åŸºå‡†ï¼Œä»åå·®ç±»å‹ã€ç¤¾ä¼šç¾¤ä½“å’Œä»»åŠ¡ä¸‰ä¸ªç»´åº¦æè¿°æ¯ä¸ªæ•°æ®é›†ã€‚",
                            "en": "CEB evaluates LLM bias compositionally, featuring 11k samples characterized across bias types, social groups, and tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ceb'. Error: Path opencompass/ceb is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1838",
                    "name": "LLM-BabyBench",
                    "version": "1.0.0",
                    "description": "LLM-BabyBench is a benchmark suite designed to evaluate Large Language Models (LLMs) on grounded planning and reasoning tasks.",
                    "url": "opencompass/opencompass_1838.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1838",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1838",
                        "name": "LLM-BabyBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Strong Reasoning",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/choukrani/llm-babybench",
                        "paperLink": "https://arxiv.org/pdf/2505.12135",
                        "officialWebsiteLink": "https://huggingface.co/datasets/salem-mbzuai/LLM-BabyBench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "78",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:49",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:49",
                        "createDate": "2025-06-04 11:16:10",
                        "desc": {
                            "cn": "LLM-BabyBenchæ˜¯ä¸€ä¸ªä¸“é—¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨äº¤äº’ç¯å¢ƒä¸­è§„åˆ’å’Œæ¨ç†èƒ½åŠ›çš„æ–°åŸºå‡†æµ‹è¯•å¥—ä»¶ã€‚åŸºäºBabyAIç½‘æ ¼ä¸–ç•Œçš„æ–‡æœ¬é€‚é…ç‰ˆæœ¬ï¼Œè¯¥åŸºå‡†è¯„ä¼°LLMsåœ¨ä¸‰ä¸ªæ ¸å¿ƒæ–¹é¢çš„è¡¨ç°ï¼šé¢„æµ‹åŠ¨ä½œå¯¹ç¯å¢ƒçŠ¶æ€çš„å½±å“ï¼ˆPredictä»»åŠ¡ï¼‰ã€ç”Ÿæˆä½çº§åŠ¨ä½œåºåˆ—ä»¥å®ç°æŒ‡å®šç›®æ ‡ï¼ˆPlanä»»åŠ¡ï¼‰ã€ä»¥åŠå°†é«˜çº§æŒ‡ä»¤åˆ†è§£ä¸ºè¿è´¯çš„å­ç›®æ ‡åºåˆ—ï¼ˆDecomposeä»»åŠ¡ï¼‰ã€‚\nåŸºå‡†åŒ…å«16ä¸ªéš¾åº¦çº§åˆ«ï¼Œæä¾›ä¸‰ç§æ–‡æœ¬æ ¼å¼ï¼ˆNarrativeã€Structuredã€JSONï¼‰ï¼Œå¹¶é…å¤‡OmniBotä¸“å®¶ä»£ç†ç”¨äºç”ŸæˆåŸºå‡†æ•°æ®ã€‚",
                            "en": "LLM-BabyBench is a benchmark suite designed to evaluate Large Language Models (LLMs) on grounded planning and reasoning tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/llm_babybench'. Error: Path opencompass/llm_babybench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1678",
                    "name": "RSMMVP",
                    "version": "1.0.0",
                    "description": "This dataset follows a similar procedure to the original MMVP benchmark on natural images but directed towards the remote sensing domain. Challenging visual patterns are identified based on CLIP blind pairs, accompanied with the correpsonding questions, options and ground-truth answer.",
                    "url": "opencompass/opencompass_1678.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1678",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1678",
                        "name": "RSMMVP",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2503.15816",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "77",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-26 15:56:27",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-26 15:56:27",
                        "createDate": "2025-03-26 14:35:19",
                        "desc": {
                            "cn": "RSMMVPéµå¾ªä¸åŸå§‹ MMVP åŸºå‡†åœ¨è‡ªç„¶å›¾åƒä¸Šçš„ç±»ä¼¼æµç¨‹ï¼Œä½†é’ˆå¯¹é¥æ„Ÿé¢†åŸŸã€‚æ ¹æ® CLIP ç›²å¯¹è¯†åˆ«å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰æ¨¡å¼ï¼Œå¹¶é™„å¸¦ç›¸åº”çš„é—®é¢˜ã€é€‰é¡¹å’ŒçœŸå®ç­”æ¡ˆã€‚",
                            "en": "This dataset follows a similar procedure to the original MMVP benchmark on natural images but directed towards the remote sensing domain. Challenging visual patterns are identified based on CLIP blind pairs, accompanied with the correpsonding questions, options and ground-truth answer."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/rsmmvp'. Error: Path opencompass/rsmmvp is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2005",
                    "name": "OSS-Bench",
                    "version": "1.0.0",
                    "description": "OSS-Bench replaces functions with LLM-generated code and evaluates them using three natural metrics: compilability, functional correctness, and memory safety, leveraging robust signals like compilation failures, test-suite violations, and sanitizer alerts as ground truth.",
                    "url": "opencompass/opencompass_2005.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2005",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2005",
                        "name": "OSS-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Memory Safety",
                                "en": "Memory Safety"
                            },
                            {
                                "cn": "Open-Source Software",
                                "en": "Open-Source Software"
                            },
                            {
                                "cn": "Living Benchmark",
                                "en": "Living Benchmark"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/oss-bench/oss-bench",
                        "paperLink": "https://arxiv.org/abs/2505.12331",
                        "officialWebsiteLink": "https://oss-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "85503313",
                            "name": null,
                            "avatar": null,
                            "nickname": "jiangyc"
                        },
                        "lookNum": "77",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-01 09:36:14",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-01 09:36:14",
                        "createDate": "2025-06-30 18:41:59",
                        "desc": {
                            "cn": "OSS-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†ç”Ÿæˆå™¨ï¼Œå®ƒå¯ä»¥ä»çœŸå®çš„å¼€æºè½¯ä»¶ä¸­è‡ªåŠ¨æ„å»ºå¤§è§„æ¨¡çš„å®æ—¶è¯„ä¼°ä»»åŠ¡ã€‚OSS-Bench å°†å‡½æ•°æ›¿æ¢ä¸º LLM ç”Ÿæˆçš„ä»£ç ï¼Œå¹¶ä½¿ç”¨ä¸‰ä¸ªè‡ªç„¶æŒ‡æ ‡ï¼ˆå¯ç¼–è¯‘æ€§ã€åŠŸèƒ½æ­£ç¡®æ€§å’Œå†…å­˜å®‰å…¨æ€§ï¼‰å¯¹å…¶è¿›è¡Œè¯„ä¼°ï¼Œå¹¶åˆ©ç”¨ç¼–è¯‘å¤±è´¥ã€æµ‹è¯•å¥—ä»¶è¿è§„å’ŒSanitizerè­¦æŠ¥ç­‰ç¨³å¥ä¿¡å·ä½œä¸ºåŸºå‡†äº‹å®ã€‚",
                            "en": "OSS-Bench replaces functions with LLM-generated code and evaluates them using three natural metrics: compilability, functional correctness, and memory safety, leveraging robust signals like compilation failures, test-suite violations, and sanitizer alerts as ground truth."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/oss_bench'. Error: Path opencompass/oss_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1848",
                    "name": "MAVOS-DD",
                    "version": "1.0.0",
                    "description": "MAVOS-DD is the first large-scale open-set benchmark for multilingual audio-video deepfake detection.",
                    "url": "opencompass/opencompass_1848.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1848",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1848",
                        "name": "MAVOS-DD",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            },
                            {
                                "cn": "AVQA",
                                "en": "AVQA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/pdf/2505.11109",
                        "officialWebsiteLink": "https://huggingface.co/datasets/unibuc-cs/MAVOS-DD",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "75",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:33",
                        "createDate": "2025-06-04 11:14:23",
                        "desc": {
                            "cn": "MAVOS-DDæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šè¯­è¨€éŸ³è§†é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡250å°æ—¶çš„çœŸå®å’Œä¼ªé€ è§†é¢‘ï¼Œæ¶µç›–å…«ç§è¯­è¨€ã€‚è¯¥æ•°æ®é›†é€šè¿‡ä¸ƒç§ä¸åŒçš„æ·±åº¦ä¼ªé€ ç”Ÿæˆæ¨¡å‹ç”Ÿæˆä¼ªé€ è§†é¢‘ï¼Œè¿™äº›æ¨¡å‹åŸºäºä¸åŒçš„ç”Ÿæˆæ–¹æ³•ï¼ŒåŒ…æ‹¬è¯´è¯å¤´åƒç”Ÿæˆã€è¡¨æƒ…è½¬ç§»å’Œæ¢è„¸ã€‚MAVOS-DDè®¾è®¡äº†å¤šç§å¼€æ”¾é›†æµ‹è¯•åœºæ™¯ï¼ŒåŒ…æ‹¬å¼€æ”¾é›†æ¨¡å‹ã€å¼€æ”¾é›†è¯­è¨€å’Œå…¨å¼€æ”¾é›†ï¼Œä»¥è¯„ä¼°æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹åœ¨æœªçŸ¥æ¨¡å‹å’Œè¯­è¨€ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹åœ¨å¼€æ”¾é›†åœºæ™¯ä¸‹çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå‡¸æ˜¾äº†å¼€å‘æ›´é²æ£’æ£€æµ‹æŠ€æœ¯çš„å¿…è¦æ€§ã€‚",
                            "en": "MAVOS-DD is the first large-scale open-set benchmark for multilingual audio-video deepfake detection."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mavos_dd'. Error: Path opencompass/mavos_dd is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1742",
                    "name": "U-NIAH",
                    "version": "1.0.0",
                    "description": "U-NIAH is a framework unifying RAG and LLM for needle-in-a-haystack tasks, based on the fictional Starlight Academy dataset. It eliminates interference from pre-trained knowledge and supports diverse, complex scenarios (e.g., multi-needle, long-needle, \"needle-in-needle\").",
                    "url": "opencompass/opencompass_1742.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1742",
                    "sample_count": 1000,
                    "traits": [
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1742",
                        "name": "U-NIAH",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "é•¿æ–‡æœ¬",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "RAG",
                                "en": "RAG"
                            },
                            {
                                "cn": "Long-Context",
                                "en": "Long-Context"
                            },
                            {
                                "cn": "Needle In A Haystack",
                                "en": "Needle In A Haystack"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Tongji-KGLLM/U-NIAH",
                        "paperLink": "https://arxiv.org/abs/2503.00353",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "85508164",
                            "name": "yunfan",
                            "avatar": null,
                            "nickname": "yunfan"
                        },
                        "lookNum": "74",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-16 11:32:40",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-16 11:32:40",
                        "createDate": "2025-04-13 10:56:15",
                        "desc": {
                            "cn": "U-NIAHæ˜¯å°† RAG å’Œ LLM ç»Ÿä¸€æ˜ å°„åœ¨å¤§æµ·æé’ˆä»»åŠ¡ä¸­çš„æ¡†æ¶ã€‚æ‰€æœ‰ä»»åŠ¡åŸºäºä¸€ä¸ªè™šæ„èƒŒæ™¯ä¸‹çš„æ•°æ®é›†Starlight Academyï¼Œæ¶µç›–äº†é­”æ³•ç³»ç»Ÿã€å­¦æœ¯è¯¾ç¨‹ã€æ ¡å›­ç”Ÿæ´»ã€ç­‰å¤šä¸ªæ–¹é¢ï¼Œæ—¨åœ¨æ¶ˆé™¤é¢„è®­ç»ƒçŸ¥è¯†çš„å¹²æ‰°ï¼Œä»è€Œèƒ½å¤Ÿç‹¬ç«‹äº LLMs çš„å…ˆéªŒçŸ¥è¯†ã€‚æ¡†æ¶åŒ…å«å¤šç§è¯„ä¼°åœºæ™¯ï¼Œæ”¯æŒå¤šé’ˆï¼ˆ3ã€7ã€15ä¸ªé’ˆï¼‰å’Œé•¿é’ˆï¼ˆ400-500 tokenï¼‰é…ç½®ï¼Œè¿˜å¼•å…¥äº†â€œé’ˆä¸­é’ˆâ€ç»“æ„ï¼Œè¿›ä¸€æ­¥å¢åŠ äº†å¤æ‚æ€§ã€‚è¯¥æ•°æ®é›†é€šè¿‡å¤šæ ·åŒ–çš„åœºæ™¯å’Œåˆæˆç”Ÿæˆçš„å†…å®¹ï¼Œèƒ½å¤Ÿä»å¤šä¸ªç»´åº¦åˆ†ææ¨¡å‹åœ¨é•¿æ–‡æœ¬åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚åŒæ—¶é€šè¿‡æ¨¡å—åŒ–è®¾è®¡ï¼ŒU-NIAHå¯æŒç»­æ³¨å…¥æ–°çš„æŒ‘æˆ˜åœºæ™¯ã€‚",
                            "en": "U-NIAH is a framework unifying RAG and LLM for needle-in-a-haystack tasks, based on the fictional Starlight Academy dataset. It eliminates interference from pre-trained knowledge and supports diverse, complex scenarios (e.g., multi-needle, long-needle, \"needle-in-needle\")."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/u_niah'. Error: Path opencompass/u_niah is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1780",
                    "name": "C-FAITH",
                    "version": "1.0.0",
                    "description": "C-FAITH, a Chinese QA hallucination benchmark created from 1,399 knowledge documents obtained from web scraping, totaling 60,702 entries.",
                    "url": "opencompass/opencompass_1780.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1780",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1780",
                        "name": "C-FAITH",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/pkulcwmzx/C-FAITH",
                        "paperLink": "https://arxiv.org/abs/2504.10167",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "74",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-25 17:00:49",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-25 17:00:49",
                        "createDate": "2025-04-25 16:42:33",
                        "desc": {
                            "cn": " C-FAITHï¼Œè¿™æ˜¯ä¸€ä¸ªä¸­å›½ QA å¹»è§‰åŸºå‡†ï¼Œç”±ä»ç½‘ç»œæŠ“å–ä¸­è·å¾—çš„ 1,399 ä»½çŸ¥è¯†æ–‡æ¡£åˆ›å»ºï¼Œæ€»å…± 60,702 ä¸ªæ¡ç›®ã€‚",
                            "en": "C-FAITH, a Chinese QA hallucination benchmark created from 1,399 knowledge documents obtained from web scraping, totaling 60,702 entries."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/c_faith'. Error: Path opencompass/c_faith is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1865",
                    "name": "GitGoodBench",
                    "version": "1.0.0",
                    "description": "GitGoodBench Lite is a subset of 900 samples for evaluating the performance of AI agents in resolving git tasks (see Supported Scenarios). ",
                    "url": "opencompass/opencompass_1865.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1865",
                    "sample_count": 1000,
                    "traits": [
                        "Code",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1865",
                        "name": "GitGoodBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/JetBrains-Research/git-good-bench",
                        "paperLink": "https://arxiv.org/pdf/2505.22583",
                        "officialWebsiteLink": "https://huggingface.co/datasets/JetBrains/git_good_bench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "73",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-03 15:06:02",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-03 15:06:02",
                        "createDate": "2025-06-03 12:15:09",
                        "desc": {
                            "cn": "GitGoodBench Liteæ˜¯ä¸€ä¸ªåŒ…å«900ä¸ªæ ·æœ¬çš„å­é›†ï¼Œç”¨äºè¯„ä¼°AIæ™ºèƒ½ä½“åœ¨è§£å†³gitä»»åŠ¡æ–¹é¢çš„æ€§èƒ½ï¼ˆå‚è§æ”¯æŒçš„åœºæ™¯ï¼‰ã€‚æ•°æ®é›†ä¸­çš„æ ·æœ¬åœ¨ç¼–ç¨‹è¯­è¨€Pythonã€Javaå’ŒKotlinä»¥åŠæ ·æœ¬ç±»å‹åˆå¹¶å†²çªè§£å†³å’Œæ–‡ä»¶æäº¤è¯­æ³•ä¹‹é—´å‡åŒ€åˆ†å¸ƒã€‚å› æ­¤ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¯ç§æ ·æœ¬ç±»å‹å’Œç¼–ç¨‹è¯­è¨€å„150ä¸ªæ ·æœ¬ã€‚",
                            "en": "GitGoodBench Lite is a subset of 900 samples for evaluating the performance of AI agents in resolving git tasks (see Supported Scenarios). "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gitgoodbench'. Error: Path opencompass/gitgoodbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1681",
                    "name": "PRMBench_Preview",
                    "version": "1.0.0",
                    "description": "PRMBench is a benchmark dataset for evaluating process-level reward models (PRMs). It consists of 6,216 data instances, each containing a question, a solution process, and a modified process with errors.",
                    "url": "opencompass/opencompass_1681.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1681",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1681",
                        "name": "PRMBench_Preview",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å­¦ç§‘",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ssmisya/PRMBench",
                        "paperLink": "https://arxiv.org/abs/2501.03124",
                        "officialWebsiteLink": "https://prmbench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "73",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-26 15:56:18",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-26 15:56:18",
                        "createDate": "2025-03-26 15:07:01",
                        "desc": {
                            "cn": "PRMBench æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è¿‡ç¨‹çº§å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰çš„åŸºå‡†æ•°æ®é›†ã€‚å®ƒåŒ…å« 6,216 ä¸ªæ•°æ®å®ä¾‹ï¼Œæ¯ä¸ªå®ä¾‹åŒ…å«ä¸€ä¸ªé—®é¢˜ã€ä¸€ä¸ªè§£å†³æ–¹æ¡ˆè¿‡ç¨‹ä»¥åŠä¸€ä¸ªåŒ…å«é”™è¯¯çš„ä¿®æ”¹è¿‡ç¨‹ã€‚",
                            "en": "PRMBench is a benchmark dataset for evaluating process-level reward models (PRMs). It consists of 6,216 data instances, each containing a question, a solution process, and a modified process with errors."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/prmbench_preview'. Error: Path opencompass/prmbench_preview is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1849",
                    "name": "CleanPatrick",
                    "version": "1.0.0",
                    "description": "CleanPatrick is a large-scale, real-world image data-cleaning benchmark with 496,377 binary annotations from 933 medical crowd workers for ranking off-topic, near-duplicate, and label-error issues.",
                    "url": "opencompass/opencompass_1849.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1849",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding",
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1849",
                        "name": "CleanPatrick",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            },
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Digital-Dermatology/CleanPatrick",
                        "paperLink": "https://arxiv.org/pdf/2505.11034",
                        "officialWebsiteLink": "https://huggingface.co/datasets/Digital-Dermatology/CleanPatrick",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "72",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:38",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:38",
                        "createDate": "2025-06-04 11:14:42",
                        "desc": {
                            "cn": "CleanPatrickæ˜¯é¦–ä¸ªå¤§è§„æ¨¡å›¾åƒæ•°æ®æ¸…æ´—åŸºå‡†ï¼ŒåŸºäºå…¬å¼€çš„Fitzpatrick17kçš®è‚¤ç§‘æ•°æ®é›†æ„å»ºã€‚è¯¥åŸºå‡†åŒ…å«è¶…è¿‡50ä¸‡æ¡æ¥è‡ª933ååŒ»å­¦ä¼—åŒ…å·¥äººçš„äºŒå…ƒæ³¨é‡Šï¼Œæ¶µç›–ä¸‰ç§æ•°æ®è´¨é‡é—®é¢˜ï¼šç¦»é¢˜æ ·æœ¬ã€è¿‘ä¼¼é‡å¤æ ·æœ¬å’Œæ ‡ç­¾é”™è¯¯ã€‚é€šè¿‡åŒ»å­¦ä¸“å®¶éªŒè¯ï¼ŒCleanPatrickæä¾›äº†é«˜è´¨é‡çš„åŸºå‡†æ•°æ®ï¼Œç”¨äºè¯„ä¼°å›¾åƒæ•°æ®æ¸…æ´—ç­–ç•¥ã€‚åŸºå‡†æµ‹è¯•ç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„æ•°æ®æ¸…æ´—æ–¹æ³•åœ¨è¿‘ä¼¼é‡å¤æ£€æµ‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ ‡ç­¾é”™è¯¯æ£€æµ‹æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚CleanPatrickä¸ºæ•°æ®æ¸…æ´—æ–¹æ³•æä¾›äº†æ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œæ¨åŠ¨äº†æ›´å¯é çš„æ•°æ®ä¸­å¿ƒäººå·¥æ™ºèƒ½çš„å‘å±•ã€‚",
                            "en": "CleanPatrick is a large-scale, real-world image data-cleaning benchmark with 496,377 binary annotations from 933 medical crowd workers for ranking off-topic, near-duplicate, and label-error issues."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cleanpatrick'. Error: Path opencompass/cleanpatrick is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1981",
                    "name": "OmniBench",
                    "version": "1.0.0",
                    "description": "OmniBench is a multi-dimensional benchmark for virtual agents, designed to systematically evaluate ten core capabilities such as planning, decision-making, and instruction comprehension through automatically generated task graphs with controllable complexity. ",
                    "url": "opencompass/opencompass_1981.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1981",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1981",
                        "name": "OmniBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/antgroup/OmniBench",
                        "paperLink": "https://arxiv.org/abs/2506.08933",
                        "officialWebsiteLink": "https://omni-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "72",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-27 18:12:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-27 18:12:33",
                        "createDate": "2025-06-26 13:55:08",
                        "desc": {
                            "cn": "OmniBench æ˜¯ä¸€ä¸ªé¢å‘è™šæ‹Ÿæ™ºèƒ½ä½“çš„å¤šç»´åº¦è¯„æµ‹åŸºå‡†ï¼Œæ—¨åœ¨é€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹ç”Ÿæˆå…·æœ‰å¯æ§å¤æ‚åº¦çš„ä»»åŠ¡å›¾ï¼Œç³»ç»Ÿè¯„ä¼°æ™ºèƒ½ä½“åœ¨è®¡åˆ’ã€å†³ç­–ã€æŒ‡ä»¤ç†è§£ç­‰åä¸ªæ ¸å¿ƒèƒ½åŠ›ä¸Šçš„è¡¨ç°ã€‚è¯¥åŸºå‡†åŒ…å« 36,000 ä¸ªå›¾ç»“æ„ä»»åŠ¡ï¼Œè¦†ç›– 20 ä¸ªçœŸå®åœºæ™¯ï¼Œå¹¶å¼•å…¥ OmniEval æ¡†æ¶ï¼Œå®ç°å­ä»»åŠ¡çº§åˆ«çš„ç»†ç²’åº¦è¯„ä¼°ï¼Œæ˜¾è‘—æå‡äº†è¯„æµ‹çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚",
                            "en": "OmniBench is a multi-dimensional benchmark for virtual agents, designed to systematically evaluate ten core capabilities such as planning, decision-making, and instruction comprehension through automatically generated task graphs with controllable complexity. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/omnibench'. Error: Path opencompass/omnibench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1739",
                    "name": "CrossWordBench",
                    "version": "1.0.0",
                    "description": "CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles.",
                    "url": "opencompass/opencompass_1739.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1739",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1739",
                        "name": "CrossWordBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SeanLeng1/CrossWordBench",
                        "paperLink": "https://arxiv.org/abs/2504.00043",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "72",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:15:53",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:15:53",
                        "createDate": "2025-04-11 16:14:37",
                        "desc": {
                            "cn": "CrossWordBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡å¡«å­—æ¸¸æˆçš„æ–¹å¼æ¥è¯„ä¼°LLMså’ŒLVLMsçš„æ¨ç†èƒ½åŠ›ã€‚",
                            "en": "CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/crosswordbench'. Error: Path opencompass/crosswordbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1910",
                    "name": "MMAR",
                    "version": "1.0.0",
                    "description": "We introduce MMAR, a new benchmark comprising 1,000 meticulously curated audio-question-answer triplets, designed to evaluate the deep reasoning capabilities of Audio-Language Models (ALMs) across massive multi-disciplinary tasks. ",
                    "url": "opencompass/opencompass_1910.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1910",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1910",
                        "name": "MMAR",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "MMAR",
                                "en": "MMAR"
                            },
                            {
                                "cn": "éŸ³é¢‘è¯­è¨€æ¨¡å‹",
                                "en": "éŸ³é¢‘è¯­è¨€æ¨¡å‹"
                            },
                            {
                                "cn": "æ·±åº¦æ¨ç†",
                                "en": "æ·±åº¦æ¨ç†"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ddlBoJack/MMAR",
                        "paperLink": "https://arxiv.org/abs/2505.13032",
                        "officialWebsiteLink": "https://huggingface.co/datasets/BoJack/MMAR",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "20908931",
                            "name": null,
                            "avatar": null,
                            "nickname": "ddlBoJack"
                        },
                        "lookNum": "71",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-09 11:31:06",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-09 11:31:06",
                        "createDate": "2025-06-06 19:32:53",
                        "desc": {
                            "cn": "MMARæ˜¯ä¸€ä¸ªå…¨æ–°è¯„æµ‹åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°éŸ³é¢‘-è¯­è¨€æ¨¡å‹ï¼ˆALMsï¼‰çš„æ·±åº¦æ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«1,000ä¸ªç²¾å¿ƒæ„å»ºçš„éŸ³é¢‘ä¸é—®ç­”ï¼Œå¹¶ç»è¿‡å¤šè½®çº é”™ä¸è´¨é‡æ ¡éªŒä»¥ç¡®ä¿é«˜æ ‡å‡†ã€‚ä¸ç°æœ‰å±€é™äºç‰¹å®šå£°éŸ³ã€éŸ³ä¹æˆ–è¯­éŸ³é¢†åŸŸçš„è¯„æµ‹ä½“ç³»ä¸åŒï¼ŒMMARè¦†ç›–ç°å®åœºæ™¯ä¸­çš„æ··åˆæ¨¡æ€ï¼Œå¹¶é‡‡ç”¨å››çº§åˆ†å±‚åˆ†ç±»ä½“ç³»ï¼ˆä¿¡å·å±‚ã€æ„ŸçŸ¥å±‚ã€è¯­ä¹‰å±‚ä¸æ–‡åŒ–å±‚ï¼‰ã€‚è¯¥åŸºå‡†ä¸­çš„æ¯ä¸ªé¢˜ç›®éƒ½éœ€è¦è¶…è¶Šè¡¨å±‚ç†è§£çš„å¤šå±‚æ¬¡æ·±åº¦æ¨ç†ï¼Œéƒ¨åˆ†é—®é¢˜æ›´è¦æ±‚ç ”ç©¶ç”Ÿçº§åˆ«çš„ä¸“ä¸šé¢†åŸŸçŸ¥è¯†ä¸æ„ŸçŸ¥èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨MMARä¸Šè¯„ä¼°äº†å¤šç±»æ¨¡å‹ï¼Œæµ‹è¯•ç»“æœè¡¨æ˜è¯¥åŸºå‡†å…·æœ‰æ˜¾è‘—æŒ‘æˆ˜æ€§ï¼Œåˆ†æç»“æœè¿›ä¸€æ­¥æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨ç†è§£ä¸æ¨ç†èƒ½åŠ›ä¸Šçš„å…³é”®å±€é™ã€‚æˆ‘ä»¬æœŸå¾…MMARèƒ½æ¨åŠ¨è¿™ä¸ªé‡è¦ä½†å°šæœªå……åˆ†æ¢ç´¢çš„ç ”ç©¶é¢†åŸŸçš„å‘å±•ã€‚",
                            "en": "We introduce MMAR, a new benchmark comprising 1,000 meticulously curated audio-question-answer triplets, designed to evaluate the deep reasoning capabilities of Audio-Language Models (ALMs) across massive multi-disciplinary tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmar'. Error: Path opencompass/mmar is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1740",
                    "name": "GPT-ImgEval",
                    "version": "1.0.0",
                    "description": "GPT-ImgEval, quantitatively and qualitatively diagnoses GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis.",
                    "url": "opencompass/opencompass_1740.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1740",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1740",
                        "name": "GPT-ImgEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/PicoTrex/GPT-ImgEval",
                        "paperLink": "https://arxiv.org/abs/2504.02782",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "71",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:16:12",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:16:12",
                        "createDate": "2025-04-11 16:58:50",
                        "desc": {
                            "cn": "GPT-ImgEvalï¼Œä»ä¸‰ä¸ªå…³é”®ç»´åº¦å¯¹GPT-4oçš„æ€§èƒ½è¿›è¡Œå®šé‡å’Œå®šæ€§è¯Šæ–­ï¼šï¼ˆ1ï¼‰ç”Ÿæˆè´¨é‡ï¼Œï¼ˆ2ï¼‰ç¼–è¾‘èƒ½åŠ›ï¼Œä»¥åŠï¼ˆ3ï¼‰åŸºäºä¸–ç•ŒçŸ¥è¯†çš„è¯­ä¹‰åˆæˆèƒ½åŠ›ã€‚",
                            "en": "GPT-ImgEval, quantitatively and qualitatively diagnoses GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gpt_imgeval'. Error: Path opencompass/gpt_imgeval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1755",
                    "name": "REAL",
                    "version": "1.0.0",
                    "description": "Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites",
                    "url": "opencompass/opencompass_1755.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1755",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1755",
                        "name": "REAL",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/agi-inc/agisdk",
                        "paperLink": "https://arxiv.org/abs/2504.11543",
                        "officialWebsiteLink": "https://www.realevals.xyz/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "71",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-21 12:34:45",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-21 12:34:45",
                        "createDate": "2025-04-21 12:31:05",
                        "desc": {
                            "cn": "åœ¨çœŸå®ç½‘ç«™çš„ç¡®å®šæ€§æ¨¡æ‹Ÿä¸Šå¯¹è‡ªä¸»ä»£ç†è¿›è¡ŒåŸºå‡†æµ‹è¯•",
                            "en": "Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/real'. Error: Path opencompass/real is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1623",
                    "name": "MJ-Bench",
                    "version": "1.0.0",
                    "description": "MJ-Bench incorporates a comprehensive preference dataset to evaluate multimodal judges in providing feedback for image generation models across four key perspectives: alignment, safety, image quality, and bias. ",
                    "url": "opencompass/opencompass_1623.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1623",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1623",
                        "name": "MJ-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MJ-Bench/MJ-Bench",
                        "paperLink": "https://arxiv.org/abs/2407.04842",
                        "officialWebsiteLink": "https://mj-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "69",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-12 11:07:50",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-12 11:07:50",
                        "createDate": "2025-03-11 15:46:38",
                        "desc": {
                            "cn": "MJ-Benchï¼Œå®ƒåŒ…å«äº†ä¸€ä¸ªç»¼åˆçš„åå¥½æ•°æ®é›†ï¼Œç”¨äºä»å››ä¸ªå…³é”®è§’åº¦è¯„ä¼°å¤šæ¨¡æ€è¯„å§”åœ¨ä¸ºå›¾åƒç”Ÿæˆæ¨¡å‹æä¾›åé¦ˆæ–¹é¢çš„èƒ½åŠ›ï¼šå¯¹é½ã€å®‰å…¨æ€§ã€å›¾åƒè´¨é‡å’Œåè§ã€‚",
                            "en": "MJ-Bench incorporates a comprehensive preference dataset to evaluate multimodal judges in providing feedback for image generation models across four key perspectives: alignment, safety, image quality, and bias. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mj_bench'. Error: Path opencompass/mj_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1668",
                    "name": "TimeTravel",
                    "version": "1.0.0",
                    "description": "TimeTravel Taxonomy maps artifacts from 10 civilizations, 266 cultures, and 10k+ verified samples for AI-driven historical analysis.",
                    "url": "opencompass/opencompass_1668.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1668",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1668",
                        "name": "TimeTravel",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/mbzuai-oryx/TimeTravel",
                        "paperLink": "https://arxiv.org/abs/2502.14865",
                        "officialWebsiteLink": "https://mbzuai-oryx.github.io/TimeTravel/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "69",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-24 18:48:21",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-24 18:48:21",
                        "createDate": "2025-03-24 17:05:23",
                        "desc": {
                            "cn": "æ—¶é—´æ—…è¡Œåˆ†ç±»å°†æ¥è‡ª 10 ä¸ªæ–‡æ˜ã€266 ä¸ªæ–‡åŒ–ä»¥åŠ 10k+ä¸ªéªŒè¯æ ·æœ¬çš„æ–‡ç‰©æ˜ å°„ï¼Œç”¨äº AI é©±åŠ¨çš„å†å²åˆ†æã€‚",
                            "en": "TimeTravel Taxonomy maps artifacts from 10 civilizations, 266 cultures, and 10k+ verified samples for AI-driven historical analysis."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/timetravel'. Error: Path opencompass/timetravel is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1801",
                    "name": "OmniGIRL",
                    "version": "1.0.0",
                    "description": "A multi-modal, multi-language benchmark dataset for the GitHub Issue Resolution task.",
                    "url": "opencompass/opencompass_1801.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1801",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Code",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1801",
                        "name": "OmniGIRL",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Github Issue Resolution",
                                "en": "Github Issue Resolution"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/DeepSoftwareAnalytics/OmniGIRL",
                        "paperLink": "https://arxiv.org/abs/2505.04606",
                        "officialWebsiteLink": "https://deepsoftwareanalytics.github.io/omnigirl_leaderboard.html",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "26208047",
                            "name": null,
                            "avatar": null,
                            "nickname": "gnohgnailoug"
                        },
                        "lookNum": "68",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-05-09 21:26:49",
                        "supportOnlineEval": false,
                        "updateDate": "2025-05-09 21:26:49",
                        "createDate": "2025-05-09 20:52:15",
                        "desc": {
                            "cn": "ä¸€ä¸ªé¢å‘ GitHub Issue ResoLutionä»»åŠ¡çš„å¤šè¯­è¨€ã€å¤šæ¨¡æ€åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«ä»¥ä¸‹ç‰¹ç‚¹: 1.æ”¯æŒ Pythonã€Javaã€JSã€TS å››ç§ä¸»æµç¼–ç¨‹è¯­è¨€ï¼Œ2. è¾“å…¥ä¿¡æ¯æ¶µç›–æ–‡æœ¬ã€å›¾åƒã€ç½‘é¡µç­‰å¤šç§æ¨¡æ€ï¼Œ3. æä¾›å¯å¤ç°çš„ Docker è¯„ä¼°ç¯å¢ƒã€‚",
                            "en": "A multi-modal, multi-language benchmark dataset for the GitHub Issue Resolution task."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/omnigirl'. Error: Path opencompass/omnigirl is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1832",
                    "name": "MedBrowseComp",
                    "version": "1.0.0",
                    "description": "the first benchmark that systematically tests an agentâ€™s ability to reliably retrieve and synthesize multi-hop medical facts from live, domain-specific knowledge bases. MedBrowseComp holds 1,000+ human-curated questions that mirror clinical scenarios\nï»¿",
                    "url": "opencompass/opencompass_1832.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1832",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1832",
                        "name": "MedBrowseComp",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Medical",
                                "en": "Medical"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/shan23chen/MedBrowseComp",
                        "paperLink": "https://arxiv.org/pdf/2505.14963",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "68",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:23",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:23",
                        "createDate": "2025-06-04 11:17:23",
                        "desc": {
                            "cn": "the first benchmark that systematically tests an agentâ€™s ability to reliably retrieve and synthesize multi-hop medical facts from live, domain-specific knowledge bases. MedBrowseComp holds 1,000+ human-curated questions that mirror clinical scenarios\nï»¿",
                            "en": "the first benchmark that systematically tests an agentâ€™s ability to reliably retrieve and synthesize multi-hop medical facts from live, domain-specific knowledge bases. MedBrowseComp holds 1,000+ human-curated questions that mirror clinical scenarios\nï»¿"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medbrowsecomp'. Error: Path opencompass/medbrowsecomp is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1667",
                    "name": "MicroVQA",
                    "version": "1.0.0",
                    "description": "MicroVQA is a benchmark that evaluates LLM reasoning on multiple-choice questions about microscopy images, created by expert biologists.",
                    "url": "opencompass/opencompass_1667.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1667",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1667",
                        "name": "MicroVQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/jmhb0/microvqa",
                        "paperLink": "https://arxiv.org/abs/2503.13399",
                        "officialWebsiteLink": "https://jmhb0.github.io/microvqa/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "68",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-24 18:48:38",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-24 18:48:38",
                        "createDate": "2025-03-24 16:37:21",
                        "desc": {
                            "cn": "MicroVQAï¼Œä¸€ä¸ªè¯„ä¼°å…³äºæ˜¾å¾®é•œå›¾åƒçš„å¤šé€‰é¢˜æ¨ç†åŸºå‡†ï¼Œç”±ä¸“å®¶ç”Ÿç‰©å­¦å®¶åˆ›å»ºï¼Œæ—¨åœ¨åæ˜ ç”Ÿç‰©ç ”ç©¶ä¸­èƒ½å¤Ÿæœ‰æ„ä¹‰åœ°ååŠ©çš„ä»»åŠ¡ï¼Œæ¯ä¸ªé—®é¢˜éƒ½éœ€è¦å¤šæ¨¡æ€æ¨ç†ã€‚",
                            "en": "MicroVQA is a benchmark that evaluates LLM reasoning on multiple-choice questions about microscopy images, created by expert biologists."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/microvqa'. Error: Path opencompass/microvqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1675",
                    "name": "PokerBench",
                    "version": "1.0.0",
                    "description": "PokerBench contains natural language game scenarios and optimal decisions computed by solvers in No Limit Texas Holdâ€™em. It is divided into pre-flop and post-flop datasets, each with training and test splits. ",
                    "url": "opencompass/opencompass_1675.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1675",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1675",
                        "name": "PokerBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/pokerllm/pokerbench",
                        "paperLink": "https://arxiv.org/abs/2501.08328",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "68",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-25 10:42:49",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-25 10:42:49",
                        "createDate": "2025-03-25 10:41:14",
                        "desc": {
                            "cn": "PokerBenchåŒ…å«è‡ªç„¶è¯­è¨€æ¸¸æˆåœºæ™¯å’Œç”±æ±‚è§£å™¨åœ¨æ— é™åˆ¶å¾·å·æ‰‘å…‹ä¸­è®¡ç®—å‡ºçš„æœ€ä¼˜å†³ç­–ã€‚å®ƒåˆ†ä¸ºå‰æ³¨å’Œåæ³¨æ•°æ®é›†ï¼Œæ¯ä¸ªæ•°æ®é›†éƒ½åŒ…å«è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚",
                            "en": "PokerBench contains natural language game scenarios and optimal decisions computed by solvers in No Limit Texas Holdâ€™em. It is divided into pre-flop and post-flop datasets, each with training and test splits. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/pokerbench'. Error: Path opencompass/pokerbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1847",
                    "name": "Massive-STEPS",
                    "version": "1.0.0",
                    "description": "Massive-STEPS is a large-scale semantic trajectories dataset designed for understanding and predicting Point-of-Interest (POI) check-ins. ",
                    "url": "opencompass/opencompass_1847.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1847",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1847",
                        "name": "Massive-STEPS",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            },
                            {
                                "cn": "Embodied Decision Making",
                                "en": "Embodied Decision Making"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/cruiseresearchgroup/Massive-STEPS",
                        "paperLink": "https://arxiv.org/pdf/2505.11239",
                        "officialWebsiteLink": "https://huggingface.co/collections/CRUISEResearchGroup/massive-steps-point-of-interest-check-in-dataset-682716f625d74c2569bc7a73",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "67",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:44",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:44",
                        "createDate": "2025-06-04 11:15:18",
                        "desc": {
                            "cn": "Massive-STEPSæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„è¯­ä¹‰è½¨è¿¹æ•°æ®é›†ï¼Œæ—¨åœ¨ç†è§£å’Œé¢„æµ‹å…´è¶£ç‚¹ï¼ˆPOIï¼‰ç­¾åˆ°è¡Œä¸ºã€‚è¯¥æ•°æ®é›†åŸºäºSemantic Trailsæ•°æ®é›†æ„å»ºï¼Œè¦†ç›–12ä¸ªå…¨çƒä¸åŒåœ°åŒºçš„åŸå¸‚ï¼ŒåŒ…å«2012-2013å¹´å’Œ2017-2018å¹´çš„ç­¾åˆ°æ•°æ®ï¼Œæä¾›äº†æ›´ç°ä»£å’Œå¤šæ ·åŒ–çš„POIç­¾åˆ°ä¿¡æ¯ã€‚Massive-STEPSä¸ä»…ä¸°å¯Œäº†ç­¾åˆ°æ•°æ®çš„è¯­ä¹‰ä¿¡æ¯ï¼Œè¿˜é€šè¿‡ä¸Foursquare Open Source Placesæ•°æ®é›†å¯¹é½ï¼Œå¢åŠ äº†POIçš„åœ°ç†åæ ‡ã€åç§°å’Œåœ°å€ç­‰å…ƒæ•°æ®ã€‚",
                            "en": "Massive-STEPS is a large-scale semantic trajectories dataset designed for understanding and predicting Point-of-Interest (POI) check-ins. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/massive_steps'. Error: Path opencompass/massive_steps is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2075",
                    "name": "Arena-Hard-Auto",
                    "version": "1.0.0",
                    "description": "Arena-Hard-Auto is an automated benchmark for instruction-tuned LLMs, designed to efficiently approximate human preferences. ",
                    "url": "opencompass/opencompass_2075.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2075",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2075",
                        "name": "Arena-Hard-Auto",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/lmarena/arena-hard-auto",
                        "paperLink": "https://icml.cc/virtual/2025/poster/45630",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "å¼ å¯Œæ‰"
                        },
                        "lookNum": "66",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-23 14:49:02",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-23 14:49:02",
                        "createDate": "2025-07-23 14:48:52",
                        "desc": {
                            "cn": "Arena-Hard-Auto æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼° LLM çš„åŸºå‡†ï¼Œè‡ªåŠ¨ç”„é€‰ 500 æ¡é«˜éš¾åº¦å¼€æ”¾å¼æç¤ºï¼Œä»æ¨¡å‹åŒºåˆ†åº¦ã€äººç±»åå¥½ä¸€è‡´æ€§ä¸æç¤ºè´¨é‡ä¸‰ç»´åº¦è¿›è¡Œä¸¥è‹›è¯„æµ‹ã€‚ä¾æ‰˜ BenchBuilder ç®¡é“ã€ä¸»é¢˜å»ºæ¨¡ä¸ LLM è£åˆ¤ï¼Œå®ç°ä¼—åŒ…æ•°æ®â†’ç­›é€‰â†’è¯„åˆ†çš„å…¨è‡ªåŠ¨é—­ç¯ã€‚",
                            "en": "Arena-Hard-Auto is an automated benchmark for instruction-tuned LLMs, designed to efficiently approximate human preferences. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/arena_hard_auto'. Error: Path opencompass/arena_hard_auto is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1626",
                    "name": "CRAG",
                    "version": "1.0.0",
                    "description": "The Comprehensive RAG Benchmark (CRAG) is a rich and comprehensive factual question answering benchmark designed to advance research in RAG. Besides question-answer pairs, CRAG provides mock APIs to simulate web and knowledge graph search. ",
                    "url": "opencompass/opencompass_1626.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1626",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1626",
                        "name": "CRAG",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "RAG",
                                "en": "RAG"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/facebookresearch/CRAG",
                        "paperLink": "https://arxiv.org/abs/2406.04744",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "66",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-12 11:07:30",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-12 11:07:30",
                        "createDate": "2025-03-11 16:16:34",
                        "desc": {
                            "cn": "CRAGæ˜¯ä¸€ä¸ªä¸°å¯Œä¸”å…¨é¢çš„åŸºäºäº‹å®çš„é—®é¢˜å›ç­”åŸºå‡†ï¼Œæ—¨åœ¨æ¨è¿› RAG ç ”ç©¶ã€‚é™¤äº†é—®ç­”å¯¹ä¹‹å¤–ï¼ŒCRAG è¿˜æä¾›äº†æ¨¡æ‹Ÿç½‘é¡µå’ŒçŸ¥è¯†å›¾è°±æœç´¢çš„æ¨¡æ‹Ÿ APIã€‚",
                            "en": "The Comprehensive RAG Benchmark (CRAG) is a rich and comprehensive factual question answering benchmark designed to advance research in RAG. Besides question-answer pairs, CRAG provides mock APIs to simulate web and knowledge graph search. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/crag'. Error: Path opencompass/crag is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1738",
                    "name": "RUListening",
                    "version": "1.0.0",
                    "description": "RUListening: Robust Understanding through Listening, an automated QA generation framework for evaluating multimodal perception.",
                    "url": "opencompass/opencompass_1738.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1738",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1738",
                        "name": "RUListening",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2504.00369",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "66",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:15:12",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:15:12",
                        "createDate": "2025-04-11 16:07:18",
                        "desc": {
                            "cn": "RUListeningï¼šé€šè¿‡è†å¬çš„ç¨³å¥ç†è§£ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€æ„ŸçŸ¥çš„è‡ªåŠ¨é—®ç­”ç”Ÿæˆæ¡†æ¶ã€‚",
                            "en": "RUListening: Robust Understanding through Listening, an automated QA generation framework for evaluating multimodal perception."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/rulistening'. Error: Path opencompass/rulistening is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1915",
                    "name": "RewardBench",
                    "version": "1.0.0",
                    "description": "RewardBench is a benchmark designed to evaluate the capabilities and safety of reward models (including those trained with Direct Preference Optimization, DPO). ",
                    "url": "opencompass/opencompass_1915.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1915",
                    "sample_count": 1000,
                    "traits": [
                        "Instruct",
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1915",
                        "name": "RewardBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æŒ‡ä»¤è·Ÿéš",
                                "en": "Instruct"
                            },
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/allenai/reward-bench",
                        "paperLink": "https://arxiv.org/pdf/2506.01937",
                        "officialWebsiteLink": "https://huggingface.co/datasets/allenai/reward-bench-2",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "64",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-01 09:59:19",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-01 09:59:19",
                        "createDate": "2025-07-01 09:46:35",
                        "desc": {
                            "cn": "RewardBenchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¥–åŠ±æ¨¡å‹ï¼ˆåŒ…æ‹¬é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è®­ç»ƒçš„æ¨¡å‹ï¼‰èƒ½åŠ›å’Œå®‰å…¨æ€§çš„åŸºå‡†ã€‚",
                            "en": "RewardBench is a benchmark designed to evaluate the capabilities and safety of reward models (including those trained with Direct Preference Optimization, DPO). "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/rewardbench'. Error: Path opencompass/rewardbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1975",
                    "name": "CausalVQA",
                    "version": "1.0.0",
                    "description": "CausalVQA tests causal reasoning in videos across five question types, and state-of-the-art multimodal models still trail human performance.",
                    "url": "opencompass/opencompass_1975.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1975",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1975",
                        "name": "CausalVQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/facebookresearch/CausalVQA",
                        "paperLink": "https://arxiv.org/abs/2506.09943",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "64",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-25 15:30:44",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-25 15:30:44",
                        "createDate": "2025-06-25 10:25:30",
                        "desc": {
                            "cn": "CausalVQA æ˜¯é¢å‘è§†é¢‘é—®ç­”çš„å› æœæ¨ç†åŸºå‡†ï¼Œæ¶µç›–åäº‹å®ã€å‡è®¾ã€é¢„åˆ¤ã€è§„åˆ’ã€æè¿°äº”ç±»é—®é¢˜ï¼Œå¼ºè°ƒçœŸå®ç‰©ç†åœºæ™¯ã€‚",
                            "en": "CausalVQA tests causal reasoning in videos across five question types, and state-of-the-art multimodal models still trail human performance."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/causalvqa'. Error: Path opencompass/causalvqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1737",
                    "name": "FortisAVQA",
                    "version": "1.0.0",
                    "description": " FortisAVQA is the first dataset designed to assess the robustness of AVQA models. ",
                    "url": "opencompass/opencompass_1737.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1737",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1737",
                        "name": "FortisAVQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "AVQA",
                                "en": "AVQA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/reml-group/fortisavqa",
                        "paperLink": "https://arxiv.org/abs/2504.00487",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "64",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:14:21",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:14:21",
                        "createDate": "2025-04-11 15:55:15",
                        "desc": {
                            "cn": "FortisAVQAï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºè¯„ä¼°AVQAæ¨¡å‹é²æ£’æ€§çš„æ•°æ®é›†ã€‚",
                            "en": " FortisAVQA is the first dataset designed to assess the robustness of AVQA models. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/fortisavqa'. Error: Path opencompass/fortisavqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1781",
                    "name": "MIEB",
                    "version": "1.0.0",
                    "description": "Massive Image Embedding Benchmark (MIEB) is to evaluate the performance of image and image-text embedding models across the broadest spectrum to date. ",
                    "url": "opencompass/opencompass_1781.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1781",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1781",
                        "name": "MIEB",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/embeddings-benchmark/mteb",
                        "paperLink": "https://arxiv.org/abs/2504.10471",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "64",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-25 17:00:52",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-25 17:00:52",
                        "createDate": "2025-04-25 16:52:48",
                        "desc": {
                            "cn": "MIEBç”¨äºè¯„ä¼°å›¾åƒå’Œå›¾åƒæ–‡æœ¬åµŒå…¥æ¨¡å‹åœ¨è¿„ä»Šä¸ºæ­¢æœ€å¹¿æ³›çš„èŒƒå›´å†…çš„æ€§èƒ½ã€‚",
                            "en": "Massive Image Embedding Benchmark (MIEB) is to evaluate the performance of image and image-text embedding models across the broadest spectrum to date. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mieb'. Error: Path opencompass/mieb is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2141",
                    "name": "LEXam",
                    "version": "1.0.0",
                    "description": "LEXam is a novel benchmark derived from 340 law exams spanning 116 law school courses across a range of subjects and degree levels. The dataset comprises 4,886 law exam questions in English and German, including 2,841 long-form, open-ended questions and 2,045 multiple-choice questions.",
                    "url": "opencompass/opencompass_2141.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2141",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Examination",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2141",
                        "name": "LEXam",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "å­¦ç§‘",
                                "en": "Examination"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "é•¿æ–‡æœ¬",
                                "en": "é•¿æ–‡æœ¬"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "æ¨ç†"
                            },
                            {
                                "cn": "æ³•å¾‹",
                                "en": "æ³•å¾‹"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/LEXam-Benchmark/LEXam",
                        "paperLink": "https://arxiv.org/abs/2505.12864",
                        "officialWebsiteLink": "https://lexam-benchmark.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "91903579",
                            "name": "LEXam",
                            "avatar": null,
                            "nickname": "LEXam"
                        },
                        "lookNum": "62",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-10-15 16:28:11",
                        "supportOnlineEval": false,
                        "updateDate": "2025-10-15 16:28:11",
                        "createDate": "2025-10-13 02:40:42",
                        "desc": {
                            "cn": "LEXamæ˜¯ä¸€é¡¹å…¨æ–°çš„åŸºå‡†ï¼Œæ¥æºäº340ä»½æ³•å­¦é™¢è€ƒè¯•ï¼Œæ¶µç›–äº†116é—¨ä¸åŒå­¦ç§‘å’Œå­¦ä½å±‚çº§çš„è¯¾ç¨‹ã€‚è¯¥æ•°æ®é›†åŒ…å«4,886é“è‹±å¾·åŒè¯­çš„æ³•å­¦è€ƒè¯•é¢˜ç›®ï¼Œå…¶ä¸­åŒ…æ‹¬2,841é“é•¿ç¯‡å¼€æ”¾æ€§é—®é¢˜å’Œ2,045é“é€‰æ‹©é¢˜ã€‚é™¤å‚è€ƒç­”æ¡ˆå¤–ï¼Œå¼€æ”¾æ€§é—®é¢˜è¿˜é…æœ‰æ˜ç¡®çš„æŒ‡å¯¼ï¼Œè¯´æ˜äº†é¢„æœŸçš„æ³•å¾‹æ¨ç†æ–¹å¼ï¼Œä¾‹å¦‚é—®é¢˜è¯†åˆ«ã€è§„åˆ™å›å¿†æˆ–è§„åˆ™é€‚ç”¨ã€‚æˆ‘ä»¬åœ¨å¼€æ”¾æ€§é—®é¢˜å’Œé€‰æ‹©é¢˜ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œå½“å‰çš„LLMåœ¨è¿™ä¸€ä»»åŠ¡ä¸Šé¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç»“æ„åŒ–ã€å¤šæ­¥éª¤æ³•å¾‹æ¨ç†çš„å¼€æ”¾æ€§é—®é¢˜ä¸Šè¡¨ç°ä¸ä½³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†è¯¥æ•°æ®é›†åœ¨åŒºåˆ†ä¸åŒèƒ½åŠ›æ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
                            "en": "LEXam is a novel benchmark derived from 340 law exams spanning 116 law school courses across a range of subjects and degree levels. The dataset comprises 4,886 law exam questions in English and German, including 2,841 long-form, open-ended questions and 2,045 multiple-choice questions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/lexam'. Error: Path opencompass/lexam is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1917",
                    "name": "Orak",
                    "version": "1.0.0",
                    "description": "Orak (ì˜¤ë½) is a foundational benchmark for evaluating Large Language Model (LLM) agents in diverse popular video games. ",
                    "url": "opencompass/opencompass_1917.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1917",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1917",
                        "name": "Orak",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/krafton-ai/Orak",
                        "paperLink": "https://arxiv.org/abs/2506.03610",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "62",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-16 09:52:11",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-16 09:52:11",
                        "createDate": "2025-06-10 14:48:51",
                        "desc": {
                            "cn": "Orakæ˜¯ä¸€ä¸ªåŸºç¡€æ€§çš„åŸºå‡†,ç”¨äºè¯„ä¼°åœ¨å„ç§æµè¡Œè§†é¢‘æ¸¸æˆä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹(LLM)ä»£ç†ã€‚",
                            "en": "Orak (ì˜¤ë½) is a foundational benchmark for evaluating Large Language Model (LLM) agents in diverse popular video games. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/orak'. Error: Path opencompass/orak is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1704",
                    "name": "Mono2Stereo",
                    "version": "1.0.0",
                    "description": "For evaluating stereo image conversion, it provides test data for five different scenarios: animation, indoor, outdoor, complex, and simple, with a total of approximately 2,500 test sample pairs. It also offers evaluation metrics for assessing the stereo effect.",
                    "url": "opencompass/opencompass_1704.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1704",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1704",
                        "name": "Mono2Stereo",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Stereo Conversion",
                                "en": "Stereo Conversion"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/song2yu/Mono2Stereo",
                        "paperLink": "https://arxiv.org/abs/2503.22262",
                        "officialWebsiteLink": "https://mono2stereo-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "52300297",
                            "name": null,
                            "avatar": null,
                            "nickname": "è¿·è—"
                        },
                        "lookNum": "62",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-07 10:30:36",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-07 10:30:36",
                        "createDate": "2025-04-02 17:37:32",
                        "desc": {
                            "cn": "ç”¨äºæµ‹è¯„ç«‹ä½“å½±åƒè½¬æ¢ï¼Œæä¾›äº†åŠ¨ç”»ï¼Œå®¤å†…ï¼Œå®¤å¤–ï¼Œå¤æ‚ï¼Œç®€å•å…±äº”ç§åœºæ™¯çš„æµ‹è¯•æ•°æ®ï¼Œæ€»å…±çº¦2500å¯¹æµ‹è¯•æ ·æœ¬ã€‚å¹¶æä¾›äº†ç”¨äºæµ‹è¯„ç«‹ä½“æ•ˆæœçš„è¯„ä»·æŒ‡æ ‡-ç«‹ä½“äº¤å¹¶æ¯”ã€‚",
                            "en": "For evaluating stereo image conversion, it provides test data for five different scenarios: animation, indoor, outdoor, complex, and simple, with a total of approximately 2,500 test sample pairs. It also offers evaluation metrics for assessing the stereo effect."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mono2stereo'. Error: Path opencompass/mono2stereo is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1844",
                    "name": "MIRACL-VISION",
                    "version": "1.0.0",
                    "description": "MIRACL-VISION is a large-scale, multilingual visual document retrieval benchmark built by the NVIDIA team, extending the popular MIRACL multilingual text retrieval benchmark. It covers 18 languages and contains 211 original questions.",
                    "url": "opencompass/opencompass_1844.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1844",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1844",
                        "name": "MIRACL-VISION",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            },
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "Strong Reasoning",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/pdf/2505.11651",
                        "officialWebsiteLink": "https://huggingface.co/datasets/nvidia/miracl-vision",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "61",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-03 10:50:34",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-03 10:50:34",
                        "createDate": "2025-05-27 10:50:53",
                        "desc": {
                            "cn": "MIRACL-VISIONæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šè¯­è¨€è§†è§‰æ–‡æ¡£æ£€ç´¢åŸºå‡†æµ‹è¯•ï¼Œç”±NVIDIAå›¢é˜Ÿæ„å»ºï¼Œæ‰©å±•äº†æµè¡Œçš„MIRACLå¤šè¯­è¨€æ–‡æœ¬æ£€ç´¢åŸºå‡†ã€‚è¯¥æ•°æ®é›†è¦†ç›–18ç§è¯­è¨€ï¼ŒåŒ…å«211ä¸ªåŸåˆ›é—®é¢˜ï¼Œæ¶µç›–è¾¹ç•Œå±‚åˆ†æã€WKBæ–¹æ³•ã€éçº¿æ€§åå¾®åˆ†æ–¹ç¨‹çš„æ¸è¿‘è§£å’ŒæŒ¯è¡ç§¯åˆ†çš„æ¸è¿‘æ€§ç­‰æ ¸å¿ƒä¸»é¢˜ã€‚",
                            "en": "MIRACL-VISION is a large-scale, multilingual visual document retrieval benchmark built by the NVIDIA team, extending the popular MIRACL multilingual text retrieval benchmark. It covers 18 languages and contains 211 original questions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/miracl_vision'. Error: Path opencompass/miracl_vision is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1682",
                    "name": "MotionBench",
                    "version": "1.0.0",
                    "description": "MotionBench, is a comprehensive evaluation benchmark designed to assess the fine-grained motion comprehension of video understanding models.",
                    "url": "opencompass/opencompass_1682.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1682",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1682",
                        "name": "MotionBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/THUDM/MotionBench",
                        "paperLink": "https://arxiv.org/abs/2501.02955",
                        "officialWebsiteLink": "https://motion-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "61",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-26 15:56:14",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-26 15:56:14",
                        "createDate": "2025-03-26 15:35:43",
                        "desc": {
                            "cn": "MotionBenchæ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„è¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°è§†é¢‘ç†è§£æ¨¡å‹çš„ç»†ç²’åº¦è¿åŠ¨ç†è§£èƒ½åŠ›ã€‚",
                            "en": "MotionBench, is a comprehensive evaluation benchmark designed to assess the fine-grained motion comprehension of video understanding models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/motionbench'. Error: Path opencompass/motionbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1706",
                    "name": "KOFFVQA",
                    "version": "1.0.0",
                    "description": "KOFFVQA is a carefully crafted free-form visual question answering(VQA) benchmark in the Korean language consisting of 275 questions across 10 different tasks. ",
                    "url": "opencompass/opencompass_1706.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1706",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1706",
                        "name": "KOFFVQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/maum-ai/KOFFVQA",
                        "paperLink": "https://arxiv.org/abs/2503.23730",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "60",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-03 19:47:01",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-03 19:47:01",
                        "createDate": "2025-04-03 14:31:30",
                        "desc": {
                            "cn": "KOFFVQAæ˜¯ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„éŸ©è¯­è‡ªç”±å½¢å¼è§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«10ä¸ªä¸åŒä»»åŠ¡ä¸­çš„275ä¸ªé—®é¢˜ã€‚",
                            "en": "KOFFVQA is a carefully crafted free-form visual question answering(VQA) benchmark in the Korean language consisting of 275 questions across 10 different tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/koffvqa'. Error: Path opencompass/koffvqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1986",
                    "name": "SEC-bench",
                    "version": "1.0.0",
                    "description": "SEC-bench is a benchmark designed to evaluate large language model (LLM) agents on real-world software security tasks.",
                    "url": "opencompass/opencompass_1986.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1986",
                    "sample_count": 1000,
                    "traits": [
                        "Safety",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1986",
                        "name": "SEC-bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SEC-bench/SEC-bench",
                        "paperLink": "https://arxiv.org/abs/2506.11791",
                        "officialWebsiteLink": "https://huggingface.co/datasets/SEC-bench/SEC-bench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "60",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-27 18:13:24",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-27 18:13:24",
                        "createDate": "2025-06-26 14:35:04",
                        "desc": {
                            "cn": "SEC-bench æ˜¯ä¸€ä¸ªé¢å‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™ºèƒ½ä½“çš„è½¯ä»¶å®‰å…¨ä»»åŠ¡è¯„æµ‹åŸºå‡†ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–è¯„ä¼°æ¨¡å‹åœ¨çœŸå®æ¼æ´ç¯å¢ƒä¸­çš„èƒ½åŠ›ã€‚ è¯¥åŸºå‡†é€šè¿‡å¤šæ™ºèƒ½ä½“æ¡†æ¶è‡ªåŠ¨æ„å»ºä»£ç ä»“åº“ã€å¤ç°æ¼æ´å¹¶ç”Ÿæˆä¿®å¤è¡¥ä¸ï¼Œæ¶µç›–æ¼æ´éªŒè¯ï¼ˆPoC ç”Ÿæˆï¼‰å’Œè¡¥ä¸ä¿®å¤ä¸¤ä¸ªå…³é”®ä»»åŠ¡ï¼ŒåŒ…å«æ•°ç™¾ä¸ªçœŸå®å®‰å…¨æ¡ˆä¾‹ã€‚",
                            "en": "SEC-bench is a benchmark designed to evaluate large language model (LLM) agents on real-world software security tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/sec_bench'. Error: Path opencompass/sec_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1732",
                    "name": "SCAM",
                    "version": "1.0.0",
                    "description": "SCAM is the largest and most diverse dataset of real-world typographic attack images to date, containing 1,162 images across hundreds of object categories and attack words.",
                    "url": "opencompass/opencompass_1732.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1732",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1732",
                        "name": "SCAM",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Bliss-e-V/SCAM",
                        "paperLink": "https://arxiv.org/abs/2504.04893",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "60",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:12:53",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:12:53",
                        "createDate": "2025-04-11 14:29:41",
                        "desc": {
                            "cn": "SCAMï¼Œæ˜¯è¿„ä»Šä¸ºæ­¢è§„æ¨¡æœ€å¤§ã€å¤šæ ·æ€§æœ€ä¸°å¯Œçš„çœŸå®ä¸–ç•Œæ’ç‰ˆæ”»å‡»å›¾åƒæ•°æ®é›†ï¼ŒåŒ…å«æ•°ç™¾ä¸ªå¯¹è±¡ç±»åˆ«å’Œæ”»å‡»è¯æ±‡çš„1,162å¼ å›¾åƒã€‚",
                            "en": "SCAM is the largest and most diverse dataset of real-world typographic attack images to date, containing 1,162 images across hundreds of object categories and attack words."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/scam'. Error: Path opencompass/scam is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1907",
                    "name": "CFinBench",
                    "version": "1.0.0",
                    "description": "We present CFinBench: a meticulously crafted, the most comprehensive evaluation benchmark to date, for assessing the financial knowledge of LLMs under Chinese context. ",
                    "url": "opencompass/opencompass_1907.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1907",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1907",
                        "name": "CFinBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "NAACL 2025",
                                "en": "NAACL 2025"
                            },
                            {
                                "cn": "é‡‘è",
                                "en": "é‡‘è"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://cfinbench.github.io/",
                        "paperLink": "https://aclanthology.org/2025.naacl-long.40.pdf",
                        "officialWebsiteLink": "https://cfinbench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "52307116",
                            "name": null,
                            "avatar": null,
                            "nickname": "Bball"
                        },
                        "lookNum": "59",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-09 11:30:13",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-09 11:30:13",
                        "createDate": "2025-06-06 17:13:27",
                        "desc": {
                            "cn": "ä¸ºäº†æ›´åŠ å…¨é¢åœ°æ¢ç©¶å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸­æ–‡è´¢ç»é¢†åŸŸçš„èƒ½åŠ›ï¼Œæœ¬å·¥ä½œæå‡ºäº†ç›®å‰ä¸ºæ­¢é‡çº§æœ€å¤§çš„ä¸­æ–‡è´¢ç»è¯„æµ‹åŸºå‡†ï¼ˆCFinBenchï¼‰ã€‚è¯¥æ•°æ®é›†å…±åŒ…å«99,100ä¸ªè¯„æµ‹æ ·æœ¬ï¼Œå¹¶åŒ…å«å•é€‰é¢˜ã€å¤šé€‰é¢˜å’Œåˆ¤æ–­é¢˜åœ¨å†…çš„ä¸‰ç§é¢˜å‹ã€‚è¯¥å·¥ä½œå¯¹å½“å‰ä¸»æµçš„å¤§æ¨¡å‹ä»å››ä¸ªç»´åº¦è¿›è¡Œäº†è¯¦ç»†è¯„æµ‹ï¼šè´¢ç»å­¦ç§‘åŸºç¡€ã€è´¢ç»èµ„æ ¼è®¤è¯ã€è´¢ç»ä»ä¸šå®è·µã€è´¢ç»æ³•å¾‹æ³•è§„ã€‚æ•°æ®é›†å’Œæµ‹è¯„ä»£ç å‡å·²å¼€æºã€‚",
                            "en": "We present CFinBench: a meticulously crafted, the most comprehensive evaluation benchmark to date, for assessing the financial knowledge of LLMs under Chinese context. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cfinbench'. Error: Path opencompass/cfinbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1707",
                    "name": "RXRX3-CORE",
                    "version": "1.0.0",
                    "description": "RxRx3-core dataset is a challenge dataset in phenomics optimized for the research\ncommunity. ",
                    "url": "opencompass/opencompass_1707.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1707",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1707",
                        "name": "RXRX3-CORE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Xavi3398/fer_benchmark",
                        "paperLink": "https://arxiv.org/abs/2503.20428",
                        "officialWebsiteLink": "https://www.rxrx.ai/rxrx3-core",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "59",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-03 19:47:14",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-03 19:47:14",
                        "createDate": "2025-04-03 15:13:31",
                        "desc": {
                            "cn": "RxRx3-coreæ•°æ®é›†æ˜¯Recursionä¸ºç ”ç©¶ç¤¾åŒºä¼˜åŒ–çš„è¡¨å‹ç»„å­¦æŒ‘æˆ˜æ•°æ®é›†ã€‚",
                            "en": "RxRx3-core dataset is a challenge dataset in phenomics optimized for the research\ncommunity. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/rxrx3_core'. Error: Path opencompass/rxrx3_core is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1734",
                    "name": "Thai_local_benchmark",
                    "version": "1.0.0",
                    "description": "Thai local dialect benchmark covers Northern (Lanna), Northeastern (Isan), and Southern (Dambro) Thai, evaluating LLMs on five NLP tasks: summarization, question answering, translation, conversation, and food-related tasks.",
                    "url": "opencompass/opencompass_1734.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1734",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1734",
                        "name": "Thai_local_benchmark",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/mrpeerat/Thai_local_benchmark",
                        "paperLink": "https://arxiv.org/abs/2504.05898",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "59",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:13:26",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:13:26",
                        "createDate": "2025-04-11 14:57:01",
                        "desc": {
                            "cn": "è¿™æ˜¯ä¸€ä¸ªæ¶µç›–æ³°å›½åŒ—éƒ¨ï¼ˆå…°çº³ï¼‰ã€ä¸œåŒ—éƒ¨ï¼ˆä¼Šæ£®ï¼‰å’Œå—éƒ¨ï¼ˆä¸¹å¸ƒç½—ï¼‰æ–¹è¨€çš„æ³°å›½åœ°æ–¹æ–¹è¨€åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨äº”é¡¹è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼šæ€»ç»“ã€é—®ç­”ã€ç¿»è¯‘ã€å¯¹è¯ä»¥åŠä¸é£Ÿç‰©ç›¸å…³çš„ä»»åŠ¡ã€‚",
                            "en": "Thai local dialect benchmark covers Northern (Lanna), Northeastern (Isan), and Southern (Dambro) Thai, evaluating LLMs on five NLP tasks: summarization, question answering, translation, conversation, and food-related tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/thai_local_benchmark'. Error: Path opencompass/thai_local_benchmark is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1787",
                    "name": "LiveLongBench",
                    "version": "1.0.0",
                    "description": "LiveLongBench is the first spoken long-text benchmark designed to address the challenges of long-context understanding in real-world dialogues, characterized by speech-specific features, high redundancy, and uneven information density. Existing benchmarks fail to capture these complexities, limiting",
                    "url": "opencompass/opencompass_1787.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1787",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1787",
                        "name": "LiveLongBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Spoken text",
                                "en": "Spoken text"
                            },
                            {
                                "cn": "Long context",
                                "en": "Long context"
                            },
                            {
                                "cn": "Live streams",
                                "en": "Live streams"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Yarayx/livelongbench",
                        "paperLink": "https://arxiv.org/abs/2504.17366",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "10207810",
                            "name": null,
                            "avatar": null,
                            "nickname": "Yarayx"
                        },
                        "lookNum": "59",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-05-06 11:02:03",
                        "supportOnlineEval": false,
                        "updateDate": "2025-05-06 11:02:03",
                        "createDate": "2025-04-28 16:54:00",
                        "desc": {
                            "cn": "LiveLongBench æ˜¯é¦–ä¸ªé¢å‘å£è¯­é•¿æ–‡æœ¬ç†è§£çš„åŸºå‡†æµ‹è¯•ï¼ŒåŸºäºç›´æ’­å†…å®¹æ„å»ºï¼Œæ¶µç›–æ£€ç´¢ç±»ã€æ¨ç†ç±»åŠæ··åˆç±»ä¸‰ç§ä»»åŠ¡ç±»å‹ï¼Œé’ˆå¯¹ç°å®å¯¹è¯ä¸­å­˜åœ¨çš„è¯­éŸ³ç‰¹æ€§ã€é«˜å†—ä½™æ€§å’Œä¿¡æ¯å¯†åº¦ä¸å‡ç­‰æŒ‘æˆ˜ã€‚",
                            "en": "LiveLongBench is the first spoken long-text benchmark designed to address the challenges of long-context understanding in real-world dialogues, characterized by speech-specific features, high redundancy, and uneven information density. Existing benchmarks fail to capture these complexities, limiting"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/livelongbench'. Error: Path opencompass/livelongbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1918",
                    "name": "RDB2G-Bench",
                    "version": "1.0.0",
                    "description": "RDB2G-Bench provides comprehensive performance evaluation data for graph neural network models applied to relational database tasks. The dataset contains extensive experiments across multiple graph configurations and architectures.",
                    "url": "opencompass/opencompass_1918.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1918",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1918",
                        "name": "RDB2G-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/chlehdwon/RDB2G-Bench?tab=readme-ov-file",
                        "paperLink": "https://arxiv.org/abs/2506.01360",
                        "officialWebsiteLink": "https://huggingface.co/datasets/kaistdata/RDB2G-Bench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "58",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-30 15:32:24",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-30 15:32:24",
                        "createDate": "2025-06-30 15:06:09",
                        "desc": {
                            "cn": "RDB2G-Benchæä¾›äº†é’ˆå¯¹å…³ç³»æ•°æ®åº“ä»»åŠ¡åº”ç”¨çš„å›¾ç¥ç»ç½‘ç»œæ¨¡å‹çš„å…¨é¢æ€§èƒ½è¯„ä¼°æ•°æ®ã€‚è¯¥æ•°æ®é›†åŒ…å«äº†è·¨å¤šç§å›¾é…ç½®å’Œæ¶æ„çš„å¹¿æ³›å®éªŒã€‚",
                            "en": "RDB2G-Bench provides comprehensive performance evaluation data for graph neural network models applied to relational database tasks. The dataset contains extensive experiments across multiple graph configurations and architectures."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/rdb2g_bench'. Error: Path opencompass/rdb2g_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1954",
                    "name": "ClimateViz",
                    "version": "1.0.0",
                    "description": "ClimateViz is a large-scale multimodal benchmark designed to evaluate the scientific fact-checking and statistical reasoning capabilities of large language and vision-language models. It focuses on real-world climate science data, with over 49,000 high quality natural language claims.",
                    "url": "opencompass/opencompass_1954.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1954",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1954",
                        "name": "ClimateViz",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal Reasoning ",
                                "en": "Multimodal Reasoning "
                            },
                            {
                                "cn": "Fact-Checking",
                                "en": "Fact-Checking"
                            },
                            {
                                "cn": "Charts",
                                "en": "Charts"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Albasu120491/ClimateViz",
                        "paperLink": "https://arxiv.org/abs/2506.08700",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "90307327",
                            "name": "Alba",
                            "avatar": null,
                            "nickname": "Alba"
                        },
                        "lookNum": "58",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-20 15:05:24",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-20 15:05:24",
                        "createDate": "2025-06-18 17:56:23",
                        "desc": {
                            "cn": "ClimateViz æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åŸºå‡†æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å¤§æ¨¡å‹åœ¨æ°”å€™ç§‘å­¦å›¾è¡¨ä¸Šçš„äº‹å®æ ¸æŸ¥ä¸ç»Ÿè®¡æ¨ç†èƒ½åŠ›ã€‚æ•°æ®æ¥æºäº NOAAã€è‹±å›½æ°”è±¡å±€ç­‰æƒå¨æœºæ„ï¼Œå…±åŒ…å«çº¦ 2,800 å¼ ç§‘å­¦å›¾è¡¨ä¸è¿‘ 5 ä¸‡æ¡ä¸»å¼ ï¼Œæ ‡æ³¨ä¸ºæ”¯æŒï¼ˆsupportï¼‰ã€åé©³ï¼ˆrefuteï¼‰æˆ–ä¿¡æ¯ä¸è¶³ï¼ˆNEIï¼‰ã€‚\n\nè¯¥æ•°æ®é›†æ”¯æŒä¸‰ç§è¾“å…¥æ ¼å¼ï¼šå›¾è¡¨+ä¸»å¼ ã€è¡¨æ ¼+ä¸»å¼ ã€å›¾è¡¨æ ‡é¢˜+è¡¨æ ¼+ä¸»å¼ ï¼Œæ¶µç›–è¶‹åŠ¿è¯†åˆ«ã€æ—¶ç©ºæ¨ç†ä¸ç§‘å­¦å¯¹æ¯”ç­‰æ ¸å¿ƒä»»åŠ¡ã€‚ClimateViz é€‚ç”¨äºå¤šæ¨¡æ€å¤§æ¨¡å‹å’Œè¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿæ€§è¯„ä¼°ã€‚\nå›¾è¡¨è½¬è¡¨æ ¼ + å›¾è¡¨æ ‡é¢˜ + ä¸»å¼ ï¼ˆCaption + Table",
                            "en": "ClimateViz is a large-scale multimodal benchmark designed to evaluate the scientific fact-checking and statistical reasoning capabilities of large language and vision-language models. It focuses on real-world climate science data, with over 49,000 high quality natural language claims."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/climateviz'. Error: Path opencompass/climateviz is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2078",
                    "name": "AutoAdvExBench",
                    "version": "1.0.0",
                    "description": "AutoAdvExBench is a benchmark designed to evaluate large language models' (LLMs) ability to autonomously exploit adversarial example defenses, directly measuring LLMs' success on tasks regularly performed by machine learning security experts.",
                    "url": "opencompass/opencompass_2078.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2078",
                    "sample_count": 1000,
                    "traits": [
                        "Safety",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2078",
                        "name": "AutoAdvExBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ethz-spylab/AutoAdvExBench",
                        "paperLink": "https://icml.cc/virtual/2025/poster/45896",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "57",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:10",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:10",
                        "createDate": "2025-07-22 16:51:34",
                        "desc": {
                            "cn": "AutoAdvExBench æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªä¸»åˆ©ç”¨å¯¹æŠ—æ€§æ ·æœ¬é˜²å¾¡èƒ½åŠ›çš„åŸºå‡†ï¼Œç›´æ¥è¡¡é‡LLMsåœ¨æœºå™¨å­¦ä¹ å®‰å…¨ä¸“å®¶ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡ã€‚å®ƒä¸»è¦è¯„ä¼°æ¨¡å‹ç†è§£å­¦æœ¯è®ºæ–‡ã€ä»£ç å®ç°åŠç”Ÿæˆå¯¹æŠ—æ€§æ”»å‡»çš„èƒ½åŠ›ã€‚æµ‹è¯•é›†åŒ…å«75ä¸ªå¯¹æŠ—æ€§æ ·æœ¬é˜²å¾¡å®ç°ã€‚",
                            "en": "AutoAdvExBench is a benchmark designed to evaluate large language models' (LLMs) ability to autonomously exploit adversarial example defenses, directly measuring LLMs' success on tasks regularly performed by machine learning security experts."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/autoadvexbench'. Error: Path opencompass/autoadvexbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1840",
                    "name": "IQBench",
                    "version": "1.0.0",
                    "description": "IQBench is a novel benchmark designed to evaluate the fluid intelligence of VisionLanguage Models (VLMs) using standardized visual IQ tests. It consists of 500 manually collected and annotated visual IQ questions covering various domains.",
                    "url": "opencompass/opencompass_1840.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1840",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1840",
                        "name": "IQBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            },
                            {
                                "cn": "Strong Reasoning",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://anonymous.4open.science/r/IQBench_anonymous-3515/README.md",
                        "paperLink": "https://arxiv.org/pdf/2505.12000",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "57",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:46",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:46",
                        "createDate": "2025-06-04 11:15:30",
                        "desc": {
                            "cn": "IQBenchæ˜¯ä¸€ä¸ªæ–°åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡æ ‡å‡†åŒ–è§†è§‰æ™ºå•†æµ‹è¯•è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æµä½“æ™ºåŠ›ã€‚è¯¥åŸºå‡†åŒ…å«500ä¸ªæ‰‹åŠ¨æ”¶é›†å’Œæ³¨é‡Šçš„è§†è§‰æ™ºå•†é—®é¢˜ï¼Œæ¶µç›–æ¨¡å¼è¯†åˆ«ã€ç±»æ¯”æ¨ç†ã€è§†è§‰ç®—æœ¯ã€ç©ºé—´ç†è§£ç­‰å¤šä¸ªé¢†åŸŸã€‚ä¸ä»¥å¾€ä»…å…³æ³¨æœ€ç»ˆç­”æ¡ˆå‡†ç¡®æ€§çš„åŸºå‡†ä¸åŒï¼ŒIQBenchå¼ºè°ƒå¯¹æ¨¡å‹æ¨ç†èƒ½åŠ›çš„è¯„ä¼°ï¼Œé‡‡ç”¨åŒé‡è¯„ä¼°æ¡†æ¶ï¼šå‡†ç¡®æ€§è¯„åˆ†å’Œæ¨ç†è¯„åˆ†ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯æ€§èƒ½æœ€é«˜çš„æ¨¡å‹ï¼ˆå¦‚o4miniã€gemini2.5flashå’Œclaude3.7sonnetï¼‰ï¼Œåœ¨3Dç©ºé—´å’Œå­—æ¯é‡ç»„ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å‡ºæ˜æ˜¾ä¸è¶³ï¼Œå‡¸æ˜¾äº†å½“å‰VLMsåœ¨é€šç”¨æ¨ç†èƒ½åŠ›ä¸Šçš„å±€é™æ€§ã€‚IQBenchä¸ºå¼€å‘æ›´é€æ˜ã€æ›´å…·è®¤çŸ¥èƒ½åŠ›çš„å¤šæ¨¡æ€ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
                            "en": "IQBench is a novel benchmark designed to evaluate the fluid intelligence of VisionLanguage Models (VLMs) using standardized visual IQ tests. It consists of 500 manually collected and annotated visual IQ questions covering various domains."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/iqbench'. Error: Path opencompass/iqbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1731",
                    "name": "DOVE",
                    "version": "1.0.0",
                    "description": "DOVE (Dataset Of Variation Evaluation) is a large-scale dataset containing prompt perturbations of various evaluation benchmarks.",
                    "url": "opencompass/opencompass_1731.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1731",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1731",
                        "name": "DOVE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "LLMæ•æ„Ÿæ€§è¯„ä¼°",
                                "en": "LLMæ•æ„Ÿæ€§è¯„ä¼°"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SLAB-NLP/DOVE",
                        "paperLink": "https://arxiv.org/abs/2503.01622",
                        "officialWebsiteLink": "https://slab-nlp.github.io/DOVE/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "57",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:12:41",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:12:41",
                        "createDate": "2025-04-11 14:22:36",
                        "desc": {
                            "cn": "DOVEï¼ˆå˜å¼‚è¯„ä¼°æ•°æ®é›†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«äº†å„ç§è¯„ä¼°åŸºå‡†çš„æç¤ºæ‰°åŠ¨ã€‚",
                            "en": "DOVE (Dataset Of Variation Evaluation) is a large-scale dataset containing prompt perturbations of various evaluation benchmarks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/dove'. Error: Path opencompass/dove is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1753",
                    "name": "AgMMU",
                    "version": "1.0.0",
                    "description": "A Comprehensive Agricultural Multimodal Understanding and Reasoning Benchmark",
                    "url": "opencompass/opencompass_1753.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1753",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1753",
                        "name": "AgMMU",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "å†œä¸š",
                                "en": "å†œä¸š"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AgMMU/AgMMU",
                        "paperLink": "https://arxiv.org/abs/2504.10568",
                        "officialWebsiteLink": "https://agmmu.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "57",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-21 12:34:40",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-21 12:34:40",
                        "createDate": "2025-04-21 12:12:24",
                        "desc": {
                            "cn": "å†œä¸šç»¼åˆå¤šæ¨¡æ€ç†è§£å’Œæ¨ç†åŸºå‡†ã€‚",
                            "en": "A Comprehensive Agricultural Multimodal Understanding and Reasoning Benchmark"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/agmmu'. Error: Path opencompass/agmmu is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1834",
                    "name": "CLEVER",
                    "version": "1.0.0",
                    "description": "CLEVER is a benchmark suite for end-to-end code generation and formal verification in Lean 4, adapted from the HumanEval dataset. It requires models to generate implementations, formal specifications, and proofsâ€”all verifiable by Lean's type checker, moving beyond test-case-driven evaluation.",
                    "url": "opencompass/opencompass_1834.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1834",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1834",
                        "name": "CLEVER",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/trishullab/clever",
                        "paperLink": "https://arxiv.org/abs/2505.13938",
                        "officialWebsiteLink": "https://huggingface.co/datasets/amitayusht/clever",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "56",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:57",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:57",
                        "createDate": "2025-06-04 11:17:08",
                        "desc": {
                            "cn": "CLEVER is a benchmark suite for end-to-end code generation and formal verification in Lean 4, adapted from the HumanEval dataset. It requires models to generate implementations, formal specifications, and proofsâ€”all verifiable by Lean's type checker, moving beyond test-case-driven evaluation.",
                            "en": "CLEVER is a benchmark suite for end-to-end code generation and formal verification in Lean 4, adapted from the HumanEval dataset. It requires models to generate implementations, formal specifications, and proofsâ€”all verifiable by Lean's type checker, moving beyond test-case-driven evaluation."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/clever'. Error: Path opencompass/clever is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2025",
                    "name": "TableEval",
                    "version": "1.0.0",
                    "description": "TableEval is the first cross-lingual benchmark for tabular question answering, supporting Simplified Chinese, Traditional Chinese, and English. It is designed to evaluate model performance on real-world, complex table understanding tasks across multiple languages.",
                    "url": "opencompass/opencompass_2025.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2025",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2025",
                        "name": "TableEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "è¡¨æ ¼é—®ç­”",
                                "en": "è¡¨æ ¼é—®ç­”"
                            },
                            {
                                "cn": "è·¨è¯­è¨€",
                                "en": "è·¨è¯­è¨€"
                            },
                            {
                                "cn": "çœŸå®ä¸–ç•Œæ•°æ®",
                                "en": "çœŸå®ä¸–ç•Œæ•°æ®"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/wenge-research/TableEval",
                        "paperLink": "https://arxiv.org/abs/2506.03949",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "085564",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-QmRrrAZeT"
                        },
                        "lookNum": "56",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-11 10:29:43",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-11 10:29:43",
                        "createDate": "2025-07-04 16:32:40",
                        "desc": {
                            "cn": "TableEval æ˜¯é¦–ä¸ªæ”¯æŒç®€ä½“ä¸­æ–‡ã€ç¹ä½“ä¸­æ–‡å’Œè‹±æ–‡çš„è·¨è¯­è¨€è¡¨æ ¼é—®ç­”åŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°å¤§æ¨¡å‹åœ¨çœŸå®å¤æ‚è¡¨æ ¼ç†è§£ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚",
                            "en": "TableEval is the first cross-lingual benchmark for tabular question answering, supporting Simplified Chinese, Traditional Chinese, and English. It is designed to evaluate model performance on real-world, complex table understanding tasks across multiple languages."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/tableeval'. Error: Path opencompass/tableeval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1777",
                    "name": "ColorBench",
                    "version": "1.0.0",
                    "description": "ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. ",
                    "url": "opencompass/opencompass_1777.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1777",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1777",
                        "name": "ColorBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/tianyi-lab/ColorBench",
                        "paperLink": "https://arxiv.org/abs/2504.10514",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "55",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-25 17:00:41",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-25 17:00:41",
                        "createDate": "2025-04-25 16:14:54",
                        "desc": {
                            "cn": "ColorBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°ä¸”ç²¾å¿ƒè®¾è®¡çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°VLMsåœ¨é¢œè‰²ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬é¢œè‰²æ„ŸçŸ¥ã€æ¨ç†å’Œé²æ£’æ€§ã€‚",
                            "en": "ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/colorbench'. Error: Path opencompass/colorbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1833",
                    "name": "CSTS",
                    "version": "1.0.0",
                    "description": "CSTS is a synthetic benchmark for evaluating correlation structure discovery in multivariate time series. It features 23 distinct correlation structures with systematic data variations (distribution shifts, sparsification, downsampling) and provides ground truth labels for validation.",
                    "url": "opencompass/opencompass_1833.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1833",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1833",
                        "name": "CSTS",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/isabelladegen/corrclust-validation",
                        "paperLink": "https://arxiv.org/abs/2505.14596",
                        "officialWebsiteLink": "https://huggingface.co/datasets/idegen/csts",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "54",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:27",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:27",
                        "createDate": "2025-06-04 11:16:54",
                        "desc": {
                            "cn": "CSTS is a synthetic benchmark for evaluating correlation structure discovery in multivariate time series. It features 23 distinct correlation structures with systematic data variations (distribution shifts, sparsification, downsampling) and provides ground truth labels for validation.",
                            "en": "CSTS is a synthetic benchmark for evaluating correlation structure discovery in multivariate time series. It features 23 distinct correlation structures with systematic data variations (distribution shifts, sparsification, downsampling) and provides ground truth labels for validation."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/csts'. Error: Path opencompass/csts is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2027",
                    "name": "GroundingSuite",
                    "version": "1.0.0",
                    "description": "GroundingSuite is designed to test the localization capabilities of multimodal models. It created 3,720 pixel-level data entries based on COCO Unlabeled images. This dataset covers four dimensions: Stuff Class Object, Multi-Object, Part-Level Object, and Single Object. ",
                    "url": "opencompass/opencompass_2027.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2027",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2027",
                        "name": "GroundingSuite",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/hustvl/GroundingSuite",
                        "paperLink": "https://arxiv.org/abs/2503.10596",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "10207026",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-sBd5aPOD1"
                        },
                        "lookNum": "54",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-07 11:45:13",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-07 11:45:13",
                        "createDate": "2025-07-05 17:59:14",
                        "desc": {
                            "cn": "GroundingSuite ç”¨æ¥æµ‹è¯•å¤šæ¨¡æ€æ¨¡å‹çš„å®šä½èƒ½åŠ›ã€‚å®ƒé€šè¿‡åŠè‡ªåŠ¨æ ‡æ³¨å’Œäººå·¥ç­›é€‰åœ¨COCO Unlabelçš„å›¾ç‰‡åŸºç¡€ä¸Šåˆ›å»ºäº†3720æ¡pixel-levelçš„æ•°æ®ï¼Œè¦†ç›–Stuff Class Object, Multi Object, Part Level Object, Single Objectå››ä¸ªç»´åº¦ï¼Œæ˜¯ä¸€ä¸ªå…¨é¢è¯„æµ‹å¤šæ¨¡æ€æ¨¡å‹å®šä½èƒ½åŠ›çš„æµ‹è¯•é›†ã€‚",
                            "en": "GroundingSuite is designed to test the localization capabilities of multimodal models. It created 3,720 pixel-level data entries based on COCO Unlabeled images. This dataset covers four dimensions: Stuff Class Object, Multi-Object, Part-Level Object, and Single Object. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/groundingsuite'. Error: Path opencompass/groundingsuite is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2047",
                    "name": "LLMThinkBench",
                    "version": "1.0.0",
                    "description": "LLMThinkBench is a benchmark framework designed to evaluate large language models (LLMs) on basic math reasoning and â€œoverthinkingâ€ behaviors, targeting code-executing language models.",
                    "url": "opencompass/opencompass_2047.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2047",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2047",
                        "name": "LLMThinkBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ctrl-gaurav/LLMThinkBench",
                        "paperLink": "https://arxiv.org/abs/2507.04023",
                        "officialWebsiteLink": "https://ctrl-gaurav.github.io/llmthinkbench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "54",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-14 09:39:28",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-14 09:39:28",
                        "createDate": "2025-07-11 14:17:58",
                        "desc": {
                            "cn": "LLMThinkBench æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŸºç¡€æ•°å­¦æ¨ç†å’Œâ€œè¿‡åº¦æ€è€ƒâ€è¡Œä¸ºæ–¹é¢çš„åŸºå‡†æ¡†æ¶ï¼Œæ”¯æŒå¯¹å…·å¤‡ä»£ç æ‰§è¡Œèƒ½åŠ›çš„è¯­è¨€æ¨¡å‹è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚",
                            "en": "LLMThinkBench is a benchmark framework designed to evaluate large language models (LLMs) on basic math reasoning and â€œoverthinkingâ€ behaviors, targeting code-executing language models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/llmthinkbench'. Error: Path opencompass/llmthinkbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1874",
                    "name": "MedArabiQ",
                    "version": "1.0.0",
                    "description": "MedArabiQ introduces a new benchmark dataset consisting of seven Arabic medical tasks, covering multiple specialties and question formats:\nâœ… Multiple-choice questions\nâœï¸ Fill-in-the-blank (with and without choices)\nğŸ’¬ Patient-doctor question answering",
                    "url": "opencompass/opencompass_1874.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1874",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1874",
                        "name": "MedArabiQ",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Medical",
                                "en": "Medical"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/nyuad-cai/MedArabiQ",
                        "paperLink": "https://arxiv.org/pdf/2505.03427",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "53",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-03 17:23:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-03 17:23:33",
                        "createDate": "2025-06-03 17:18:16",
                        "desc": {
                            "cn": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—ä¿å¥åº”ç”¨ä¸­æ˜¾ç¤ºå‡ºäº†æ˜¾è‘—çš„å‰æ™¯ï¼Œä½†ç”±äºç¼ºä¹é«˜è´¨é‡çš„é¢†åŸŸç‰¹å®šæ•°æ®é›†ï¼Œå®ƒä»¬åœ¨é˜¿æ‹‰ä¼¯è¯­åŒ»å­¦é¢†åŸŸçš„è¡¨ç°åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»æœªå¾—åˆ°æ¢ç´¢ã€‚MedArabiQå¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«ä¸ƒä¸ªé˜¿æ‹‰ä¼¯è¯­åŒ»å­¦ä»»åŠ¡ï¼Œæ¶µç›–å¤šä¸ªä¸“ä¸šå’Œé—®é¢˜æ ¼å¼ï¼š\nâœ… å¤šé¡¹é€‰æ‹©é¢˜\nâœï¸ å¡«ç©ºé¢˜ï¼ˆæœ‰é€‰é¡¹å’Œæ— é€‰é¡¹ï¼‰\nğŸ’¬ æ‚£è€…-åŒ»ç”Ÿé—®ç­”\n\nè¯¥æ•°æ®é›†ä½¿ç”¨è¿‡å¾€åŒ»å­¦è€ƒè¯•å’Œå…¬å¼€å¯ç”¨èµ„æºæ„å»ºï¼Œå¹¶è¿›è¡Œäº†ä¿®æ”¹ä»¥è¯„ä¼°LLMsåœ¨å„ç§èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ï¼ŒåŒ…æ‹¬åè§ç¼“è§£ã€‚\n",
                            "en": "MedArabiQ introduces a new benchmark dataset consisting of seven Arabic medical tasks, covering multiple specialties and question formats:\nâœ… Multiple-choice questions\nâœï¸ Fill-in-the-blank (with and without choices)\nğŸ’¬ Patient-doctor question answering"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medarabiq'. Error: Path opencompass/medarabiq is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1991",
                    "name": "AssetOpsBench",
                    "version": "1.0.0",
                    "description": "AssetOpsBench is a comprehensive benchmark designed to evaluate the performance of large language models (LLMs) and AI agents in complex asset operation and maintenance tasks.",
                    "url": "opencompass/opencompass_1991.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1991",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1991",
                        "name": "AssetOpsBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/IBM/AssetOpsBench",
                        "paperLink": "https://arxiv.org/abs/2506.03828",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "53",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-27 18:12:23",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-27 18:12:23",
                        "createDate": "2025-06-27 14:46:33",
                        "desc": {
                            "cn": "AssetOpsBench æ˜¯ä¸€ä¸ªä¸“æ³¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ™ºèƒ½ä½“åœ¨èµ„äº§è¿ç»´é¢†åŸŸå¤æ‚ä»»åŠ¡ä¸­å®é™…è¡¨ç°çš„å¤šç»´åº¦è¯„æµ‹åŸºå‡†ã€‚è¯¥åŸºå‡†æ—¨åœ¨æ£€éªŒæ¨¡å‹åœ¨å·¥ä¸šåœºæ™¯ä¸‹çš„ä»»åŠ¡è§„åˆ’ã€å¤šæ­¥æ¨ç†ã€å·¥å…·è°ƒç”¨ã€å®‰å…¨åˆè§„æ€§ä»¥åŠé¢†åŸŸçŸ¥è¯†ç†è§£ç­‰æ ¸å¿ƒèƒ½åŠ›ï¼Œè¦†ç›–è®¾å¤‡ç»´æŠ¤ã€å¼‚å¸¸è¯Šæ–­ã€é£é™©è¯„ä¼°ç­‰å…¸å‹è¿ç»´åœºæ™¯ã€‚æµ‹è¯•é›†åŒ…å« 1,000 ä¸ªé«˜è´¨é‡æ ·æœ¬ï¼Œæ¶‰åŠ 5 å¤§ç±»ä»»åŠ¡å’Œ 20 ä½™ç§ç»†åˆ†é¢†åŸŸï¼Œæ•°æ®æ¥æºäºçœŸå®è¿ç»´æ‰‹å†Œã€å·¥å•è®°å½•åŠä¸“å®¶éªŒè¯æ¡ˆä¾‹ã€‚",
                            "en": "AssetOpsBench is a comprehensive benchmark designed to evaluate the performance of large language models (LLMs) and AI agents in complex asset operation and maintenance tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/assetopsbench'. Error: Path opencompass/assetopsbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1779",
                    "name": "MLRC-Bench",
                    "version": "1.0.0",
                    "description": "MLRC-Bench, a benchmark designed to quantify how effectively language agents can tackle challenging Machine Learning (ML) Research Competitions. ",
                    "url": "opencompass/opencompass_1779.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1779",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1779",
                        "name": "MLRC-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/yunx-z/MLRC-Bench",
                        "paperLink": "https://arxiv.org/abs/2504.09702",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "53",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-25 17:00:47",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-25 17:00:47",
                        "createDate": "2025-04-25 16:34:27",
                        "desc": {
                            "cn": " MLRC-Benchæ—¨åœ¨é‡åŒ–å¤§æ¨¡å‹ä»£ç†å¦‚ä½•æœ‰æ•ˆåœ°åº”å¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„æœºå™¨å­¦ä¹  ï¼ˆMLï¼‰ ç ”ç©¶ç«èµ›ã€‚",
                            "en": "MLRC-Bench, a benchmark designed to quantify how effectively language agents can tackle challenging Machine Learning (ML) Research Competitions. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mlrc_bench'. Error: Path opencompass/mlrc_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1754",
                    "name": "OpenTuringBench",
                    "version": "1.0.0",
                    "description": "OpenTuringBench, a new benchmark based on OLLMs, designed to train and evaluate machine-generated text detectors on the Turing Test and Authorship Attribution problems. ",
                    "url": "opencompass/opencompass_1754.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1754",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1754",
                        "name": "OpenTuringBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2504.11369",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "52",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-21 12:34:42",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-21 12:34:42",
                        "createDate": "2025-04-21 12:26:14",
                        "desc": {
                            "cn": "OpenTuringBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºOLLMsçš„æ–°åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è®­ç»ƒå’Œè¯„ä¼°æœºå™¨ç”Ÿæˆæ–‡æœ¬æ£€æµ‹å™¨åœ¨å›¾çµæµ‹è¯•å’Œä½œè€…å½’å±é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚",
                            "en": "OpenTuringBench, a new benchmark based on OLLMs, designed to train and evaluate machine-generated text detectors on the Turing Test and Authorship Attribution problems. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/openturingbench'. Error: Path opencompass/openturingbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1846",
                    "name": "TGLG",
                    "version": "1.0.0",
                    "description": "Temporally-Grounded Language Generation (TGLG) is a benchmark for real-time vision-language models (VLMs) that focus on two key capabilities: perceptual updating and contingency awareness. ",
                    "url": "opencompass/opencompass_1846.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1846",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1846",
                        "name": "TGLG",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "Strong Reasoning",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/yukw777/tglg",
                        "paperLink": "https://arxiv.org/pdf/2505.11326",
                        "officialWebsiteLink": "https://huggingface.co/datasets/kpyu/tglg",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "51",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-03 10:02:19",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-03 10:02:19",
                        "createDate": "2025-05-27 11:15:19",
                        "desc": {
                            "cn": "åŸºäºæ—¶é—´çš„è¯­è¨€ç”Ÿæˆï¼ˆTGLGï¼‰æ˜¯å®æ—¶è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„åŸºå‡†ï¼Œä¾§é‡äºä¸¤ä¸ªå…³é”®åŠŸèƒ½ï¼šæ„ŸçŸ¥æ›´æ–°å’Œåº”æ€¥æ„è¯†ã€‚è¯¥å­˜å‚¨åº“è¿˜åŒ…å«TGLGçš„åŸºçº¿å®æ—¶VLMä»£ç ï¼Œå³å…·æœ‰æ—¶é—´åŒæ­¥äº¤ç»‡çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLM-TSIï¼‰ã€‚",
                            "en": "Temporally-Grounded Language Generation (TGLG) is a benchmark for real-time vision-language models (VLMs) that focus on two key capabilities: perceptual updating and contingency awareness. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/tglg'. Error: Path opencompass/tglg is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1851",
                    "name": "MMLongBench",
                    "version": "1.0.0",
                    "description": "MMLongBench is a benchmark that evaluates long-context vision-language models across various tasks, image types, and input lengths, revealing that single-task performance is insufficient for gauging overall vision-language long-context capability.",
                    "url": "opencompass/opencompass_1851.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1851",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Knowledge",
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1851",
                        "name": "MMLongBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            },
                            {
                                "cn": "é•¿æ–‡æœ¬",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "multimodal",
                                "en": "multimodal"
                            },
                            {
                                "cn": "Long-Context",
                                "en": "Long-Context"
                            },
                            {
                                "cn": "Vision-Language",
                                "en": "Vision-Language"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/EdinburghNLP/MMLongBench",
                        "paperLink": "https://arxiv.org/abs/2505.10610",
                        "officialWebsiteLink": "https://zhaowei-wang-nlp.github.io/MMLongBench-page/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "43305250",
                            "name": null,
                            "avatar": null,
                            "nickname": "zhaowei-wang-nlp"
                        },
                        "lookNum": "51",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-24 18:08:10",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-24 18:08:10",
                        "createDate": "2025-05-27 18:50:31",
                        "desc": {
                            "cn": "MMLongBench æ˜¯ä¸€ä¸ªé’ˆå¯¹é•¿ä¸Šä¸‹æ–‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„åŸºå‡†ï¼Œè¦†ç›–å¤šç§ä»»åŠ¡ã€å›¾åƒç±»å‹å’Œè¾“å…¥é•¿åº¦ã€‚è¯„æµ‹ç»“æœè¡¨æ˜ï¼Œå•ä¸€ä»»åŠ¡çš„è¡¨ç°ä¸è¶³ä»¥è¡¡é‡æ¨¡å‹çš„æ•´ä½“é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ã€‚",
                            "en": "MMLongBench is a benchmark that evaluates long-context vision-language models across various tasks, image types, and input lengths, revealing that single-task performance is insufficient for gauging overall vision-language long-context capability."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmlongbench'. Error: Path opencompass/mmlongbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1783",
                    "name": "NPPC",
                    "version": "1.0.0",
                    "description": "Nondeterministic Polynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark for LLMs. ",
                    "url": "opencompass/opencompass_1783.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1783",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1783",
                        "name": "NPPC",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SMU-DIGA/nppc",
                        "paperLink": "https://arxiv.org/abs/2504.11239",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "51",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-28 11:36:56",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-28 11:36:56",
                        "createDate": "2025-04-28 11:16:50",
                        "desc": {
                            "cn": "éç¡®å®šæ€§å¤šé¡¹å¼æ—¶é—´é—®é¢˜æŒ‘æˆ˜ ï¼ˆNPPCï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸æ–­æ‰©å±•çš„ LLM æ¨ç†åŸºå‡†ã€‚",
                            "en": "Nondeterministic Polynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark for LLMs. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/nppc'. Error: Path opencompass/nppc is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2039",
                    "name": "TransLaw",
                    "version": "1.0.0",
                    "description": "æœ¬æ–‡æå‡ºTransLawâ€”â€”ä¸“ä¸ºé¦™æ¸¯åˆ¤ä¾‹ç¿»è¯‘è®¾è®¡çš„ååŒäº¤äº’å¼å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å°†ä¼ ç»Ÿç¿»è¯‘æµç¨‹æ‹†è§£ä¸ºç¿»è¯‘ã€é”™è¯¯æ ‡æ³¨åŠæ ¡å¯¹ä¿®æ­£ä¸‰å¤§å­ä»»åŠ¡ï¼Œå¹¶åˆ†é…ä¸‰ä¸ªæ™ºèƒ½ä½“ååŒæ‰§è¡Œã€‚ä¸ºè¯„ä¼°æ¡†æ¶æ€§èƒ½ï¼Œæˆ‘ä»¬æ„å»ºäº†å¤§è§„æ¨¡åŒè¯­åŸºå‡†æ•°æ®é›†BJC Judgmentsï¼Œå¯¹13ä¸ªå¼€æºä¸å•†ä¸šå¤§è¯­è¨€æ¨¡å‹ï¼ˆä½œä¸ºæ™ºèƒ½ä½“ï¼‰å±•å¼€è¯„æµ‹ã€‚å®éªŒç»“æœéªŒè¯äº†ååŒç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼šåœ¨å¤šæ™ºèƒ½ä½“åä½œæ˜¾è‘—æå‡æ•ˆæœçš„åŒæ—¶ï¼Œæä¾›äº†å…·æœ‰å‚è€ƒä»·å€¼çš„LLMæ€§èƒ½æ¨ªå‘å¯¹æ¯”ã€‚é€šè¿‡é”™è¯¯ç±»å‹å­¦åˆ†æï¼Œæœ¬ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†äºŸå¾…è§£å†³çš„å…³é”®ç¿»è¯‘æŒ‘æˆ˜ã€‚æœªæ¥å·¥ä½œå°†èšç„¦äºä¼˜åŒ–æ™ºèƒ½ä½“æ¶æ„ä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼ŒåŒæ—¶å¼€å‘æ›´å…¨é¢ã€ä½æˆæœ¬çš„è¯„ä¼°åŸºå‡†ã€‚",
                    "url": "opencompass/opencompass_2039.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2039",
                    "sample_count": 1000,
                    "traits": [
                        "Language",
                        "Agent",
                        "Instruct"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2039",
                        "name": "TransLaw",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "è¯­è¨€",
                                "en": "Language"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            },
                            {
                                "cn": "æŒ‡ä»¤è·Ÿéš",
                                "en": "Instruct"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Legal AI",
                                "en": "Legal AI"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/pdf/2507.00875",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "18909912",
                            "name": null,
                            "avatar": null,
                            "nickname": "Oss"
                        },
                        "lookNum": "51",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-11 10:29:52",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-11 10:29:52",
                        "createDate": "2025-07-09 02:45:37",
                        "desc": {
                            "cn": "æœ¬æ–‡æå‡ºTransLawâ€”â€”ä¸“ä¸ºé¦™æ¸¯åˆ¤ä¾‹ç¿»è¯‘è®¾è®¡çš„ååŒäº¤äº’å¼å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å°†ä¼ ç»Ÿç¿»è¯‘æµç¨‹æ‹†è§£ä¸ºç¿»è¯‘ã€é”™è¯¯æ ‡æ³¨åŠæ ¡å¯¹ä¿®æ­£ä¸‰å¤§å­ä»»åŠ¡ï¼Œå¹¶åˆ†é…ä¸‰ä¸ªæ™ºèƒ½ä½“ååŒæ‰§è¡Œã€‚ä¸ºè¯„ä¼°æ¡†æ¶æ€§èƒ½ï¼Œæˆ‘ä»¬æ„å»ºäº†å¤§è§„æ¨¡åŒè¯­åŸºå‡†æ•°æ®é›†BJC Judgmentsï¼Œå¯¹13ä¸ªå¼€æºä¸å•†ä¸šå¤§è¯­è¨€æ¨¡å‹ï¼ˆä½œä¸ºæ™ºèƒ½ä½“ï¼‰å±•å¼€è¯„æµ‹ã€‚å®éªŒç»“æœéªŒè¯äº†ååŒç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼šåœ¨å¤šæ™ºèƒ½ä½“åä½œæ˜¾è‘—æå‡æ•ˆæœçš„åŒæ—¶ï¼Œæä¾›äº†å…·æœ‰å‚è€ƒä»·å€¼çš„LLMæ€§èƒ½æ¨ªå‘å¯¹æ¯”ã€‚é€šè¿‡é”™è¯¯ç±»å‹å­¦åˆ†æï¼Œæœ¬ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†äºŸå¾…è§£å†³çš„å…³é”®ç¿»è¯‘æŒ‘æˆ˜ã€‚æœªæ¥å·¥ä½œå°†èšç„¦äºä¼˜åŒ–æ™ºèƒ½ä½“æ¶æ„ä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼ŒåŒæ—¶å¼€å‘æ›´å…¨é¢ã€ä½æˆæœ¬çš„è¯„ä¼°åŸºå‡†ã€‚",
                            "en": "æœ¬æ–‡æå‡ºTransLawâ€”â€”ä¸“ä¸ºé¦™æ¸¯åˆ¤ä¾‹ç¿»è¯‘è®¾è®¡çš„ååŒäº¤äº’å¼å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å°†ä¼ ç»Ÿç¿»è¯‘æµç¨‹æ‹†è§£ä¸ºç¿»è¯‘ã€é”™è¯¯æ ‡æ³¨åŠæ ¡å¯¹ä¿®æ­£ä¸‰å¤§å­ä»»åŠ¡ï¼Œå¹¶åˆ†é…ä¸‰ä¸ªæ™ºèƒ½ä½“ååŒæ‰§è¡Œã€‚ä¸ºè¯„ä¼°æ¡†æ¶æ€§èƒ½ï¼Œæˆ‘ä»¬æ„å»ºäº†å¤§è§„æ¨¡åŒè¯­åŸºå‡†æ•°æ®é›†BJC Judgmentsï¼Œå¯¹13ä¸ªå¼€æºä¸å•†ä¸šå¤§è¯­è¨€æ¨¡å‹ï¼ˆä½œä¸ºæ™ºèƒ½ä½“ï¼‰å±•å¼€è¯„æµ‹ã€‚å®éªŒç»“æœéªŒè¯äº†ååŒç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼šåœ¨å¤šæ™ºèƒ½ä½“åä½œæ˜¾è‘—æå‡æ•ˆæœçš„åŒæ—¶ï¼Œæä¾›äº†å…·æœ‰å‚è€ƒä»·å€¼çš„LLMæ€§èƒ½æ¨ªå‘å¯¹æ¯”ã€‚é€šè¿‡é”™è¯¯ç±»å‹å­¦åˆ†æï¼Œæœ¬ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†äºŸå¾…è§£å†³çš„å…³é”®ç¿»è¯‘æŒ‘æˆ˜ã€‚æœªæ¥å·¥ä½œå°†èšç„¦äºä¼˜åŒ–æ™ºèƒ½ä½“æ¶æ„ä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼ŒåŒæ—¶å¼€å‘æ›´å…¨é¢ã€ä½æˆæœ¬çš„è¯„ä¼°åŸºå‡†ã€‚"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/translaw'. Error: Path opencompass/translaw is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2048",
                    "name": "RISEBench",
                    "version": "1.0.0",
                    "description": "RISEBench is a benchmark for evaluating large multimodal models (LMMs) on reasoning-informed visual editing tasks, targeting models with image understanding and generation capabilities. ",
                    "url": "opencompass/opencompass_2048.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2048",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning",
                        "Instruct"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2048",
                        "name": "RISEBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "æŒ‡ä»¤è·Ÿéš",
                                "en": "Instruct"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/PhoenixZ810/RISEBench",
                        "paperLink": "https://arxiv.org/pdf/2504.02826",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "50",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-11 14:28:48",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-11 14:28:48",
                        "createDate": "2025-07-11 14:25:32",
                        "desc": {
                            "cn": "RISEBench æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨æ¨ç†é©±åŠ¨è§†è§‰ç¼–è¾‘ä»»åŠ¡ä¸­èƒ½åŠ›çš„åŸºå‡†ï¼Œé¢å‘å…·å¤‡å›¾åƒç†è§£ä¸ç”Ÿæˆèƒ½åŠ›çš„æ¨¡å‹ã€‚",
                            "en": "RISEBench is a benchmark for evaluating large multimodal models (LMMs) on reasoning-informed visual editing tasks, targeting models with image understanding and generation capabilities. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/risebench'. Error: Path opencompass/risebench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1926",
                    "name": "VideoMathQA",
                    "version": "1.0.0",
                    "description": "VideoMathQA is a benchmark designed to evaluate mathematical reasoning in real-world educational videos. It requires models to interpret and integrate information from three modalities, visuals, audio, and text, across time. ",
                    "url": "opencompass/opencompass_1926.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1926",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1926",
                        "name": "VideoMathQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/mbzuai-oryx/VideoMathQA",
                        "paperLink": "https://arxiv.org/abs/2506.05349",
                        "officialWebsiteLink": "https://mbzuai-oryx.github.io/VideoMathQA/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "å¼ å¯Œæ‰"
                        },
                        "lookNum": "50",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-16 09:50:53",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-16 09:50:53",
                        "createDate": "2025-06-12 11:57:53",
                        "desc": {
                            "cn": "VideoMathQAæ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å®é™…æ•™è‚²è§†é¢‘ä¸­æ•°å­¦æ¨ç†èƒ½åŠ›çš„åŸºå‡†ã€‚å®ƒè¦æ±‚æ¨¡å‹è§£é‡Šå’Œæ•´åˆæ¥è‡ªä¸‰ç§æ¨¡æ€(è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬)éšæ—¶é—´å˜åŒ–çš„ä¿¡æ¯ã€‚è¯¥åŸºå‡†è§£å†³äº†\"å¤šæ¨¡æ€é’ˆå †\"é—®é¢˜,å³å…³é”®ä¿¡æ¯ç¨€ç–ä¸”åˆ†æ•£åœ¨è§†é¢‘çš„ä¸åŒæ¨¡æ€å’Œæ—¶åˆ»ã€‚",
                            "en": "VideoMathQA is a benchmark designed to evaluate mathematical reasoning in real-world educational videos. It requires models to interpret and integrate information from three modalities, visuals, audio, and text, across time. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/videomathqa'. Error: Path opencompass/videomathqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1985",
                    "name": "CVDP",
                    "version": "1.0.0",
                    "description": "CVDP is a next-generation benchmark for evaluating large language models (LLMs) and agents in hardware design and verification, comprising 783 problems across 13 task categories, including RTL generation, verification, debugging, specification alignment, and technical Q&A. ",
                    "url": "opencompass/opencompass_1985.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1985",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1985",
                        "name": "CVDP",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/NVlabs/cvdp_benchmark",
                        "paperLink": "https://arxiv.org/abs/2506.14074",
                        "officialWebsiteLink": "https://huggingface.co/datasets/nvidia/cvdp-benchmark-dataset",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "50",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-27 18:13:12",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-27 18:13:12",
                        "createDate": "2025-06-26 14:21:49",
                        "desc": {
                            "cn": "CVDP æ˜¯ä¸€ä¸ªé¢å‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ™ºèƒ½ä½“çš„ä¸‹ä¸€ä»£ç¡¬ä»¶è®¾è®¡ä¸éªŒè¯è¯„æµ‹åŸºå‡†ï¼Œæ¶µç›– 13 ç±»ä»»åŠ¡å…± 783 ä¸ªé—®é¢˜ï¼Œæ¶‰åŠ RTL ç”Ÿæˆã€éªŒè¯ã€è°ƒè¯•ã€è§„èŒƒå¯¹é½å’ŒæŠ€æœ¯é—®ç­”ç­‰ã€‚",
                            "en": "CVDP is a next-generation benchmark for evaluating large language models (LLMs) and agents in hardware design and verification, comprising 783 problems across 13 task categories, including RTL generation, verification, debugging, specification alignment, and technical Q&A. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cvdp'. Error: Path opencompass/cvdp is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1969",
                    "name": "ALE-Bench",
                    "version": "1.0.0",
                    "description": "ALE-Bench is a benchmark for evaluating AI systems on score-based algorithmic programming contests.",
                    "url": "opencompass/opencompass_1969.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1969",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1969",
                        "name": "ALE-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SakanaAI/ALE-Bench",
                        "paperLink": "https://arxiv.org/abs/2506.09050",
                        "officialWebsiteLink": "https://huggingface.co/datasets/SakanaAI/ALE-Bench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "49",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-23 20:41:05",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-23 20:41:05",
                        "createDate": "2025-06-23 15:33:02",
                        "desc": {
                            "cn": "ALE-Bench æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼° AI ç³»ç»Ÿåœ¨åŸºäºåˆ†æ•°çš„ç®—æ³•ç¼–ç¨‹ç«èµ›ä¸­çš„åŸºå‡†æµ‹è¯•ã€‚",
                            "en": "ALE-Bench is a benchmark for evaluating AI systems on score-based algorithmic programming contests."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ale_bench'. Error: Path opencompass/ale_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1970",
                    "name": "EditInspector",
                    "version": "1.0.0",
                    "description": "EditInspector is a novel benchmark for evaluation of text-guided image edits, based on human annotations collected using an extensive template for edit verification. We leverage EditInspector to evaluate the performance of state-of-the-art (SoTA) vision and language models.",
                    "url": "opencompass/opencompass_1970.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1970",
                    "sample_count": 1000,
                    "traits": [
                        "Creation",
                        "Instruct"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1970",
                        "name": "EditInspector",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "åˆ›ä½œ",
                                "en": "Creation"
                            },
                            {
                                "cn": "æŒ‡ä»¤è·Ÿéš",
                                "en": "Instruct"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/editinspector/EditInspector",
                        "paperLink": "https://arxiv.org/abs/2506.09988",
                        "officialWebsiteLink": "https://editinspector.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "49",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-23 20:40:59",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-23 20:40:59",
                        "createDate": "2025-06-23 15:40:22",
                        "desc": {
                            "cn": "EditInspector è¯„ä¼°æœ€å…ˆè¿›ï¼ˆSoTAï¼‰è§†è§‰å’Œè¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªç»´åº¦ä¸Šè¯„ä¼°ç¼–è¾‘çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å‡†ç¡®æ€§ã€ç‘•ç–µæ£€æµ‹ã€è§†è§‰è´¨é‡ã€ä¸å›¾åƒåœºæ™¯çš„æ— ç¼èåˆã€éµå¾ªå¸¸è¯†ä»¥åŠæè¿°ç¼–è¾‘å¼•èµ·å˜åŒ–çš„èƒ½åŠ›ã€‚",
                            "en": "EditInspector is a novel benchmark for evaluation of text-guided image edits, based on human annotations collected using an extensive template for edit verification. We leverage EditInspector to evaluate the performance of state-of-the-art (SoTA) vision and language models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/editinspector'. Error: Path opencompass/editinspector is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1992",
                    "name": "HtFLlib",
                    "version": "1.0.0",
                    "description": "HtFLlib is a benchmark for heterogeneous federated learning that examines how 40 vision, NLP and sensor models and 10 algorithms collaborate under non-IID data. ",
                    "url": "opencompass/opencompass_1992.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1992",
                    "sample_count": 1000,
                    "traits": [
                        "Safety",
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1992",
                        "name": "HtFLlib",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            },
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/TsingZ0/HtFLlib",
                        "paperLink": "https://arxiv.org/abs/2506.03954",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "49",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-27 18:12:30",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-27 18:12:30",
                        "createDate": "2025-06-27 14:50:47",
                        "desc": {
                            "cn": "HtFLlib æ˜¯ä¸€ä¸ªé¢å‘å¼‚æ„è”é‚¦å­¦ä¹ ç®—æ³•çš„ç»¼åˆè¯„æµ‹åŸºå‡†ï¼Œæ—¨åœ¨è¡¡é‡ä¸åŒæ¨¡å‹æ¶æ„åœ¨é IID æ•°æ®ç¯å¢ƒä¸­çš„ååŒå­¦ä¹ èƒ½åŠ›ã€‚è¯„æµ‹å¯¹è±¡è¦†ç›–å›¾åƒã€æ–‡æœ¬ä¸ä¼ æ„Ÿä¿¡å·ä¸‰ç±»æ¨¡å‹ï¼Œæ€»è®¡ 40 ä¸ªæ¶æ„åŠ 10 ç§ä»£è¡¨æ€§æ–¹æ³•ã€‚",
                            "en": "HtFLlib is a benchmark for heterogeneous federated learning that examines how 40 vision, NLP and sensor models and 10 algorithms collaborate under non-IID data. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/htfllib'. Error: Path opencompass/htfllib is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1784",
                    "name": "xVerify",
                    "version": "1.0.0",
                    "description": "xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions",
                    "url": "opencompass/opencompass_1784.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1784",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1784",
                        "name": "xVerify",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/IAAR-Shanghai/xVerify",
                        "paperLink": "https://arxiv.org/abs/2504.10481",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "49",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-28 11:36:52",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-28 11:36:52",
                        "createDate": "2025-04-28 11:24:15",
                        "desc": {
                            "cn": "xVerifyï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ¨ç†æ¨¡å‹è¯„ä¼°çš„é«˜æ•ˆç­”æ¡ˆéªŒè¯å™¨ã€‚xVerify åœ¨ç­‰ä»·åˆ¤æ–­æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°ç¡®å®šæ¨ç†æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦ç­‰åŒäºå„ç§ç±»å‹å®¢è§‚é—®é¢˜çš„å‚è€ƒç­”æ¡ˆã€‚",
                            "en": "xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/xverify'. Error: Path opencompass/xverify is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1786",
                    "name": "HypoEval",
                    "version": "1.0.0",
                    "description": "HypoEval, Hypothesis-guided Evaluation framework, which first uses a small corpus of human evaluations to generate more detailed rubrics for human judgments and then incorporates a checklist-like approach to combine LLM's assigned scores on each decomposed dimension to acquire overall scores. ",
                    "url": "opencompass/opencompass_1786.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1786",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1786",
                        "name": "HypoEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ChicagoHAI/HypoEval",
                        "paperLink": "https://arxiv.org/abs/2504.07174",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "49",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-28 11:36:49",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-28 11:36:49",
                        "createDate": "2025-04-28 11:34:44",
                        "desc": {
                            "cn": "HypoEvalï¼Œå³å‡è®¾æŒ‡å¯¼çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é¦–å…ˆä½¿ç”¨ä¸€å°éƒ¨åˆ†äººå·¥è¯„ä¼°æ¥ç”Ÿæˆæ›´è¯¦ç»†çš„äººç±»åˆ¤æ–­é‡è§„ï¼Œç„¶åé‡‡ç”¨ç±»ä¼¼æ¸…å•çš„æ–¹æ³•ï¼Œå°† LLM åœ¨æ¯ä¸ªåˆ†è§£ç»´åº¦ä¸Šçš„åˆ†é…åˆ†æ•°ç»“åˆèµ·æ¥ï¼Œä»¥è·å¾—æ€»åˆ†ã€‚",
                            "en": "HypoEval, Hypothesis-guided Evaluation framework, which first uses a small corpus of human evaluations to generate more detailed rubrics for human judgments and then incorporates a checklist-like approach to combine LLM's assigned scores on each decomposed dimension to acquire overall scores. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/hypoeval'. Error: Path opencompass/hypoeval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2077",
                    "name": "ITBench",
                    "version": "1.0.0",
                    "description": "ITBench is a benchmark designed to evaluate the performance of AI agents in real-world IT automation tasks. ",
                    "url": "opencompass/opencompass_2077.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2077",
                    "sample_count": 1000,
                    "traits": [
                        "Code",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2077",
                        "name": "ITBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/itbench-hub/ITBench",
                        "paperLink": "https://icml.cc/virtual/2025/poster/44303",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "48",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:04",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:04",
                        "createDate": "2025-07-22 16:51:52",
                        "desc": {
                            "cn": "ITBench æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼° AI æ™ºèƒ½ä½“åœ¨çœŸå®ä¸–ç•Œ IT è‡ªåŠ¨åŒ–ä»»åŠ¡ä¸­è¡¨ç°çš„åŸºå‡†ã€‚å®ƒæ¶µç›–äº†ç«™ç‚¹å¯é æ€§å·¥ç¨‹ã€åˆè§„ä¸å®‰å…¨è¿è¥ä»¥åŠè´¢åŠ¡è¿è¥ç­‰å…³é”®ç»´åº¦ï¼Œå¹¶åŒ…å« 102 ä¸ªçœŸå®åœºæ™¯ã€‚è¯¥åŸºå‡†æä¾›äº†ä¸€ä¸ªå¼€æºæ¡†æ¶å’Œå¤šç§åŸºçº¿æ™ºèƒ½ä½“å®ç°ï¼Œå¹¶é›†æˆäº†CrewAIç­‰å·¥å…·ï¼Œä»¥ä¿ƒè¿›AIé©±åŠ¨çš„ITè‡ªåŠ¨åŒ–å‘å±•ã€‚",
                            "en": "ITBench is a benchmark designed to evaluate the performance of AI agents in real-world IT automation tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/itbench'. Error: Path opencompass/itbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2079",
                    "name": "MER-UniBench",
                    "version": "1.0.0",
                    "description": "MER-UniBench is a benchmark designed to evaluate multimodal large language models (MLLMs) for their emotion understanding capabilities across typical multimodal emotion recognition (MER) tasks. ",
                    "url": "opencompass/opencompass_2079.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2079",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2079",
                        "name": "MER-UniBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "MER",
                                "en": "MER"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/zeroQiaoba/AffectGPT",
                        "paperLink": "https://icml.cc/virtual/2025/poster/43565",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "47",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:16",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:16",
                        "createDate": "2025-07-22 16:51:18",
                        "desc": {
                            "cn": "MER-UniBench æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å…¸å‹å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ï¼ˆMERï¼‰ä»»åŠ¡ä¸­æƒ…æ„Ÿç†è§£èƒ½åŠ›çš„è¯„æµ‹åŸºå‡†ã€‚å®ƒæ¶µç›–äº†ç»†ç²’åº¦æƒ…æ„Ÿè¯†åˆ«ã€åŸºæœ¬æƒ…æ„Ÿè¯†åˆ«å’Œæƒ…æ„Ÿåˆ†æä¸‰ä¸ªä¸»è¦ç»´åº¦ã€‚è¯¥åŸºå‡†åˆ©ç”¨äº†åŒ…æ‹¬ MER-Caption åœ¨å†…çš„å¤šä¸ªæ•°æ®é›†ï¼Œå…¶ä¸­ MER-Caption æ‹¥æœ‰è¶…è¿‡ 2000 ç§ç»†ç²’åº¦æƒ…æ„Ÿç±»åˆ«å’Œ 11.5 ä¸‡ä¸ªæ ·æœ¬ã€‚",
                            "en": "MER-UniBench is a benchmark designed to evaluate multimodal large language models (MLLMs) for their emotion understanding capabilities across typical multimodal emotion recognition (MER) tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mer_unibench'. Error: Path opencompass/mer_unibench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2014",
                    "name": "UTBoost",
                    "version": "1.0.0",
                    "description": "UTBoost generates unit tests using LLMs to augment the test cases for certain instances in SWE-Bench, enabling a more rigorous use of SWE-Bench to evaluate the performance of Code Agents.",
                    "url": "opencompass/opencompass_2014.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2014",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2014",
                        "name": "UTBoost",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Code Agent",
                                "en": "Code Agent"
                            },
                            {
                                "cn": "SWE-Bench",
                                "en": "SWE-Bench"
                            },
                            {
                                "cn": "Automatic test augmentation",
                                "en": "Automatic test augmentation"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/CUHK-Shenzhen-SE/UTBoost",
                        "paperLink": "https://arxiv.org/abs/2506.09289",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "85501967",
                            "name": "BoxiYu",
                            "avatar": null,
                            "nickname": "è¡Œè€…"
                        },
                        "lookNum": "47",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-01 15:40:25",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-01 15:40:25",
                        "createDate": "2025-07-01 15:27:00",
                        "desc": {
                            "cn": "UTBoosté€šè¿‡LLMç”Ÿæˆçš„å•å…ƒæµ‹è¯•ï¼Œå¢å¼ºäº†SWE-Benchä¸­ä¸€äº›instancesçš„æµ‹è¯•ç”¨ä¾‹ï¼Œèƒ½ä¸¥è°¨çš„ä½¿ç”¨SWE-Benchæ¥è¯„ä¼°Code Agentsçš„è¡¨ç°ã€‚",
                            "en": "UTBoost generates unit tests using LLMs to augment the test cases for certain instances in SWE-Bench, enabling a more rigorous use of SWE-Bench to evaluate the performance of Code Agents."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/utboost'. Error: Path opencompass/utboost is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1944",
                    "name": "LoopNav",
                    "version": "1.0.0",
                    "description": "A video-action dataset containing many loop-based navigation dataset in Minecraft environment, aiming to boost the spatial consistency and providing insight for the design of memory module",
                    "url": "opencompass/opencompass_1944.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1944",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Long-Context",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1944",
                        "name": "LoopNav",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "é•¿æ–‡æœ¬",
                                "en": "Long-Context"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "navigation",
                                "en": "navigation"
                            },
                            {
                                "cn": "consistency",
                                "en": "consistency"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Kevin-lkw/LoopNav",
                        "paperLink": "https://arxiv.org/abs/2505.22976",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "29904400",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-no7Zd0NrI"
                        },
                        "lookNum": "46",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-20 15:14:30",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-20 15:14:30",
                        "createDate": "2025-06-17 09:30:27",
                        "desc": {
                            "cn": "ä¸€ä¸ªè§†é¢‘-åŠ¨ä½œå¯¼èˆªæ•°æ®é›†ï¼ŒåŒ…æ‹¬äº†åœ¨Minecraftç¯å¢ƒä¸‹å¤§é‡åŸºäºå›ç¯çš„å¯¼èˆªæ•°æ®ï¼Œèƒ½å¤Ÿä¿ƒè¿›ä¸–ç•Œæ¨¡å‹ç­‰è§†é¢‘æ¨¡å‹ç©ºé—´ä¸€è‡´æ€§çš„è®­ç»ƒï¼Œå¯å‘è®°å¿†æ¨¡å—çš„è®¾è®¡ã€‚",
                            "en": "A video-action dataset containing many loop-based navigation dataset in Minecraft environment, aiming to boost the spatial consistency and providing insight for the design of memory module"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/loopnav'. Error: Path opencompass/loopnav is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1980",
                    "name": "FalseReject",
                    "version": "1.0.0",
                    "description": "Safety alignment approaches in large language models (LLMs) often lead\nto the over-refusal of benign queries, significantly diminishing their utility\nin sensitive scenarios. To address this challenge, we introduce FalseReject,\na comprehensive resource containing 16k seemingly toxic queries accompani",
                    "url": "opencompass/opencompass_1980.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1980",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning",
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1980",
                        "name": "FalseReject",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://false-reject.github.io/",
                        "paperLink": "https://arxiv.org/pdf/2505.08054",
                        "officialWebsiteLink": "https://false-reject.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "38705030",
                            "name": null,
                            "avatar": null,
                            "nickname": "èƒ¥ä¼Ÿæ°"
                        },
                        "lookNum": "46",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-26 14:32:37",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-26 14:32:37",
                        "createDate": "2025-06-26 07:09:15",
                        "desc": {
                            "cn": "FalseReject æ„å»ºäº†åŒ…å« 16 000 æ¡è¡¨é¢â€œæœ‰æ¯’â€ä½†å®ä¸ºè‰¯æ€§çš„æŸ¥è¯¢æ ·æœ¬ï¼Œè¦†ç›– 44 ä¸ªå®‰å…¨ç›¸å…³ç±»åˆ«ï¼Œå¹¶æå‡ºä¸€ç§åŸºäºå›¾ä¿¡æ¯çš„å¯¹æŠ—å¤šæ™ºèƒ½ä½“äº¤äº’æ¡†æ¶ï¼Œç”¨ä»¥ç”Ÿæˆå¤šæ ·ä¸”å¤æ‚çš„æç¤ºâ€“å“åº”å¯¹ï¼Œå¹¶åœ¨å“åº”ä¸­å¼•å…¥æ˜¾å¼æ¨ç†é“¾ï¼Œå¸®åŠ©æ¨¡å‹æ›´å‡†ç¡®åœ°åŒºåˆ†å®‰å…¨ä¸ä¸å®‰å…¨ä¸Šä¸‹æ–‡ï¼›è¯¥å·¥ä½œè¿˜ä¸ºæ ‡å‡†æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹å’Œæ¨ç†å¯¼å‘æ¨¡å‹åˆ†åˆ«å‡†å¤‡äº†ä¸“é¡¹è®­ç»ƒé›†ï¼Œå¹¶é™„å¸¦äººå·¥æ ‡æ³¨çš„åŸºå‡†æµ‹è¯•é›†ï¼Œé’ˆå¯¹ 29 æ¬¾æœ€å…ˆè¿› LLM è¿›è¡Œäº†å¤§è§„æ¨¡è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ç» FalseReject ç›‘ç£å¾®è°ƒåï¼Œæ¨¡å‹åœ¨æ˜¾è‘—å‡å°‘å¯¹è‰¯æ€§æŸ¥è¯¢çš„è¿‡åº¦æ‹’ç»çš„åŒæ—¶ï¼Œä¸ä»…æœªæŸå¤±æ•´ä½“å®‰å…¨æ€§ï¼Œä¹Ÿä¿æŒäº†è¯­è¨€ç”Ÿæˆèƒ½åŠ›ï¼Œä¸ºæ•æ„Ÿåœºæ™¯ä¸‹æå‡ LLM å¯ç”¨æ€§æä¾›äº†é¦–ä¸ªç³»ç»ŸåŒ–ã€å¯å¤ç°çš„èµ„æºä¸æ–¹æ³•æ¡†",
                            "en": "Safety alignment approaches in large language models (LLMs) often lead\nto the over-refusal of benign queries, significantly diminishing their utility\nin sensitive scenarios. To address this challenge, we introduce FalseReject,\na comprehensive resource containing 16k seemingly toxic queries accompani"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/falsereject'. Error: Path opencompass/falsereject is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1993",
                    "name": "ViStoryBench",
                    "version": "1.0.0",
                    "description": "ViStoryBench is a benchmark designed to rigorously evaluate the capabilities of multimodal generative models (e.g., diffusion models, LLM-based agents) in synthesizing visually coherent image sequences from textual narratives and reference images.",
                    "url": "opencompass/opencompass_1993.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1993",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Creation"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1993",
                        "name": "ViStoryBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "åˆ›ä½œ",
                                "en": "Creation"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/vistorybench/vistorybench",
                        "paperLink": "https://arxiv.org/abs/2505.24862",
                        "officialWebsiteLink": "https://huggingface.co/datasets/ViStoryBench/ViStoryBench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "46",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-30 14:40:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-30 14:40:55",
                        "createDate": "2025-06-30 14:27:22",
                        "desc": {
                            "cn": "ViStoryBench æ˜¯ä¸€ä¸ªé¢å‘æ•…äº‹å¯è§†åŒ–ä»»åŠ¡çš„ç»¼åˆæ€§è¯„æµ‹åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹è§†é¢‘ç”Ÿæˆæ¨¡å‹ç­‰ï¼‰æ ¹æ®ç»™å®šå™äº‹æ–‡æœ¬å’Œå‚è€ƒå›¾åƒç”Ÿæˆè§†è§‰è¿è´¯ä¸”æƒ…èŠ‚ä¸€è‡´çš„å›¾åƒåºåˆ—çš„èƒ½åŠ›ã€‚",
                            "en": "ViStoryBench is a benchmark designed to rigorously evaluate the capabilities of multimodal generative models (e.g., diffusion models, LLM-based agents) in synthesizing visually coherent image sequences from textual narratives and reference images."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/vistorybench'. Error: Path opencompass/vistorybench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1782",
                    "name": "HypoBench",
                    "version": "1.0.0",
                    "description": "HypoBench, a novel benchmark designed to evaluate LLMs and hypothesis generation methods across multiple aspects, including practical utility, generalizability, and hypothesis discovery rate. HypoBench includes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets. ",
                    "url": "opencompass/opencompass_1782.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1782",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1782",
                        "name": "HypoBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ChicagoHAI/HypoBench",
                        "paperLink": "https://arxiv.org/abs/2504.11524",
                        "officialWebsiteLink": "https://chicagohai.github.io/HypoBench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "46",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-28 11:36:59",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-28 11:36:59",
                        "createDate": "2025-04-28 11:08:05",
                        "desc": {
                            "cn": "HypoBenchï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åŸºå‡†ï¼Œæ—¨åœ¨ä»å¤šä¸ªæ–¹é¢è¯„ä¼° LLM å’Œå‡è®¾ç”Ÿæˆæ–¹æ³•ï¼ŒåŒ…æ‹¬å®ç”¨æ€§ã€æ³›åŒ–æ€§å’Œå‡è®¾å‘ç°ç‡ã€‚HypoBench åŒ…æ‹¬ 7 ä¸ªçœŸå®ä»»åŠ¡å’Œ 5 ä¸ªåˆæˆä»»åŠ¡ï¼Œå…·æœ‰ 194 ä¸ªä¸åŒçš„æ•°æ®é›†ã€‚",
                            "en": "HypoBench, a novel benchmark designed to evaluate LLMs and hypothesis generation methods across multiple aspects, including practical utility, generalizability, and hypothesis discovery rate. HypoBench includes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/hypobench'. Error: Path opencompass/hypobench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1924",
                    "name": "LaMP-QA",
                    "version": "1.0.0",
                    "description": "The benchmark covers questions from three major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal Development, and (3) Society & Culture, encompassing over 45 subcategories in total. ",
                    "url": "opencompass/opencompass_1924.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1924",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1924",
                        "name": "LaMP-QA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/LaMP-Benchmark/LaMP-QA",
                        "paperLink": "https://arxiv.org/abs/2506.00137",
                        "officialWebsiteLink": "https://huggingface.co/datasets/alireza7/LaMP-QA",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "45",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-16 09:51:39",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-16 09:51:39",
                        "createDate": "2025-06-12 11:07:04",
                        "desc": {
                            "cn": "åŸºå‡†æ—¨åœ¨è¯„ä¼°ä¸ªæ€§åŒ–é•¿ç¯‡ç­”æ¡ˆç”Ÿæˆã€‚è¯¥åŸºå‡†æ¶µç›–ä¸‰å¤§ç±»é—®é¢˜:(1)è‰ºæœ¯ä¸å¨±ä¹,(2)ç”Ÿæ´»ä¸ä¸ªäººå‘å±•,(3)ç¤¾ä¼šä¸æ–‡åŒ–,å…±åŒ…å«45ä¸ªä»¥ä¸Šçš„å­ç±»åˆ«ã€‚",
                            "en": "The benchmark covers questions from three major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal Development, and (3) Society & Culture, encompassing over 45 subcategories in total. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/lamp_qa'. Error: Path opencompass/lamp_qa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1983",
                    "name": "ByteMorph",
                    "version": "1.0.0",
                    "description": "ByteMorph is a benchmark for instruction-guided image editing, focusing on evaluating modelsâ€™ capabilities in handling non-rigid motions such as camera viewpoint changes, object deformations, human articulations, and complex interactions. ",
                    "url": "opencompass/opencompass_1983.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1983",
                    "sample_count": 1000,
                    "traits": [
                        "Creation",
                        "Instruct"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1983",
                        "name": "ByteMorph",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "åˆ›ä½œ",
                                "en": "Creation"
                            },
                            {
                                "cn": "æŒ‡ä»¤è·Ÿéš",
                                "en": "Instruct"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ByteDance-Seed/BM-code",
                        "paperLink": "https://arxiv.org/abs/2506.03107",
                        "officialWebsiteLink": "https://huggingface.co/datasets/ByteDance-Seed/BM-Bench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "45",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-30 11:44:45",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-30 11:44:45",
                        "createDate": "2025-06-30 11:44:24",
                        "desc": {
                            "cn": "ByteMorph æ˜¯ä¸€ä¸ªé¢å‘æŒ‡ä»¤é©±åŠ¨å›¾åƒç¼–è¾‘çš„åŸºå‡†ï¼Œä¸“æ³¨äºè¯„ä¼°æ¨¡å‹åœ¨å¤„ç†éåˆšæ€§è¿åŠ¨ï¼ˆå¦‚ç›¸æœºè§†è§’å˜åŒ–ã€ç‰©ä½“å˜å½¢ã€äººç±»åŠ¨ä½œå’Œå¤æ‚äº¤äº’ï¼‰æ–¹é¢çš„èƒ½åŠ›ã€‚ è¯¥åŸºå‡†åŒ…æ‹¬è¶…è¿‡ 600 ä¸‡å¯¹é«˜åˆ†è¾¨ç‡å›¾åƒç¼–è¾‘æ ·æœ¬ï¼Œæ¶µç›–å¤šç§åŠ¨æ€ç¼–è¾‘åœºæ™¯ï¼Œæ”¯æŒå¯¹æ¨¡å‹åœ¨å¤šç§éåˆšæ€§è¿åŠ¨ç±»å‹ä¸‹çš„è¡¨ç°è¿›è¡Œç»†ç²’åº¦è¯„ä¼°ã€‚",
                            "en": "ByteMorph is a benchmark for instruction-guided image editing, focusing on evaluating modelsâ€™ capabilities in handling non-rigid motions such as camera viewpoint changes, object deformations, human articulations, and complex interactions. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/bytemorph'. Error: Path opencompass/bytemorph is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1989",
                    "name": "SWE-Factory",
                    "version": "1.0.0",
                    "description": "SWE-Factory is a benchmark for evaluating large language models on software issue fixing tasks. ",
                    "url": "opencompass/opencompass_1989.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1989",
                    "sample_count": 1000,
                    "traits": [
                        "Code",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1989",
                        "name": "SWE-Factory",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/DeepSoftwareAnalytics/swe-factory",
                        "paperLink": "https://arxiv.org/abs/2506.10954",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "45",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-27 18:13:21",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-27 18:13:21",
                        "createDate": "2025-06-26 16:21:27",
                        "desc": {
                            "cn": "SWE-Factory æ˜¯ä¸€ä¸ªé¢å‘å¤§å‹è¯­è¨€æ¨¡å‹çš„è½¯ä»¶é—®é¢˜ä¿®å¤è¯„æµ‹åŸºå‡†ï¼Œæ—¨åœ¨æå‡æ„å»ºæ•ˆç‡ä¸è¯„ä¼°å‡†ç¡®æ€§ã€‚è¯¥åŸºå‡†é›†æˆå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ SWE-Builder è‡ªåŠ¨æ­å»ºä»»åŠ¡ç¯å¢ƒï¼Œé‡‡ç”¨é€€å‡ºç è‡ªåŠ¨è¯„åˆ†ï¼Œå¹¶é€šè¿‡ fail2pass æµç¨‹éªŒè¯ä¿®å¤æœ‰æ•ˆæ€§ï¼Œç¡®ä¿è¯„æµ‹å¯é ã€‚SWE-Factory è¦†ç›–å››ç§è¯­è¨€å…± 671 ä¸ªé—®é¢˜ï¼Œæ”¯æŒé«˜æ•ˆã€è‡ªåŠ¨åŒ–ã€å¯æ‰©å±•çš„ LLM è¯„ä¼°æµç¨‹ã€‚",
                            "en": "SWE-Factory is a benchmark for evaluating large language models on software issue fixing tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/swe_factory'. Error: Path opencompass/swe_factory is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1938",
                    "name": "WorldGenBench",
                    "version": "1.0.0",
                    "description": "Evaluation dataset: WorldGenBench",
                    "url": "opencompass/opencompass_1938.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1938",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1938",
                        "name": "WorldGenBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2505.01490",
                        "officialWebsiteLink": "https://dwanzhang-ai.github.io/WorldGenBench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "38703035",
                            "name": "JiangC1233",
                            "avatar": null,
                            "nickname": "JiangC1233"
                        },
                        "lookNum": "44",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-20 15:08:20",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-20 15:08:20",
                        "createDate": "2025-06-16 13:51:32",
                        "desc": {
                            "cn": null,
                            "en": null
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/worldgenbench'. Error: Path opencompass/worldgenbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1990",
                    "name": "OPT-BENCH",
                    "version": "1.0.0",
                    "description": "OPT-BENCH is a comprehensive benchmark designed to evaluate large language model (LLM) agents on large-scale search space optimization problems, focusing on their iterative reasoning and problem-solving capabilities.",
                    "url": "opencompass/opencompass_1990.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1990",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1990",
                        "name": "OPT-BENCH",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OliverLeeXZ/OPT-BENCH",
                        "paperLink": "https://arxiv.org/abs/2506.10764",
                        "officialWebsiteLink": "https://huggingface.co/datasets/OPT-Bench/OPT-Bench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "44",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-01 09:44:01",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-01 09:44:01",
                        "createDate": "2025-07-01 09:43:53",
                        "desc": {
                            "cn": "OPT-BENCH æ˜¯ä¸€ä¸ªé¢å‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™ºèƒ½ä½“çš„å¤§è§„æ¨¡æœç´¢ç©ºé—´ä¼˜åŒ–è¯„æµ‹åŸºå‡†ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°æ¨¡å‹åœ¨è¿­ä»£æ¨ç†å’Œè§£å†³å¤æ‚ä¼˜åŒ–é—®é¢˜ä¸­çš„èƒ½åŠ›ã€‚ è¯¥åŸºå‡†åŒ…å« 30 ä¸ªä»»åŠ¡ï¼ŒåŒ…æ‹¬ 20 ä¸ªæ¥è‡ª Kaggle çš„çœŸå®æœºå™¨å­¦ä¹ ä»»åŠ¡å’Œ 10 ä¸ªç»å…¸ NP é—®é¢˜ï¼Œæ¶µç›–é¢„æµ‹å»ºæ¨¡ã€å›¾è®ºå’Œç»„åˆä¼˜åŒ–ç­‰é¢†åŸŸã€‚",
                            "en": "OPT-BENCH is a comprehensive benchmark designed to evaluate large language model (LLM) agents on large-scale search space optimization problems, focusing on their iterative reasoning and problem-solving capabilities."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/opt_bench'. Error: Path opencompass/opt_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2126",
                    "name": "KMMLU-Redux",
                    "version": "1.0.0",
                    "description": "KMMLU-Redux is a reconstructed version of the existing KMMLU, comprising 2,587 problems from Korean National Technical Qualification (KNTQ) exams. ",
                    "url": "opencompass/opencompass_2126.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2126",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2126",
                        "name": "KMMLU-Redux",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2507.08924",
                        "officialWebsiteLink": "https://huggingface.co/datasets/LGAI-EXAONE/KMMLU-Redux",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "å¼ å¯Œæ‰"
                        },
                        "lookNum": "42",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-28 10:05:11",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-28 10:05:11",
                        "createDate": "2025-07-28 09:45:22",
                        "desc": {
                            "cn": "KMMLU-Reduxæ˜¯ç°æœ‰ KMMLU çš„ä¸€ä¸ªé‡å»ºç‰ˆæœ¬ï¼ŒåŒ…å«æ¥è‡ªéŸ©å›½å›½å®¶æŠ€æœ¯èµ„æ ¼ï¼ˆKNTQï¼‰è€ƒè¯•çš„ 2,587 ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬å‘ç°äº† KMMLU ä¸­çš„ä¸€äº›å…³é”®é—®é¢˜ï¼ŒåŒ…æ‹¬æ³„éœ²çš„ç­”æ¡ˆã€ç¼ºä¹æ¸…æ™°åº¦ã€é—®é¢˜è¡¨è¿°ä¸å½“ã€ç¬¦å·é”™è¯¯å’Œæ±¡æŸ“é£é™©ã€‚",
                            "en": "KMMLU-Redux is a reconstructed version of the existing KMMLU, comprising 2,587 problems from Korean National Technical Qualification (KNTQ) exams. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/kmmlu_redux'. Error: Path opencompass/kmmlu_redux is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1900",
                    "name": "VideoReasonBench",
                    "version": "1.0.0",
                    "description": "VideoReasonBench is designed to evaluate vision-centric complex video reasoning.",
                    "url": "opencompass/opencompass_1900.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1900",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1900",
                        "name": "VideoReasonBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¼ºæ¨ç†",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "video reasoning",
                                "en": "video reasoning"
                            },
                            {
                                "cn": "MLLMs",
                                "en": "MLLMs"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/llyx97/video_reason_bench?tab=readme-ov-file",
                        "paperLink": "https://huggingface.co/papers/2505.23359",
                        "officialWebsiteLink": "https://llyx97.github.io/video_reason_bench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "53003130",
                            "name": null,
                            "avatar": null,
                            "nickname": "llyx97"
                        },
                        "lookNum": "42",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-06 14:46:13",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-06 14:46:13",
                        "createDate": "2025-06-05 11:49:20",
                        "desc": {
                            "cn": "VideoReasonBenchæ˜¯ä¸€ä¸ªç”¨äºè¯„æµ‹è§†è§‰ä¸ºä¸­å¿ƒã€å¤æ‚è§†é¢‘æ¨ç†çš„åŸºå‡†ã€‚",
                            "en": "VideoReasonBench is designed to evaluate vision-centric complex video reasoning."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/videoreasonbench'. Error: Path opencompass/videoreasonbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1920",
                    "name": "MedBookVQA",
                    "version": "1.0.0",
                    "description": "MedBookVQA is a medical visual question answering (VQA) benchmark constructed from open-access medical textbooks. It includes 5,000 questions across five clinical task types and is hierarchically organized by imaging modality, anatomical structure, and clinical specialty.",
                    "url": "opencompass/opencompass_1920.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1920",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1920",
                        "name": "MedBookVQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Medical",
                                "en": "Medical"
                            },
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            },
                            {
                                "cn": "MLLM",
                                "en": "MLLM"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/slyipae1/MedBookVQA",
                        "paperLink": "https://arxiv.org/abs/2506.00855",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50213650",
                            "name": null,
                            "avatar": null,
                            "nickname": "slyipae1"
                        },
                        "lookNum": "42",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-16 09:51:52",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-16 09:51:52",
                        "createDate": "2025-06-10 16:22:39",
                        "desc": {
                            "cn": "MedBookVQA æ˜¯ä¸€ä¸ªåŸºäºå¼€æ”¾è·å–åŒ»å­¦æ•™ç§‘ä¹¦æ„å»ºçš„åŒ»å­¦è§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æ•°æ®é›†ã€‚å®ƒåŒ…å« 5,000 ä¸ªé—®é¢˜ï¼Œæ¶µç›–äº”ç§ä¸´åºŠä»»åŠ¡ç±»å‹ï¼Œå¹¶æŒ‰ç…§å½±åƒæ¨¡æ€ã€è§£å‰–ç»“æ„å’Œä¸´åºŠä¸“ç§‘è¿›è¡Œåˆ†å±‚ç»„ç»‡ã€‚",
                            "en": "MedBookVQA is a medical visual question answering (VQA) benchmark constructed from open-access medical textbooks. It includes 5,000 questions across five clinical task types and is hierarchically organized by imaging modality, anatomical structure, and clinical specialty."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medbookvqa'. Error: Path opencompass/medbookvqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2081",
                    "name": "General-Bench",
                    "version": "1.0.0",
                    "description": "General Bench is a set of universal evaluation benchmarks for multimodal large models, covering language, image, video, audio, and 3D five modalities, with a total of 145 skills, over 700 tasks, and 325800 samples.",
                    "url": "opencompass/opencompass_2081.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2081",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2081",
                        "name": "General-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/path2generalist/General-Level",
                        "paperLink": "https://icml.cc/virtual/2025/poster/45047",
                        "officialWebsiteLink": "https://generalist.top/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "41",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:28",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:28",
                        "createDate": "2025-07-22 16:50:17",
                        "desc": {
                            "cn": "General-Bench æ˜¯ä¸€å¥—é¢å‘å¤šæ¨¡æ€å¤§æ¨¡å‹çš„é€šç”¨è¯„æµ‹åŸºå‡†ï¼Œæ¶µç›–è¯­è¨€ã€å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘å’Œ 3D äº”å¤§æ¨¡æ€ï¼Œå…±è®¡ 145 é¡¹æŠ€èƒ½ã€700 ä½™ä¸ªä»»åŠ¡ï¼ŒåŒ…å« 325 800 æ¡æ ·æœ¬ã€‚",
                            "en": "General Bench is a set of universal evaluation benchmarks for multimodal large models, covering language, image, video, audio, and 3D five modalities, with a total of 145 skills, over 700 tasks, and 325800 samples."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/general_bench'. Error: Path opencompass/general_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1892",
                    "name": "ER-Reason",
                    "version": "1.0.0",
                    "description": "ER-Reason is a large-scale benchmark suite for evaluating the clinical reasoning capabilities of large language models (LLMs) in the emergency room (ER) â€” a high-stakes environment where clinicians make rapid, life-critical decisions.",
                    "url": "opencompass/opencompass_1892.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1892",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1892",
                        "name": "ER-Reason",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Medical",
                                "en": "Medical"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AlaaLab/ER-Reason",
                        "paperLink": "https://arxiv.org/pdf/2505.22919",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "41",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-05 10:00:58",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-05 10:00:58",
                        "createDate": "2025-06-04 17:53:21",
                        "desc": {
                            "cn": "ER-Reason æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†å¥—ä»¶ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ€¥è¯Šå®¤ï¼ˆERï¼‰ä¸­çš„ä¸´åºŠæ¨ç†èƒ½åŠ›ã€‚æ€¥è¯Šå®¤æ˜¯ä¸€ä¸ªé«˜é£é™©ç¯å¢ƒï¼Œä¸´åºŠåŒ»ç”Ÿéœ€è¦å¿«é€Ÿåšå‡ºå…³ä¹ç”Ÿå‘½çš„å…³é”®å†³ç­–ã€‚",
                            "en": "ER-Reason is a large-scale benchmark suite for evaluating the clinical reasoning capabilities of large language models (LLMs) in the emergency room (ER) â€” a high-stakes environment where clinicians make rapid, life-critical decisions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/er_reason'. Error: Path opencompass/er_reason is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1893",
                    "name": "MEDAL",
                    "version": "1.0.0",
                    "description": "MEDAL is a framework for generating and evaluating multilingual open-domain chatbots and their evaluators. This framework supports various language models and provides a structured approach to creating conversational datasets.\n\n",
                    "url": "opencompass/opencompass_1893.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1893",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1893",
                        "name": "MEDAL",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/johndmendonca/medal",
                        "paperLink": "https://arxiv.org/pdf/2505.22777",
                        "officialWebsiteLink": "https://huggingface.co/datasets/Johndfm/medal",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "41",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-25 15:30:50",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-25 15:30:50",
                        "createDate": "2025-06-25 15:30:29",
                        "desc": {
                            "cn": "MEDALæ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆå’Œè¯„ä¼°å¤šè¯­è¨€å¼€æ”¾åŸŸèŠå¤©æœºå™¨äººåŠå…¶è¯„ä¼°å™¨çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ”¯æŒå„ç§è¯­è¨€æ¨¡å‹,å¹¶æä¾›äº†ä¸€ç§ç»“æ„åŒ–çš„æ–¹æ³•æ¥åˆ›å»ºå¯¹è¯æ•°æ®é›†ã€‚",
                            "en": "MEDAL is a framework for generating and evaluating multilingual open-domain chatbots and their evaluators. This framework supports various language models and provides a structured approach to creating conversational datasets.\n\n"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medal'. Error: Path opencompass/medal is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1943",
                    "name": "CombiBench",
                    "version": "1.0.0",
                    "description": "We introduce CombiBench, a comprehensive benchmark comprising 100 combinatorial problems, each formalized in Lean4 and paired with its corresponding informal statement. The problem set covers a wide spectrum of difficulty levels, ranging from middle school to IMO and university level.",
                    "url": "opencompass/opencompass_1943.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1943",
                    "sample_count": 1000,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1943",
                        "name": "CombiBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ•°å­¦",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Combinatorics",
                                "en": "Combinatorics"
                            },
                            {
                                "cn": "Lean4",
                                "en": "Lean4"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MoonshotAI/CombiBench",
                        "paperLink": "https://arxiv.org/abs/2505.03171",
                        "officialWebsiteLink": "https://moonshotai.github.io/CombiBench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "38702983",
                            "name": null,
                            "avatar": null,
                            "nickname": "192016"
                        },
                        "lookNum": "41",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-20 15:11:44",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-20 15:11:44",
                        "createDate": "2025-06-16 18:39:52",
                        "desc": {
                            "cn": "CombiBenchæ˜¯ä¸€ä¸ªåŒ…å«100ä¸ªç»„åˆé—®é¢˜çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œæ¯ä¸ªé—®é¢˜éƒ½ç”¨Lean4è¿›è¡Œäº†å½¢å¼åŒ–ï¼Œå¹¶é™„æœ‰å…¶å¯¹åº”çš„éå½¢å¼åŒ–è¡¨è¿°ã€‚è¿™äº›é—®é¢˜æ¶µç›–äº†ä»ä¸­å­¦ç”Ÿåˆ°å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ç«èµ›ï¼ˆIMOï¼‰ä»¥åŠå¤§å­¦æ°´å¹³çš„å¹¿æ³›éš¾åº¦èŒƒå›´ã€‚",
                            "en": "We introduce CombiBench, a comprehensive benchmark comprising 100 combinatorial problems, each formalized in Lean4 and paired with its corresponding informal statement. The problem set covers a wide spectrum of difficulty levels, ranging from middle school to IMO and university level."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/combibench'. Error: Path opencompass/combibench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1995",
                    "name": "AIRTBench",
                    "version": "1.0.0",
                    "description": "AIRTBench is a benchmark designed to evaluate large language models (LLMs) on their autonomous AI red teaming capabilities.",
                    "url": "opencompass/opencompass_1995.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1995",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1995",
                        "name": "AIRTBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/dreadnode/AIRTBench-Code",
                        "paperLink": "https://arxiv.org/abs/2506.14682",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "41",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-30 10:06:16",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-30 10:06:16",
                        "createDate": "2025-06-27 19:55:37",
                        "desc": {
                            "cn": "AIRTBench æ˜¯ä¸€ä¸ªä¸“ä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨â€œçº¢é˜Ÿâ€å®‰å…¨ä»»åŠ¡ä¸­çš„è‡ªä¸»æ”»å‡»èƒ½åŠ›è€Œè®¾è®¡çš„è¯„æµ‹åŸºå‡†ã€‚æœ¬åŸºå‡†åŒ…å« 70 ä¸ªé»‘ç›’ CTFï¼ˆå¤ºæ——èµ›ï¼‰æŒ‘æˆ˜ï¼Œæ¨¡æ‹ŸçœŸå® AI/ML ç³»ç»Ÿæ¼æ´ç¯å¢ƒï¼Œè¦æ±‚æ¨¡å‹ç‹¬ç«‹ç¼–å†™ Python ä»£ç è¿›è¡Œæ¼æ´å‘ç°ã€åˆ©ç”¨ä¸å¤ºæ——æ“ä½œï¼Œä½“ç°å…¶è®¡åˆ’ã€æ¨ç†ä¸ç³»ç»Ÿæ“æ§ç­‰ç»¼åˆèƒ½åŠ›ã€‚",
                            "en": "AIRTBench is a benchmark designed to evaluate large language models (LLMs) on their autonomous AI red teaming capabilities."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/airtbench'. Error: Path opencompass/airtbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2088",
                    "name": "LMAct",
                    "version": "1.0.0",
                    "description": "LMAct is a benchmark for evaluating frontier multimodal models' in-context imitation learning capabilities in long contexts. ",
                    "url": "opencompass/opencompass_2088.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2088",
                    "sample_count": 1000,
                    "traits": [
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2088",
                        "name": "LMAct",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "é•¿æ–‡æœ¬",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/google-deepmind/lm_act",
                        "paperLink": "https://icml.cc/virtual/2025/poster/46274",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "40",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:25",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:25",
                        "createDate": "2025-07-22 16:47:51",
                        "desc": {
                            "cn": "LMActæ˜¯è¯„ä¼°å‰æ²¿å¤šæ¨¡æ€æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­çš„ä¸Šä¸‹æ–‡æ¨¡ä»¿å­¦ä¹ èƒ½åŠ›çš„åŸºå‡†ã€‚å®ƒè¯„ä¼°äº†è¯¸å¦‚äº•å­—æ£‹ã€å›½é™…è±¡æ£‹å’Œé›…è¾¾åˆ©ç­‰äº¤äº’å¼ä»»åŠ¡çš„å¤šæ¨¡å¼å†³ç­–ã€‚è¯¥æµ‹è¯•é›†åŒ…å«å¤šè¾¾100ä¸‡ä¸ªä»¤ç‰Œä¸Šä¸‹æ–‡å’Œ512ä¸ªä¸“å®¶æ¼”ç¤ºé›†ã€‚",
                            "en": "LMAct is a benchmark for evaluating frontier multimodal models' in-context imitation learning capabilities in long contexts. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/lmact'. Error: Path opencompass/lmact is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2128",
                    "name": "lm-evaluation-harness",
                    "version": "1.0.0",
                    "description": "AI Language Proficiency Monitor is a multilingual benchmark platform that systematically assesses LLM performance across up to 200 languages.",
                    "url": "opencompass/opencompass_2128.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2128",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2128",
                        "name": "lm-evaluation-harness",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OpenGPTX/lm-evaluation-harness",
                        "paperLink": "https://arxiv.org/abs/2507.08538",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "å¼ å¯Œæ‰"
                        },
                        "lookNum": "40",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-28 10:05:17",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-28 10:05:17",
                        "createDate": "2025-07-28 09:58:05",
                        "desc": {
                            "cn": "AI Language Proficiency Monitoræ˜¯ä¸€ä¸ªå¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹è¯„æµ‹å¹³å°ï¼Œç³»ç»Ÿæ€§è¯„ä¼°æ¨¡å‹åœ¨å¤šè¾¾200ç§è¯­è¨€ä¸Šçš„æ€§èƒ½è¡¨ç°ï¼Œç‰¹åˆ«å…³æ³¨ä½èµ„æºè¯­è¨€ï¼Œæ•´åˆFLORES+ã€MMLUã€GSM8Kã€TruthfulQAå’ŒARCç­‰æ•°æ®é›†è¯„æµ‹ç¿»è¯‘ã€é—®ç­”ã€æ•°å­¦æ¨ç†å’Œäº‹å®æ€§ç­‰èƒ½åŠ›ï¼Œæä¾›å¼€æºè‡ªåŠ¨æ›´æ–°æ’è¡Œæ¦œå’Œäº¤äº’å¼ä»ªè¡¨æ¿ï¼Œè¦†ç›–å…¨çƒ80-95%äººå£ä½¿ç”¨çš„è¯­è¨€ã€‚",
                            "en": "AI Language Proficiency Monitor is a multilingual benchmark platform that systematically assesses LLM performance across up to 200 languages."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/lm_evaluation_harness'. Error: Path opencompass/lm_evaluation_harness is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1876",
                    "name": "Aneumo",
                    "version": "1.0.0",
                    "description": "Based on 427 real aneurysm geometries, we synthesized 10,660 3D shapes via controlled deformation to simulate aneurysm evolution. CFD computations were performed on each shape under eight steady-state mass flow conditions, generating a total of 85,280 blood flow dynamics data covering key parameters",
                    "url": "opencompass/opencompass_1876.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1876",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1876",
                        "name": "Aneumo",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Medical Imagery",
                                "en": "Medical Imagery"
                            },
                            {
                                "cn": "Aneurysm",
                                "en": "Aneurysm"
                            },
                            {
                                "cn": "Computational Fluid Dynamics",
                                "en": "Computational Fluid Dynamics"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Xigui-Li/Aneumo",
                        "paperLink": "https://arxiv.org/pdf/2505.14717",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "52305425",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-nkWkVBdJ1"
                        },
                        "lookNum": "40",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-03 19:48:15",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-03 19:48:15",
                        "createDate": "2025-06-03 19:25:50",
                        "desc": {
                            "cn": "Based on 427 real aneurysm geometries, we synthesized 10,660 3D shapes via controlled deformation to simulate aneurysm evolution. CFD computations were performed on each shape under eight steady-state mass flow conditions, generating a total of 85,280 blood flow dynamics data covering key parameters",
                            "en": "Based on 427 real aneurysm geometries, we synthesized 10,660 3D shapes via controlled deformation to simulate aneurysm evolution. CFD computations were performed on each shape under eight steady-state mass flow conditions, generating a total of 85,280 blood flow dynamics data covering key parameters"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/aneumo'. Error: Path opencompass/aneumo is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1966",
                    "name": "AMSbench",
                    "version": "1.0.0",
                    "description": "AMSbench is a benchmark of ~8000 questions to evaluate multi-modal LLMs on analog/mixed-signal circuit tasks like schematic recognition, analysis, and design. ",
                    "url": "opencompass/opencompass_1966.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1966",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding",
                        "Creation"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1966",
                        "name": "AMSbench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            },
                            {
                                "cn": "åˆ›ä½œ",
                                "en": "Creation"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "AMS",
                                "en": "AMS"
                            },
                            {
                                "cn": "Circuit",
                                "en": "Circuit"
                            },
                            {
                                "cn": "EDA",
                                "en": "EDA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Why0912/AMSBench",
                        "paperLink": "https://arxiv.org/abs/2505.24138",
                        "officialWebsiteLink": "https://amsbench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "85502520",
                            "name": null,
                            "avatar": null,
                            "nickname": "è„†çš®çƒ¤åœŸè±†"
                        },
                        "lookNum": "40",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-23 10:04:59",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-23 10:04:59",
                        "createDate": "2025-06-21 16:15:54",
                        "desc": {
                            "cn": "AMSbench æ˜¯ä¸€ä¸ªåŒ…å«çº¦8000é“é¢˜ç›®çš„åŸºå‡†æµ‹è¯•é›†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨¡æ‹Ÿ/æ··åˆä¿¡å·ç”µè·¯ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬è¯†å›¾ã€åˆ†æä¸è®¾è®¡ã€‚",
                            "en": "AMSbench is a benchmark of ~8000 questions to evaluate multi-modal LLMs on analog/mixed-signal circuit tasks like schematic recognition, analysis, and design. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/amsbench'. Error: Path opencompass/amsbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2089",
                    "name": "LaRA",
                    "version": "1.0.0",
                    "description": "LaRA is a focused benchmark for testing Retrieval-Augmented Generation (RAG) and long-context LLMs. ",
                    "url": "opencompass/opencompass_2089.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2089",
                    "sample_count": 1000,
                    "traits": [
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2089",
                        "name": "LaRA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "é•¿æ–‡æœ¬",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Alibaba-NLP/LaRA",
                        "paperLink": "https://icml.cc/virtual/2025/poster/46069",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "39",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:14",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:14",
                        "createDate": "2025-07-22 16:47:29",
                        "desc": {
                            "cn": "LaRA æ˜¯ä¸“ä¸ºè¯„ä¼°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œé•¿ä¸Šä¸‹æ–‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è€Œæ‰“é€ çš„åŸºå‡†ã€‚å®ƒå›´ç»•ä¿¡æ¯å®šä½ã€ç‰‡æ®µå¯¹æ¯”ã€å†…å®¹æ¨ç†å’Œå¹»è§‰æ£€æµ‹å››å¤§èƒ½åŠ›ï¼Œé€šè¿‡ 2 326 æ¡æµ‹è¯•ç”¨ä¾‹ï¼Œå¯¹å››ç±»é—®ç­”ä»»åŠ¡å’Œä¸‰ç§é•¿æ–‡æœ¬åœºæ™¯ï¼ˆå°è¯´ã€å­¦æœ¯è®ºæ–‡ã€è´¢åŠ¡æŠ¥è¡¨ï¼‰è¿›è¡Œå…¨é¢æµ‹è¯„ã€‚",
                            "en": "LaRA is a focused benchmark for testing Retrieval-Augmented Generation (RAG) and long-context LLMs. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/lara'. Error: Path opencompass/lara is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1976",
                    "name": "IntPhys2",
                    "version": "1.0.0",
                    "description": " IntPhys2 tests intuitive physicsâ€”permanence, immutability, continuity and solidityâ€”using synthetic videos. SOTA models perform around chance (~50 %), far below human level.",
                    "url": "opencompass/opencompass_1976.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1976",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1976",
                        "name": "IntPhys2",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "è§†é¢‘ç”Ÿæˆ",
                                "en": "è§†é¢‘ç”Ÿæˆ"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/facebookresearch/IntPhys2",
                        "paperLink": "https://arxiv.org/abs/2506.09849",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "38",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-25 15:30:59",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-25 15:30:59",
                        "createDate": "2025-06-25 10:34:04",
                        "desc": {
                            "cn": "IntPhys 2ï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°æ·±åº¦å­¦ä¹ æ¨¡å‹ç›´è§‚ç‰©ç†ç†è§£èƒ½åŠ›çš„è§†é¢‘åŸºå‡†ã€‚å®ƒå›´ç»•æ°¸æ’æ€§ã€ä¸å¯å˜æ€§ã€æ—¶ç©ºè¿ç»­æ€§å’Œå®ä½“æ€§å››ä¸ªæ ¸å¿ƒåŸåˆ™ï¼Œæµ‹è¯•æ¨¡å‹åŒºåˆ†å¯èƒ½ä¸ä¸å¯èƒ½äº‹ä»¶çš„èƒ½åŠ›ã€‚",
                            "en": " IntPhys2 tests intuitive physicsâ€”permanence, immutability, continuity and solidityâ€”using synthetic videos. SOTA models perform around chance (~50 %), far below human level."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/intphys2'. Error: Path opencompass/intphys2 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2045",
                    "name": "DRAGON",
                    "version": "1.0.0",
                    "description": "DRAGON æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨ä¿„è¯­æ–°é—»è¯­å¢ƒä¸­äº‹å®æ€§ä¸æ£€ç´¢èƒ½åŠ›çš„åŠ¨æ€åŸºå‡†ï¼Œæ”¯æŒå¯¹æ£€ç´¢å™¨ä¸ç”Ÿæˆå™¨ç»„ä»¶çš„å…¨é¢è¯„ä¼°ã€‚",
                    "url": "opencompass/opencompass_2045.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2045",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2045",
                        "name": "DRAGON",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "RAG",
                                "en": "RAG"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/RussianNLP/DRAGON",
                        "paperLink": "https://arxiv.org/abs/2507.05713",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "38",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-14 09:39:22",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-14 09:39:22",
                        "createDate": "2025-07-11 11:00:03",
                        "desc": {
                            "cn": "DRAGON is a dynamic benchmark for evaluating Retrieval-Augmented Generation (RAG) systems in Russian news contexts, supporting comprehensive assessment of both retriever and generator components. ",
                            "en": "DRAGON æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨ä¿„è¯­æ–°é—»è¯­å¢ƒä¸­äº‹å®æ€§ä¸æ£€ç´¢èƒ½åŠ›çš„åŠ¨æ€åŸºå‡†ï¼Œæ”¯æŒå¯¹æ£€ç´¢å™¨ä¸ç”Ÿæˆå™¨ç»„ä»¶çš„å…¨é¢è¯„ä¼°ã€‚"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/dragon'. Error: Path opencompass/dragon is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2061",
                    "name": "AgentHazard",
                    "version": "1.0.0",
                    "description": "ç§»åŠ¨ç«¯ GUI Agent é€šè¿‡ä¸è®¾å¤‡ç¯å¢ƒçš„äº¤äº’æ¥å®Œæˆä»»åŠ¡ï¼Œåœ¨å®Œæˆä»»åŠ¡çš„è¿‡ç¨‹ä¸­ä¼šé‡åˆ°ä¸€äº›æœªçŸ¥æˆ–ä¸å¯ä¿¡çš„ä¿¡æ¯æ¥æºï¼Œè¿™äº›ä¿¡æ¯å¯èƒ½å«æœ‰æ”»å‡»æ€§çš„å†…å®¹ï¼Œè‡´ä½¿ Agent æ— æ³•æ­£å¸¸å®Œæˆä»»åŠ¡ï¼Œç”šè‡³å¯¹ç”¨æˆ·çš„éšç§å’Œè´¢äº§å¸¦æ¥å±å®³ã€‚æœ¬è¯„æµ‹é›†å…¼å…·åŠ¨æ€æ‰§è¡Œç¯å¢ƒå’Œé™æ€è¯„æµ‹æ•°æ®é›†ï¼Œæ—¨åœ¨ä¸ºç§»åŠ¨ç«¯ GUI Agent æä¾›ä¸€ä¸ªä»¿çœŸåº¦é«˜çš„æ¨¡æ‹Ÿç¯å¢ƒï¼Œä»¥è¯„ä¼°å…¶åœ¨çœŸå®åœºæ™¯ä¸‹æ‰§è¡Œçš„è¡Œä¸ºå’Œå®‰å…¨æ€§ã€‚",
                    "url": "opencompass/opencompass_2061.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2061",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Safety",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2061",
                        "name": "AgentHazard",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Zsbyqx20/AgentHazard",
                        "paperLink": "https://arxiv.org/abs/2507.04227",
                        "officialWebsiteLink": "https://agenthazard.github.io",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "20905480",
                            "name": null,
                            "avatar": null,
                            "nickname": "Zsbyqx20"
                        },
                        "lookNum": "37",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-17 09:29:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-17 09:29:55",
                        "createDate": "2025-07-16 22:06:35",
                        "desc": {
                            "cn": "ç§»åŠ¨ç«¯ GUI Agent é€šè¿‡ä¸è®¾å¤‡ç¯å¢ƒçš„äº¤äº’æ¥å®Œæˆä»»åŠ¡ï¼Œåœ¨å®Œæˆä»»åŠ¡çš„è¿‡ç¨‹ä¸­ä¼šé‡åˆ°ä¸€äº›æœªçŸ¥æˆ–ä¸å¯ä¿¡çš„ä¿¡æ¯æ¥æºï¼Œè¿™äº›ä¿¡æ¯å¯èƒ½å«æœ‰æ”»å‡»æ€§çš„å†…å®¹ï¼Œè‡´ä½¿ Agent æ— æ³•æ­£å¸¸å®Œæˆä»»åŠ¡ï¼Œç”šè‡³å¯¹ç”¨æˆ·çš„éšç§å’Œè´¢äº§å¸¦æ¥å±å®³ã€‚æœ¬è¯„æµ‹é›†å…¼å…·åŠ¨æ€æ‰§è¡Œç¯å¢ƒå’Œé™æ€è¯„æµ‹æ•°æ®é›†ï¼Œæ—¨åœ¨ä¸ºç§»åŠ¨ç«¯ GUI Agent æä¾›ä¸€ä¸ªä»¿çœŸåº¦é«˜çš„æ¨¡æ‹Ÿç¯å¢ƒï¼Œä»¥è¯„ä¼°å…¶åœ¨çœŸå®åœºæ™¯ä¸‹æ‰§è¡Œçš„è¡Œä¸ºå’Œå®‰å…¨æ€§ã€‚",
                            "en": "ç§»åŠ¨ç«¯ GUI Agent é€šè¿‡ä¸è®¾å¤‡ç¯å¢ƒçš„äº¤äº’æ¥å®Œæˆä»»åŠ¡ï¼Œåœ¨å®Œæˆä»»åŠ¡çš„è¿‡ç¨‹ä¸­ä¼šé‡åˆ°ä¸€äº›æœªçŸ¥æˆ–ä¸å¯ä¿¡çš„ä¿¡æ¯æ¥æºï¼Œè¿™äº›ä¿¡æ¯å¯èƒ½å«æœ‰æ”»å‡»æ€§çš„å†…å®¹ï¼Œè‡´ä½¿ Agent æ— æ³•æ­£å¸¸å®Œæˆä»»åŠ¡ï¼Œç”šè‡³å¯¹ç”¨æˆ·çš„éšç§å’Œè´¢äº§å¸¦æ¥å±å®³ã€‚æœ¬è¯„æµ‹é›†å…¼å…·åŠ¨æ€æ‰§è¡Œç¯å¢ƒå’Œé™æ€è¯„æµ‹æ•°æ®é›†ï¼Œæ—¨åœ¨ä¸ºç§»åŠ¨ç«¯ GUI Agent æä¾›ä¸€ä¸ªä»¿çœŸåº¦é«˜çš„æ¨¡æ‹Ÿç¯å¢ƒï¼Œä»¥è¯„ä¼°å…¶åœ¨çœŸå®åœºæ™¯ä¸‹æ‰§è¡Œçš„è¡Œä¸ºå’Œå®‰å…¨æ€§ã€‚"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/agenthazard'. Error: Path opencompass/agenthazard is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2083",
                    "name": "SAEBench",
                    "version": "1.0.0",
                    "description": "SAEBench is a comprehensive benchmark designed to evaluate and compare the performance of language model sparse autoencoders (SAEs). This benchmark provides over 200 SAE models covering seven architectures for systematic comparison, and has open-source code and models.",
                    "url": "opencompass/opencompass_2083.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2083",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2083",
                        "name": "SAEBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/adamkarvonen/SAEBench",
                        "paperLink": "https://icml.cc/virtual/2025/poster/43918",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "37",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:07",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:07",
                        "createDate": "2025-07-22 16:49:23",
                        "desc": {
                            "cn": "SAEBench æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å’Œæ¯”è¾ƒè¯­è¨€æ¨¡å‹ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEsï¼‰æ€§èƒ½çš„ç»¼åˆæ€§åŸºå‡†ã€‚è¯¥åŸºå‡†æä¾›è¶…è¿‡200ä¸ªSAEæ¨¡å‹ï¼Œæ¶µç›–ä¸ƒç§æ¶æ„ï¼Œä»¥å®ç°ç³»ç»Ÿæ€§æ¯”è¾ƒï¼Œå¹¶ä¸”å¼€æºäº†ä»£ç å’Œæ¨¡å‹ã€‚",
                            "en": "SAEBench is a comprehensive benchmark designed to evaluate and compare the performance of language model sparse autoencoders (SAEs). This benchmark provides over 200 SAE models covering seven architectures for systematic comparison, and has open-source code and models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/saebench'. Error: Path opencompass/saebench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2086",
                    "name": "OR-Bench",
                    "version": "1.0.0",
                    "description": "OR Bench is a large-scale benchmark designed to evaluate the excessive rejection behavior of large language models.",
                    "url": "opencompass/opencompass_2086.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2086",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2086",
                        "name": "OR-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/justincui03/or-bench",
                        "paperLink": "https://icml.cc/virtual/2025/poster/46052",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "37",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:40",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:40",
                        "createDate": "2025-07-22 16:48:34",
                        "desc": {
                            "cn": "OR-Bench æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹è¿‡åº¦æ‹’ç»è¡Œä¸ºçš„å¤§è§„æ¨¡åŸºå‡†ã€‚å®ƒè¡¡é‡LLMåœ¨è¿‡åº¦æ‹’ç»å’Œæœ‰å®³æç¤ºæ‹’ç»æ–¹é¢çš„è¡¨ç°ï¼Œæ¶µç›–æš´åŠ›ã€éšç§ç­‰10ä¸ªç±»åˆ«ã€‚åŸºå‡†åŒ…å«8ä¸‡ä¸ªã€1åƒä¸ªå›°éš¾åŠ6ç™¾ä¸ªæœ‰å®³æç¤ºï¼Œé€šè¿‡Mixtralç­‰å·¥å…·è‡ªåŠ¨åŒ–ç”Ÿæˆã€‚å®ƒä¸ºæœªæ¥å®‰å…¨å¯¹é½ç ”ç©¶æä¾›å¼ºå¤§æµ‹è¯•å¹³å°ã€‚",
                            "en": "OR Bench is a large-scale benchmark designed to evaluate the excessive rejection behavior of large language models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/or_bench'. Error: Path opencompass/or_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1875",
                    "name": "VIBE",
                    "version": "1.0.0",
                    "description": "SVRPBench is an open and extensible benchmark for the Stochastic Vehicle Routing Problem (SVRP). It includes 500+ instances spanning small to large scales (10â€“1000 customers), designed to evaluate algorithms under realistic urban logistics conditions with uncertainty and operational constraints.",
                    "url": "opencompass/opencompass_1875.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1875",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1875",
                        "name": "VIBE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/yehias21/vrp-benchmarks",
                        "paperLink": "https://arxiv.org/pdf/2505.21887",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "37",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-03 20:02:21",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-03 20:02:21",
                        "createDate": "2025-06-03 18:28:29",
                        "desc": {
                            "cn": "SVRPBenchæ˜¯ä¸€ä¸ªé’ˆå¯¹éšæœºè½¦è¾†è·¯å¾„é—®é¢˜ï¼ˆSVRPï¼‰çš„å¼€æ”¾ä¸”å¯æ‰©å±•çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚å®ƒåŒ…å«500å¤šä¸ªå®ä¾‹ï¼Œæ¶µç›–å°åˆ°å¤§è§„æ¨¡ï¼ˆ10-1000ä¸ªå®¢æˆ·ï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°ç®—æ³•åœ¨å…·æœ‰ä¸ç¡®å®šæ€§å’Œæ“ä½œçº¦æŸçš„ç°å®åŸå¸‚ç‰©æµæ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚",
                            "en": "SVRPBench is an open and extensible benchmark for the Stochastic Vehicle Routing Problem (SVRP). It includes 500+ instances spanning small to large scales (10â€“1000 customers), designed to evaluate algorithms under realistic urban logistics conditions with uncertainty and operational constraints."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/vibe'. Error: Path opencompass/vibe is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1927",
                    "name": "GMAIMMBench",
                    "version": "1.0.0",
                    "description": "GMAI-MMBench is the most comprehensive and structured benchmark developed to evaluate Large Vision-Language Models (LVLMs) in general medical artificial intelligence (GMAI) applications. It addresses the limitations of existing benchmarks that typically focus on narrow domains and lack perceptual di",
                    "url": "opencompass/opencompass_1927.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1927",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1927",
                        "name": "GMAIMMBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "å¤šæ¨¡æ€"
                            },
                            {
                                "cn": "åŒ»ç–—",
                                "en": "åŒ»ç–—"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/uni-medical/GMAI-MMBench",
                        "paperLink": "https://arxiv.org/pdf/2408.03361",
                        "officialWebsiteLink": "https://uni-medical.github.io/GMAI-MMBench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "155355",
                            "name": "kennyutc",
                            "avatar": null,
                            "nickname": "kennyutc"
                        },
                        "lookNum": "34",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-16 09:50:40",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-16 09:50:40",
                        "createDate": "2025-06-12 16:05:21",
                        "desc": {
                            "cn": "GMAI-MMBench æ˜¯ç›®å‰æœ€å…¨é¢ã€ç»“æ„åŒ–æœ€å®Œå–„çš„é€šç”¨åŒ»å­¦äººå·¥æ™ºèƒ½ï¼ˆGMAIï¼‰åŸºå‡†æ•°æ®é›†ï¼Œä¸“ä¸ºè¯„ä¼°å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨åŒ»ç–—é¢†åŸŸä¸­çš„è¡¨ç°è€Œè®¾è®¡ã€‚é’ˆå¯¹ç°æœ‰åŒ»å­¦åŸºå‡†é€šå¸¸å±€é™äºç‰¹å®šå­¦ç§‘ã€æ„ŸçŸ¥ç²’åº¦å•ä¸€çš„é—®é¢˜ï¼ŒGMAI-MMBench ä»285ä¸ªçœŸå®åŒ»å­¦æ•°æ®é›†æ„å»ºè€Œæˆï¼Œæ¶µç›–39ç§åŒ»å­¦å½±åƒæ¨¡æ€ã€18ä¸ªä¸´åºŠä»»åŠ¡ã€18ä¸ªåŒ»å­¦ç§‘å®¤ï¼Œä»¥åŠ4ç§ä¸åŒçš„æ„ŸçŸ¥ç²’åº¦ã€‚æ‰€æœ‰ä»»åŠ¡é‡‡ç”¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å½¢å¼ç»„ç»‡ï¼Œå…·å¤‡è‰¯å¥½çš„äº¤äº’æ€§å’Œé€šç”¨æ€§ã€‚å…¶ç‹¬ç‰¹çš„è¯æ±‡æ ‘ç»“æ„å…è®¸ç”¨æˆ·é’ˆå¯¹å…·ä½“ç ”ç©¶éœ€æ±‚çµæ´»å®šåˆ¶è¯„ä¼°è·¯å¾„ï¼Œæ”¯æŒå¤šæ ·åŒ–çš„è¯„æµ‹åœºæ™¯ã€‚æˆ‘ä»¬å¯¹50ä¸ªä¸»æµLVLMsè¿›è¡Œäº†ç³»ç»Ÿæµ‹è¯•ï¼Œå‘ç°å³ä½¿æ˜¯ç›®å‰æœ€å…ˆè¿›çš„ GPT-4oï¼Œå…¶å‡†ç¡®ç‡ä¹Ÿä»…ä¸º5",
                            "en": "GMAI-MMBench is the most comprehensive and structured benchmark developed to evaluate Large Vision-Language Models (LVLMs) in general medical artificial intelligence (GMAI) applications. It addresses the limitations of existing benchmarks that typically focus on narrow domains and lack perceptual di"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gmaimmbench'. Error: Path opencompass/gmaimmbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2046",
                    "name": "CREW-WILDFIRE",
                    "version": "1.0.0",
                    "description": "CREW-WILDFIRE is a benchmark designed to evaluate large language model-based multi-agent systemsâ€™ collaboration capabilities in complex, dynamic tasks, targeting agentic frameworks with perception, planning, and execution abilities.",
                    "url": "opencompass/opencompass_2046.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2046",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2046",
                        "name": "CREW-WILDFIRE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/generalroboticslab/CREW/tree/main/crew-algorithms/crew_algorithms/wildfire_alg",
                        "paperLink": "https://arxiv.org/abs/2507.05178",
                        "officialWebsiteLink": "http://www.generalroboticslab.com/blogs/blog/2025-07-05-crewwildfire/index.html",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "33",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-14 09:39:25",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-14 09:39:25",
                        "createDate": "2025-07-11 14:15:45",
                        "desc": {
                            "cn": "CREW-WILDFIRE æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤æ‚åŠ¨æ€ä»»åŠ¡ä¸­åä½œèƒ½åŠ›çš„åŸºå‡†ï¼Œé¢å‘å…·å¤‡æ„ŸçŸ¥ã€è§„åˆ’ä¸æ‰§è¡Œèƒ½åŠ›çš„æ™ºèƒ½ä½“æ¡†æ¶ã€‚",
                            "en": "CREW-WILDFIRE is a benchmark designed to evaluate large language model-based multi-agent systemsâ€™ collaboration capabilities in complex, dynamic tasks, targeting agentic frameworks with perception, planning, and execution abilities."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/crew_wildfire'. Error: Path opencompass/crew_wildfire is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2052",
                    "name": "ArtifactsBench",
                    "version": "1.0.0",
                    "description": "ArtifactsBench is a benchmark with 1,825 tasks for evaluating LLM-generated visual and interactive code. It addresses the gap of traditional benchmarks that focus only on algorithmic correctness by assessing visual fidelity and user interaction.",
                    "url": "opencompass/opencompass_2052.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2052",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2052",
                        "name": "ArtifactsBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "ArtifactsBench",
                                "en": "ArtifactsBench"
                            },
                            {
                                "cn": "ä»£ç å¯è§†åŒ–",
                                "en": "ä»£ç å¯è§†åŒ–"
                            },
                            {
                                "cn": "ä»£ç ç”Ÿæˆ",
                                "en": "ä»£ç ç”Ÿæˆ"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Tencent-Hunyuan/ArtifactsBenchmark",
                        "paperLink": "https://arxiv.org/abs/2507.04952",
                        "officialWebsiteLink": "https://artifactsbenchmark.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "90307965",
                            "name": null,
                            "avatar": null,
                            "nickname": "xxzcc"
                        },
                        "lookNum": "32",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-14 09:39:37",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-14 09:39:37",
                        "createDate": "2025-07-12 13:22:30",
                        "desc": {
                            "cn": "rtifactsBench æ˜¯ä¸€ä¸ªä¸“æ³¨äºå¼¥åˆä¼ ç»Ÿä»£ç è¯„æµ‹ä¸­â€œè§†è§‰-äº¤äº’â€é¸¿æ²Ÿçš„æ–°å‹åŸºå‡†ã€‚å®ƒæ—¨åœ¨å…¨é¢è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”ŸæˆåŠ¨æ€å¯è§†åŒ–ä¸äº¤äº’å¼ä»£ç çš„èƒ½åŠ›ï¼Œè€Œéä»…ä»…è€ƒæ ¸ç®—æ³•æ­£ç¡®æ€§ã€‚\nè¯¥åŸºå‡†åŒ…å«1825ä¸ªçœŸå®ä¸–ç•Œçš„ä»»åŠ¡ï¼Œå¹¶å¼€åˆ›äº†ä¸€å¥—è‡ªåŠ¨åŒ–å¤šæ¨¡æ€è¯„ä¼°æµç¨‹ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨æ¸²æŸ“ä»£ç ã€æ•æ‰å…¶è§†è§‰ä¸äº¤äº’è¡Œä¸ºï¼Œå†ç”±ä¸€ä¸ªå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆMLLMï¼‰ä¾æ®è¯¦ç»†æ¸…å•è¿›è¡Œè¯„åˆ†ã€‚è¯¥æµç¨‹ä¸äººç±»ä¸“å®¶åˆ¤æ–­çš„ä¸€è‡´æ€§é«˜è¾¾94.4%ï¼Œè¯æ˜äº†å…¶é«˜åº¦å¯é æ€§ã€‚\nArtifactsBench å·²å°†æ•°æ®é›†ã€è¯„ä¼°æ¡†æ¶åŠå·¥å…·é“¾å®Œå…¨å¼€æºï¼Œä¸ºç¤¾åŒºæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”ç²¾å‡†çš„å·¥å…·ï¼Œä»¥æ¨åŠ¨èƒ½åˆ›é€ ä¸°å¯Œç”¨æˆ·ä½“éªŒçš„æ–°ä¸€ä»£ç”Ÿæˆæ¨¡å‹çš„å‘å±•ã€‚",
                            "en": "ArtifactsBench is a benchmark with 1,825 tasks for evaluating LLM-generated visual and interactive code. It addresses the gap of traditional benchmarks that focus only on algorithmic correctness by assessing visual fidelity and user interaction."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/artifactsbench'. Error: Path opencompass/artifactsbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2063",
                    "name": "VISCO",
                    "version": "1.0.0",
                    "description": "VISCO aims to evaluate the critique and correction capabilities of VLMs, which are two essential building blocks towards VLM self-improvement. VISCO requires VLMs to critique the correctness of each step in CoT, provide natural language explanation, and corrects the CoT based on the critique.",
                    "url": "opencompass/opencompass_2063.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2063",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2063",
                        "name": "VISCO",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "çŸ¥è¯†",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "self-critique",
                                "en": "self-critique"
                            },
                            {
                                "cn": "VLM",
                                "en": "VLM"
                            },
                            {
                                "cn": "VLM reasoning",
                                "en": "VLM reasoning"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/PlusLabNLP/VISCO",
                        "paperLink": "https://arxiv.org/abs/2412.02172",
                        "officialWebsiteLink": "https://visco-benchmark.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "20905070",
                            "name": null,
                            "avatar": null,
                            "nickname": "shirley-wu"
                        },
                        "lookNum": "32",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-17 10:17:50",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-17 10:17:50",
                        "createDate": "2025-07-17 09:56:30",
                        "desc": {
                            "cn": "VISCO æ—¨åœ¨è¯„ä¼° VLM çš„è¯„åˆ¤ï¼ˆcritiqueï¼‰å’Œçº æ­£ï¼ˆcorrectionï¼‰èƒ½åŠ›ï¼Œè¿™ä¸¤ä¸ªèƒ½åŠ›æ˜¯ VLM è‡ªä¸»æå‡æ¨ç†æ€§èƒ½çš„åŸºç¡€ã€‚å¯¹äºä¸€ä¸ªè§†è§‰æ¨ç†é—®é¢˜ï¼Œç»™å®šæ¨¡å‹ç”Ÿæˆçš„ CoTï¼ŒVISCO è¯„æµ‹é›†è¦æ±‚ VLM è¯„åˆ¤ CoT ä¸­æ¯ä¸ªæ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œæä¾›è‡ªç„¶è¯­è¨€è§£é‡Šï¼Œå¹¶æ ¹æ®è¯„åˆ¤ç»“æœå¯¹ CoT è¿›è¡Œä¿®æ­£ã€‚",
                            "en": "VISCO aims to evaluate the critique and correction capabilities of VLMs, which are two essential building blocks towards VLM self-improvement. VISCO requires VLMs to critique the correctness of each step in CoT, provide natural language explanation, and corrects the CoT based on the critique."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/visco'. Error: Path opencompass/visco is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2085",
                    "name": "MIB",
                    "version": "1.0.0",
                    "description": "MIB is a benchmark designed to evaluate mechanistic interpretability methods for neural language models.",
                    "url": "opencompass/opencompass_2085.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2085",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2085",
                        "name": "MIB",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/aaronmueller/mib",
                        "paperLink": "https://icml.cc/virtual/2025/poster/43836",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "31",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:19",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:19",
                        "createDate": "2025-07-22 16:48:53",
                        "desc": {
                            "cn": "MIB æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹ä¸­æœºæ¢°å¯è§£é‡Šæ€§æ–¹æ³•çš„åŸºå‡†ã€‚å®ƒä¸»è¦è¯„ä¼°æ–¹æ³•åœ¨ç²¾ç¡®åœ°å®šä½å› æœè·¯å¾„ï¼ˆç”µè·¯å®šä½ï¼‰å’Œç‰¹å®šæ¦‚å¿µï¼ˆå› æœå˜é‡å®šä½ï¼‰æ–¹é¢çš„èƒ½åŠ›ï¼Œä¾§é‡äºå¿ å®åº¦å’Œæœ€å°åŒ–ç”µè·¯è§„æ¨¡ã€‚è¯¥åŸºå‡†æ¶µç›–äº†IOIå’Œç®—æœ¯ç­‰å››ä¸ªä»»åŠ¡ï¼Œå¹¶è¯„ä¼°äº†Llama-3.1å’ŒGemma-2ç­‰å››ç§æ¨¡å‹ã€‚\n",
                            "en": "MIB is a benchmark designed to evaluate mechanistic interpretability methods for neural language models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mib'. Error: Path opencompass/mib is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2091",
                    "name": "IS-Bench",
                    "version": "1.0.0",
                    "description": "IS-Bench is the first evaluation benchmark dedicated to assessing the safety of embodied agents during their interaction with home environments. It encompasses 161 high-risk domestic scenarios, spanning 10 hazard categories, including food poisoning, fire, electric shock, and more. ",
                    "url": "opencompass/opencompass_2091.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2091",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Safety",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2091",
                        "name": "IS-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "å®‰å…¨",
                                "en": "Safety"
                            },
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "task planning",
                                "en": "task planning"
                            },
                            {
                                "cn": "safety",
                                "en": "safety"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AI45Lab/IS-Bench",
                        "paperLink": "https://www.arxiv.org/abs/2506.16402",
                        "officialWebsiteLink": "https://ursulalujun.github.io/isbench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "62403124",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-PwR4JLkwp"
                        },
                        "lookNum": "31",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-21 10:18:50",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-21 10:18:50",
                        "createDate": "2025-07-19 20:51:53",
                        "desc": {
                            "cn": "IS-Bench æ˜¯é¦–ä¸ªä¸“æ³¨äºå…·èº«æ™ºèƒ½ä½“ä¸å®¶ç”¨ç¯å¢ƒäº¤äº’è¿‡ç¨‹å®‰å…¨æ€§çš„è¯„æµ‹åŸºå‡†ã€‚å®ƒåŒ…å« 161 ä¸ªé«˜é£é™©å®¶å±…åœºæ™¯åŠç›¸å…³æ—¥å¸¸ä»»åŠ¡ï¼Œè¦†ç›–é£Ÿç‰©ä¸­æ¯’ã€ç«ç¾ã€è§¦ç”µç­‰ 10 å¤§ç±»å¸¸è§é£é™©ã€‚é€šè¿‡è´¯ç©¿æ•´ä¸ªäº¤äº’è¿‡ç¨‹çš„åŠ¨æ€è¯„æµ‹æ¡†æ¶ï¼Œå…¨æ–¹ä½è¯„ä¼°å…·èº«æ™ºèƒ½ä½“çš„å®‰å…¨ç´ å…»ã€‚",
                            "en": "IS-Bench is the first evaluation benchmark dedicated to assessing the safety of embodied agents during their interaction with home environments. It encompasses 161 high-risk domestic scenarios, spanning 10 hazard categories, including food poisoning, fire, electric shock, and more. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/is_bench'. Error: Path opencompass/is_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2154",
                    "name": "MotionMillion",
                    "version": "1.0.0",
                    "description": "MotionMillion: The largest open-sourced 3D human motion dataset with text annotation, including 2.5k hours 1.9M episodes.",
                    "url": "opencompass/opencompass_2154.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2154",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2154",
                        "name": "MotionMillion",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "",
                        "officialWebsiteLink": "https://huggingface.co/datasets/InternRobotics/MotionMillion",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50207630",
                            "name": null,
                            "avatar": null,
                            "nickname": "fak111"
                        },
                        "lookNum": "31",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-08-15 16:40:09",
                        "supportOnlineEval": false,
                        "updateDate": "2025-08-15 16:40:09",
                        "createDate": "2025-08-15 16:38:27",
                        "desc": {
                            "cn": "MotionMillionï¼šç›®å‰æœ€å¤§çš„å¼€æºå¸¦æ–‡æœ¬æ ‡æ³¨çš„ 3D äººä½“åŠ¨ä½œæ•°æ®é›†ï¼ŒåŒ…å« 2500 å°æ—¶ã€190 ä¸‡ä¸ªç‰‡æ®µã€‚",
                            "en": "MotionMillion: The largest open-sourced 3D human motion dataset with text annotation, including 2.5k hours 1.9M episodes."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/motionmillion'. Error: Path opencompass/motionmillion is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2080",
                    "name": "PhyGenBench",
                    "version": "1.0.0",
                    "description": "PhyGenBench is a benchmark designed to evaluate physical commonsense correctness in text-to-video generation models. I",
                    "url": "opencompass/opencompass_2080.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2080",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2080",
                        "name": "PhyGenBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OpenGVLab/PhyGenBench",
                        "paperLink": "https://icml.cc/virtual/2025/poster/44642",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "30",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:22",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:22",
                        "createDate": "2025-07-22 16:50:31",
                        "desc": {
                            "cn": "PhyGenBench æ˜¯ä¸€ä¸ªè¯„ä¼°T2Væ¨¡å‹ç‰©ç†å¸¸è¯†çš„åŸºå‡†ã€‚å®ƒæ¶µç›–åŠ›å­¦ã€å…‰å­¦ã€çƒ­å­¦å’Œææ–™ç‰¹æ€§å››å¤§é¢†åŸŸ27ç§ç‰©ç†å®šå¾‹ï¼ŒåŒ…å«160ä¸ªæç¤ºã€‚ç»“åˆPhyGenEvalæ¡†æ¶ï¼Œåˆ©ç”¨è§†å¬å’Œå¤§è¯­è¨€æ¨¡å‹è‡ªåŠ¨åŒ–è¯„ä¼°æ¨¡å‹çš„ç‰©ç†ç†è§£èƒ½åŠ›ã€‚",
                            "en": "PhyGenBench is a benchmark designed to evaluate physical commonsense correctness in text-to-video generation models. I"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/phygenbench'. Error: Path opencompass/phygenbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2084",
                    "name": "AXBENCH",
                    "version": "1.0.0",
                    "description": "AXBENCH is a benchmark for large-scale evaluation of Language Model (LLM) control methods using synthetic data, focusing on fine-grained steering for safety and reliability.",
                    "url": "opencompass/opencompass_2084.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2084",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2084",
                        "name": "AXBENCH",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/stanfordnlp/axbench",
                        "paperLink": "https://icml.cc/virtual/2025/poster/45658",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "30",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:37",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:37",
                        "createDate": "2025-07-22 16:49:08",
                        "desc": {
                            "cn": "AXBENCH æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LLMæ§åˆ¶èƒ½åŠ›çš„åŸºå‡†ã€‚å®ƒé€šè¿‡æ¦‚å¿µæ£€æµ‹å’Œæ¨¡å‹æ“æ§ï¼ˆåŒ…å«æ¦‚å¿µã€æŒ‡ä»¤ã€æµç•…åº¦ï¼‰è¯„ä¼°ï¼Œæ—¨åœ¨å®ç°å®‰å…¨å¯é çš„ç»†ç²’åº¦æ“æ§ã€‚åŸºå‡†ä½¿ç”¨å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼Œå¹¶é›†æˆäº†å¤šç§åŸºçº¿æ–¹æ³•ã€‚",
                            "en": "AXBENCH is a benchmark for large-scale evaluation of Language Model (LLM) control methods using synthetic data, focusing on fine-grained steering for safety and reliability."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/axbench'. Error: Path opencompass/axbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2129",
                    "name": "LangNavBench",
                    "version": "1.0.0",
                    "description": "LangNavBench is a benchmark for evaluating natural language understanding in semantic navigation, built upon the manually verified LangNav open-set dataset.",
                    "url": "opencompass/opencompass_2129.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2129",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2129",
                        "name": "LangNavBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/3dlg-hcvc/langmonmap",
                        "paperLink": "https://arxiv.org/abs/2507.07299",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "å¼ å¯Œæ‰"
                        },
                        "lookNum": "29",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-28 10:05:09",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-28 10:05:09",
                        "createDate": "2025-07-28 10:01:42",
                        "desc": {
                            "cn": "LangNavBenchæ˜¯ä¸€ä¸ªè¯­ä¹‰å¯¼èˆªä¸­è‡ªç„¶è¯­è¨€ç†è§£è¯„æµ‹åŸºå‡†ï¼ŒåŸºäºæ‰‹å·¥éªŒè¯çš„LangNavå¼€æ”¾é›†æ•°æ®é›†ï¼Œè¯„æµ‹å…·èº«æ™ºèƒ½ä½“åœ¨è‡ªç„¶è¯­è¨€æŒ‡ä»¤å¼•å¯¼ä¸‹çš„ç›®æ ‡å®šä½èƒ½åŠ›ï¼Œæ¶µç›–ç±»åˆ«å±‚æ¬¡ç†è§£ã€å¯¹è±¡å±æ€§è¯†åˆ«å’Œç©ºé—´å…³ç³»æ¨ç†ç­‰ç»´åº¦.",
                            "en": "LangNavBench is a benchmark for evaluating natural language understanding in semantic navigation, built upon the manually verified LangNav open-set dataset."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/langnavbench'. Error: Path opencompass/langnavbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2032",
                    "name": "DICE-BENCH",
                    "version": "1.0.0",
                    "description": "DICE-BENCH is a benchmark for evaluating large language modelsâ€™ tool-use capabilities in multi-round, multi-party dialogues, focusing on function selection, parameter filling, and dialogue context comprehension.",
                    "url": "opencompass/opencompass_2032.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2032",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2032",
                        "name": "DICE-BENCH",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/snuhcc/DICE-Bench",
                        "paperLink": "https://arxiv.org/abs/2506.22853",
                        "officialWebsiteLink": "https://huggingface.co/datasets/OfficerChul/DICE-BENCH",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "å¼ å¯Œæ‰"
                        },
                        "lookNum": "29",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-07 11:44:26",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-07 11:44:26",
                        "createDate": "2025-07-07 10:40:34",
                        "desc": {
                            "cn": "DICE-BENCH æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®ã€å¤šæ–¹å¯¹è¯ä¸­å·¥å…·è°ƒç”¨èƒ½åŠ›çš„åŸºå‡†ï¼Œæ¶µç›–å‡½æ•°é€‰æ‹©ã€å‚æ•°å¡«å……å’Œå¯¹è¯ä¸Šä¸‹æ–‡ç†è§£ç­‰ç»´åº¦ã€‚",
                            "en": "DICE-BENCH is a benchmark for evaluating large language modelsâ€™ tool-use capabilities in multi-round, multi-party dialogues, focusing on function selection, parameter filling, and dialogue context comprehension."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/dice_bench'. Error: Path opencompass/dice_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1979",
                    "name": "OpenUnlearning",
                    "version": "1.0.0",
                    "description": "OpenUnlearning is an efficient and modular benchmark platform designed to support and drive research on \"unlearning\" in large language models (LLMs).",
                    "url": "opencompass/opencompass_1979.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1979",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1979",
                        "name": "OpenUnlearning",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/locuslab/open-unlearning",
                        "paperLink": "https://arxiv.org/abs/2506.12618",
                        "officialWebsiteLink": "https://huggingface.co/open-unlearning",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass å¸å—"
                        },
                        "lookNum": "28",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-27 18:12:41",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-27 18:12:41",
                        "createDate": "2025-06-25 17:24:57",
                        "desc": {
                            "cn": "OpenUnlearning æ˜¯ä¸€ä¸ªé«˜æ•ˆä¸”æ¨¡å—åŒ–çš„åŸºå‡†å¹³å°ï¼Œæ—¨åœ¨æ”¯æŒå’Œæ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„â€œé—å¿˜â€ï¼ˆunlearningï¼‰ç ”ç©¶ã€‚",
                            "en": "OpenUnlearning is an efficient and modular benchmark platform designed to support and drive research on \"unlearning\" in large language models (LLMs)."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/openunlearning'. Error: Path opencompass/openunlearning is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2087",
                    "name": "PertEval-scFM",
                    "version": "1.0.0",
                    "description": "PertEval-scFM is a standardized framework to evaluate single-cell foundation models (scFMs) for predicting cellular perturbation effects. ",
                    "url": "opencompass/opencompass_2087.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2087",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2087",
                        "name": "PertEval-scFM",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/aaronwtr/PertEval",
                        "paperLink": "https://icml.cc/virtual/2025/poster/43799",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "27",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:34",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:34",
                        "createDate": "2025-07-22 16:48:13",
                        "desc": {
                            "cn": "PertEval-scFM æ˜¯ä¸€ä¸ªè¯„ä¼°å•ç»†èƒåŸºç¡€æ¨¡å‹ï¼ˆscFMï¼‰æ‰°åŠ¨æ•ˆåº”é¢„æµ‹çš„æ ‡å‡†åŒ–æ¡†æ¶ã€‚å®ƒä»åˆ†å¸ƒåç§»æ³›åŒ–æ€§ã€æ‰°åŠ¨å¼ºåº¦åŠä¸Šä¸‹æ–‡å¯¹é½ä¸‰æ–¹é¢è¯„æµ‹ã€‚æµ‹è¯•é›†åŒ…å«Normanã€Replogleç­‰æ•°æ®é›†ï¼Œæ¶µç›–æ•°åƒæ‰°åŠ¨æ ·æœ¬å’ŒåŸºå› ã€‚\n",
                            "en": "PertEval-scFM is a standardized framework to evaluate single-cell foundation models (scFMs) for predicting cellular perturbation effects. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/perteval_scfm'. Error: Path opencompass/perteval_scfm is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2082",
                    "name": "StructTokenBench",
                    "version": "1.0.0",
                    "description": "StructTokenBench is a benchmark framework designed to comprehensively evaluate the quality and efficiency of protein structure tokenization methods, particularly focusing on fine-grained local substructures. ",
                    "url": "opencompass/opencompass_2082.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2082",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2082",
                        "name": "StructTokenBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/KatarinaYuan/StructTokenBench",
                        "paperLink": "https://icml.cc/virtual/2025/poster/44084",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "26",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:31",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:31",
                        "createDate": "2025-07-22 16:49:57",
                        "desc": {
                            "cn": "StructTokenBench æ˜¯ä¸€ä¸ªè¯„ä¼°è›‹ç™½è´¨ç»“æ„æ ‡è®°åŒ–æ–¹æ³•è´¨é‡æ•ˆç‡çš„åŸºå‡†ã€‚å®ƒå…³æ³¨ç»†ç²’åº¦å±€éƒ¨å­ç»“æ„ï¼Œè¯„æµ‹ç»´åº¦åŒ…æ‹¬ä¸‹æ¸¸æœ‰æ•ˆæ€§ã€æ•æ„Ÿæ€§ã€ç‹¬ç‰¹æ€§å’Œç æœ¬åˆ©ç”¨æ•ˆç‡ã€‚\n",
                            "en": "StructTokenBench is a benchmark framework designed to comprehensively evaluate the quality and efficiency of protein structure tokenization methods, particularly focusing on fine-grained local substructures. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/structtokenbench'. Error: Path opencompass/structtokenbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2127",
                    "name": "clembench",
                    "version": "1.0.0",
                    "description": "clembench is a benchmark framework for evaluating LLMs through dialogue game-based interactions, assessing chat-optimized models as conversational agents via multi-turn interactive game scenario.",
                    "url": "opencompass/opencompass_2127.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2127",
                    "sample_count": 1000,
                    "traits": [
                        "Creation"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2127",
                        "name": "clembench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "åˆ›ä½œ",
                                "en": "Creation"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/clembench/clembench",
                        "paperLink": "https://arxiv.org/abs/2507.08491",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "å¼ å¯Œæ‰"
                        },
                        "lookNum": "26",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-28 10:05:14",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-28 10:05:14",
                        "createDate": "2025-07-28 09:49:10",
                        "desc": {
                            "cn": "clembenchæ˜¯ä¸€ä¸ªåŸºäºå¯¹è¯æ¸¸æˆçš„å¤§è¯­è¨€æ¨¡å‹è¯„æµ‹æ¡†æ¶ï¼Œé€šè¿‡å¤šå›åˆäº¤äº’å¼æ¸¸æˆåœºæ™¯è¯„æµ‹èŠå¤©ä¼˜åŒ–æ¨¡å‹çš„ä¼šè¯æ™ºèƒ½ä½“èƒ½åŠ›ï¼ŒåŒ…å«Wordleã€Tabooç­‰å¤šç§æ¸¸æˆä»»åŠ¡ï¼Œè¯„æµ‹æŒ‡ä»¤éµå¾ªã€ç›®æ ‡å¯¼å‘è¡Œä¸ºå’Œç²¾ç»†åŒ–äº¤äº’ç†è§£ç­‰æ ¸å¿ƒç»´åº¦ï¼Œæä¾›å¯é‡å¤ã€å¯æ§åˆ¶çš„è‡ªç©è¯„æµ‹ç¯å¢ƒå’Œç»¼åˆå¾—åˆ†æœºåˆ¶ï¼Œæ”¯æŒè‹±è¯­åŠå¤šè¯­è¨€æ‰©å±•çš„å¼€æºè¯„æµ‹å¹³å°ã€‚",
                            "en": "clembench is a benchmark framework for evaluating LLMs through dialogue game-based interactions, assessing chat-optimized models as conversational agents via multi-turn interactive game scenario."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/clembench'. Error: Path opencompass/clembench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2151",
                    "name": "InternData-A1",
                    "version": "1.0.0",
                    "description": "InternData-A1: A hybrid synthetic-real manipulation dataset integrating 5 heterogeneous robots, 15 skills, and 200+ scenes, emphasizing multi-robot collaboration under dynamic scenarios.",
                    "url": "opencompass/opencompass_2151.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2151",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2151",
                        "name": "InternData-A1",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "video",
                                "en": "video"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "",
                        "officialWebsiteLink": "https://huggingface.co/datasets/InternRobotics/InternData-A1",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50207630",
                            "name": null,
                            "avatar": null,
                            "nickname": "fak111"
                        },
                        "lookNum": "26",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-08-15 16:40:00",
                        "supportOnlineEval": false,
                        "updateDate": "2025-08-15 16:40:00",
                        "createDate": "2025-08-15 16:39:27",
                        "desc": {
                            "cn": "InternData-A1ï¼šä¸€ä¸ªèåˆäº† 5 ç§å¼‚æ„æœºå™¨äººã€15 é¡¹æŠ€èƒ½å’Œ 200+ åœºæ™¯çš„æ··åˆåˆæˆ-çœŸå®æ“ä½œæ•°æ®é›†ï¼Œé‡ç‚¹å…³æ³¨åŠ¨æ€åœºæ™¯ä¸‹çš„å¤šæœºå™¨äººåä½œã€‚",
                            "en": "InternData-A1: A hybrid synthetic-real manipulation dataset integrating 5 heterogeneous robots, 15 skills, and 200+ scenes, emphasizing multi-robot collaboration under dynamic scenarios."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/interndata_a1'. Error: Path opencompass/interndata_a1 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2029",
                    "name": "SMMILE",
                    "version": "1.0.0",
                    "description": "SMMILE is a benchmark for evaluating multimodal large language models (MLLMs) in medical in-context learning tasks, constructed by medical experts. ",
                    "url": "opencompass/opencompass_2029.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2029",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2029",
                        "name": "SMMILE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "åŒ»å­¦",
                                "en": "åŒ»å­¦"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/eth-medical-ai-lab/smmile",
                        "paperLink": "https://arxiv.org/abs/2506.21355",
                        "officialWebsiteLink": "https://smmile-benchmark.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "å¼ å¯Œæ‰"
                        },
                        "lookNum": "26",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-07 11:44:42",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-07 11:44:42",
                        "createDate": "2025-07-07 10:06:59",
                        "desc": {
                            "cn": "SMMILE æ˜¯ä¸€ä¸ªç”±åŒ»å­¦ä¸“å®¶ä¸»å¯¼æ„å»ºçš„å¤šæ¨¡æ€åŒ»ç–—ä¸Šä¸‹æ–‡å­¦ä¹ è¯„æµ‹åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»å­¦ä»»åŠ¡ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«111ä¸ªé—®é¢˜ï¼ˆå…±517ä¸ªå›¾æ–‡é—®ç­”ä¸‰å…ƒç»„ï¼‰ï¼Œæ¶µç›–6ä¸ªåŒ»å­¦ä¸“ç§‘å’Œ13ç§å½±åƒæ¨¡æ€ï¼Œå¦æä¾›æ‰©å±•ç‰ˆæœ¬ SMMILE++ï¼ŒåŒ…å«1038ä¸ªå˜æ¢é—®é¢˜ã€‚",
                            "en": "SMMILE is a benchmark for evaluating multimodal large language models (MLLMs) in medical in-context learning tasks, constructed by medical experts. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/smmile'. Error: Path opencompass/smmile is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2030",
                    "name": "DABstep",
                    "version": "1.0.0",
                    "description": "DABstep is a benchmark for evaluating AI agentsâ€™ multi-step reasoning and planning abilities in realistic data analysis tasks, targeting code-executing language model agents. ",
                    "url": "opencompass/opencompass_2030.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2030",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2030",
                        "name": "DABstep",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "æ™ºèƒ½ä½“",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2506.23719",
                        "officialWebsiteLink": "https://huggingface.co/datasets/adyen/DABstep",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "å¼ å¯Œæ‰"
                        },
                        "lookNum": "26",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-07 11:44:38",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-07 11:44:38",
                        "createDate": "2025-07-07 10:26:30",
                        "desc": {
                            "cn": "DABstep æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼° AI æ™ºèƒ½ä½“åœ¨ç°å®å¤šæ­¥éª¤æ•°æ®åˆ†æä»»åŠ¡ä¸­æ¨ç†ä¸è§„åˆ’èƒ½åŠ›çš„åŸºå‡†ï¼Œé¢å‘å…·å¤‡ä»£ç æ‰§è¡Œèƒ½åŠ›çš„è¯­è¨€æ¨¡å‹ä»£ç†ã€‚è¯¥åŸºå‡†åŒ…å«450å¤šä¸ªä»»åŠ¡ï¼Œæºè‡ªé‡‘èåˆ†æå¹³å°ï¼Œæ¶µç›–ç»“æ„åŒ–æ•°æ®å¤„ç†ã€éç»“æ„åŒ–æ–‡æ¡£ç†è§£å’Œè·¨æºä¿¡æ¯æ•´åˆã€‚",
                            "en": "DABstep is a benchmark for evaluating AI agentsâ€™ multi-step reasoning and planning abilities in realistic data analysis tasks, targeting code-executing language model agents. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/dabstep'. Error: Path opencompass/dabstep is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2067",
                    "name": "LongVALE",
                    "version": "1.0.0",
                    "description": "LongVALE is the first long video benchmark integrating fine-grained omni modalities (video, audio, speech) information in videos. It comprises 105K omni-modal events with precise temporal boundaries and detailed omni-modal captions, aiming to advance comprehensive multi-modal video understanding.",
                    "url": "opencompass/opencompass_2067.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2067",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding",
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2067",
                        "name": "LongVALE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            },
                            {
                                "cn": "é•¿æ–‡æœ¬",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "long video understanding",
                                "en": "long video understanding"
                            },
                            {
                                "cn": "omni-modal",
                                "en": "omni-modal"
                            },
                            {
                                "cn": "fine-grained temporal understanding",
                                "en": "fine-grained temporal understanding"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ttgeng233/LongVALE",
                        "paperLink": "https://openaccess.thecvf.com/content/CVPR2025/papers/Geng_LongVALE_Vision-Audio-Language-Event_Benchmark_Towards_Time-Aware_Omni-Modal_Perception_of_Long_Videos_CVPR_2025_paper.pdf",
                        "officialWebsiteLink": "https://ttgeng233.github.io/LongVALE/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "20905098",
                            "name": null,
                            "avatar": null,
                            "nickname": "ttgeng233"
                        },
                        "lookNum": "25",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-18 14:31:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-18 14:31:33",
                        "createDate": "2025-07-17 16:31:11",
                        "desc": {
                            "cn": "LongVALE æ˜¯é¦–ä¸ªé›†æˆè§†é¢‘ä¸­ç»†ç²’åº¦å…¨æ¨¡æ€ï¼ˆè§†é¢‘ã€éŸ³é¢‘ã€è¯­éŸ³ï¼‰ä¿¡æ¯çš„é•¿è§†é¢‘ç†è§£åŸºå‡†ã€‚å®ƒåŒ…å« 10.5 ä¸‡ä¸ªå…·æœ‰ç²¾ç¡®æ—¶é—´è¾¹ç•Œå’Œè¯¦ç»†çš„å…¨æ¨¡æ€æè¿°çš„äº‹ä»¶æ ‡æ³¨ï¼Œè‡´åŠ›äºå…¨é¢æå‡å¤šæ¨¡æ€è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„è·¨æ¨¡æ€æ¨ç†ä¸ç»†ç²’åº¦æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›ã€‚",
                            "en": "LongVALE is the first long video benchmark integrating fine-grained omni modalities (video, audio, speech) information in videos. It comprises 105K omni-modal events with precise temporal boundaries and detailed omni-modal captions, aiming to advance comprehensive multi-modal video understanding."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/longvale'. Error: Path opencompass/longvale is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2031",
                    "name": "HERB",
                    "version": "1.0.0",
                    "description": "HERB comprises 39,190 artifactsâ€”including documents, meeting transcripts, Slack messages, GitHub content, and URLsâ€”simulating business workflows across product planning, development, and support stages, with noisy, multi-hop QA tasks featuring both answerable and unanswerable queries. ",
                    "url": "opencompass/opencompass_2031.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2031",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2031",
                        "name": "HERB",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "RAG",
                                "en": "RAG"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2506.23139",
                        "officialWebsiteLink": "https://huggingface.co/datasets/Salesforce/HERB",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "å¼ å¯Œæ‰"
                        },
                        "lookNum": "25",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-07 11:44:29",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-07 11:44:29",
                        "createDate": "2025-07-07 10:31:04",
                        "desc": {
                            "cn": "HERBåŸºå‡†åŒ…å«39,190ä¸ªä¼ä¸šæ–‡æ¡£ã€ä¼šè®®è®°å½•ã€Slackæ¶ˆæ¯ã€GitHubå†…å®¹å’Œç½‘é¡µé“¾æ¥ï¼Œæ¨¡æ‹Ÿäº§å“è§„åˆ’ã€å¼€å‘ä¸æ”¯æŒç­‰ä¸šåŠ¡æµç¨‹ï¼Œç”ŸæˆåŒ…å«å™ªå£°çš„å¤šè·³é—®ç­”ä»»åŠ¡ï¼Œæ¶µç›–å¯å›ç­”ä¸ä¸å¯å›ç­”æŸ¥è¯¢ã€‚",
                            "en": "HERB comprises 39,190 artifactsâ€”including documents, meeting transcripts, Slack messages, GitHub content, and URLsâ€”simulating business workflows across product planning, development, and support stages, with noisy, multi-hop QA tasks featuring both answerable and unanswerable queries. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/herb'. Error: Path opencompass/herb is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2033",
                    "name": "RExBench",
                    "version": "1.0.0",
                    "description": "RExBench is a benchmark for evaluating large language model agentsâ€™ ability to autonomously implement AI research extensions, focusing on code generation, experimental design, and research comprehension. ",
                    "url": "opencompass/opencompass_2033.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2033",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2033",
                        "name": "RExBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ä»£ç ",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/tinlaboratory/RexBench",
                        "paperLink": "https://arxiv.org/abs/2506.22598",
                        "officialWebsiteLink": "https://huggingface.co/datasets/tin-lab/RExBench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "å¼ å¯Œæ‰"
                        },
                        "lookNum": "25",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-07 11:44:23",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-07 11:44:23",
                        "createDate": "2025-07-07 10:46:09",
                        "desc": {
                            "cn": "RExBench æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†åœ¨è‡ªåŠ¨å®ç° AI ç ”ç©¶æ‰©å±•èƒ½åŠ›çš„åŸºå‡†ï¼Œæ¶µç›–ä»£ç ç”Ÿæˆã€å®éªŒè®¾è®¡å’Œç ”ç©¶ç†è§£ç­‰ç»´åº¦ã€‚è¯¥åŸºå‡†åŒ…å«12ä¸ªåŸºäºçœŸå®è®ºæ–‡å’Œä»£ç åº“çš„ä»»åŠ¡ï¼Œæ¯é¡¹ä»»åŠ¡ç”±é¢†åŸŸä¸“å®¶æä¾›æ‰©å±•æŒ‡ä»¤ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨åŒ–åŸºç¡€è®¾æ–½æ‰§è¡Œä»£ç†è¾“å‡ºä»¥éªŒè¯æˆåŠŸæ ‡å‡†ã€‚",
                            "en": "RExBench is a benchmark for evaluating large language model agentsâ€™ ability to autonomously implement AI research extensions, focusing on code generation, experimental design, and research comprehension. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/rexbench'. Error: Path opencompass/rexbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2034",
                    "name": "MTEB",
                    "version": "1.0.0",
                    "description": "MTEB is a benchmark for evaluating text embedding models, aiming to enhance long-term usability and reproducibility, focusing on task extensibility, data integrity validation, and result generalizability. ",
                    "url": "opencompass/opencompass_2034.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2034",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2034",
                        "name": "MTEB",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "æ–‡æœ¬åµŒå…¥",
                                "en": "æ–‡æœ¬åµŒå…¥"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/embeddings-benchmark/mteb",
                        "paperLink": "https://arxiv.org/abs/2506.21182",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "å¼ å¯Œæ‰"
                        },
                        "lookNum": "25",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-07 11:44:19",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-07 11:44:19",
                        "createDate": "2025-07-07 10:50:59",
                        "desc": {
                            "cn": "MTEB æ˜¯ä¸€ä¸ªè¯„ä¼°æ–‡æœ¬åµŒå…¥æ¨¡å‹çš„åŸºå‡†ï¼Œæ—¨åœ¨æå‡å…¶é•¿æœŸå¯ç”¨æ€§å’Œå¯å¤ç°æ€§ï¼Œæ¶µç›–ä»»åŠ¡æ‰©å±•æ€§ã€æ•°æ®å®Œæ•´æ€§éªŒè¯å’Œç»“æœé€šç”¨æ€§ç­‰ç»´åº¦ã€‚è¯¥åŸºå‡†åŒ…å«åŒ…æ‹¬åˆ†ç±»ã€æ£€ç´¢å’Œèšç±»åœ¨å†…çš„å¤šç§ä»»åŠ¡ï¼Œè¦†ç›–å¤šä¸ªè¯­è¨€å’Œé¢†åŸŸã€‚MTEB å¼•å…¥äº†è‡ªåŠ¨åŒ–æµ‹è¯•ç®¡é“ã€æŒç»­é›†æˆæ¡†æ¶å’Œç¤¾åŒºè´¡çŒ®æœºåˆ¶ï¼Œä»¥æ”¯æŒåŸºå‡†çš„å¯æ‰©å±•æ€§å’Œè´¨é‡æ§åˆ¶ã€‚",
                            "en": "MTEB is a benchmark for evaluating text embedding models, aiming to enhance long-term usability and reproducibility, focusing on task extensibility, data integrity validation, and result generalizability. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mteb'. Error: Path opencompass/mteb is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2153",
                    "name": "InternData-M1",
                    "version": "1.0.0",
                    "description": "InternData-M1: A large-scale synthetic dataset for generalizable pick-and-place over 80K objects, with open-ended instructions covering object recognition, spatial and commonsense reasoning, and long-horizon tasks.",
                    "url": "opencompass/opencompass_2153.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2153",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2153",
                        "name": "InternData-M1",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "",
                        "officialWebsiteLink": "https://huggingface.co/datasets/InternRobotics/InternData-M1",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50207630",
                            "name": null,
                            "avatar": null,
                            "nickname": "fak111"
                        },
                        "lookNum": "24",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-08-15 16:40:04",
                        "supportOnlineEval": false,
                        "updateDate": "2025-08-15 16:40:04",
                        "createDate": "2025-08-15 16:38:52",
                        "desc": {
                            "cn": "InternData-M1ï¼šä¸€ä¸ªå¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼Œç”¨äºå¯æ³›åŒ–çš„æŠ“å–ä¸æ”¾ç½®ä»»åŠ¡ï¼Œæ¶µç›– 8 ä¸‡+ å¯¹è±¡ï¼Œé…æœ‰å¼€æ”¾å¼æŒ‡ä»¤ï¼Œæ¶‰åŠç‰©ä½“è¯†åˆ«ã€ç©ºé—´ä¸å¸¸è¯†æ¨ç†ä»¥åŠé•¿æ—¶ä»»åŠ¡ã€‚",
                            "en": "InternData-M1: A large-scale synthetic dataset for generalizable pick-and-place over 80K objects, with open-ended instructions covering object recognition, spatial and commonsense reasoning, and long-horizon tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/interndata_m1'. Error: Path opencompass/interndata_m1 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2035",
                    "name": "Gym4ReaL",
                    "version": "1.0.0",
                    "description": "Gym4ReaL is a benchmark suite designed to evaluate reinforcement learning algorithms in real-world scenarios, addressing challenges such as non-stationarity, partial observability, and large state-action spaces. ",
                    "url": "opencompass/opencompass_2035.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2035",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2035",
                        "name": "Gym4ReaL",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "å¼ºåŒ–å­¦ä¹ ",
                                "en": "å¼ºåŒ–å­¦ä¹ "
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Daveonwave/gym4ReaL",
                        "paperLink": "https://arxiv.org/abs/2507.00257",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "å¼ å¯Œæ‰"
                        },
                        "lookNum": "24",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-07 11:44:15",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-07 11:44:15",
                        "createDate": "2025-07-07 10:54:22",
                        "desc": {
                            "cn": "Gym4ReaL æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­è¡¨ç°çš„åŸºå‡†å¥—ä»¶ï¼Œæ¶µç›–éå¹³ç¨³æ€§ã€éƒ¨åˆ†å¯è§‚æµ‹æ€§å’Œå¤§çŠ¶æ€-åŠ¨ä½œç©ºé—´ç­‰æŒ‘æˆ˜ã€‚",
                            "en": "Gym4ReaL is a benchmark suite designed to evaluate reinforcement learning algorithms in real-world scenarios, addressing challenges such as non-stationarity, partial observability, and large state-action spaces. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gym4real'. Error: Path opencompass/gym4real is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2050",
                    "name": "THUNDER",
                    "version": "1.0.0",
                    "description": "THUNDER is a benchmark designed to evaluate digital pathology foundation models on tile-level image understanding tasks, facilitating comparative analysis across various downstream tasks and models. ",
                    "url": "opencompass/opencompass_2050.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2050",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding",
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2050",
                        "name": "THUNDER",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            },
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MICS-Lab/thunder",
                        "paperLink": "https://arxiv.org/abs/2507.07860",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "23",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-14 09:39:19",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-14 09:39:19",
                        "createDate": "2025-07-11 17:55:24",
                        "desc": {
                            "cn": "THUNDER æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ•°å­—ç—…ç†å­¦åŸºç¡€æ¨¡å‹åœ¨åˆ‡ç‰‡çº§å›¾åƒç†è§£ä»»åŠ¡ä¸­è¡¨ç°çš„åŸºå‡†ï¼Œæ—¨åœ¨æ”¯æŒå¤šç§ä¸‹æ¸¸ä»»åŠ¡å’Œæ¨¡å‹çš„å¯¹æ¯”åˆ†æã€‚",
                            "en": "THUNDER is a benchmark designed to evaluate digital pathology foundation models on tile-level image understanding tasks, facilitating comparative analysis across various downstream tasks and models. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/thunder'. Error: Path opencompass/thunder is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2076",
                    "name": "K-Sort-Arena",
                    "version": "1.0.0",
                    "description": "K-Sort Arena employs K-wise comparison, allowing K models to participate in a free-for-all, providing richer information than pairwise comparison. It also designs a matching algorithm based on exploration-exploitation and probabilistic modeling to achieve more efficient and reliable model ranking. ",
                    "url": "opencompass/opencompass_2076.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2076",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2076",
                        "name": "K-Sort-Arena",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "å›¾åƒè¯„ä¼°",
                                "en": "å›¾åƒè¯„ä¼°"
                            },
                            {
                                "cn": "è§†é¢‘è¯„ä¼°",
                                "en": "è§†é¢‘è¯„ä¼°"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_K-Sort_Arena_Efficient_and_Reliable_Benchmarking_for_Generative_Models_via_CVPR_2025_paper.pdf",
                        "officialWebsiteLink": "https://huggingface.co/spaces/ksort/K-Sort-Arena",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "62402831",
                            "name": "lizhikai",
                            "avatar": null,
                            "nickname": "lizhikai"
                        },
                        "lookNum": "23",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-18 14:31:04",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-18 14:31:04",
                        "createDate": "2025-07-18 10:43:15",
                        "desc": {
                            "cn": "æœ¬é¡¹ç›®æå‡ºK-Sort Arenaï¼Œé‡‡ç”¨ K-wise æ¯”è¾ƒï¼Œå…è®¸ K ä¸ªæ¨¡å‹å‚ä¸è‡ªç”±æ··æˆ˜ï¼Œæä¾›æ¯”æˆå¯¹æ¯”è¾ƒæ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼Œå¹¶è®¾è®¡åŸºäºæ¢ç´¢-åˆ©ç”¨çš„åŒ¹é…ç®—æ³•å’Œæ¦‚ç‡å»ºæ¨¡ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆå’Œæ›´å¯é çš„æ¨¡å‹æ’åã€‚ç›®å‰ï¼ŒK-Sort Arena å·²æ”¶é›†å‡ åƒæ¬¡é«˜è´¨é‡æŠ•ç¥¨å¹¶æ„å»ºäº†å…¨é¢çš„æ¨¡å‹æ’è¡Œæ¦œï¼Œå·²ç”¨äºè¯„ä¼°å‡ åç§æœ€å…ˆè¿›çš„è§†è§‰ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬æ–‡ç”Ÿå›¾å’Œæ–‡ç”Ÿè§†é¢‘æ¨¡å‹ã€‚K-Sort Arenaå·²ç»å†æ•°æœˆçš„é¡¹ç›®å†…æµ‹ï¼ŒæœŸé—´æ”¶åˆ°æ¥è‡ªåŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡, æ–°åŠ å¡å›½ç«‹å¤§å­¦, å¡å†…åŸºæ¢…éš†å¤§å­¦, æ–¯å¦ç¦å¤§å­¦, æ™®æ—æ–¯é¡¿å¤§å­¦, åŒ—äº¬å¤§å­¦ç­‰æ•°åå®¶æœºæ„çš„ä¸“ä¸šäººå‘˜çš„æŠ€æœ¯åé¦ˆï¼Œç°å·²å…¬å¼€çº¿ä¸Šå‘å¸ƒã€‚",
                            "en": "K-Sort Arena employs K-wise comparison, allowing K models to participate in a free-for-all, providing richer information than pairwise comparison. It also designs a matching algorithm based on exploration-exploitation and probabilistic modeling to achieve more efficient and reliable model ranking. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/k_sort_arena'. Error: Path opencompass/k_sort_arena is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2142",
                    "name": "OmniMMI",
                    "version": "1.0.0",
                    "description": "OmniMMI is a cutting-edge benchmark for multi-modal interaction, specifically designed for OmniLLMs in streaming video environments. It includes 1,121 videos and 2,290 questions, tackling the challenges of streaming video understanding and proactive reasoning across six unique subtasks.",
                    "url": "opencompass/opencompass_2142.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2142",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2142",
                        "name": "OmniMMI",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OmniMMI/OmniMMI",
                        "paperLink": "https://arxiv.org/abs/2503.22952",
                        "officialWebsiteLink": "https://omnimmi.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "000196",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-WxyZUppDS"
                        },
                        "lookNum": "23",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-08-06 14:59:17",
                        "supportOnlineEval": false,
                        "updateDate": "2025-08-06 14:59:17",
                        "createDate": "2025-08-05 13:52:24",
                        "desc": {
                            "cn": "OmniMMI is a cutting-edge benchmark for multi-modal interaction, specifically designed for OmniLLMs in streaming video environments. It includes 1,121 videos and 2,290 questions, tackling the challenges of streaming video understanding and proactive reasoning across six unique subtasks.",
                            "en": "OmniMMI is a cutting-edge benchmark for multi-modal interaction, specifically designed for OmniLLMs in streaming video environments. It includes 1,121 videos and 2,290 questions, tackling the challenges of streaming video understanding and proactive reasoning across six unique subtasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/omnimmi'. Error: Path opencompass/omnimmi is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2130",
                    "name": "JOB-Complex",
                    "version": "1.0.0",
                    "description": "JOB-Complex is a challenging benchmark for traditional and learned query optimizers, containing 30 SQL queries and a plan-selection benchmark with nearly 6000 execution plans.",
                    "url": "opencompass/opencompass_2130.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2130",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2130",
                        "name": "JOB-Complex",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å…¶ä»–",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/DataManagementLab/JOB-Complex",
                        "paperLink": "https://arxiv.org/abs/2507.07471",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "å¼ å¯Œæ‰"
                        },
                        "lookNum": "21",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-28 10:05:03",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-28 10:05:03",
                        "createDate": "2025-07-28 10:04:11",
                        "desc": {
                            "cn": "JOB-Complexæ˜¯ä¸€ä¸ªé¢å‘ä¼ ç»Ÿä¸å­¦ä¹ å¼æŸ¥è¯¢ä¼˜åŒ–å™¨çš„æŒ‘æˆ˜æ€§æ•°æ®åº“åŸºå‡†ï¼ŒåŒ…å«30ä¸ªSQLæŸ¥è¯¢å’Œè¿‘6000ä¸ªæ‰§è¡Œè®¡åˆ’çš„è®¡åˆ’é€‰æ‹©åŸºå‡†ã€‚",
                            "en": "JOB-Complex is a challenging benchmark for traditional and learned query optimizers, containing 30 SQL queries and a plan-selection benchmark with nearly 6000 execution plans."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/job_complex'. Error: Path opencompass/job_complex is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2092",
                    "name": "Spatial457",
                    "version": "1.0.0",
                    "description": "Spatial457 systematically introduces four key capabilitiesâ€”multi-object understanding, 2D and 3D localization, and 3D orientationâ€”across five difficulty levels and seven question types, from simple object recognition to complex 6DoF spatial reasoning tasks. ",
                    "url": "opencompass/opencompass_2092.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2092",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2092",
                        "name": "Spatial457",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "å¤šæ¨¡æ€",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "æ¨ç†",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "ç†è§£",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "3D Spatial Reasoning",
                                "en": "3D Spatial Reasoning"
                            },
                            {
                                "cn": "Spatial Intelligence",
                                "en": "Spatial Intelligence"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/XingruiWang/Spatial457",
                        "paperLink": "https://arxiv.org/abs/2502.08636",
                        "officialWebsiteLink": "https://xingruiwang.github.io/projects/Spatial457/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "20903864",
                            "name": null,
                            "avatar": null,
                            "nickname": "XingruiWang"
                        },
                        "lookNum": "18",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-08-15 16:37:31",
                        "supportOnlineEval": false,
                        "updateDate": "2025-08-15 16:37:31",
                        "createDate": "2025-07-20 08:57:13",
                        "desc": {
                            "cn": "Spatial457 èšç„¦äºç©ºé—´æ¨ç†çš„å››é¡¹æ ¸å¿ƒèƒ½åŠ›ï¼šå¤šç‰©ä½“ç†è§£ã€äºŒç»´ä½ç½®è¯†åˆ«ã€ä¸‰ç»´ä½ç½®è¯†åˆ«ï¼Œä»¥åŠä¸‰ç»´æœå‘åˆ¤æ–­ã€‚è¿™äº›èƒ½åŠ›å¯¹ç°å®ä¸–ç•Œä¸­å¤æ‚åœºæ™¯çš„è®¤çŸ¥å’Œç†è§£è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ç§å±‚çº§é€’è¿›çš„è¯„ä¼°ç»“æ„ï¼Œå°†é—®é¢˜åˆ’åˆ†ä¸º 7 ç±»é—®é¢˜ç±»å‹ï¼Œè¦†ç›–ä»åŸºç¡€çš„å•ç‰©ä½“è¯†åˆ«ä»»åŠ¡ï¼Œåˆ°æˆ‘ä»¬é¦–æ¬¡æå‡ºçš„æ›´å…·æŒ‘æˆ˜æ€§çš„ 6DoF ç©ºé—´æ¨ç†ä»»åŠ¡ã€‚æ•´ä¸ªæ•°æ®é›†æŒ‰ 5 ä¸ªéš¾åº¦ç­‰çº§ç»„ç»‡ï¼Œå¸®åŠ©ç³»ç»Ÿæ€§åœ°è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒå±‚æ¬¡ç©ºé—´ç†è§£ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚\n\nSpatial457 ä¸ä»…æå‡äº†ç©ºé—´æ¨ç†è¯„æµ‹çš„è¦†ç›–é¢å’ŒæŒ‘æˆ˜æ€§ï¼Œä¹Ÿä¸ºåç»­ç ”ç©¶æä¾›äº†ç»Ÿä¸€çš„åŸºå‡†ï¼Œæœ‰åŠ©äºæ¨åŠ¨å¤šæ¨¡æ€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œç©ºé—´è®¤çŸ¥ä»»åŠ¡ä¸­çš„å‘å±•ã€‚",
                            "en": "Spatial457 systematically introduces four key capabilitiesâ€”multi-object understanding, 2D and 3D localization, and 3D orientationâ€”across five difficulty levels and seven question types, from simple object recognition to complex 6DoF spatial reasoning tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/spatial457'. Error: Path opencompass/spatial457 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                }
            ]
        }
    },
    "update_policy": {
        "check_interval_hours": 24,
        "auto_update": false,
        "notify_on_update": true,
        "backup_before_update": true,
        "rollback_on_failure": true
    },
    "authentication": {
        "required_for": [
            "bud_custom"
        ],
        "method": "oauth2",
        "token_endpoint": "https://auth.bud.eco/token"
    },
    "migration": null,
    "changelog": {
        "2025.01.05": [
            "Added eval_type support for datasets to specify evaluation configurations",
            "GSM8K dataset now includes gen evaluation type configuration"
        ],
        "2025.01.04": [
            "Added GSM8K dataset for mathematical reasoning",
            "Added HumanEval dataset for code generation",
            "Added MMLU dataset for broad knowledge evaluation",
            "Added Bud Multilingual QA dataset",
            "Added Bud Safety Evaluation dataset",
            "Enhanced metadata with token estimates"
        ]
    }
}
