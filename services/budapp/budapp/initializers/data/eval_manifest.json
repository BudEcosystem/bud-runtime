{
    "manifest_version": "1.0.10",
    "last_updated": "2025-10-24T11:44:12Z",
    "schema_version": "1.0",
    "repository": {
        "name": "Bud Evaluation Datasets",
        "description": "Official evaluation datasets for model testing - 446 datasets from OpenCompass",
        "maintainer": "Bud Ecosystem",
        "base_url": "https://eval-datasets.bud.eco/v2",
        "bundle_url": null,
        "bundle_checksum": null,
        "bundle_size_mb": null
    },
    "version_info": {
        "current_version": "1.0.10",
        "previous_versions": [
            {
                "version": "1.0.0",
                "deprecated": false,
                "migration_required": false,
                "updated_at": "2025-10-24T07:02:14Z"
            },
            {
                "version": "1.0.1",
                "deprecated": false,
                "migration_required": false,
                "updated_at": "2025-10-24T07:02:55Z"
            },
            {
                "version": "1.0.2",
                "deprecated": false,
                "migration_required": false,
                "updated_at": "2025-10-24T07:18:48Z"
            },
            {
                "version": "1.0.3",
                "deprecated": false,
                "migration_required": false,
                "updated_at": "2025-10-24T07:36:22Z"
            },
            {
                "version": "1.0.4",
                "deprecated": false,
                "migration_required": false,
                "updated_at": "2025-10-24T09:30:50Z"
            },
            {
                "version": "1.0.5",
                "deprecated": false,
                "migration_required": false,
                "updated_at": "2025-10-24T09:53:53Z"
            },
            {
                "version": "1.0.6",
                "deprecated": false,
                "migration_required": false,
                "updated_at": "2025-10-24T10:35:39Z"
            },
            {
                "version": "1.0.7",
                "deprecated": false,
                "migration_required": false,
                "updated_at": "2025-10-24T11:12:11Z"
            },
            {
                "version": "1.0.8",
                "deprecated": false,
                "migration_required": false,
                "updated_at": "2025-11-14T09:01:01Z"
            },
            {
                "version": "1.0.9",
                "deprecated": false,
                "migration_required": false,
                "updated_at": "2025-11-17T12:08:21Z"
            }
        ]
    },
    "traits": {
        "version": "1.0.0",
        "checksum": "sha256:913a03cac67794ef",
        "url": "traits/traits_v1.json",
        "count": 23,
        "definitions": [
            {
                "name": "Strong Reasoning",
                "description": "Evaluation trait: Strong Reasoning",
                "slogan": "",
                "icon": "icons/traits/strong_reasoning.png"
            },
            {
                "name": "Multimodal",
                "description": "Evaluation trait: Multimodal",
                "slogan": "",
                "icon": "icons/traits/multimodal.png"
            },
            {
                "name": "Science",
                "description": "Evaluation trait: Science",
                "slogan": "",
                "icon": "icons/traits/science.png"
            },
            {
                "name": "Reasoning",
                "description": "Evaluation trait: Reasoning",
                "slogan": "",
                "icon": "icons/traits/reasoning.png"
            },
            {
                "name": "Agent",
                "description": "Evaluation trait: Agent",
                "slogan": "",
                "icon": "icons/traits/agent.png"
            },
            {
                "name": "Code",
                "description": "Evaluation trait: Code",
                "slogan": "",
                "icon": "icons/traits/code.png"
            },
            {
                "name": "Math",
                "description": "Evaluation trait: Math",
                "slogan": "",
                "icon": "icons/traits/math.png"
            },
            {
                "name": "Instruct",
                "description": "Evaluation trait: Instruct",
                "slogan": "",
                "icon": "icons/traits/instruct.png"
            },
            {
                "name": "Examination",
                "description": "Evaluation trait: Examination",
                "slogan": "",
                "icon": "icons/traits/examination.png"
            },
            {
                "name": "Knowledge",
                "description": "Evaluation trait: Knowledge",
                "slogan": "",
                "icon": "icons/traits/knowledge.png"
            },
            {
                "name": "Medical",
                "description": "Evaluation trait: Medical",
                "slogan": "",
                "icon": "icons/traits/medical.png"
            },
            {
                "name": "Understanding",
                "description": "Evaluation trait: Understanding",
                "slogan": "",
                "icon": "icons/traits/understanding.png"
            },
            {
                "name": "Language",
                "description": "Evaluation trait: Language",
                "slogan": "",
                "icon": "icons/traits/language.png"
            },
            {
                "name": "Safety",
                "description": "Evaluation trait: Safety",
                "slogan": "",
                "icon": "icons/traits/safety.png"
            },
            {
                "name": "Long-Context",
                "description": "Evaluation trait: Long-Context",
                "slogan": "",
                "icon": "icons/traits/long_context.png"
            },
            {
                "name": "Creation",
                "description": "Evaluation trait: Creation",
                "slogan": "",
                "icon": "icons/traits/creation.png"
            },
            {
                "name": "Hallucination",
                "description": "Evaluation trait: Hallucination",
                "slogan": "",
                "icon": "icons/traits/hallucination.png"
            },
            {
                "name": "Alignment",
                "description": "Evaluation trait: Alignment",
                "slogan": "",
                "icon": "icons/traits/alignment.png"
            },
            {
                "name": "Visual-Qa",
                "description": "Evaluation trait: Visual-Qa",
                "slogan": "",
                "icon": "icons/traits/visual_qa.png"
            },
            {
                "name": "Visual-Localization",
                "description": "Evaluation trait: Visual-Localization",
                "slogan": "",
                "icon": "icons/traits/visual_localization.png"
            },
            {
                "name": "Spatial-Understanding",
                "description": "Evaluation trait: Spatial-Understanding",
                "slogan": "",
                "icon": "icons/traits/spatial_understanding.png"
            },
            {
                "name": "Video-Understanding",
                "description": "Evaluation trait: Video-Understanding",
                "slogan": "",
                "icon": "icons/traits/video_understanding.png"
            },
            {
                "name": "Other",
                "description": "Evaluation trait: Other",
                "slogan": "",
                "icon": "icons/traits/other.png"
            }
        ]
    },
    "datasets": {
        "opencompass": {
            "version": "2.0.0",
            "license": "Various - See individual dataset licenses",
            "source": "OpenCompass Evaluation Platform",
            "checksum": "sha256:placeholder_opencompass_v2",
            "count": 445,
            "datasets": [
                {
                    "id": "opencompass_2152",
                    "name": "InternData-N1",
                    "version": "1.0.0",
                    "description": "InternData-N1: A high-quality navigation dataset with the most diverse scenes and extensive randomization across embodiments/viewpoints, including 3k+ scenes and 830k VLN data.",
                    "url": "opencompass/opencompass_2152.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2152",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2152",
                        "name": "InternData-N1",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "",
                        "officialWebsiteLink": "https://huggingface.co/datasets/InternRobotics/InternData-N1",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50207630",
                            "name": null,
                            "avatar": null,
                            "nickname": "fak111"
                        },
                        "lookNum": "293",
                        "top": true,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-08-15 16:39:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-08-15 16:39:55",
                        "createDate": "2025-08-15 16:39:42",
                        "desc": {
                            "cn": "InternData-N1：一个高质量导航数据集，具有最丰富的场景和跨具身/视角的广泛随机化，包含 3000+ 场景和 83 万条 VLN 数据。",
                            "en": "InternData-N1: A high-quality navigation dataset with the most diverse scenes and extensive randomization across embodiments/viewpoints, including 3k+ scenes and 830k VLN data."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/interndata_n1'. Error: Path opencompass/interndata_n1 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1922",
                    "name": "MMSI-Bench",
                    "version": "1.0.0",
                    "description": "MMSI-Bench is a novel Visual Question Answering (VQA) benchmark specifically designed to evaluate Multi-image Spatial Intelligence in multimodal large language models (MLLMs). Unlike traditional datasets that focus on spatial reasoning within a single image, MMSI-Bench emphasizes real-world inspired",
                    "url": "opencompass/opencompass_1922.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1922",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1922",
                        "name": "MMSI-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "空间智能",
                                "en": "空间智能"
                            },
                            {
                                "cn": "多模态",
                                "en": "多模态"
                            },
                            {
                                "cn": "跨视角",
                                "en": "跨视角"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OpenRobotLab/MMSI-Bench",
                        "paperLink": "https://arxiv.org/pdf/2505.23764",
                        "officialWebsiteLink": "https://runsenxu.com/projects/MMSI_Bench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "155355",
                            "name": "kennyutc",
                            "avatar": null,
                            "nickname": "kennyutc"
                        },
                        "lookNum": "619",
                        "top": true,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-11 14:16:32",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-11 14:16:32",
                        "createDate": "2025-06-11 14:11:18",
                        "desc": {
                            "cn": "MMSI-Bench 是一个全新的多模态空间智能视觉问答（VQA）基准数据集，专为评估多图像空间推理能力而设计。与专注于单图像关系推理的传统数据集不同，MMSI-Bench 更贴近现实世界，聚焦于需在多张图像之间进行逻辑推理的复杂场景。该数据集由六位三维视觉专家耗时超过300小时构建，精心整理出包含1,000个高质量选择题的问题集，题目来自12万余张图像，并附有精心设计的误导选项与逐步推理过程。对34个主流开源和闭源多模态大语言模型的实证评估显示：最先进的模型准确率仅为30%至40%，而人类表现高达97%，揭示该任务的巨大挑战性与模型发展空间。此外，MMSI-Bench 配套提供自动化错误分析",
                            "en": "MMSI-Bench is a novel Visual Question Answering (VQA) benchmark specifically designed to evaluate Multi-image Spatial Intelligence in multimodal large language models (MLLMs). Unlike traditional datasets that focus on spatial reasoning within a single image, MMSI-Bench emphasizes real-world inspired"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmsi_bench'. Error: Path opencompass/mmsi_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1942",
                    "name": "OmniDocBench",
                    "version": "1.0.0",
                    "description": "OmniDocBench is a comprehensive benchmark for evaluating document parsing in real-world scenarios. It includes 981 PDF pages across 9 document types, annotated with dense paragraph-level bboxes with text and attributes. Along with its designed evaluation methods, it provides Fine-grained results.",
                    "url": "opencompass/opencompass_1942.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1942",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1942",
                        "name": "OmniDocBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "长文本",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Document content extraction",
                                "en": "Document content extraction"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/opendatalab/OmniDocBench/tree/main?tab=readme-ov-file",
                        "paperLink": "https://arxiv.org/pdf/2412.07626",
                        "officialWebsiteLink": "https://huggingface.co/datasets/opendatalab/OmniDocBench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "032787",
                            "name": "ouyanglinke",
                            "avatar": null,
                            "nickname": "ouyanglinke"
                        },
                        "lookNum": "346",
                        "top": true,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-16 17:13:23",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-16 17:13:23",
                        "createDate": "2025-06-16 14:34:38",
                        "desc": {
                            "cn": "OmniDocBench是一个用于评估真实场景下多样性文档解析效果的评测集，它包含了981个页面，覆盖9种文档类型（包括研报、教材、报纸、手写笔记、杂志等），具有段落级别的位置标注和内容标注，还有阅读顺序标注和属性标注，并开发了配套的评测方法，使其既具备单模块的评测能力（包括布局检测，公式识别，表格识别，文本识别），又具备端到端的评测能力，针对不同元素提供了分页面以及分属性的精细化评测结果，精准定位模型文档解析的痛点问题。",
                            "en": "OmniDocBench is a comprehensive benchmark for evaluating document parsing in real-world scenarios. It includes 981 PDF pages across 9 document types, annotated with dense paragraph-level bboxes with text and attributes. Along with its designed evaluation methods, it provides Fine-grained results."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/omnidocbench'. Error: Path opencompass/omnidocbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1206",
                    "name": "MMBench",
                    "version": "1.0.0",
                    "description": "MMBench is a collection of benchmarks to evaluate the multi-modal understanding capability of large vision language models (LVLMs). This benchmark contains 3,000 multiple-choice questions covering 20 fine-grained assessment dimensions.",
                    "url": "opencompass/opencompass_1206.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1206",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1206",
                        "name": "MMBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 3,
                        "githubLink": "https://github.com/open-compass/MMBench",
                        "paperLink": "https://arxiv.org/abs/2307.06281",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1912",
                        "top": true,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-30 16:30:49",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-30 16:30:49",
                        "createDate": "2024-12-30 16:16:18",
                        "desc": {
                            "cn": "MMBench是OpenCompass 研究团队自建的视觉语言模型评测数据集，可实现从感知到认知能力逐级细分评估。此评测基准包含3000 道单项选择题 ，覆盖 20个细粒度评估维度。",
                            "en": "MMBench is a collection of benchmarks to evaluate the multi-modal understanding capability of large vision language models (LVLMs). This benchmark contains 3,000 multiple-choice questions covering 20 fine-grained assessment dimensions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmbench'. Error: Path opencompass/mmbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1694",
                    "name": "MaritimeBench",
                    "version": "1.0.0",
                    "description": "MaritimeBench builds a scientific, fair maritime knowledge assessment system. With 1,888 MCQs based on industry standards, we evaluate models' capabilities across shipping domains.",
                    "url": "opencompass/opencompass_1694.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1694",
                    "sample_count": 1000,
                    "traits": [
                        "Examination",
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1694",
                        "name": "MaritimeBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "学科",
                                "en": "Examination"
                            },
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "航运",
                                "en": "航运"
                            },
                            {
                                "cn": "知识",
                                "en": "知识"
                            },
                            {
                                "cn": "海运",
                                "en": "海运"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 4,
                        "githubLink": "",
                        "paperLink": "",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "067099",
                            "name": "wangxiangyu",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/067099-bd016c2a-6b89-4ba3-aaa1-6cc12a7ca88f.png",
                            "nickname": "Hi-Dolphin"
                        },
                        "lookNum": "779",
                        "top": true,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-22 11:41:39",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-22 11:41:39",
                        "createDate": "2025-04-01 09:13:05",
                        "desc": {
                            "cn": "MaritimeBench 致力于构建一套科学、公平且严谨的航运知识评估体系。基于行业权威标准，我们持续维护并更新高质量的航运数据集——其中包含1,888道客观选择题（MCQ格式），以全面、多维度地量化模型在航运各领域的能力表现。",
                            "en": "MaritimeBench builds a scientific, fair maritime knowledge assessment system. With 1,888 MCQs based on industry standards, we evaluate models' capabilities across shipping domains."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/maritimebench'. Error: Path opencompass/maritimebench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1558",
                    "name": "MM-AlignBench",
                    "version": "1.0.0",
                    "description": "A benchmark for evaluating MLLMs' alignment with human preferences. It includes 252 high-quality, human-annotated samples with diverse image types and open-ended questions. Modeled after Arena-style benchmarks, it uses GPT-4o as the judge model and Claude-Sonnet-3 as the reference model.",
                    "url": "opencompass/opencompass_1558.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1558",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1558",
                        "name": "MM-AlignBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/PhoenixZ810/OmniAlign-V",
                        "paperLink": "https://arxiv.org/abs/2502.18411",
                        "officialWebsiteLink": "https://phoenixz810.github.io/OmniAlign-V/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1137",
                        "top": true,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-05 09:55:00",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-05 09:55:00",
                        "createDate": "2025-03-05 09:54:39",
                        "desc": {
                            "cn": "用于评估 MLLM 与人类偏好的一致性的基准。它包含 252 个高质量、人类标注的样本，具有不同的图像类型和开放式问题。它仿照 Arena 风格的基准，使用 GPT-4o 作为评判模型，Claude-Sonnet-3 作为参考模型。",
                            "en": "A benchmark for evaluating MLLMs' alignment with human preferences. It includes 252 high-quality, human-annotated samples with diverse image types and open-ended questions. Modeled after Arena-style benchmarks, it uses GPT-4o as the judge model and Claude-Sonnet-3 as the reference model."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mm_alignbench'. Error: Path opencompass/mm_alignbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1509",
                    "name": "MVBench",
                    "version": "1.0.0",
                    "description": "MVBench can test MLLMs' temporal understanding in the dynamic video tasks. It covers 20 challenging video tasks that cannot be effectively solved with a single frame.",
                    "url": "opencompass/opencompass_1509.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1509",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1509",
                        "name": "MVBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/OpenGVLab/Ask-Anything",
                        "paperLink": "https://arxiv.org/abs/2311.17005",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "690",
                        "top": true,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-02-19 14:28:53",
                        "supportOnlineEval": false,
                        "updateDate": "2025-02-19 14:28:53",
                        "createDate": "2025-02-17 15:17:00",
                        "desc": {
                            "cn": "MVBench用于评估多模态大模型在动态视频任务中的时间理解能力，由20个单帧内容无法有效解决的挑战性的视频任务组成。",
                            "en": "MVBench can test MLLMs' temporal understanding in the dynamic video tasks. It covers 20 challenging video tasks that cannot be effectively solved with a single frame."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mvbench'. Error: Path opencompass/mvbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1375",
                    "name": "VBench",
                    "version": "1.0.0",
                    "description": "VBench is a comprehensive benchmark evaluates video generation quality. It comprises 16 dimensions in video generation, and also provides a dataset of human preference annotations. ",
                    "url": "opencompass/opencompass_1375.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1375",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Creation"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1375",
                        "name": "VBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "创作",
                                "en": "Creation"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Vchitect/VBench",
                        "paperLink": "https://arxiv.org/abs/2311.17982",
                        "officialWebsiteLink": "https://vchitect.github.io/VBench-project/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "670",
                        "top": true,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:36",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:36",
                        "createDate": "2025-01-10 16:36:24",
                        "desc": {
                            "cn": "VBench用于评估多模态大模型的视频生成质量，包含16个视频生成维度及1个人类偏好注释数据集。",
                            "en": "VBench is a comprehensive benchmark evaluates video generation quality. It comprises 16 dimensions in video generation, and also provides a dataset of human preference annotations. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/vbench'. Error: Path opencompass/vbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1397",
                    "name": "LiveMathBench",
                    "version": "1.0.0",
                    "description": "LiveMathBench can capture LLM capabilities in complex reasoning tasks, including challenging latest question sets from various mathematical competitions. ",
                    "url": "opencompass/opencompass_1397.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1397",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Math"
                    ],
                    "eval_type": {
                        "gen": "LiveMathBench_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1397",
                        "name": "LiveMathBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 3,
                        "githubLink": "https://github.com/open-compass/GPassK",
                        "paperLink": "https://arxiv.org/abs/2412.13147",
                        "officialWebsiteLink": "https://open-compass.github.io/GPassK/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1365",
                        "top": true,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-16 20:30:57",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-16 20:30:57",
                        "createDate": "2025-01-15 18:21:51",
                        "desc": {
                            "cn": "LiveMathBench用于评估大语言模型在复杂推理方面的表现，由极具挑战性的现代数学问题组成。",
                            "en": "LiveMathBench can capture LLM capabilities in complex reasoning tasks, including challenging latest question sets from various mathematical competitions. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/livemathbench'. Error: Path opencompass/livemathbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1370",
                    "name": "MathVision",
                    "version": "1.0.0",
                    "description": "MathVision measures multimodal mathematical reasoning capabilities through a meticulously curated collection of 3,040 high-quality mathematical problems spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty.",
                    "url": "opencompass/opencompass_1370.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1370",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1370",
                        "name": "MathVision",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/mathllm/MATH-V",
                        "paperLink": "https://arxiv.org/abs/2402.14804",
                        "officialWebsiteLink": "https://mathllm.github.io/mathvision/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "908",
                        "top": true,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-10 18:23:49",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-10 18:23:49",
                        "createDate": "2025-01-10 18:19:40",
                        "desc": {
                            "cn": "MathVision用于评估多模态大模型的数学推理能力，由涵盖16个数学领域、跨越5个难度级别的3040个高质量数学问题组成。",
                            "en": "MathVision measures multimodal mathematical reasoning capabilities through a meticulously curated collection of 3,040 high-quality mathematical problems spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mathvision'. Error: Path opencompass/mathvision is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1371",
                    "name": "MathVerse",
                    "version": "1.0.0",
                    "description": "MathVerse is intended for evaluating MLLMs' visual math problem-solving, containing 2,612 high-quality, multi-subject math problems with diagrams. ",
                    "url": "opencompass/opencompass_1371.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1371",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1371",
                        "name": "MathVerse",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/ZrrSkywalker/MathVerse",
                        "paperLink": "https://arxiv.org/abs/2403.14624",
                        "officialWebsiteLink": "https://mathverse-cuhk.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "459",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-10 18:00:01",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-10 18:00:01",
                        "createDate": "2025-01-10 14:29:47",
                        "desc": {
                            "cn": "MathVerse用于评估多模态大模型的视觉数学问题解决能力，包含2612个高质量、多主题的数学问题。",
                            "en": "MathVerse is intended for evaluating MLLMs' visual math problem-solving, containing 2,612 high-quality, multi-subject math problems with diagrams. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mathverse'. Error: Path opencompass/mathverse is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1374",
                    "name": "DynaMath",
                    "version": "1.0.0",
                    "description": "DynaMath is a dynamic visual math benchmark designed for in-depth assessment of VLMs. It includes 501 high-quality, multi-topic seed questions, each represented as a Python program enabling the automatic generation of a much larger set of concrete questions.",
                    "url": "opencompass/opencompass_1374.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1374",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1374",
                        "name": "DynaMath",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/DynaMath/DynaMath",
                        "paperLink": "https://arxiv.org/abs/2411.00836",
                        "officialWebsiteLink": "https://dynamath.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "321",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-10 18:25:13",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-10 18:25:13",
                        "createDate": "2025-01-10 18:24:27",
                        "desc": {
                            "cn": " DynaMath用于评估多模态大模型的数学能力，包括501个高质量、多主题的种子问题，每个问题都以Python程序表示，能够自动生成大量具体的多样化问题。",
                            "en": "DynaMath is a dynamic visual math benchmark designed for in-depth assessment of VLMs. It includes 501 high-quality, multi-topic seed questions, each represented as a Python program enabling the automatic generation of a much larger set of concrete questions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/dynamath'. Error: Path opencompass/dynamath is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_539",
                    "name": "BBH",
                    "version": "1.0.0",
                    "description": "BIG-Bench Hard (BBH) is a subset of the BIG-Bench, a diverse evaluation suite for language models. BBH focuses on a suite of 23 challenging tasks from BIG-Bench that were found to be beyond the capabilities of current language models.",
                    "url": "opencompass/opencompass_539.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_539",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "539",
                        "name": "BBH",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "reasoning",
                                "en": "reasoning"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/suzgunmirac/BIG-Bench-Hard",
                        "paperLink": "https://arxiv.org/pdf/2210.09261.pdf",
                        "officialWebsiteLink": "https://github.com/suzgunmirac/BIG-Bench-Hard",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "17498",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:31:41",
                        "supportOnlineEval": true,
                        "updateDate": "2024-09-12 19:31:41",
                        "createDate": "2024-09-12 19:26:57",
                        "desc": {
                            "cn": "BIG Bench-Hard（BBH）是BIG Bench的一个子集，它是一个用于语言模型的多样化评估套件。BBH专注于BIG Bench的23项具有挑战性的任务，这些任务被发现超出了当前语言模型的能力。",
                            "en": "BIG-Bench Hard (BBH) is a subset of the BIG-Bench, a diverse evaluation suite for language models. BBH focuses on a suite of 23 challenging tasks from BIG-Bench that were found to be beyond the capabilities of current language models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/bbh'. Error: No data found in /home/budadmin/.cache/opencompass/./data/BBH/data for split 'test'. Please check if the split exists and contains data files.\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_534",
                    "name": "MATH",
                    "version": "1.0.0",
                    "description": "MATH is a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution.",
                    "url": "opencompass/opencompass_534.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_534",
                    "sample_count": 5000,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {
                        "gen": "math_gen",
                        "agent": "math_agent"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 100,
                        "estimated_output_tokens": 50
                    },
                    "original_data": {
                        "id": "534",
                        "name": "MATH",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "math",
                                "en": "math"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/hendrycks/math",
                        "paperLink": "https://arxiv.org/pdf/2103.03874.pdf",
                        "officialWebsiteLink": "https://huggingface.co/datasets/hendrycks/competition_math",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "13270",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:31:29",
                        "supportOnlineEval": true,
                        "updateDate": "2024-09-12 19:31:29",
                        "createDate": "2024-09-12 19:28:19",
                        "desc": {
                            "cn": "MATH 是一个包含 12,500 个具有挑战性的竞赛数学问题的新数据集。 MATH 中的每个问题都有完整的分步解决方案。",
                            "en": "MATH is a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "{\"problem\": \"What fraction of the form $\\\\frac{A}{x + 3}$ can be added to $\\\\frac{6x}{x^2 + 2x - 3}$ so that the result reduces to a fraction of the form $\\\\frac{B}{x - 1}$?  Here $A$ and $B$ are real",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "symbolic algebra",
                                        "equation solving"
                                    ],
                                    "domains": [
                                        "Algebra",
                                        "Precalculus"
                                    ],
                                    "skills": [
                                        "algebraic manipulation",
                                        "problem solving",
                                        "critical thinking"
                                    ],
                                    "concepts": [
                                        "Rational expressions",
                                        "Factorization of quadratics",
                                        "Equation solving"
                                    ],
                                    "qualification": [
                                        "High school graduate",
                                        "College mathematics (optional)"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 13,
                                        "avg": 16,
                                        "max": 99
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "{\"problem\": \"Evaluate $|\\\\sqrt5+2i|$.\", \"level\": \"Level 1\", \"type\": \"Intermediate Algebra\", \"solution\": \"We have $|\\\\sqrt5+2i| = \\\\sqrt{(\\\\sqrt5)^2 + 2^2} = \\\\sqrt{5+4} = \\\\sqrt9 = \\\\boxed{3}$.\", \"_so",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "algebra",
                                        "numeric_calculation"
                                    ],
                                    "domains": [
                                        "Mathematics",
                                        "Algebra"
                                    ],
                                    "skills": [
                                        "basic arithmetic",
                                        "complex number basics"
                                    ],
                                    "concepts": [
                                        "complex number modulus",
                                        "Pythagorean theorem",
                                        "square root"
                                    ],
                                    "qualification": [
                                        "High School Diploma"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 15,
                                        "avg": 16,
                                        "max": 80
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "{\"problem\": \"Simplify $\\\\cos 10^\\\\circ \\\\cos 30^\\\\circ \\\\cos 50^\\\\circ \\\\cos 70^\\\\circ.$\", \"level\": \"Level 3\", \"type\": \"Precalculus\", \"solution\": \"From the product-to-sum formula, $\\\\cos 50^\\\\circ \\\\c",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "math",
                                        "problem_solving"
                                    ],
                                    "domains": [
                                        "Trigonometry",
                                        "Mathematics"
                                    ],
                                    "skills": [
                                        "problem_solving",
                                        "mathematical_reasoning",
                                        "algebraic_manipulation",
                                        "critical_thinking"
                                    ],
                                    "concepts": [
                                        "product-to-sum formula",
                                        "cosine addition identity",
                                        "cosine of special angles",
                                        "basic algebraic manipulation"
                                    ],
                                    "qualification": [
                                        "high_school",
                                        "undergraduate"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 15,
                                        "avg": 17,
                                        "max": 30
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "{\"problem\": \"Suppose $z$ is a complex number such that $z^3 = 100+75i$.  Find $|z|$.\", \"level\": \"Level 3\", \"type\": \"Intermediate Algebra\", \"solution\": \"Since $z^3 = 100+75i$, we must have $|z^3| = |10",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "algebra",
                                        "problem_solving"
                                    ],
                                    "domains": [
                                        "Mathematics"
                                    ],
                                    "skills": [
                                        "Algebraic manipulation",
                                        "Understanding of complex number properties",
                                        "Iteration of modulus operations"
                                    ],
                                    "concepts": [
                                        "Complex numbers",
                                        "Modulus of a complex number",
                                        "powers and roots of complex numbers",
                                        "properties of absolute value"
                                    ],
                                    "qualification": [
                                        "High_school senior",
                                        "College freshman (Mathematics)"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 16,
                                        "avg": 18,
                                        "max": 25
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "{\"problem\": \"A point has rectangular coordinates $(2,-1,-2)$ and spherical coordinates $(\\\\rho, \\\\theta, \\\\phi).$  Find the rectangular coordinates of the point with spherical coordinates $(\\\\rho, \\\\t",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "math_problem_solving",
                                        "coordinate_conversion"
                                    ],
                                    "domains": [
                                        "Mathematics"
                                    ],
                                    "skills": [
                                        "algebra",
                                        "trigonometry",
                                        "problem_solving",
                                        "reasoning"
                                    ],
                                    "concepts": [
                                        "Spherical Coordinates",
                                        "Trigonometric Identities",
                                        "Coordinate Geometry"
                                    ],
                                    "qualification": [
                                        "high_school graduate",
                                        "undergraduate mathematics student"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 16,
                                        "avg": 18,
                                        "max": 25
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "{\"problem\": \"How many subsets of the set of divisors of $72$ contain only composite numbers? For example, $\\\\{8,9\\\\}$ and $\\\\{4,8,12\\\\}$ are two such sets. Include the empty set in your count.\", \"leve",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "counting",
                                        "combinatorics",
                                        "math"
                                    ],
                                    "domains": [
                                        "Number Theory",
                                        "Combinatorics"
                                    ],
                                    "skills": [
                                        "problem_solving",
                                        "basic_algebra",
                                        "arithmetic",
                                        "logical_reasoning",
                                        "combinatorics"
                                    ],
                                    "concepts": [
                                        "Prime factorization",
                                        "Divisors",
                                        "Composite numbers",
                                        "Subsets",
                                        "Counting subsets"
                                    ],
                                    "qualification": [
                                        "High school"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 14,
                                        "avg": 18,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "{\"problem\": \"$12! = 47a001600$, for some digit $a$. What is the value of $a$?\", \"level\": \"Level 3\", \"type\": \"Number Theory\", \"solution\": \"Testing for divisibility by 9 does not work, because the sum o",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "number_theory",
                                        "problem_solving"
                                    ],
                                    "domains": [
                                        "Mathematics",
                                        "Number Theory"
                                    ],
                                    "skills": [
                                        "arithmetic_reasoning",
                                        "logical_reasoning",
                                        "knowledge_of_divisibility_tests",
                                        "pattern_recognition"
                                    ],
                                    "concepts": [
                                        "Factorial",
                                        "Divisibility rule for 9",
                                        "Divisibility rule for 11",
                                        "Alternating digit sum",
                                        "Decimal representation"
                                    ],
                                    "qualification": [
                                        "High_school_math"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 10,
                                        "avg": 14,
                                        "max": 80
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "{\"problem\": \"A four-digit perfect square number is created by placing two positive two-digit perfect square numbers next to each other. What is the four-digit square number?\", \"level\": \"Level 3\", \"typ",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "Math Problem Solving",
                                        "Number Theory"
                                    ],
                                    "domains": [
                                        "Number Theory"
                                    ],
                                    "skills": [
                                        "Algebra",
                                        "Critical Thinking",
                                        "Mathematical Reasoning",
                                        "Factorization"
                                    ],
                                    "concepts": [
                                        "Perfect Square",
                                        "Concatenation",
                                        "Factorization",
                                        "Diophantine Equation"
                                    ],
                                    "qualification": [
                                        "High school mathematics student",
                                        "Undergraduate mathematics student"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 14,
                                        "avg": 17,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "{\"problem\": \"If $n$ is a positive integer such that $2n$ has 28 positive divisors and $3n$ has 30 positive divisors, then how many positive divisors does $6n$ have?\", \"level\": \"Level 5\", \"type\": \"Numb",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "math_question_answering",
                                        "math_reasoning"
                                    ],
                                    "domains": [
                                        "Number Theory",
                                        "Mathematics"
                                    ],
                                    "skills": [
                                        "problem_solving",
                                        "critical_thinking",
                                        "algebra",
                                        "number_theory",
                                        "arithmetics",
                                        "combinatorics"
                                    ],
                                    "concepts": [
                                        "prime_factorization",
                                        "divisor_count_formula",
                                        "exponents",
                                        "systems_of_equations"
                                    ],
                                    "qualification": [
                                        "high_school",
                                        "undergraduate"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 13,
                                        "avg": 15,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "{\"problem\": \"Let $f(n)$ return the number of times $n$ appears in Pascal's Triangle. For example, $f(3) = 2$ and $f(4) = 2$. If $n$ is a positive integer, what is the minimum value of $f(n)$?\", \"level",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "counting",
                                        "probability",
                                        "math_problem"
                                    ],
                                    "domains": [
                                        "Combinatorics",
                                        "Discrete Mathematics"
                                    ],
                                    "skills": [
                                        "problem_solving",
                                        "reasoning",
                                        "combinatorial reasoning",
                                        "mathematical inference"
                                    ],
                                    "concepts": [
                                        "Pascal's Triangle",
                                        "Binomial Coefficients",
                                        "Symmetry",
                                        "Counting Occurrences"
                                    ],
                                    "qualification": [
                                        "high_school",
                                        "college_undergraduate"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 15,
                                        "avg": 17,
                                        "max": 100
                                    },
                                    "domain_expertise_required": true
                                }
                            ],
                            "total_questions": 5000,
                            "question_format": "Algebra, Algebra Problem Solving, Algebra problem solving, Algebraic manipulation, Algebraic_Problem, Arithmetical_reasoning, Combinatorics, Computation, Counting, Counting & Probability, Enumeration, Equation Solving, Equation solving, Geometry, Inequality Problem, Math Calculation, Math Problem, Math problem solving, Math_Problem, Mathematical problem solving, Number Theory, Number_Theory, Optimization, Prealgebra, Precalculus, Probability, Problem_Solving, Product calculation, QuestionAnswer, Question_and_Answer, Sequence Analysis, Simplification, algebra, algebra_problem, algebraic manipulation, algebraic_equation_solving, algebraic_manipulation, algebraic_reasoning, analysis, area_calculation, arithmetic, arithmetical_sum, arithmetics, basic_algebra, basic_calculation, calculation, calculations, classification, combinatorics, complex-number-problem, conceptual_reasoning, conic sections, coordinate geometry, coordinate_conversion, coordinate_geometry, counting, counting_and_probability, divisibility, enumeration, equation_solving, factoring, function_analysis, functional equation analysis, geometric_problem, geometry, geometry_problem_solving, inequality, intermediate_algebra, math, math problem solving, math_algebra, math_comprehension, math_evaluation, math_problem, math_problem_solving, math_reasoning, mathematical problem solving, mathematical_reasoning, mathematics, mathematics problem solving, mathematics_problem_solving, matrix_problem, number theory, number_theory, numerical_reasoning, optimization, polynomial, prealgebra, precalculus, probability, probability_problem, problem solving, problem_solving, qa, question_answer, question_answering, ratio_problem, reasoning, sequence_analysis, simple math, simplification, summation, trigonometry, unit_conversion",
                            "difficulty_levels": [
                                "Advanced",
                                "Beginner",
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Large, diverse set of 5,000 problems covering a wide range of math topics.",
                                "Includes both conceptual and calculation‑heavy questions, testing multiple skills.",
                                "Clear difficulty labels (Beginner, Intermediate) allow fine‑grained analysis.",
                                "Rich feature set (domains, skills) aids in diagnosing specific strengths/weaknesses."
                            ],
                            "disadvantages": [
                                "No items beyond the Intermediate difficulty level, limiting assessment of advanced problem solving.",
                                "Problems are mostly textbook‑style; lacks real‑world applied math contexts.",
                                "Solution formats vary; inconsistencies may introduce noise for evaluation scripts.",
                                "High reliance on symbolic manipulation may expose ceiling effects for many existing LLMs."
                            ]
                        },
                        "age_distribution": {
                            "data": [
                                13,
                                40,
                                50,
                                79,
                                17,
                                1,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0
                            ],
                            "categories": [
                                "10",
                                "12",
                                "14",
                                "16",
                                "18",
                                "20",
                                "22",
                                "24",
                                "26",
                                "28",
                                "30",
                                "32",
                                "34",
                                "36",
                                "38",
                                "40",
                                "42",
                                "44",
                                "46",
                                "48",
                                "50",
                                "52",
                                "54",
                                "56",
                                "58",
                                "60"
                            ]
                        },
                        "why_run_this_eval": [
                            "Run this evaluation when you want to benchmark a model’s mathematical problem‑solving skills, assess its ability to carry out multi‑step algebraic reasoning, test its arithmetic precision, and gauge its performance on number‑theoretic and combinatorial questions.",
                            "It is particularly useful for models intended for educational or tutoring applications, or for any system that must handle math‑related queries."
                        ],
                        "what_to_expect": [
                            "When a language model is evaluated on MATH, the resulting score indicates how well it can parse mathematical sentences, apply algebraic rules, perform symbolic calculations, and reason about number theory or trigonometric identities.",
                            "A high score shows the model can reliably understand problem contexts, carry out multi‑step reasoning, and produce correct solutions.",
                            "A low score suggests weaknesses in mathematical reasoning or symbolic manipulation."
                        ],
                        "evaluation_description": "The MATH dataset contains 5,000 handwritten and digitally typed math problems that span typical middle‑school to early college topics. Each row includes a problem statement, sometimes a difficulty label, the domain it belongs to, and a solution or partial solution. The problems cover Algebra, Combinatorics, Number Theory, Trigonometry, Precalculus, as well as discrete mathematics concepts. Most items are at the Beginner or Intermediate level, designed to test algebraic manipulation, arithmetic reasoning, critical thinking, and pattern recognition.",
                        "top_5_task_types": [
                            "algebra",
                            "math_problem",
                            "question_answering",
                            "math_problem_solving",
                            "qa"
                        ],
                        "top_5_domains": [
                            "Mathematics",
                            "Algebra",
                            "Geometry",
                            "Number Theory",
                            "Trigonometry"
                        ],
                        "top_5_skills": [
                            "problem_solving",
                            "critical_thinking",
                            "arithmetic",
                            "algebraic manipulation",
                            "critical thinking"
                        ],
                        "top_5_concepts": [
                            "multiplication",
                            "Pythagorean theorem",
                            "division",
                            "fractions",
                            "linear equations"
                        ],
                        "top_5_qualifications": [
                            "high_school",
                            "middle_school",
                            "undergraduate",
                            "High School Diploma",
                            "High School"
                        ],
                        "top_5_languages": [
                            "English"
                        ]
                    },
                    "analysis_file": "analysis/opencompass_534_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 200,
                        "successful": 200,
                        "failed": 0
                    }
                },
                {
                    "id": "opencompass_542",
                    "name": "LongBench",
                    "version": "1.0.0",
                    "description": "LongBench is a benchmark for bilingual, multitask, and comprehensive assessment of long context understanding capabilities of large language models.",
                    "url": "opencompass/opencompass_542.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_542",
                    "sample_count": 1000,
                    "traits": [
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "542",
                        "name": "LongBench",
                        "emoji": "🪑",
                        "dimensions": [
                            {
                                "cn": "长文本",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "long-context",
                                "en": "long-context"
                            }
                        ],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/THUDM/LongBench",
                        "paperLink": "https://arxiv.org/pdf/2308.14508.pdf",
                        "officialWebsiteLink": "https://huggingface.co/datasets/THUDM/LongBench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "8912",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:32:08",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-12 19:32:08",
                        "createDate": "2024-09-12 19:26:26",
                        "desc": {
                            "cn": "LongBench 是一个多任务、中英双语、针对大语言模型长文本理解能力的评测基准。",
                            "en": "LongBench is a benchmark for bilingual, multitask, and comprehensive assessment of long context understanding capabilities of large language models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/longbench'. Error: Path opencompass/longbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_535",
                    "name": "GSM8K",
                    "version": "1.0.0",
                    "description": "GSM8K is a dataset of 8,500 high quality linguistically diverse grade school math word problems created by human problem writers. The dataset is segmented into 7,500 training problems and 1,000 test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ − × ÷) to reach the final answer.",
                    "url": "opencompass/opencompass_535.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_535",
                    "sample_count": 1319,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {
                        "gen": "gsm8k_gen",
                        "agent": "gsm8k_agent"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 55,
                        "estimated_output_tokens": 95
                    },
                    "original_data": {
                        "id": "535",
                        "name": "GSM8K",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "math",
                                "en": "math"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/openai/grade-school-math",
                        "paperLink": "https://arxiv.org/pdf/2110.14168.pdf",
                        "officialWebsiteLink": "https://huggingface.co/datasets/gsm8k",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "8894",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:31:32",
                        "supportOnlineEval": true,
                        "updateDate": "2024-09-12 19:31:32",
                        "createDate": "2024-09-12 19:27:59",
                        "desc": {
                            "cn": "GSM8K 是一个包含 8,500 个高质量、语言多样化的小学数学单词问题的数据集，由人类问题编写者创建。该数据集分为 7,500 个训练问题和 1,000 个测试问题。这些问题的解题步骤在 2 到 8 步之间，解题过程主要涉及使用基本算术运算（+ - × ÷）进行一连串的基本计算，从而得出最终答案。一个聪明的初中生应该能够解决每一个问题。它可用于多步数学推理。",
                            "en": "GSM8K is a dataset of 8,500 high quality linguistically diverse grade school math word problems created by human problem writers. The dataset is segmented into 7,500 training problems and 1,000 test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ − × ÷) to reach the final answer."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "Jared is trying to increase his typing speed. He starts with 47 words per minute (WPM). After some lessons the next time he tests his typing speed it has increased to 52 WPM. If he continues to increa",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "math"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic_math",
                                        "arithmetic",
                                        "average_calculation"
                                    ],
                                    "concepts": [
                                        "addition",
                                        "average",
                                        "arithmetic_mean"
                                    ],
                                    "qualification": [
                                        "middle_school"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 8,
                                        "avg": 12,
                                        "max": 70
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "Jordan has 2 children who wear diapers.  Each child requires 5 diaper changes per day.  Jordan's wife changes half of the diapers.  How many diapers does Jordan change per day?",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "question_answering",
                                        "math"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic_arithmetic",
                                        "problem_solving",
                                        "logical_reasoning"
                                    ],
                                    "concepts": [
                                        "addition",
                                        "division",
                                        "basic_arithmetic"
                                    ],
                                    "qualification": [
                                        "middle_school"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 8,
                                        "avg": 12,
                                        "max": 90
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "A wooden bridge can carry no more than 5000 pounds. A delivery truck filled with identical boxes, each weighing 15 pounds, will pass over the bridge. The combined weight of the driver and the empty tr",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "math_problem",
                                        "qa"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "arithmetic",
                                        "division",
                                        "basic algebra",
                                        "problem_solving"
                                    ],
                                    "concepts": [
                                        "arithmetic calculation",
                                        "division",
                                        "capacity constraint"
                                    ],
                                    "qualification": [
                                        "high_school"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 10,
                                        "avg": 12,
                                        "max": 70
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "Tim has a box with 7 blue shoe boxes and 9 red shoe boxes. If he uses 3 blue shoeboxes and 1/3 red of his shoeboxes to go fishing, how many red and blue shoe boxes are left in Tim's box?",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "question_answering",
                                        "arithmetic"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic_arithmetic",
                                        "fraction_calculation",
                                        "subtraction"
                                    ],
                                    "concepts": [
                                        "basic arithmetic",
                                        "fractions",
                                        "subtraction",
                                        "counting"
                                    ],
                                    "qualification": [
                                        "primary school",
                                        "grade 4"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 8,
                                        "avg": 10,
                                        "max": 70
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "Dominick went to his team's changing room and saw half as many robots as helmets and half as many helmets as footballs kept there. If there were 20 helmets, calculate the total number of items Dominic",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "question_answering",
                                        "mathematical_problem_solving"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic_calculation",
                                        "number_reasoning",
                                        "logic"
                                    ],
                                    "concepts": [
                                        "basic arithmetic",
                                        "fraction (halves)",
                                        "simple addition"
                                    ],
                                    "qualification": [
                                        "5th grade (or equal basic math proficiency)"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 8,
                                        "avg": 12,
                                        "max": 80
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "The school auditorium has 4 rows of seats. There are 18 seats in each row. One-fourth of the seats were occupied by the administrators. One-third of the remaining seats were occupied by the parents an",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "numeric reasoning",
                                        "word problem"
                                    ],
                                    "domains": [
                                        "Mathematics"
                                    ],
                                    "skills": [
                                        "basic arithmetic",
                                        "fraction skills",
                                        "reading comprehension"
                                    ],
                                    "concepts": [
                                        "fractions",
                                        "multiplication",
                                        "subtraction"
                                    ],
                                    "qualification": [
                                        "Primary school student (5th grade)"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 7,
                                        "avg": 9,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "A pet store currently has 5 dogs, 2 cats, and 10 birds. How many legs in total do the pets in the store have?",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "question_answering",
                                        "math"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic_arithmetic",
                                        "reading_comprehension",
                                        "problem_solving"
                                    ],
                                    "concepts": [
                                        "basic_multiplication",
                                        "addition",
                                        "counting",
                                        "reading_comprehension",
                                        "simple_word_problem"
                                    ],
                                    "qualification": [
                                        "1st_grade",
                                        "2nd_grade",
                                        "3rd_grade",
                                        "4th_grade"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 7,
                                        "avg": 10,
                                        "max": 99
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "A car in the fast lane is traveling at 60 miles/hour. A car in the slow lane is traveling at half that speed. If the car in the fast lane traveled for a total of 480 miles, calculate the time the car ",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "qa",
                                        "calculations"
                                    ],
                                    "domains": [
                                        "Mathematics"
                                    ],
                                    "skills": [
                                        "basic_arithmetic",
                                        "problem_solving",
                                        "rate_time_distance_calculation"
                                    ],
                                    "concepts": [
                                        "distance",
                                        "speed",
                                        "time",
                                        "rate"
                                    ],
                                    "qualification": [
                                        "middle_school"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 8,
                                        "avg": 12,
                                        "max": 80
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "Joe has $50 to buy an outfit for his new field trip. There is a 30% off sale at the clothing store. The shirt he picks out has a price of $25. He also picks out a pair of shorts for $35. Assuming that",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "qa",
                                        "calculation",
                                        "math_problem"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic_arithmetic",
                                        "percentage_calculation",
                                        "financial_reasoning",
                                        "reading_comprehension"
                                    ],
                                    "concepts": [
                                        "percentage_discount",
                                        "sales_tax",
                                        "simple_arithmetic",
                                        "budgeting"
                                    ],
                                    "qualification": [
                                        "6th_grade_math"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 8,
                                        "avg": 12,
                                        "max": 17
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "A glass of milk is 8 ounces of milk.  John drinks 2 glasses of milk.  If milk has 3 calories per ounce how many calories did he consume?",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "qa"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic_math",
                                        "comprehension"
                                    ],
                                    "concepts": [
                                        "multiplication",
                                        "unit conversion",
                                        "basic arithmetic"
                                    ],
                                    "qualification": [
                                        "Kindergarten"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 6,
                                        "avg": 8,
                                        "max": 100
                                    },
                                    "domain_expertise_required": false
                                }
                            ],
                            "total_questions": 1319,
                            "question_format": "Algebra, Basic_Math_Problem, Calculations, Math problem solving, Mathematical Reasoning, QA, Word problem, addition, algebra, arithmetic, arithmetical reasoning, arithmetics, basic_algebra, basic_arithmetic, basic_calculation, basic_computation, basic_math, basic_math_problem, basic_problem_solving, calculation, calculational, calculations, computation, counting, financial_calculation, financial_math, math, math problem solving, math word problem, math_basic, math_calculation, math_problem, math_problem_solving, math_problems, math_question, math_riddle, math_word_problem, mathematics, problem solving, problem_solving, puzzle, qa, question_answer, question_answering, reading_comprehension, reasoning, riddle, simple_arithmetic, simple_calculation, simple_computation, simple_math, word_problem",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Large sample size (1319 questions) provides robust statistical evidence.",
                                "Problems are beginner‑level, so even early‑stage models can achieve meaningful scores, enabling quick iterative testing.",
                                "Dataset is domain‑specific to Mathematics, reducing noise from unrelated content.",
                                "Explicit representation of skill types (arithmetic, logical reasoning, etc.) allows targeted analysis of model strengths and weaknesses.",
                                "Publicly available and widely used, facilitating comparison with other studies."
                            ],
                            "disadvantages": [
                                "Only covers beginner-level problems; does not challenge models with advanced algebra, calculus, or domain‑specific math contexts.",
                                "All questions are in English, limiting evaluation of multilingual numeric reasoning capabilities.",
                                "Potential for bias toward certain arithmetic patterns (e.g., many problems involve subtraction of small numbers).",
                                "Because the dataset is smallish compared to generative‑QA benchmarks, statistical power for niche skills may be limited.",
                                "Does not test the model’s ability to handle ambiguous or multi‑step reasoning beyond the beginner scope."
                            ]
                        },
                        "age_distribution": {
                            "data": [
                                116,
                                65,
                                11,
                                3,
                                0,
                                0,
                                0,
                                1,
                                0,
                                0,
                                4,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0
                            ],
                            "categories": [
                                "10",
                                "12",
                                "14",
                                "16",
                                "18",
                                "20",
                                "22",
                                "24",
                                "26",
                                "28",
                                "30",
                                "32",
                                "34",
                                "36",
                                "38",
                                "40",
                                "42",
                                "44",
                                "46",
                                "48",
                                "50",
                                "52",
                                "54",
                                "56",
                                "58",
                                "60"
                            ]
                        },
                        "why_run_this_eval": [
                            "Run this evaluation to assess a model’s:\n- Basic arithmetic capability (addition, subtraction, multiplication, division).\n- Handling of fractions, percentages, and simple algebraic expressions.\n- Ability to read and interpret plain‑text word problems.\n- Consistency in applying rules of calculation across varying contexts.",
                            "These skills are essential for educational applications, tutoring assistants, or any task requiring quick numeric reasoning."
                        ],
                        "what_to_expect": [
                            "When a large language model is scored on GSM8K, the percentage of correctly solved problems indicates its proficiency in basic numeric reasoning, arithmetic manipulation, and word‑problem interpretation.",
                            "A high score (e.g., >70%) suggests that the model can reliably parse and solve straightforward mathematical questions and perform simple calculations, while a low score points to weaknesses in number sense, arithmetic accuracy, or understanding of the problem context."
                        ],
                        "evaluation_description": "GSM8K is an open‑source collection of 1,319 arithmetic word problems written in English. Each row contains a single question that tests basic mathematical skills such as addition, subtraction, multiplication, division, fractions, percentages, and simple algebraic reasoning. The problems are designed for a beginner difficulty level, making the dataset suitable for early‑stage model evaluation or educational tools aimed at young learners. The dataset focuses exclusively on the Mathematics domain and covers a broad range of arithmetic and number‑reasoning skills, including reading comprehension of word problems and logical reasoning about quantities.",
                        "top_5_task_types": [
                            "qa",
                            "math_problem",
                            "math",
                            "question_answering",
                            "calculations"
                        ],
                        "top_5_domains": [
                            "Mathematics"
                        ],
                        "top_5_skills": [
                            "problem_solving",
                            "basic_arithmetic",
                            "basic arithmetic",
                            "reading_comprehension",
                            "problem solving"
                        ],
                        "top_5_concepts": [
                            "addition",
                            "multiplication",
                            "subtraction",
                            "basic_arithmetic",
                            "fraction"
                        ],
                        "top_5_qualifications": [
                            "5th grade",
                            "1st grade",
                            "Middle school",
                            "High school",
                            "primary_school"
                        ],
                        "top_5_languages": [
                            "English"
                        ]
                    },
                    "analysis_file": "analysis/opencompass_535_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 200,
                        "successful": 200,
                        "failed": 0
                    }
                },
                {
                    "id": "opencompass_512",
                    "name": "TriviaQA",
                    "version": "1.0.0",
                    "description": "TriviaqQA is a reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaqQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions.",
                    "url": "opencompass/opencompass_512.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_512",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {
                        "gen": "TriviaQA_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "512",
                        "name": "TriviaQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/mandarjoshi90/triviaqa",
                        "paperLink": "https://arxiv.org/abs/1705.03551",
                        "officialWebsiteLink": "http://nlp.cs.washington.edu/triviaqa/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "6251",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": true,
                        "updateDate": "2024-08-02 09:45:49",
                        "createDate": "2024-01-11 14:10:43",
                        "desc": {
                            "cn": "TriviaqQA是一个阅读理解数据集，包含超过65万个问题-答案-证据三元组。其包括95K个问答对，由冷知识爱好者和独立收集的事实性文档撰写，平均每个问题6个，为回答问题提供高质量的远程监督。\n",
                            "en": "TriviaqQA is a reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaqQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/triviaqa'. Error: Path opencompass/triviaqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_540",
                    "name": "T-Eval",
                    "version": "1.0.0",
                    "description": "T-Eval evaluates the tool utilization capabilities of LLMs and decomposing them into instruction following, planning, reasoning, retrieval, understanding, and review.",
                    "url": "opencompass/opencompass_540.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_540",
                    "sample_count": 1000,
                    "traits": [
                        "agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "540",
                        "name": "T-Eval",
                        "emoji": "🗽",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 3,
                        "githubLink": "https://github.com/open-compass/T-Eval",
                        "paperLink": "https://arxiv.org/abs/2312.14033",
                        "officialWebsiteLink": "https://open-compass.github.io/T-Eval/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "5850",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-01-25 15:15:04",
                        "createDate": "2024-01-25 15:15:04",
                        "desc": {
                            "cn": "T-Eval 评估了 LLM 的工具使用能力，并将其分解为指令遵循、规划、推理、检索、理解和审查等子能力",
                            "en": "T-Eval evaluates the tool utilization capabilities of LLMs and decomposing them into instruction following, planning, reasoning, retrieval, understanding, and review."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/t_eval'. Error: Path opencompass/t_eval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_895",
                    "name": "Fin-Eva",
                    "version": "1.0.0",
                    "description": "Ant Group and Shanghai University of Finance and Economics jointly launched the financial benchmark，Fin-Eva Version 1.0, covering multiple financial scenarios and subjects such as wealth management, insurance, investment research. The number of this benchmark's questions reaches 13,000+.",
                    "url": "opencompass/opencompass_895.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_895",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "895",
                        "name": "Fin-Eva",
                        "emoji": "📈",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "knowledge;Finance",
                                "en": "knowledge;Finance"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 4,
                        "githubLink": "https://github.com/alipay/financial_evaluation_dataset",
                        "paperLink": "",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50074087",
                            "name": "Ant_Group",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50074087-492c3cdd-1529-4e49-84ad-8cc2f8c86893.png",
                            "nickname": "蚂蚁集团"
                        },
                        "lookNum": "5750",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:40:45",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-12 19:40:45",
                        "createDate": "2024-09-12 19:36:26",
                        "desc": {
                            "cn": "蚂蚁集团、上海财经大学联合推出金融评测集Fin-Eva Version 1.0，覆盖财富管理、保险、投资研究等多个金融场景以及金融专业主题学科，总评测题数目达到13,000+。",
                            "en": "Ant Group and Shanghai University of Finance and Economics jointly launched the financial benchmark，Fin-Eva Version 1.0, covering multiple financial scenarios and subjects such as wealth management, insurance, investment research. The number of this benchmark's questions reaches 13,000+."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/fin_eva'. Error: Path opencompass/fin_eva is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_498",
                    "name": "MMLU",
                    "version": "1.0.0",
                    "description": "MMLU (Massive Multitask Language Understanding) is a new benchmark designed to measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the benchmark more challenging and more similar to how we evaluate humans. The benchmark covers 57 subjects across STEM, the humanities, the social sciences, and more. It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and probl",
                    "url": "opencompass/opencompass_498.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_498",
                    "sample_count": 14042,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 202,
                        "estimated_output_tokens": 1
                    },
                    "original_data": {
                        "id": "498",
                        "name": "MMLU",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "学科",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/hendrycks/test",
                        "paperLink": "https://arxiv.org/abs/2009.03300",
                        "officialWebsiteLink": "https://github.com/hendrycks/test",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "5070",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": true,
                        "updateDate": "2024-08-02 09:45:42",
                        "createDate": "2024-01-11 14:09:29",
                        "desc": {
                            "cn": "MMLU (Massive Multitask Language Understanding) 是一个新的基准测试，旨在通过在零次学习和少次学习的环境中评估模型来测量预训练期间获得的知识。这使得基准测试更具挑战性，且更接近我们评估人类的方式。该基准测试涵盖了STEM、人文学科、社会科学等57个主题。其难度范围从小学级别到专业级别，旨在测试世界知识和解决问题的能力。测试主题范围从传统领域，如数学和历史，到更专业的领域，如法律和伦理学。题目的精细度和广度使该基准测试成为识别模型盲点的理想选择。",
                            "en": "MMLU (Massive Multitask Language Understanding) is a new benchmark designed to measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the benchmark more challenging and more similar to how we evaluate humans. The benchmark covers 57 subjects across STEM, the humanities, the social sciences, and more. It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and problem solving ability. Subjects range from traditional areas, such as mathematics and history, to more specialized areas like law and ethics. The granularity and breadth of the subjects makes the benchmark ideal for identifying a model’s blind spots."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "The market for denim jeans is in equilibrium, and the price of polyester pants, a substitute good, rises. In the jean market\n\nOptions:\nA) supply falls, increasing the price and decreasing the quantity",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "qa",
                                        "multiple_choice"
                                    ],
                                    "domains": [
                                        "Economics"
                                    ],
                                    "skills": [
                                        "critical_thinking",
                                        "economic_reasoning",
                                        "conceptual_understanding"
                                    ],
                                    "concepts": [
                                        "Supply and Demand",
                                        "Substitute Goods",
                                        "Market Equilibrium",
                                        "Demand Curve Shift"
                                    ],
                                    "qualification": [
                                        "high_school",
                                        "undergraduate_intro_economics",
                                        "college_major_economics"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 14,
                                        "avg": 18,
                                        "max": 80
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "The code fragment below is intended to display \"odd\" if the positive number num is odd.\n IF (<MISSING CONDITION>)\n DISPLAY “odd”\n\n Which of the following can be used to replace <MISSING CONDITION> so ",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "coding",
                                        "multiple_choice"
                                    ],
                                    "domains": [
                                        "Computer Science"
                                    ],
                                    "skills": [
                                        "logical reasoning",
                                        "basic programming",
                                        "modular arithmetic"
                                    ],
                                    "concepts": [
                                        "modulus operation",
                                        "odd/even detection",
                                        "conditional logic"
                                    ],
                                    "qualification": [
                                        "High school graduate",
                                        "Basic programming knowledge"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 10,
                                        "avg": 12,
                                        "max": 90
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "Experimenter A uses a very small test charge qo, and experimenter B uses a test charge 2qo to measure an electric field produced by two parallel plates. A finds a field that is\n\nOptions:\nA) greater th",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "multiple_choice",
                                        "question_and_answer"
                                    ],
                                    "domains": [
                                        "Physics"
                                    ],
                                    "skills": [
                                        "conceptual understanding",
                                        "basic physics knowledge",
                                        "critical thinking"
                                    ],
                                    "concepts": [
                                        "Electric field",
                                        "Test charge",
                                        "Force",
                                        "Acceleration",
                                        "F = ma"
                                    ],
                                    "qualification": [
                                        "High School Physics"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 14,
                                        "avg": 16,
                                        "max": 21
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "What was the first American college to become coeducational?\n\nOptions:\nA) Oberlin College\nB) Dartmouth College\nC) Grinnell College\nD) Antioch College",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "qa"
                                    ],
                                    "domains": [
                                        "History",
                                        "Education"
                                    ],
                                    "skills": [
                                        "reading_comprehension",
                                        "recall"
                                    ],
                                    "concepts": [
                                        "coeducation",
                                        "American college history",
                                        "Oberlin College",
                                        "women's education in the U.S."
                                    ],
                                    "qualification": [
                                        "high_school_graduate"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 16,
                                        "max": 90
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "A supplier of ink for printers sent the following letter to all of its customers:\"Closeout special! We have decided to no longer stock green ink cartridges. We have on hand a limited supply of green i",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "qa",
                                        "legal_reasoning"
                                    ],
                                    "domains": [
                                        "Law",
                                        "Contract Law"
                                    ],
                                    "skills": [
                                        "critical_thinking",
                                        "legal_reasoning",
                                        "analysis"
                                    ],
                                    "concepts": [
                                        "contract formation",
                                        "breach of contract",
                                        "damages",
                                        "incidental damages",
                                        "consequential damages",
                                        "rescission",
                                        "last-inventory"
                                    ],
                                    "qualification": [
                                        "high_school",
                                        "undergraduate"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 16,
                                        "avg": 21,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "Which of these is an Italian Design firm?\n\nOptions:\nA) Escada\nB) Fendi\nC) Ghost\nD) Mainbocher",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "question_answering",
                                        "multiple_choice"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "knowledge recall",
                                        "cultural literacy"
                                    ],
                                    "concepts": [
                                        "fashion brands",
                                        "Italian design"
                                    ],
                                    "qualification": [
                                        "High School Diploma"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 17,
                                        "max": 70
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "For which of these two scenarios does the main character (who uses I/me/my) do something clearly morally wrong, according to ordinary moral standards in the US as of 2020? Scenario 1 | I put my three ",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "moral_judgment",
                                        "classification",
                                        "reasoning"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "critical_thinking",
                                        "empathetic_understanding",
                                        "cultural_familiarity",
                                        "ethical_analysis"
                                    ],
                                    "concepts": [
                                        "moral reasoning",
                                        "child discipline",
                                        "personal autonomy",
                                        "cultural norms"
                                    ],
                                    "qualification": [
                                        "high_school_graduate",
                                        "undergraduate",
                                        "master's",
                                        "professional"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 14,
                                        "avg": 35,
                                        "max": 80
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": " Aquinas's moral theory is a version of\n\nOptions:\nA) consequentialism.\nB) natural law theory.\nC) rights-based theory.\nD) virtue ethics.",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "question_answering"
                                    ],
                                    "domains": [
                                        "Philosophy",
                                        "Theology"
                                    ],
                                    "skills": [
                                        "knowledge recall",
                                        "critical thinking",
                                        "conceptual understanding"
                                    ],
                                    "concepts": [
                                        "Natural law theory",
                                        "Aquinas's moral theory",
                                        "Ethics"
                                    ],
                                    "qualification": [
                                        "high_school",
                                        "undergraduate",
                                        "PhD"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 16,
                                        "avg": 25,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "Observational epidemiology studies have produced useful in identifying health effects of many food contaminants, but not for assessing risk, because:\n\n\nOptions:\nA) It is never possible to know whether",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "quiz",
                                        "multiple_choice_question",
                                        "knowledge_verification"
                                    ],
                                    "domains": [
                                        "Epidemiology",
                                        "Public Health",
                                        "Environmental Health",
                                        "Toxicology"
                                    ],
                                    "skills": [
                                        "critical_thinking",
                                        "domain_knowledge in epidemiology and toxicology",
                                        "interpretation of scientific options"
                                    ],
                                    "concepts": [
                                        "Observational Epidemiology",
                                        "Risk Assessment",
                                        "Exposure Assessment",
                                        "Dose-Response Relationship",
                                        "Confounding Factors"
                                    ],
                                    "qualification": [
                                        "High school graduate with biology background (minimum)",
                                        "Bachelor's degree in Public Health, Epidemiology, or related field"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 16,
                                        "avg": 22,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "The \"c\" in the word cat is best described as a\n\nOptions:\nA) phoneme.\nB) morpheme.\nC) holophrase.\nD) syllable.",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "qa"
                                    ],
                                    "domains": [
                                        "Linguistics",
                                        "Phonetics"
                                    ],
                                    "skills": [
                                        "reading_comprehension",
                                        "basic_linguistics_understanding"
                                    ],
                                    "concepts": [
                                        "phoneme",
                                        "morpheme",
                                        "holophrase",
                                        "syllable",
                                        "phonological units"
                                    ],
                                    "qualification": [
                                        "middle_school",
                                        "high_school"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 15,
                                        "max": 90
                                    },
                                    "domain_expertise_required": true
                                }
                            ],
                            "total_questions": 14042,
                            "question_format": "Classification, Legal Analysis, Legal Reasoning, Logic, Moral_Judgment, Multiple Choice, Multiple Choice Question, Multiple Choice Question Analysis, Multiple Choice Reasoning, QA, Question Answering, QuestionAnswering, Reader Comprehension, ReadingComprehension, algebra, analysis, arithmetic, basic_math, calculation, case_analysis, chemistry_facts, classification, conceptual_understanding, contract_analysis, critical_thinking, decision_making, economics, ethical_judgment, fact recall, knowledge_assessment, knowledge_based, knowledge_based_question, knowledge_check, knowledge_extraction, knowledge_question, knowledge_recall, knowledge_recognition, knowledge_verification, law problem solving, legal analysis, legal reasoning, legal_analysis, legal_reasoning, logic_symbolization, logic_translation, logical_reasoning, math, math_problem, math_problem_solving, medical reasoning, medical_ethical_reasoning, ml_test, moral reasoning, moral_judgement, moral_judgment, moral_reasoning, moral_sentiment_analysis, multiple-choice, multiple_choice, multiple_choice_answer, multiple_choice_qa, multiple_choice_question, multiple_choice_question_answering, multiple_choice_quiz, negotiated_qa, polynomial_root_finding, problem_solving, professional_consultation, q_a, qa, qc, question answering, question_answer, question_answering, quiz, quiz_answering, reading_comprehension, reasoning, regulatory_advisory, statistics, trivia",
                            "difficulty_levels": [
                                "Advanced",
                                "Beginner",
                                "Expert",
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Extremely large (14,042 questions) covering vast subject matter.",
                                "Rich diversity in domain, task type, and difficulty level.",
                                "Supports multi‑choice and reasoning‑heavy scenarios.",
                                "Facilitates weak‑spot detection by skill and domain.",
                                "Open‑source and widely adopted in the research community."
                            ],
                            "disadvantages": [
                                "Highly uneven domain and skill frequency, leading to bias toward over‑represented topics.",
                                "Redundant and inconsistent skill labels (e.g., multiple variants of “critical thinking”), making fine‑grained analysis difficult.",
                                "Age distribution limited to a narrow range, lacking broader demographic granularity.",
                                "No contextual metadata for questions (e.g., image, code snippets), restricting analysis of multimodal capabilities.",
                                "Potential platform bias where the dataset was originally curated for English‑speaking academic contexts."
                            ]
                        },
                        "age_distribution": {
                            "data": [
                                6,
                                13,
                                27,
                                51,
                                13,
                                28,
                                16,
                                18,
                                1,
                                2,
                                12,
                                1,
                                10,
                                0,
                                0,
                                1,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0
                            ],
                            "categories": [
                                "10",
                                "12",
                                "14",
                                "16",
                                "18",
                                "20",
                                "22",
                                "24",
                                "26",
                                "28",
                                "30",
                                "32",
                                "34",
                                "36",
                                "38",
                                "40",
                                "42",
                                "44",
                                "46",
                                "48",
                                "50",
                                "52",
                                "54",
                                "56",
                                "58",
                                "60"
                            ]
                        },
                        "why_run_this_eval": [
                            "The dataset is a benchmark for general language model competency and generalized reasoning.",
                            "Running an evaluation helps developers gauge model readiness for real‑world knowledge transfer, identify capability gaps, guide targeted training or fine‑tuning, and provide interpretable metrics that stakeholders can trust for educational or professional applications."
                        ],
                        "what_to_expect": [
                            "Evaluating MMLU will reveal how well a model performs across a wide spectrum of academic and professional knowledge areas, difficulty levels, and question types.",
                            "Analysts will see which domains and skill categories (e.g., logical reasoning, domain-specific facts, procedural knowledge) the model excels or struggles with, detect patterns of over- or under‑representation in the age distribution, and assess how the dataset tests reasoning and factual recall rather than rote memorization.",
                            "The exercise offers insights into model strengths on advanced, intermediate, and beginner content and yields a granular breakdown of performance by domain, skill, and task type."
                        ],
                        "top_5_task_types": [
                            "qa",
                            "multiple_choice",
                            "question_answering",
                            "programming_knowledge",
                            "Legal Analysis"
                        ],
                        "top_5_domains": [
                            "Economics",
                            "Computer Science",
                            "Programming",
                            "Physics",
                            "Contract Law"
                        ],
                        "top_5_skills": [
                            "critical_thinking",
                            "reading_comprehension",
                            "basic_economic_analysis",
                            "reasoning",
                            "Logical reasoning"
                        ],
                        "top_5_concepts": [
                            "supply and demand",
                            "equilibrium",
                            "substitute goods",
                            "cross-price elasticity",
                            "Modulus operator"
                        ],
                        "top_5_qualifications": [
                            "high_school_graduate",
                            "high_school",
                            "High School",
                            "Middle School (Grade 8+)",
                            "high school physics"
                        ],
                        "top_5_languages": [
                            "English"
                        ]
                    },
                    "analysis_file": "analysis/opencompass_498_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 200,
                        "successful": 199,
                        "failed": 1
                    }
                },
                {
                    "id": "opencompass_496",
                    "name": "C-Eval",
                    "version": "1.0.0",
                    "description": "C-Eval is a comprehensive Chinese evaluation suite for foundation models. It consists of 13948 multi-choice questions spanning 52 diverse disciplines and four difficulty levels.",
                    "url": "opencompass/opencompass_496.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_496",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "496",
                        "name": "C-Eval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "学科",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/SJTU-LIT/ceval",
                        "paperLink": "https://arxiv.org/abs/2305.08322",
                        "officialWebsiteLink": "https://cevalbenchmark.com/index.html",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "5034",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": true,
                        "updateDate": "2024-01-11 14:09:22",
                        "createDate": "2024-01-11 14:09:22",
                        "desc": {
                            "cn": "C-Eval 是一个全面的中文基础模型评估套件。它包含了13948个多项选择题，涵盖了52个不同的学科和四个难度级别。",
                            "en": "C-Eval is a comprehensive Chinese evaluation suite for foundation models. It consists of 13948 multi-choice questions spanning 52 diverse disciplines and four difficulty levels."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/c_eval'. Error: Path opencompass/c_eval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_541",
                    "name": "L-Eval",
                    "version": "1.0.0",
                    "description": "L-Eval is a comprehensive Long Context Language Models (LCLMs) evaluation suite with 20 sub-tasks, 508 long documents, and over 2,000 human-labeled query-response pairs encompassing diverse question styles, domains, and input length (3k～200k tokens).",
                    "url": "opencompass/opencompass_541.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_541",
                    "sample_count": 1000,
                    "traits": [
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "541",
                        "name": "L-Eval",
                        "emoji": "🦾",
                        "dimensions": [
                            {
                                "cn": "长文本",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "long-context",
                                "en": "long-context"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/OpenLMLab/LEval",
                        "paperLink": "https://arxiv.org/pdf/2307.11088",
                        "officialWebsiteLink": "https://huggingface.co/datasets/L4NLP/LEval",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "5031",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:32:04",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-12 19:32:04",
                        "createDate": "2024-09-12 19:26:40",
                        "desc": {
                            "cn": "L-Eval 是一个全面的长上下文语言模型（LCLMs）评估套件，包括 20 个子任务、508 个长文档和超过 2,000 个人工标记的查询-响应对。它涵盖了多种问答风格、领域和输入长度（3,000 至 200,000 个 token）。",
                            "en": "L-Eval is a comprehensive Long Context Language Models (LCLMs) evaluation suite with 20 sub-tasks, 508 long documents, and over 2,000 human-labeled query-response pairs encompassing diverse question styles, domains, and input length (3k～200k tokens)."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/l_eval'. Error: Path opencompass/l_eval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_948",
                    "name": "SecBench",
                    "version": "1.0.0",
                    "description": "Tencent Zhuque Lab and Tencent Security Keen Lab, together with Tencent Huyuan Team, Professor Jiang Yong/Professor Xia Shutao's team from Tsinghua University, Professor Luo Xiapu's research team from Hong Kong Polytechnic University, and OpenCompass team from Shanghai Artificial Intelligence Laboratory, have jointly built a safety benchmark, SecBench. We provide fair, impartial, objective, and comprehensive evaluation capabilities and promote the construction of large model in security dimensio",
                    "url": "opencompass/opencompass_948.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_948",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "948",
                        "name": "SecBench",
                        "emoji": "🔍",
                        "dimensions": [
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "Safety",
                                "en": "Safety"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 4,
                        "githubLink": "",
                        "paperLink": "",
                        "officialWebsiteLink": "https://secbench.org/dataset",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50163763",
                            "name": "Tencent",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50163763-2065e33a-96ba-4182-9708-b1967a29e6a4.png",
                            "nickname": "Tencent"
                        },
                        "lookNum": "4545",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:40:48",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-12 19:40:48",
                        "createDate": "2024-09-12 19:35:12",
                        "desc": {
                            "cn": "腾讯朱雀实验室和腾讯安全科恩实验室联合腾讯混元大模型团队、清华大学江勇教授/夏树涛教授团队、香港理工大学罗夏朴教授研究团队以及上海人工智能实验室OpenCompass团队，通过建设安全大模型评测基准SecBench，为安全大模型研发提供公平、公正、客观、全面的评测能力，推动安全大模型建设。",
                            "en": "Tencent Zhuque Lab and Tencent Security Keen Lab, together with Tencent Huyuan Team, Professor Jiang Yong/Professor Xia Shutao's team from Tsinghua University, Professor Luo Xiapu's research team from Hong Kong Polytechnic University, and OpenCompass team from Shanghai Artificial Intelligence Laboratory, have jointly built a safety benchmark, SecBench. We provide fair, impartial, objective, and comprehensive evaluation capabilities and promote the construction of large model in security dimension."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/secbench'. Error: Path opencompass/secbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_537",
                    "name": "HumanEval",
                    "version": "1.0.0",
                    "description": "The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases.",
                    "url": "opencompass/opencompass_537.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_537",
                    "sample_count": 164,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 132,
                        "estimated_output_tokens": 50
                    },
                    "original_data": {
                        "id": "537",
                        "name": "HumanEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "code",
                                "en": "code"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/openai/human-eval",
                        "paperLink": "https://arxiv.org/pdf/2107.03374.pdf",
                        "officialWebsiteLink": "https://huggingface.co/datasets/openai_humaneval",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "3963",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:31:35",
                        "supportOnlineEval": true,
                        "updateDate": "2024-09-12 19:31:35",
                        "createDate": "2024-09-12 19:27:36",
                        "desc": {
                            "cn": "这是 \"Evaluating Large Language Models Trained on Code\" 论文中描述的 HumanEval 问题解决数据集的评估工具包。它用于测量从文档脚本合成程序的功能正确性。它由 164 个原始编程问题组成，评估语言理解能力、算法和简单数学，其中一些问题与简单的软件面试题类似。",
                            "en": "The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "\n\ndef correct_bracketing(brackets: str):\n    \"\"\" brackets is a string of \"(\" and \")\".\n    return True if every opening bracket has a corresponding closing bracket.\n\n    >>> correct_bracketing(\"(\")\n   ",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "coding"
                                    ],
                                    "domains": [
                                        "Computer Science",
                                        "Programming"
                                    ],
                                    "skills": [
                                        "algorithmic problem solving",
                                        "Python programming",
                                        "logical reasoning"
                                    ],
                                    "concepts": [
                                        "bracket balancing",
                                        "string processing",
                                        "algorithmic logic"
                                    ],
                                    "qualification": [
                                        "High school graduate with programming experience",
                                        "Undergraduate student in Computer Science",
                                        "Junior software developer"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 14,
                                        "avg": 18,
                                        "max": 80
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "\ndef unique_digits(x):\n    \"\"\"Given a list of positive integers x. return a sorted list of all \n    elements that hasn't any even digit.\n\n    Note: Returned list should be sorted in increasing order.\n",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "coding",
                                        "function_calling"
                                    ],
                                    "domains": [
                                        "Computer Science",
                                        "Programming"
                                    ],
                                    "skills": [
                                        "Programming",
                                        "Problem solving",
                                        "Algorithmic thinking",
                                        "Debugging",
                                        "Code comprehension"
                                    ],
                                    "concepts": [
                                        "Loops",
                                        "Conditionals",
                                        "Integer manipulation",
                                        "Digit extraction",
                                        "List comprehension",
                                        "Sorting"
                                    ],
                                    "qualification": [
                                        "High School (with programming exposure)",
                                        "Undergraduate (Computer Science or related)",
                                        "Programmer",
                                        "Software Engineer"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 15,
                                        "avg": 25,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "\ndef by_length(arr):\n    \"\"\"\n    Given an array of integers, sort the integers that are between 1 and 9 inclusive,\n    reverse the resulting array, and then replace each digit by its corresponding nam",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "coding"
                                    ],
                                    "domains": [
                                        "Computer Science"
                                    ],
                                    "skills": [
                                        "Problem solving",
                                        "Basic algorithm design",
                                        "Code implementation",
                                        "Debugging"
                                    ],
                                    "concepts": [
                                        "Array manipulation",
                                        "Sorting",
                                        "Reversing",
                                        "String mapping",
                                        "Conditional filtering"
                                    ],
                                    "qualification": [
                                        "High school graduation",
                                        "Undergraduate in Computer Science or related field"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 13,
                                        "avg": 20,
                                        "max": 65
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "\n\ndef incr_list(l: list):\n    \"\"\"Return list with elements incremented by 1.\n    >>> incr_list([1, 2, 3])\n    [2, 3, 4]\n    >>> incr_list([5, 3, 5, 2, 3, 3, 9, 0, 123])\n    [6, 4, 6, 3, 4, 4, 10, 1, 1",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "coding"
                                    ],
                                    "domains": [
                                        "Computer Science"
                                    ],
                                    "skills": [
                                        "basic programming",
                                        "Python syntax",
                                        "list manipulation",
                                        "problem solving"
                                    ],
                                    "concepts": [
                                        "lists",
                                        "iteration",
                                        "increment",
                                        "function definition"
                                    ],
                                    "qualification": [
                                        "High school"
                                    ],
                                    "language": [
                                        "English",
                                        "Python"
                                    ],
                                    "age_range": {
                                        "min": 11,
                                        "avg": 15,
                                        "max": 80
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "\ndef order_by_points(nums):\n    \"\"\"\n    Write a function which sorts the given list of integers\n    in ascending order according to the sum of their digits.\n    Note: if there are several items with s",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "coding"
                                    ],
                                    "domains": [
                                        "Computer Science"
                                    ],
                                    "skills": [
                                        "problem solving",
                                        "basic programming",
                                        "algorithmic thinking",
                                        "understanding of sort stability",
                                        "numeric manipulation"
                                    ],
                                    "concepts": [
                                        "sorting algorithms",
                                        "stable sorting",
                                        "digit sum calculation",
                                        "list manipulation",
                                        "negative number handling",
                                        "basic programming in Python"
                                    ],
                                    "qualification": [
                                        "High school graduate with experience in introductory programming",
                                        "College undergraduate in Computer Science or related field"
                                    ],
                                    "language": [
                                        "Python"
                                    ],
                                    "age_range": {
                                        "min": 13,
                                        "avg": 17,
                                        "max": 80
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "\ndef tri(n):\n    \"\"\"Everyone knows Fibonacci sequence, it was studied deeply by mathematicians in \n    the last couple centuries. However, what people don't know is Tribonacci sequence.\n    Tribonacci",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "coding",
                                        "algorithm"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic_programming",
                                        "recursion",
                                        "logic",
                                        "debugging"
                                    ],
                                    "concepts": [
                                        "recursion",
                                        "conditional statements",
                                        "list construction",
                                        "function definition"
                                    ],
                                    "qualification": [
                                        "high_school"
                                    ],
                                    "language": [
                                        "English",
                                        "Python"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 15,
                                        "max": 20
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "\ndef sort_array(array):\n    \"\"\"\n    Given an array of non-negative integers, return a copy of the given array after sorting,\n    you will sort the given array in ascending order if the sum( first inde",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "coding"
                                    ],
                                    "domains": [
                                        "Computer Science"
                                    ],
                                    "skills": [
                                        "problem_solving",
                                        "logical_reasoning",
                                        "basic_programming",
                                        "data_structures",
                                        "algorithmic_knowledge",
                                        "attention_to_detail"
                                    ],
                                    "concepts": [
                                        "array",
                                        "sorting",
                                        "conditional",
                                        "indices",
                                        "sum"
                                    ],
                                    "qualification": [
                                        "high_school",
                                        "undergraduate"
                                    ],
                                    "language": [
                                        "English",
                                        "Python"
                                    ],
                                    "age_range": {
                                        "min": 10,
                                        "avg": 15,
                                        "max": 80
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "from typing import List\n\n\ndef string_xor(a: str, b: str) -> str:\n    \"\"\" Input are two strings a and b consisting only of 1s and 0s.\n    Perform binary XOR on these inputs and return result also as a ",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "coding"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic_programming",
                                        "string_manipulation",
                                        "logical_reasoning",
                                        "problem_solving"
                                    ],
                                    "concepts": [
                                        "bitwise XOR",
                                        "binary",
                                        "string manipulation"
                                    ],
                                    "qualification": [
                                        "high_school",
                                        "undergraduate",
                                        "programmer"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 14,
                                        "max": 80
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "\ndef compare_one(a, b):\n    \"\"\"\n    Create a function that takes integers, floats, or strings representing\n    real numbers, and returns the larger variable in its given variable type.\n    Return None",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "coding",
                                        "function_creation"
                                    ],
                                    "domains": [
                                        "Computer Science",
                                        "Mathematics"
                                    ],
                                    "skills": [
                                        "programming",
                                        "problem_solving",
                                        "logic_and_reasoning",
                                        "basic_math",
                                        "debugging"
                                    ],
                                    "concepts": [
                                        "Data Types",
                                        "Type Conversion",
                                        "String Parsing",
                                        "Conditional Logic",
                                        "Return Statement",
                                        "NoneType"
                                    ],
                                    "qualification": [
                                        "High School Graduate with Basic Programming",
                                        "Undergraduate Degree in Computer Science",
                                        "Self‑taught Programmer"
                                    ],
                                    "language": [
                                        "English",
                                        "Python"
                                    ],
                                    "age_range": {
                                        "min": 14,
                                        "avg": 25,
                                        "max": 60
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "\ndef digitSum(s):\n    \"\"\"Task\n    Write a function that takes a string as input and returns the sum of the upper characters only'\n    ASCII codes.\n\n    Examples:\n        digitSum(\"\") => 0\n        digi",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "coding",
                                        "function_implementation"
                                    ],
                                    "domains": [
                                        "Computer Science"
                                    ],
                                    "skills": [
                                        "programming",
                                        "algorithmic thinking",
                                        "basic logic",
                                        "string processing"
                                    ],
                                    "concepts": [
                                        "ASCII",
                                        "String Manipulation",
                                        "Conditional Statements",
                                        "Iteration"
                                    ],
                                    "qualification": [
                                        "High School Diploma",
                                        "Undergraduate Degree in Computer Science or related field",
                                        "Coding Experience"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 18,
                                        "max": 60
                                    },
                                    "domain_expertise_required": true
                                }
                            ],
                            "total_questions": 164,
                            "question_format": "Coding, Function_creation, Programming, algorithm, algorithm_design, algorithm_implementation, algorithmic, algorithmic problem solving, algorithmic_problem, algorithmic_problem_solving, best_practices, coding, docstring, function implementation, function writing, function_creation, function_definition, function_development, function_implementation, hashing, implementation, list_manipulation, math, plat, problem_solving, programming, programming_logic, sorting, string_manipulation, string_parsing, string_processing, unit_test, unit_testing",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Tests domain-specific knowledge in Information Security, Computer Science, Mathematics",
                                "Evaluates algorithmic thinking, conditional_statement_usage, Type handling skills",
                                "Covers multiple difficulty levels: Beginner, Intermediate",
                                "Diverse task types: hashing, unit_test, docstring"
                            ],
                            "disadvantages": [
                                "Requires knowledge across multiple specialized domains"
                            ]
                        },
                        "age_distribution": {
                            "data": [
                                1,
                                4,
                                32,
                                63,
                                24,
                                25,
                                9,
                                5,
                                0,
                                1,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0
                            ],
                            "categories": [
                                "10",
                                "12",
                                "14",
                                "16",
                                "18",
                                "20",
                                "22",
                                "24",
                                "26",
                                "28",
                                "30",
                                "32",
                                "34",
                                "36",
                                "38",
                                "40",
                                "42",
                                "44",
                                "46",
                                "48",
                                "50",
                                "52",
                                "54",
                                "56",
                                "58",
                                "60"
                            ]
                        },
                        "why_run_this_eval": [
                            "Run this evaluation to evaluate algorithmic thinking, conditional_statement_usage, Type handling capabilities, to assess domain knowledge in Information Security, Computer Science, to test hashing, unit_test performance."
                        ],
                        "what_to_expect": [
                            "Expect questions at Beginner, Intermediate levels, diverse tasks including hashing, unit_test, requiring Information Security, Computer Science knowledge, suitable for computer_science, high_school_graduation level."
                        ],
                        "top_5_task_types": [
                            "coding",
                            "algorithm",
                            "function_definition",
                            "function_implementation",
                            "algorithm_design"
                        ],
                        "top_5_domains": [
                            "Computer Science",
                            "Programming",
                            "Mathematics",
                            "Algorithms",
                            "Software Engineering"
                        ],
                        "top_5_skills": [
                            "problem_solving",
                            "programming",
                            "algorithmic thinking",
                            "debugging",
                            "basic_programming"
                        ],
                        "top_5_concepts": [
                            "iteration",
                            "string manipulation",
                            "function definition",
                            "conditional logic",
                            "String manipulation"
                        ],
                        "top_5_qualifications": [
                            "high_school",
                            "undergraduate",
                            "High School",
                            "Undergraduate",
                            "High School Diploma"
                        ],
                        "top_5_languages": [
                            "English",
                            "Python"
                        ]
                    },
                    "analysis_file": "analysis/opencompass_537_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 164,
                        "successful": 164,
                        "failed": 0
                    }
                },
                {
                    "id": "opencompass_945",
                    "name": "Flames",
                    "version": "1.0.0",
                    "description": "Flames is a highly adversarial benchmark in Chinese for LLM's value alignment evaluation developed by Shanghai AI Lab and Fudan NLP Group. Flames meticulously designs a dataset of 2,251 highly adversarial, manually crafted prompts, each tailored to probe a specific value dimension (i.e., Fairness, Safety, Morality, Legality, Data protection). Currently,  Flames releases 1,000 prompts for public use (Flames_1k_Chinese).",
                    "url": "opencompass/opencompass_945.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_945",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "945",
                        "name": "Flames",
                        "emoji": "💡",
                        "dimensions": [
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "Safety",
                                "en": "Safety"
                            }
                        ],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NAACL 2024",
                                "en": "NAACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 4,
                        "githubLink": "https://github.com/AIFlames/Flames",
                        "paperLink": "https://arxiv.org/abs/2311.06899",
                        "officialWebsiteLink": "https://flames.opencompass.org.cn/leaderboard",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "3536",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-16 11:02:53",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-16 11:02:53",
                        "createDate": "2024-10-15 15:34:25",
                        "desc": {
                            "cn": "Flames 是上海人工智能实验室和复旦大学 NLP团队开发的 LLM 价值对齐方向的中文高度对抗性基准。Flames 精心设计了一个由 2,251 个高度对抗性、人工创建的提示词成的评测集，每个提示词都经过精心设计，以探究特定的价值维度（即公平、安全、道德、合法、数据保护）。目前，Flames 发布了 1,000 个提示词供公众使用（Flames_1k_Chinese）。",
                            "en": "Flames is a highly adversarial benchmark in Chinese for LLM's value alignment evaluation developed by Shanghai AI Lab and Fudan NLP Group. Flames meticulously designs a dataset of 2,251 highly adversarial, manually crafted prompts, each tailored to probe a specific value dimension (i.e., Fairness, Safety, Morality, Legality, Data protection). Currently,  Flames releases 1,000 prompts for public use (Flames_1k_Chinese)."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/flames'. Error: Path opencompass/flames is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_499",
                    "name": "CMMLU",
                    "version": "1.0.0",
                    "description": "CMMLU is a comprehensive Chinese evaluation benchmark specifically designed to assess the knowledge and reasoning abilities of language models in the context of the Chinese language. CMMLU covers 67 topics ranging from basic subjects to advanced professional levels. It includes tasks that require calculations and reasoning in natural sciences, as well as tasks involving knowledge from humanities, social sciences, and practical aspects like Chinese driving rules. Moreover, many tasks within CMMLU",
                    "url": "opencompass/opencompass_499.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_499",
                    "sample_count": 11649,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 100,
                        "estimated_output_tokens": 50
                    },
                    "original_data": {
                        "id": "499",
                        "name": "CMMLU",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "学科",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/haonan-li/CMMLU/tree/1b8ce5813be6a011b1042f2d8fc7f0806e2ac0bc",
                        "paperLink": "https://arxiv.org/abs/2306.09212",
                        "officialWebsiteLink": "https://github.com/haonan-li/CMMLU/tree/1b8ce5813be6a011b1042f2d8fc7f0806e2ac0bc",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "3028",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": true,
                        "updateDate": "2024-08-02 09:46:14",
                        "createDate": "2024-01-11 14:09:33",
                        "desc": {
                            "cn": "CMMLU是一个综合性的中文评估基准，专门用于评估语言模型在中文语境下的知识和推理能力。CMMLU涵盖了从基础学科到高级专业水平的67个主题。它包括：需要计算和推理的自然科学，需要知识的人文科学和社会科学,以及需要生活常识的中国驾驶规则等。此外，CMMLU中的许多任务具有中国特定的答案，可能在其他地区或语言中并不普遍适用。因此是一个完全中国化的中文测试基准。",
                            "en": "CMMLU is a comprehensive Chinese evaluation benchmark specifically designed to assess the knowledge and reasoning abilities of language models in the context of the Chinese language. CMMLU covers 67 topics ranging from basic subjects to advanced professional levels. It includes tasks that require calculations and reasoning in natural sciences, as well as tasks involving knowledge from humanities, social sciences, and practical aspects like Chinese driving rules. Moreover, many tasks within CMMLU have answers specific to China, which might not be universally applicable in other regions or languages. As a result, CMMLU serves as a fully localized Chinese evaluation benchmark."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "{\"0\": \"205\", \"1\": \"中华民族在五千多年的发展中形成的伟大民族精神的核心是\", \"2\": \"国际主义\", \"3\": \"爱国主义\", \"4\": \"团结统一\", \"5\": \"爱好和平\", \"6\": \"B\"}",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "qa"
                                    ],
                                    "domains": [
                                        "Chinese history",
                                        "Cultural studies",
                                        "Political ideology"
                                    ],
                                    "skills": [
                                        "reading_comprehension",
                                        "knowledge_recall",
                                        "cultural_understanding"
                                    ],
                                    "concepts": [
                                        "national spirit",
                                        "patriotism",
                                        "internationalism",
                                        "unity",
                                        "peace"
                                    ],
                                    "qualification": [
                                        "high_school",
                                        "undergraduate_in_chinese_history",
                                        "history_enthusiast"
                                    ],
                                    "language": [
                                        "Mandarin"
                                    ],
                                    "age_range": {
                                        "min": 16,
                                        "avg": 17,
                                        "max": 65
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "{\"0\": \"1\", \"1\": \"下列现象中，属于⽤热传递的⽅式改变物体内能的是\", \"2\": \"两⼿互相摩擦时⼿发热\", \"3\": \"在炉⼦上烧开⽔\", \"4\": \"菜⼑在砂轮上磨得发烫\", \"5\": \"⽤打⽓筒打⽓时筒壁发热\", \"6\": \"B\"}",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "question_answering",
                                        "multiple_choice"
                                    ],
                                    "domains": [
                                        "Physics"
                                    ],
                                    "skills": [
                                        "reading_comprehension",
                                        "physics_reasoning",
                                        "critical_thinking"
                                    ],
                                    "concepts": [
                                        "heat transfer",
                                        "internal energy",
                                        "friction",
                                        "conduction"
                                    ],
                                    "qualification": [
                                        "high_school_physics"
                                    ],
                                    "language": [
                                        "Chinese"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 15,
                                        "max": 80
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "{\"0\": \"26\", \"1\": \"编制施工组织总设计时，编制资源需求量计划前必须完成的工作是\", \"2\": \"计算技术经济指标\", \"3\": \"绘制施工总平面图\", \"4\": \"编制施工总进度计划\", \"5\": \"编制施工准备工作计划\", \"6\": \"C\"}",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "multiple_choice_question",
                                        "exam"
                                    ],
                                    "domains": [
                                        "Construction Management",
                                        "Civil Engineering",
                                        "Project Management"
                                    ],
                                    "skills": [
                                        "domain knowledge",
                                        "critical thinking",
                                        "reading comprehension",
                                        "basic project management",
                                        "construction scheduling"
                                    ],
                                    "concepts": [
                                        "Resource Requirement Planning",
                                        "Construction Organization Design",
                                        "Overall Construction Schedule",
                                        "Technical Economic Indicators",
                                        "Construction Layout Planning",
                                        "Preparation Work Planning"
                                    ],
                                    "qualification": [
                                        "Bachelor's Degree in Civil Engineering or Construction Management",
                                        "Construction Engineer",
                                        "Site Project Manager",
                                        "Master's Degree in Construction Management"
                                    ],
                                    "language": [
                                        "Chinese"
                                    ],
                                    "age_range": {
                                        "min": 22,
                                        "avg": 28,
                                        "max": 60
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "{\"0\": \"159\", \"1\": \"下列表述正确的是\", \"2\": \"法是由国家强制力保证实施的，具有国家强制力\", \"3\": \"国家强制力是保证法实施的唯一力量\", \"4\": \"法是由国家强制力保证实施的，具有国家强制性\", \"5\": \"任何情况下法的实施都必须借助于国家强制力\", \"6\": \"C\"}",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "multiple_choice_question",
                                        "reading_comprehension",
                                        "knowledge_check"
                                    ],
                                    "domains": [
                                        "Law",
                                        "Political Science",
                                        "Legal Studies"
                                    ],
                                    "skills": [
                                        "reading_comprehension",
                                        "critical_thinking",
                                        "legal_theory_understanding",
                                        "language_proficiency"
                                    ],
                                    "concepts": [
                                        "state coercive power",
                                        "legal authority",
                                        "law enforcement",
                                        "mandatory nature of law"
                                    ],
                                    "qualification": [
                                        "High School Diploma",
                                        "Undergraduate Degree in Law or related field",
                                        "Law School Student"
                                    ],
                                    "language": [
                                        "Chinese"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 18,
                                        "max": 65
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "{\"0\": \"89\", \"1\": \"组织生命周期理论认为，组织在成熟阶段后期，官僚主义盛行，文山会海普遍存在，组织将陷于\", \"2\": \"领导危机\", \"3\": \"自主性危机\", \"4\": \"失控危机\", \"5\": \"硬化危机\", \"6\": \"D\"}",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "question_answering"
                                    ],
                                    "domains": [
                                        "Business",
                                        "Management",
                                        "Organizational Behavior"
                                    ],
                                    "skills": [
                                        "critical_thinking",
                                        "reading_comprehension",
                                        "domain_knowledge"
                                    ],
                                    "concepts": [
                                        "组织生命周期理论",
                                        "成熟阶段",
                                        "官僚主义",
                                        "文山会海",
                                        "危机类型",
                                        "硬化危机"
                                    ],
                                    "qualification": [
                                        "Undergraduate degree in Business Administration",
                                        "MBA",
                                        "Management or Organizational Behavior specialization"
                                    ],
                                    "language": [
                                        "Mandarin Chinese"
                                    ],
                                    "age_range": {
                                        "min": 18,
                                        "avg": 25,
                                        "max": 55
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "{\"0\": \"264\", \"1\": \"公法和私法的划分来源于\", \"2\": \"英国\", \"3\": \"罗马\", \"4\": \"美国\", \"5\": \"古希腊\", \"6\": \"B\"}",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "qa",
                                        "multiple_choice"
                                    ],
                                    "domains": [
                                        "Legal Studies",
                                        "History"
                                    ],
                                    "skills": [
                                        "knowledge_retrieval",
                                        "basic_reasoning"
                                    ],
                                    "concepts": [
                                        "Public Law",
                                        "Private Law",
                                        "Roman Law"
                                    ],
                                    "qualification": [
                                        "high_school",
                                        "undergraduate",
                                        "law"
                                    ],
                                    "language": [
                                        "Chinese"
                                    ],
                                    "age_range": {
                                        "min": 15,
                                        "avg": 18,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "{\"0\": \"93\", \"1\": \"在社会调查中，要求以事物和现象的真实状况为调查的前提和依据，它所体现的基本原则是\", \"2\": \"层次性原则\", \"3\": \"整体性原则\", \"4\": \"相关性原则\", \"5\": \"客观性原则\", \"6\": \"D\"}",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "qa"
                                    ],
                                    "domains": [
                                        "Sociology",
                                        "Social Research Methods"
                                    ],
                                    "skills": [
                                        "reading_comprehension",
                                        "basic_social_science_knowledge",
                                        "critical_thinking"
                                    ],
                                    "concepts": [
                                        "Objectivity Principle",
                                        "Social Survey",
                                        "Research Methodology"
                                    ],
                                    "qualification": [
                                        "high_school",
                                        "undergraduate",
                                        "masters"
                                    ],
                                    "language": [
                                        "Chinese"
                                    ],
                                    "age_range": {
                                        "min": 15,
                                        "avg": 22,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "{\"0\": \"29\", \"1\": \"当我们构造线性模型时, 我们注意变量间的相关性. 在相关矩阵中搜索相关系数时, 如果我们发现3对变量的相关系数是(Var1 和Var2, Var2和Var3, Var3和Var1)是-0.98, 0.45, 1.23 . 我们可以得出什么结论\", \"2\": \"以上都是\", \"3\": \"因为Var1和Var2是非常相关的, 我们可以去除其中一个\", \"4\": \"V",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "multiple_choice"
                                    ],
                                    "domains": [
                                        "Statistics",
                                        "Data Science"
                                    ],
                                    "skills": [
                                        "Statistical reasoning",
                                        "Critical thinking",
                                        "Basic knowledge of linear modeling"
                                    ],
                                    "concepts": [
                                        "Correlation coefficient",
                                        "Correlation bounds (−1 to 1)",
                                        "Multicollinearity in linear models"
                                    ],
                                    "qualification": [
                                        "High school diploma (advanced math)",
                                        "Undergraduate degree in Statistics/Data Science/Mathematics",
                                        "B.Sc. in a related field"
                                    ],
                                    "language": [
                                        "Chinese"
                                    ],
                                    "age_range": {
                                        "min": 16,
                                        "avg": 22,
                                        "max": 50
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "{\"0\": \"159\", \"1\": \"裸露病毒体的结构是\", \"2\": \"核酸+包膜\", \"3\": \"核心+衣壳+包膜\", \"4\": \"核心+衣壳\", \"5\": \"核衣壳+包膜\", \"6\": \"A\"}",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "quiz",
                                        "multiple_choice",
                                        "knowledge_recall"
                                    ],
                                    "domains": [
                                        "Biology",
                                        "Virology"
                                    ],
                                    "skills": [
                                        "reading_comprehension",
                                        "factual_recall",
                                        "basic_biology_knowledge"
                                    ],
                                    "concepts": [
                                        "Virus structure",
                                        "Enveloped vs. naked viruses",
                                        "Capsid",
                                        "Nucleic acid",
                                        "Nucleocapsid"
                                    ],
                                    "qualification": [
                                        "High_School",
                                        "Undergraduate_Biology",
                                        "College_Student"
                                    ],
                                    "language": [
                                        "Chinese"
                                    ],
                                    "age_range": {
                                        "min": 13,
                                        "avg": 16,
                                        "max": 80
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "{\"0\": \"21\", \"1\": \"《论语·八佾》记录孔子的话：“周监于二代，郁郁乎文哉！吾从周。”《荀子》说：“由士以上则必以礼节之。”对以上材料理解准确的是：a“周监于二代”中的“二代”是指夏商两代；b“吾从周”指孔子尊崇周礼；c夏商政治文化已成熟；d《荀子》指出了礼乐制在维护分封制、宗法制方面的重大作用\", \"2\": \"abc\", \"3\": \"abd\", \"4\": \"bcd\", \"5\": \"",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "reading_comprehension",
                                        "multiple_choice"
                                    ],
                                    "domains": [
                                        "Chinese Classics",
                                        "Chinese History",
                                        "Chinese Philosophy"
                                    ],
                                    "skills": [
                                        "critical_thinking",
                                        "reading_comprehension",
                                        "historical_contextualization",
                                        "interpretation of classical Chinese texts",
                                        "comparative analysis of philosophical texts"
                                    ],
                                    "concepts": [
                                        "周礼 (Zhou rites)",
                                        "礼乐制 (ritual and music system)",
                                        "周监于二代 (Zhou's oversight of two dynasties)",
                                        "孔子对周礼的尊崇",
                                        "荀子对礼节的论述",
                                        "分封制与宗法制"
                                    ],
                                    "qualification": [
                                        "High_school",
                                        "Undergraduate in Chinese Literature or History",
                                        "Graduate-level exposure to Classical Chinese"
                                    ],
                                    "language": [
                                        "Chinese",
                                        "Classical Chinese"
                                    ],
                                    "age_range": {
                                        "min": 15,
                                        "avg": 17,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                }
                            ],
                            "total_questions": 11649,
                            "question_format": "Multiple Choice, MultipleChoice, Multiple_Choice, Multiple_Choice_QA, QA, Question_Answering, algebra, analysis, biology, choice_of_law_examination, conceptual summarisation, conceptual_question, content_analysis, content_creation, data_entry, domain_specific_knowledge, equation solving, essay_writing, knowledge_assessment, knowledge_check, knowledge_question, knowledge_recall, language_lookup, language_understanding, legal_analysis, lexicographic_lookup, math, math problem, math_computation, math_problem, medical_question_answering, multichoice, multiple_choice, multiple_choice_QA, multiple_choice_answering, multiple_choice_classification, multiple_choice_comprehension, multiple_choice_question, multiple_choice_question_answering, multiple_choice_quiz, probability_theory, programming, qa, question answering, question_answer, question_answering, quiz, quiz_answering, quiz_solution, reading comprehension, reading_comprehension, rule_explanation, short_answer_selection, statistical_analysis",
                            "difficulty_levels": [
                                "Advanced",
                                "Beginner",
                                "Expert",
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Large, realistic question set (11.6k items) with proven exam‑style difficulty.",
                                "Broad domain spectrum – useful for multi‑disciplinary skill assessment.",
                                "Clear, quantitative metric (accuracy) easy to interpret.",
                                "Supports both English and Chinese contexts, giving a multilingual perspective.",
                                "Free‐to‐use and openly available for research and industry testing."
                            ],
                            "disadvantages": [
                                "Only multiple‑choice format; no assessment of free‑text generation quality.",
                                "Questions are domain‑monitored, so they may bias toward certain curricula or cultural viewpoints (e.g., many Chinese‑history and Chinese‑classics items).",
                                "Difficulty is marked only as Beginner/Intermediate; there is no fine‑grained difficulty ladder or adaptive testing.",
                                "The dataset doesn’t test reasoning steps – only the final answer, so subtle understanding or reasoning quality might be missed.",
                                "If a model relies on heavy pre‑training data overlap, CMMLU results may reflect memorization rather than true generalization."
                            ]
                        },
                        "age_distribution": {
                            "data": [
                                3,
                                8,
                                22,
                                73,
                                23,
                                14,
                                18,
                                20,
                                1,
                                8,
                                5,
                                0,
                                4,
                                0,
                                1,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0
                            ],
                            "categories": [
                                "10",
                                "12",
                                "14",
                                "16",
                                "18",
                                "20",
                                "22",
                                "24",
                                "26",
                                "28",
                                "30",
                                "32",
                                "34",
                                "36",
                                "38",
                                "40",
                                "42",
                                "44",
                                "46",
                                "48",
                                "50",
                                "52",
                                "54",
                                "56",
                                "58",
                                "60"
                            ]
                        },
                        "why_run_this_eval": [
                            "Running CMMLU is useful if you want to benchmark or monitor:\n1. **Domain coverage** – does the model know enough about biology, law, physics, etc.?\n2. **Critical thinking & reasoning** – can it choose the correct answer when reasoning patterns differ across subjects? \n3. **Language proficiency** – the dataset includes both English and Chinese contexts, so it tests multilingual understanding.\n4. **Benchmarking against peer models** – many research papers use CMMLU, enabling direct comparison.\n5. **Assessing knowledge updating** – when a model is fine‑tuned or updated, CMMLU can quickly show if factual knowledge has improved or degraded."
                        ],
                        "what_to_expect": [
                            "When an LLM is evaluated on CMMLU, the system calculates the **accuracy** – the percentage of questions correctly answered.",
                            "A high accuracy means the model can reliably retrieve factual information, reason through problem‑solving steps, and understand context within the given domains.",
                            "Conversely, lower scores reveal gaps in knowledge or reasoning abilities in specific subjects or formats.",
                            "Because the dataset uses only multiple‑choice questions, the score specifically reflects the model’s ability to identify the best answer given the options, rather than free‑form generation quality."
                        ],
                        "evaluation_description": "CMMLU (Comprehensive Multi‑Domain Language Understanding) is a benchmark dataset designed to test general knowledge and reasoning in multiple academic and professional domains.  It contains **11,649 multiple‑choice questions** that cover 16 distinct domains such as Biology, Civil Engineering, Chinese Classics, History, Law, Physics, Sociology, and Virology.  The questions were collected from real exams, quizzes, and knowledge checks and are categorized as beginner or intermediate difficulty.  Each entry includes a question stem, up to four answer choices, and the correct answer label (e.g., \"A\", \"B\", \"C\", or \"D\").  The dataset is broadly suitable for evaluating an LLM’s factual recall, reasoning, domain expertise, and language comprehension across a spectrum of topics.",
                        "top_5_task_types": [
                            "multiple_choice",
                            "qa",
                            "question_answering",
                            "multiple_choice_question",
                            "multiple_choice_quiz"
                        ],
                        "top_5_domains": [
                            "Chinese History",
                            "Chinese Culture",
                            "Nationalism Studies",
                            "Physics",
                            "Construction Engineering"
                        ],
                        "top_5_skills": [
                            "reading_comprehension",
                            "critical_thinking",
                            "reading comprehension",
                            "critical thinking",
                            "natural_language_understanding"
                        ],
                        "top_5_concepts": [
                            "National Spirit",
                            "爱国主义",
                            "中华民族",
                            "文化认同",
                            "Heat transfer"
                        ],
                        "top_5_qualifications": [
                            "High School Diploma",
                            "high_school",
                            "Middle School Graduation",
                            "High School Graduation",
                            "Undergraduate Degree (optional)"
                        ],
                        "top_5_languages": [
                            "Chinese",
                            "Mandarin Chinese"
                        ]
                    },
                    "analysis_file": "analysis/opencompass_499_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 200,
                        "successful": 200,
                        "failed": 0
                    }
                },
                {
                    "id": "opencompass_1003",
                    "name": "S-Eval",
                    "version": "1.0.0",
                    "description": "S-Eval is a new comprehensive, multi-dimensional and open-ended safety evaluation benchmark for LLMs consisting of 220,000 evaluation prompts (still in active expansion) across 102 risk subcategories and 10 advanced jailbreak attacks.",
                    "url": "opencompass/opencompass_1003.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1003",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1003",
                        "name": "S-Eval",
                        "emoji": "⚖️",
                        "dimensions": [
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "LLM",
                                "en": "LLM"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 4,
                        "githubLink": "https://github.com/IS2Lab/S-Eval",
                        "paperLink": "https://dl.acm.org/doi/10.1145/3728971",
                        "officialWebsiteLink": "https://s-eval.github.io",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50151947",
                            "name": "IS2Lab",
                            "avatar": null,
                            "nickname": "IS2Lab"
                        },
                        "lookNum": "2725",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-10-15 16:28:34",
                        "supportOnlineEval": false,
                        "updateDate": "2025-10-15 16:28:34",
                        "createDate": "2025-10-14 20:59:45",
                        "desc": {
                            "cn": "S-Eval 是一个针对 LLM 的全新全面、多维、开放式安全评估基准，包含 102 个风险子类别的 220,000 个评估提示（仍在积极扩展中）和 10 个高级越狱攻击。",
                            "en": "S-Eval is a new comprehensive, multi-dimensional and open-ended safety evaluation benchmark for LLMs consisting of 220,000 evaluation prompts (still in active expansion) across 102 risk subcategories and 10 advanced jailbreak attacks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/s_eval'. Error: Path opencompass/s_eval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_531",
                    "name": "HellaSwag",
                    "version": "1.0.0",
                    "description": "HellaSwag is a challenge dataset for evaluating commonsense natural language inference, which is specially hard for state-of-the-art models, though its questions are trivial for humans (>95% accuracy). It consists of 70k multiple choice questions, each with a scenario and four possible endings, which requires to select the most reasonable ending. These questions come from two domains:activitynet and wikihow, involving video and text scenarios respectively. The correct answers of these questions ",
                    "url": "opencompass/opencompass_531.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_531",
                    "sample_count": 10042,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {
                        "gen": "hellaswag_gen",
                        "ppl": "hellaswag_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 43,
                        "estimated_output_tokens": 50
                    },
                    "original_data": {
                        "id": "531",
                        "name": "HellaSwag",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "reasoning",
                                "en": "reasoning"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/rowanz/hellaswag",
                        "paperLink": "https://arxiv.org/abs/1905.07830",
                        "officialWebsiteLink": "https://allenai.org/data/hellaswag",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "2352",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:31:26",
                        "supportOnlineEval": true,
                        "updateDate": "2024-09-12 19:31:26",
                        "createDate": "2024-09-12 19:28:41",
                        "desc": {
                            "cn": "HellaSwag 是一个用于评估常识性自然语言推理的数据集，HellaSwag的问题对于最先进的模型来说是特别困难的，尽管它的问题对于人类来说非常轻松就能回答的（> 95% 的准确率）。它由7万多道多项选择题组成，每道题都有一个场景和四种可能的答案，需要选择最合理的答案。这些问题来自两个领域：activitynet和wikihow，分别涉及视频和文本场景。这些问题的正确答案是下一个事件的真实句子，而错误答案是通过对抗技术生成的并经过人类验证，这些答案可以欺骗机器但不能欺骗人类。",
                            "en": "HellaSwag is a challenge dataset for evaluating commonsense natural language inference, which is specially hard for state-of-the-art models, though its questions are trivial for humans (>95% accuracy). It consists of 70k multiple choice questions, each with a scenario and four possible endings, which requires to select the most reasonable ending. These questions come from two domains:activitynet and wikihow, involving video and text scenarios respectively. The correct answers of these questions are the real sentences for the next event, while the incorrect answers are adversarially generated and human verified, so as to fool machines but not humans."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the righ",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "informational content",
                                        "advice",
                                        "personal care education"
                                    ],
                                    "domains": [
                                        "Fashion & Apparel",
                                        "Body Image Studies",
                                        "Health & Wellness"
                                    ],
                                    "skills": [
                                        "communication",
                                        "body measurement",
                                        "critical thinking",
                                        "empathy",
                                        "basic physics of fit"
                                    ],
                                    "concepts": [
                                        "Bra fitting",
                                        "Band and cup sizing",
                                        "Push‑up technology",
                                        "Underwire support",
                                        "Padding",
                                        "Proper adjustment techniques"
                                    ],
                                    "qualification": [
                                        "High school graduate",
                                        "Certified Lingerie Specialist",
                                        "Fashion design student",
                                        "Health and Wellness Coach"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 15,
                                        "avg": 28,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "Washing face: A girl stands in front of a bathroom mirror and vigorously rubs her face. The girl turns on the faucet. The girl",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "reading_comprehension",
                                        "story_understanding"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic_reading",
                                        "simple_comprehension"
                                    ],
                                    "concepts": [
                                        "washing",
                                        "face",
                                        "mirror",
                                        "faucet",
                                        "daily routine"
                                    ],
                                    "qualification": [
                                        "No formal educational requirement"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 3,
                                        "avg": 10,
                                        "max": 90
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "Home and Garden: How to paint basement stairs. Remove any carpet or overlaid material from your basement stairs. Remove staples left from the carpet installation with pliers. Look over all areas of th",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "manual_task"
                                    ],
                                    "domains": [
                                        "Home Improvement"
                                    ],
                                    "skills": [
                                        "mechanical aptitude",
                                        "tool handling",
                                        "attention to detail",
                                        "basic safety practices"
                                    ],
                                    "concepts": [
                                        "carpet removal",
                                        "staple removal",
                                        "surface inspection",
                                        "stairs painting prep"
                                    ],
                                    "qualification": [
                                        "High school graduate",
                                        "DIY enthusiast"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 25,
                                        "max": 80
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "Zumba: A dance team dressed in black with pink letters shows a routine put to reggae music. The women jump bounce in place before the lyrics of the song starts. When the lyrics",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "description",
                                        "summarization"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "reading_comprehension",
                                        "basic_writing",
                                        "creative_description"
                                    ],
                                    "concepts": [],
                                    "qualification": [],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 20,
                                        "max": 90
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "Family Life: How to deal with a child not wanting to go to school. Track how often they resist school. There are some times that it's common for students not to want to go to school. They may feel tha",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "qa",
                                        "advice"
                                    ],
                                    "domains": [
                                        "Parenting",
                                        "Child Psychology",
                                        "Education"
                                    ],
                                    "skills": [
                                        "problem_solving",
                                        "critical_thinking",
                                        "empathy",
                                        "communication",
                                        "observational_skills",
                                        "data_tracking"
                                    ],
                                    "concepts": [
                                        "school refusal",
                                        "motivation",
                                        "parent-child relationship",
                                        "child development"
                                    ],
                                    "qualification": [
                                        "high_school"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 18,
                                        "avg": 35,
                                        "max": 80
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "Cheerleading: A group of cheerleaders are seen walking on stage while the audience cheers. The group",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "creative_writing",
                                        "description"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "creative_writing",
                                        "narrative description",
                                        "imagery",
                                        "contextual understanding"
                                    ],
                                    "concepts": [
                                        "Cheerleading",
                                        "stage performance",
                                        "audience engagement"
                                    ],
                                    "qualification": [
                                        "High school graduate"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 10,
                                        "avg": 16,
                                        "max": 80
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "Layup drill in basketball: A coach gives instructions to boys in a basketball court. Then",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "sport_coaching",
                                        "instruction_delivery",
                                        "athletic_training"
                                    ],
                                    "domains": [
                                        "Basketball",
                                        "Physical Education",
                                        "Coaching"
                                    ],
                                    "skills": [
                                        "Clear communication",
                                        "Instructional design",
                                        "Physical education knowledge",
                                        "Basketball skill analysis",
                                        "Motivational coaching",
                                        "Observation and feedback",
                                        "Time management"
                                    ],
                                    "concepts": [
                                        "Layup technique",
                                        "Footwork",
                                        "Dribbling",
                                        "Shooting fundamentals",
                                        "Athletic instruction",
                                        "Team communication"
                                    ],
                                    "qualification": [
                                        "High School Diploma",
                                        "Undergraduate Degree in Physical Education or Sports Science (optional)",
                                        "Basketball Coaching Certification (e.g., AAU, WNBA, NCAA assistant coach certification)",
                                        "Volunteer or Youth Coach Experience"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 16,
                                        "avg": 26,
                                        "max": 55
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "Personal Care and Style: How to apply mascara. Decide what you want out of your mascara. There is a mascara for just about any kind of lash enhancement--volumizing, lengthening, defining, faster growt",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "instructional",
                                        "advice"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "reading comprehension",
                                        "critical decision making",
                                        "fine motor coordination"
                                    ],
                                    "concepts": [
                                        "Product selection",
                                        "Lash enhancement categories",
                                        "Application technique"
                                    ],
                                    "qualification": [
                                        "High school",
                                        "General consumer knowledge"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 10,
                                        "avg": 25,
                                        "max": 80
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "Ice fishing: A man is kneeling on a frozen lake. A video is shown of the cold waters below. A fish",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "reading_comprehension",
                                        "summarization"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic_reading",
                                        "inference",
                                        "observation"
                                    ],
                                    "concepts": [],
                                    "qualification": [
                                        "1st Grade",
                                        "2nd Grade",
                                        "3rd Grade"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 6,
                                        "avg": 10,
                                        "max": 18
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "Finance and Business: How to obtain full custody. Sit down and talk. Although it may be enormously difficult to sit and talk with the other parent, especially during a divorce, you should nevertheless",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "question_answering"
                                    ],
                                    "domains": [
                                        "Family Law",
                                        "Legal Negotiation",
                                        "Child Welfare"
                                    ],
                                    "skills": [
                                        "communication",
                                        "negotiation",
                                        "empathy",
                                        "basic legal knowledge",
                                        "problem solving"
                                    ],
                                    "concepts": [
                                        "custody",
                                        "visitation",
                                        "mediation",
                                        "negotiation",
                                        "divorce",
                                        "parenting plan",
                                        "child support"
                                    ],
                                    "qualification": [
                                        "High School Graduate",
                                        "Some College",
                                        "Licensed Attorney",
                                        "Certified Family Mediator"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 18,
                                        "avg": 35,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                }
                            ],
                            "total_questions": 10042,
                            "question_format": "Advice, Advice_Generation, Basic_electrical_instructions, ColorMixing, DIY, DIY_assembly, Information_Response, Informational, Instruction, Instruction following, Instruction_Following, Instructional, Manual_Task, MaterialApproximation, Measurement, Music_Performance, Organization, Physical Performance Description, Planning, Problem_solving, Q&A, Self‑care, Skill_based, StyleRecommendation, Summarization, action_description, advice, advice_and_instructions, advisory, advisory_content, analysis, audio_connection, beauty_tutorial, behavioral_assessment, business_strategy, caption_generation, captioning, caregiving, classification, cleaning, coaching, communication, completion, comprehension, consulting, content creation, content_creation, content_generation, contextual_understanding, conversation, cooking, copywriting, counseling, crafting, creative, creative_writing, decision_making, description, description_generation, design, discussion, documentation, domestic_chores, education, educational, emotional support, emotional_support, explanation, fact-checking, faq, fashion_advice, fill_in_the_blank, gardening, generation, guidance, handcrafting, hardware_setup, health_advice, how-to, how-to guide, how_to, how_to_guidance, how_to_guide, hr_strategy, information, information retrieval, information_discovery, information_extraction, information_gathering, information_generation, information_retrieval, informational, instruction, instruction_following, instruction_generation, instructional, instructional guidance, instructional_description, interpersonal skill development, job guidance, knowledge_based_explanation, knowledge_based_qa, knowledge_sharing, language_comprehension, learning_strategies, lesson_plan, manual_guide, manual_labor, manual_task, marketing, marketing_strategy, media_commentary, narration, narrative, narrative creation, narrative_generation, observation_description, organization, outdoor_sport_description, performance, performance_description, personal_care, pet_care, physical_activity, physical_exercise, physical_task, physical_work, planning, policy_drafting, practical, problem_solving, public_health_policy, public_speaking, qa, question_answer, question_answering, reading comprehension, reading_comprehension, recipe_execution, recipe_following, recipe_instruction, recommendation, recruitment, relationship counseling, relationship guidance, research, rewriting, routine, scene_description, script_writing, scriptwriting, self_service, shopping_advice, simple_user_guide, skill_development, skill_instruction, sports, story_continuation, story_generation, storytelling, style_advice, style_guidance, summarisation, summarization, summary, support, surveying, teaching_material_creation, technical_writing, text_completion, text_comprehension, text_generation, video_editing, video_production, visual observation, visual_description, writing",
                            "difficulty_levels": [
                                "Advanced",
                                "Beginner",
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Large number of examples (10k+) covers many real‑world topics.",
                                "Clear, single‑choice format makes scoring straightforward.",
                                "Balanced difficulty (Beginner & Intermediate) lets you see progress at different stages.",
                                "Domain diversity (from personal care to sports, law, and education) tests contextual flexibility.",
                                "Crowd‑annotated, so the correct choice reflects common human intuition."
                            ],
                            "disadvantages": [
                                "Only four candidate sentences – no open‑ended generation to test creativity.",
                                "Potential bias from the age distribution (mostly younger participants).",
                                "Sensitive or awkward topics (e.g., body image, legal advice) may not be suitable for all audiences.",
                                "Evaluation is strictly accuracy; it does not capture partial reasoning or explanations.",
                                "The dataset is not rich in multimodal content (images, audio)."
                            ]
                        },
                        "age_distribution": {
                            "data": [
                                10,
                                22,
                                21,
                                18,
                                8,
                                11,
                                2,
                                29,
                                6,
                                10,
                                25,
                                2,
                                34,
                                1,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0
                            ],
                            "categories": [
                                "10",
                                "12",
                                "14",
                                "16",
                                "18",
                                "20",
                                "22",
                                "24",
                                "26",
                                "28",
                                "30",
                                "32",
                                "34",
                                "36",
                                "38",
                                "40",
                                "42",
                                "44",
                                "46",
                                "48",
                                "50",
                                "52",
                                "54",
                                "56",
                                "58",
                                "60"
                            ]
                        },
                        "why_run_this_eval": [
                            "Run this evaluation if you want to gauge:\n1.",
                            "Commonsense reasoning across a wide range of domains.\n2.",
                            "The model’s ability to pick the most plausible everyday continuation.\n3.",
                            "Its performance on both simple (routine) and more challenging (context‑rich) examples.\n4.",
                            "How well the model handles varying age perspectives and communication styles."
                        ],
                        "what_to_expect": [
                            "When an LLM is evaluated on HellaSwag, the main metric is the accuracy (%) of correctly chosen continuations.",
                            "A high score means the model can understand context, reason about everyday events, and choose the most believable outcome – a key trait for natural‑language reasoning.",
                            "A low score would suggest gaps in commonsense knowledge and contextual understanding."
                        ],
                        "evaluation_description": "HellaSwag is a crowdsourced evaluation benchmark that contains 10,042 examples.  Each example is a short paragraph followed by four possible continuations.  The model is expected to pick the most plausible one.  The stories cover everyday situations such as personal care, home improvement, parenting, sports coaching, and more.  The dataset was annotated by respondents aged 10–34, providing a range of perspectives.  Tasks include answering, advice, instruction delivery, creative writing, summarization, and question answering.  The difficulty level is split between ‘Beginner’ and ‘Intermediate’ examples.",
                        "top_5_task_types": [
                            "advice",
                            "reading_comprehension",
                            "description",
                            "summarization",
                            "informational content"
                        ],
                        "top_5_domains": [
                            "Fashion & Apparel",
                            "Body Image Studies",
                            "Health & Wellness",
                            "Home Improvement",
                            "Parenting"
                        ],
                        "top_5_skills": [
                            "communication",
                            "empathy",
                            "basic_reading",
                            "body measurement",
                            "critical thinking"
                        ],
                        "top_5_concepts": [
                            "Bra fitting",
                            "Band and cup sizing",
                            "Push‑up technology",
                            "Underwire support",
                            "Padding"
                        ],
                        "top_5_qualifications": [
                            "High school graduate",
                            "Certified Lingerie Specialist",
                            "Fashion design student",
                            "Health and Wellness Coach",
                            "No formal educational requirement"
                        ],
                        "top_5_languages": [
                            "English"
                        ]
                    },
                    "analysis_file": "analysis/opencompass_531_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 200,
                        "successful": 199,
                        "failed": 1
                    }
                },
                {
                    "id": "opencompass_692",
                    "name": "ChemBench",
                    "version": "1.0.0",
                    "description": "ChemBench is a large-scale chemistry competency evaluation benchmark for language models, which includes nine chemistry core tasks and 4100 high-quality single-choice questions and answers.",
                    "url": "opencompass/opencompass_692.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_692",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "692",
                        "name": "ChemBench",
                        "emoji": "🧪",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "Name_Conversion",
                                "en": "Name_Conversion"
                            },
                            {
                                "cn": "Property_Prediction",
                                "en": "Property_Prediction"
                            },
                            {
                                "cn": "Mol2caption",
                                "en": "Mol2caption"
                            },
                            {
                                "cn": "Caption2mol",
                                "en": "Caption2mol"
                            },
                            {
                                "cn": "Product_Prediction",
                                "en": "Product_Prediction"
                            },
                            {
                                "cn": "Retrosynthesis",
                                "en": "Retrosynthesis"
                            },
                            {
                                "cn": "Yield_Prediction",
                                "en": "Yield_Prediction"
                            },
                            {
                                "cn": "Temperature_Prediction",
                                "en": "Temperature_Prediction"
                            },
                            {
                                "cn": "Solvent_Prediction",
                                "en": "Solvent_Prediction"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 4,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2402.06852",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50029510",
                            "name": "OpenScienceLab",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50029510-baa30340-1e57-432f-9b2b-6f656752c65e.png",
                            "nickname": "OpenScienceLab"
                        },
                        "lookNum": "2296",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:51:38",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-12 19:51:38",
                        "createDate": "2024-09-12 19:49:11",
                        "desc": {
                            "cn": "ChemBench是一个包含了九项化学核心任务，4100个高质量单选问答的大语言模型化学能力评测基准.",
                            "en": "ChemBench is a large-scale chemistry competency evaluation benchmark for language models, which includes nine chemistry core tasks and 4100 high-quality single-choice questions and answers."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/chembench'. Error: Path opencompass/chembench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_538",
                    "name": "MBPP",
                    "version": "1.0.0",
                    "description": "The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases.",
                    "url": "opencompass/opencompass_538.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_538",
                    "sample_count": 974,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {
                        "gen": "MBPP_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 13,
                        "estimated_output_tokens": 50
                    },
                    "original_data": {
                        "id": "538",
                        "name": "MBPP",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "code",
                                "en": "code"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/google-research/google-research/tree/master/mbpp",
                        "paperLink": "https://arxiv.org/pdf/2108.07732v1",
                        "officialWebsiteLink": "https://huggingface.co/datasets/mbpp",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "2291",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:31:38",
                        "supportOnlineEval": true,
                        "updateDate": "2024-09-12 19:31:38",
                        "createDate": "2024-09-12 19:27:17",
                        "desc": {
                            "cn": "该基准测试由大约1000个入门级程序员可以解决的众包Python编程问题组成，涵盖编程基础知识、标准库功能等。每个问题都由任务描述、代码解决方案和3个自动化测试用例组成。",
                            "en": "The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "Write a function to check if the triangle is valid or not.",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "coding"
                                    ],
                                    "domains": [
                                        "Computer Science",
                                        "Mathematics"
                                    ],
                                    "skills": [
                                        "problem_solving",
                                        "basic_algorithm_design",
                                        "programming_syntax"
                                    ],
                                    "concepts": [
                                        "triangle inequality",
                                        "basic algorithm",
                                        "function definition",
                                        "input validation"
                                    ],
                                    "qualification": [
                                        "high_school",
                                        "undergraduate"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 16,
                                        "max": 65
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "Write a function to sort the given array by using counting sort.",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "coding"
                                    ],
                                    "domains": [
                                        "Computer Science"
                                    ],
                                    "skills": [
                                        "Problem Solving",
                                        "Algorithm Understanding",
                                        "Programming Logic",
                                        "Attention to Detail",
                                        "Debugging"
                                    ],
                                    "concepts": [
                                        "Counting Sort",
                                        "Array Manipulation",
                                        "Loop constructs",
                                        "Time Complexity",
                                        "Space Complexity"
                                    ],
                                    "qualification": [
                                        "High School Diploma with Computer Science",
                                        "Undergraduate CS degree",
                                        "Beginner programming proficiency in Python/C++/Java"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 18,
                                        "max": 60
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "Write a function to find the list of lists with maximum length.",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "coding"
                                    ],
                                    "domains": [
                                        "Computer Science",
                                        "Programming"
                                    ],
                                    "skills": [
                                        "problem_solving",
                                        "algorithm_design",
                                        "list_manipulation",
                                        "coding"
                                    ],
                                    "concepts": [
                                        "lists",
                                        "array_iteration",
                                        "length_computation",
                                        "max_value"
                                    ],
                                    "qualification": [
                                        "high_school",
                                        "undergraduate",
                                        "programming_student",
                                        "software_developer"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 13,
                                        "avg": 25,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "Write a function to separate and print the numbers and their position of a given string.",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "coding",
                                        "function_definition"
                                    ],
                                    "domains": [
                                        "Computer Science",
                                        "Programming"
                                    ],
                                    "skills": [
                                        "problem_solving",
                                        "programming",
                                        "algorithmic_thinking",
                                        "string_manipulation",
                                        "debugging"
                                    ],
                                    "concepts": [
                                        "String Manipulation",
                                        "Indexing",
                                        "Iteration"
                                    ],
                                    "qualification": [
                                        "High School Graduate"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 14,
                                        "avg": 18,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "Write a function to find the size of the given tuple.",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "coding"
                                    ],
                                    "domains": [
                                        "Computer Science",
                                        "Programming"
                                    ],
                                    "skills": [
                                        "basic programming",
                                        "function definition",
                                        "understanding of tuples",
                                        "use of built-in functions"
                                    ],
                                    "concepts": [
                                        "tuple",
                                        "size",
                                        "function definition",
                                        "built-in len function"
                                    ],
                                    "qualification": [
                                        "High School Diploma",
                                        "College Student",
                                        "Junior Developer"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 13,
                                        "avg": 18,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "Write a function to find the minimum value in a given heterogeneous list.",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "coding"
                                    ],
                                    "domains": [
                                        "Computer Science"
                                    ],
                                    "skills": [
                                        "problem_solving",
                                        "algorithm_design",
                                        "basic_programming",
                                        "conditional_logic",
                                        "list_processing"
                                    ],
                                    "concepts": [
                                        "iteration",
                                        "comparison",
                                        "heterogeneous data handling",
                                        "minimum computation",
                                        "array/list processing"
                                    ],
                                    "qualification": [
                                        "High School Graduate",
                                        "Undergraduate Coursework in CS",
                                        "Entry-Level Programmer"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 18,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "Write a python function to check whether the given number is co-prime or not.",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "coding"
                                    ],
                                    "domains": [
                                        "Mathematics",
                                        "Computer Science"
                                    ],
                                    "skills": [
                                        "basic programming",
                                        "algorithmic thinking",
                                        "basic math"
                                    ],
                                    "concepts": [
                                        "Co-prime",
                                        "GCD",
                                        "Euclidean algorithm"
                                    ],
                                    "qualification": [
                                        "High school graduate",
                                        "Basic programming knowledge"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 15,
                                        "max": 80
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "Write a function to check if all the elements in tuple have same data type or not.",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "coding",
                                        "function_definition"
                                    ],
                                    "domains": [
                                        "Computer Science"
                                    ],
                                    "skills": [
                                        "problem_solving",
                                        "basic programming",
                                        "logical reasoning",
                                        "understanding of data types"
                                    ],
                                    "concepts": [
                                        "data types",
                                        "tuples",
                                        "iteration",
                                        "conditional statements"
                                    ],
                                    "qualification": [
                                        "High school diploma",
                                        "Basic programming knowledge"
                                    ],
                                    "language": [
                                        "Python"
                                    ],
                                    "age_range": {
                                        "min": 15,
                                        "avg": 17,
                                        "max": 80
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "Write a function to find the sum of geometric progression series.",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "coding"
                                    ],
                                    "domains": [
                                        "Mathematics",
                                        "Computer Science"
                                    ],
                                    "skills": [
                                        "Problem_Solving",
                                        "Mathematical_Thinking",
                                        "Programming",
                                        "Algorithmic_Design"
                                    ],
                                    "concepts": [
                                        "Geometric Series",
                                        "Summation Formula",
                                        "Exponentiation",
                                        "Programming"
                                    ],
                                    "qualification": [
                                        "High_School_Mathematics",
                                        "Undergraduate_Computer_Science",
                                        "Programmer"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 15,
                                        "avg": 22,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "Write a function to round the given number to the nearest multiple of a specific number.",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "coding"
                                    ],
                                    "domains": [
                                        "Computer Science",
                                        "Mathematics"
                                    ],
                                    "skills": [
                                        "Programming",
                                        "Algorithmic thinking",
                                        "Mathematical reasoning",
                                        "Attention to detail"
                                    ],
                                    "concepts": [
                                        "Rounding",
                                        "Modular arithmetic",
                                        "Multiples",
                                        "Basic algorithm design"
                                    ],
                                    "qualification": [
                                        "High School Diploma",
                                        "Associate Degree in Computer Science",
                                        "Undergraduate Degree in Computer Science or related field",
                                        "Entry-level Software Engineer",
                                        "Intern"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 20,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                }
                            ],
                            "total_questions": 974,
                            "question_format": "Coding, Function Definition, Function_Creation, Programming, algorithm, algorithm_design, algorithm_implementation, algorithmic_problem, algorithmic_problem_solving, coding, function, function implementation, function-writing, function_creation, function_defining, function_definition, function_design, function_development, function_generation, function_implementation, lambda_function, lambda_usage, map_function, math_problem_solving, programming, python, recursion, regex, regex_application, string_processing, string_search",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Large, well‑curated set of problems guarantees diverse coverage of programming fundamentals.",
                                "Automated, objective scoring via hidden test cases eliminates human bias.",
                                "Python is the most common language for LLM evaluation, making results comparable across studies.",
                                "Tasks are labelled by difficulty, so you can test models at both beginner and intermediate levels."
                            ],
                            "disadvantages": [
                                "The dataset focuses on *simple* and *intermediate* problems only; it contains no advanced topics like concurrent programming or machine‑learning pipelines.",
                                "All tests are in Python – models trained on other languages may not be reflected accurately.",
                                "Evaluations rely solely on functional correctness; code quality, readability, security, and documentation are not assessed.",
                                "Some prompts exhibit duplicate or very similar wording, which can bias a few models that remember training examples."
                            ]
                        },
                        "age_distribution": {
                            "data": [
                                0,
                                5,
                                36,
                                69,
                                28,
                                31,
                                18,
                                11,
                                0,
                                1,
                                1,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0
                            ],
                            "categories": [
                                "10",
                                "12",
                                "14",
                                "16",
                                "18",
                                "20",
                                "22",
                                "24",
                                "26",
                                "28",
                                "30",
                                "32",
                                "34",
                                "36",
                                "38",
                                "40",
                                "42",
                                "44",
                                "46",
                                "48",
                                "50",
                                "52",
                                "54",
                                "56",
                                "58",
                                "60"
                            ]
                        },
                        "why_run_this_eval": [
                            "Run this evaluation if you want to:\n\n- Measure how well a model can translate problem statements into working code.\n- Check the model’s proficiency in core programming concepts such as conditionals, loops, and function design.\n- Assess debugging and error‑handling abilities via the hidden test cases.\n- Benchmark different models or prompt‑engineering strategies on a standard, widely‑used coding dataset."
                        ],
                        "what_to_expect": [
                            "When an LLM is evaluated on MBPP, the model generates code for each prompt and the resulting code is run against the hidden test cases.",
                            "The evaluation score reflects the percentage of functions that compile and satisfy all tests.",
                            "A higher score demonstrates stronger skills in algorithmic thinking, debugging, and producing syntactically correct, logically sound Python code.",
                            "An assessment that fails many tests highlights weaknesses in understanding program logic, edge‑case handling, or proper use of data types."
                        ],
                        "evaluation_description": "MBPP (Mini-Benchmarks for Programming Problems) is a collection of **974** programming challenges written in Python. The problems cover the basics of computer science, mathematics, and programming, ranging from very simple (Beginner) to moderately complex (Intermediate). Users will find tasks centred on functions, list manipulation, basic algorithms, debugging, and logic. Each problem is paired with test cases that automatically verify the correctness of a submitted function.",
                        "top_5_task_types": [
                            "coding",
                            "function_definition"
                        ],
                        "top_5_domains": [
                            "Computer Science",
                            "Mathematics",
                            "Programming"
                        ],
                        "top_5_skills": [
                            "problem_solving",
                            "basic programming",
                            "algorithm_design",
                            "Programming",
                            "basic_algorithm_design"
                        ],
                        "top_5_concepts": [
                            "function definition",
                            "iteration",
                            "triangle inequality",
                            "basic algorithm",
                            "input validation"
                        ],
                        "top_5_qualifications": [
                            "high_school",
                            "undergraduate",
                            "High School Graduate",
                            "High School Diploma",
                            "Basic programming knowledge"
                        ],
                        "top_5_languages": [
                            "English",
                            "Python"
                        ]
                    },
                    "analysis_file": "analysis/opencompass_538_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 200,
                        "successful": 200,
                        "failed": 0
                    }
                },
                {
                    "id": "opencompass_497",
                    "name": "AGIEval",
                    "version": "1.0.0",
                    "description": "AGIEval is a human-centric benchmark specifically designed to evaluate the general abilities of foundation models in tasks pertinent to human cognition and problem-solving. This benchmark is derived from 20 official, public, and high-standard admission and qualification exams intended for general human test-takers, such as general college admission tests (e.g., Chinese College Entrance Exam (Gaokao) and American SAT), law school admission tests, math competitions, lawyer qualification tests, and",
                    "url": "opencompass/opencompass_497.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_497",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "497",
                        "name": "AGIEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "学科",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "examination",
                                "en": "examination"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/ruixiangcui/AGIEval",
                        "paperLink": "https://arxiv.org/pdf/2304.06364",
                        "officialWebsiteLink": "https://github.com/ruixiangcui/AGIEval",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1885",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:31:13",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-12 19:31:13",
                        "createDate": "2024-09-12 19:30:02",
                        "desc": {
                            "cn": "AGIEval是一个以人为中心的基准测试，专门设计用于评估基础模型在涉及人类认知和问题解决的任务中的一般能力。该基准测试源自20个官方、公开和高标准的入学和资格考试，例如普通大学入学考试（例如中国高考和美国SAT）、法学院入学考试、数学竞赛、律师资格考试以及国家公务员考试",
                            "en": "AGIEval is a human-centric benchmark specifically designed to evaluate the general abilities of foundation models in tasks pertinent to human cognition and problem-solving. This benchmark is derived from 20 official, public, and high-standard admission and qualification exams intended for general human test-takers, such as general college admission tests (e.g., Chinese College Entrance Exam (Gaokao) and American SAT), law school admission tests, math competitions, lawyer qualification tests, and national civil service exams."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "No valid url for ./data/AGIEval/data/v1/!\nPlease make sure  `./data/AGIEval/data/v1/` is correct"
                    }
                },
                {
                    "id": "opencompass_505",
                    "name": "CHID",
                    "version": "1.0.0",
                    "description": " CHID is a chinese idiom reading comprehension task, which requires to select the correct idiom to fill in the blank according to the context, with 10 candidate idioms.",
                    "url": "opencompass/opencompass_505.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_505",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "505",
                        "name": "CHID",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/chujiezheng/ChID-Dataset",
                        "paperLink": "https://arxiv.org/abs/1906.01265",
                        "officialWebsiteLink": "https://github.com/chujiezheng/ChID-Dataset",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1844",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:48:03",
                        "createDate": "2024-01-11 14:10:02",
                        "desc": {
                            "cn": "CHID是一个中文成语阅读理解任务，要求根据上下文选择正确的成语填空，共有10个候选成语。",
                            "en": " CHID is a chinese idiom reading comprehension task, which requires to select the correct idiom to fill in the blank according to the context, with 10 candidate idioms."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/chid'. Error: Path opencompass/chid is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_502",
                    "name": "ARC-c",
                    "version": "1.0.0",
                    "description": "The AI2’s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. ",
                    "url": "opencompass/opencompass_502.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_502",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {
                        "gen": "ARC-c_gen",
                        "ppl": "ARC-c_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "502",
                        "name": "ARC-c",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "学科",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://allenai.org/data/arc",
                        "paperLink": "https://arxiv.org/pdf/1803.05457.pdf",
                        "officialWebsiteLink": "https://allenai.org/data/arc",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1815",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:48:39",
                        "createDate": "2024-01-11 14:09:48",
                        "desc": {
                            "cn": "AI2的推理挑战（ARC）数据集是一个多项选择问题回答数据集，包含了从三年级到九年级的科学考试中提取的问题。该数据集分为两个部分：简单和挑战，其中后者包含了需要推理能力的更难的问题。大多数问题有4个答案选项，仅有不到1％的问题有3个或5个答案选项。",
                            "en": "The AI2’s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/arc_c'. Error: Path opencompass/arc_c is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_564",
                    "name": "LV-Eval",
                    "version": "1.0.0",
                    "description": "LV-Eval is a challenging long-context benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up to 256k words. The average number of words is 102,380, and the Min/Max number of words is 11,896/387,406. It features two main tasks, single-hop QA and multi-hop QA, comprising 11 bilingual datasets.",
                    "url": "opencompass/opencompass_564.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_564",
                    "sample_count": 1000,
                    "traits": [
                        "long-context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "564",
                        "name": "LV-Eval",
                        "emoji": "🗽",
                        "dimensions": [
                            {
                                "cn": "长文本",
                                "en": "long-context"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "Question Answering",
                                "en": "Question Answering"
                            },
                            {
                                "cn": "Synthetic",
                                "en": "Synthetic"
                            },
                            {
                                "cn": "Confusing Evidence",
                                "en": "Confusing Evidence"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 4,
                        "githubLink": "https://github.com/infinigence/LVEval",
                        "paperLink": "https://arxiv.org/abs/2402.05136",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "066910",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-KGJYN5OmL"
                        },
                        "lookNum": "1782",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 10:32:21",
                        "createDate": "2024-02-18 11:35:12",
                        "desc": {
                            "cn": "LV-Eval是一个具备5个长度等级（16k、32k、64k、128k和256k）、最大文本测试长度达到256k的长文本评测基准。LV-Eval的平均文本长度达到102,380字，最小/最大文本长度为11,896/387,406字。LV-Eval主要有两类评测任务——单跳QA和多跳QA，共包含11个涵盖中英文的评测数据子集。",
                            "en": "LV-Eval is a challenging long-context benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up to 256k words. The average number of words is 102,380, and the Min/Max number of words is 11,896/387,406. It features two main tasks, single-hop QA and multi-hop QA, comprising 11 bilingual datasets."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/lv_eval'. Error: Path opencompass/lv_eval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_529",
                    "name": "COPA",
                    "version": "1.0.0",
                    "description": "COPA is a causal inference task, which requires to select the correct causal relation based on the given premise.",
                    "url": "opencompass/opencompass_529.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_529",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {
                        "gen": "SuperGLUE_COPA_gen",
                        "ppl": "XCOPA_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "529",
                        "name": "COPA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://people.ict.usc.edu/~gordon/copa.html",
                        "paperLink": "https://arxiv.org/abs/1905.00537",
                        "officialWebsiteLink": "https://people.ict.usc.edu/~gordon/copa.html",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1746",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:51:45",
                        "createDate": "2024-01-11 14:11:52",
                        "desc": {
                            "cn": "COPA是一个因果推断任务，要求根据给定的前提，选择正确的因果关系。\n",
                            "en": "COPA is a causal inference task, which requires to select the correct causal relation based on the given premise."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/copa'. Error: Path opencompass/copa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_631",
                    "name": "OpenFinData",
                    "version": "1.0.0",
                    "description": "OpenFinData is an open-source financial evaluation dataset jointly released by EastMoney and Shanghai Artificial Intelligence Laboratory. This dataset represents the most realistic industrial scenario requirements and is currently the most comprehensive and professional financial evaluation dataset.",
                    "url": "opencompass/opencompass_631.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_631",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "631",
                        "name": "OpenFinData",
                        "emoji": "🦾",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "Finance",
                                "en": "Finance"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 4,
                        "githubLink": "https://github.com/open-compass/OpenFinData",
                        "paperLink": "",
                        "officialWebsiteLink": "https://openfindata.org/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50029256",
                            "name": "eastmoney",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50029256-0729a0fd-9e97-4c38-b9f9-a580ad6f6a54.png",
                            "nickname": "eastmoney"
                        },
                        "lookNum": "1700",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:40:43",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-12 19:40:43",
                        "createDate": "2024-09-12 19:39:10",
                        "desc": {
                            "cn": "OpenFinData是由东方财富与上海人工智能实验室联合发布的开源金融评测数据集。该数据集代表了最真实的产业场景需求，是目前场景最全、专业性最深的金融评测数据集。它基于东方财富实际金融业务的多样化丰富场景，旨在为金融科技领域的研究者和开发者提供一个高质量的数据资源。",
                            "en": "OpenFinData is an open-source financial evaluation dataset jointly released by EastMoney and Shanghai Artificial Intelligence Laboratory. This dataset represents the most realistic industrial scenario requirements and is currently the most comprehensive and professional financial evaluation dataset."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/openfindata'. Error: Path opencompass/openfindata is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_518",
                    "name": "OpenbookQA",
                    "version": "1.0.0",
                    "description": "OpenBookQA contains questions that require multi-step reasoning, application of common-sense knowledge, and in-depth comprehension of text. It is a new type of question-answering dataset, modeled after open-book exams, designed to assess human understanding of a specific topic.",
                    "url": "opencompass/opencompass_518.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_518",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {
                        "_gen": "openbookqa_gen",
                        "_ppl": "openbookqa_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "518",
                        "name": "OpenbookQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/allenai/OpenBookQA",
                        "paperLink": "https://arxiv.org/abs/1809.02789",
                        "officialWebsiteLink": "https://allenai.org/data/open-book-qa",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1661",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:48:08",
                        "createDate": "2024-01-11 14:11:07",
                        "desc": {
                            "cn": "OpenBookQA包含需要多步推理、运用常识知识、深入理解文本等能力的问题，是一种新型的问答数据集，其模式借鉴了开放式书本考试，用于评估人类对某一主题理解的程度。",
                            "en": "OpenBookQA contains questions that require multi-step reasoning, application of common-sense knowledge, and in-depth comprehension of text. It is a new type of question-answering dataset, modeled after open-book exams, designed to assess human understanding of a specific topic."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/openbookqa'. Error: Path opencompass/openbookqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_511",
                    "name": "CommonSenseQA",
                    "version": "1.0.0",
                    "description": "\nCommonsenseQA is a multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers. \n",
                    "url": "opencompass/opencompass_511.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_511",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "511",
                        "name": "CommonSenseQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/jonathanherzig/commonsenseqa",
                        "paperLink": "https://arxiv.org/abs/1811.00937",
                        "officialWebsiteLink": "https://www.tau-nlp.sites.tau.ac.il/commonsenseqa",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1582",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:48:32",
                        "createDate": "2024-01-11 14:10:39",
                        "desc": {
                            "cn": "CommonsenseQA是一个选择题数据集，它需要不同类型的常识知识来预测正确答案。它包含12,102个问题，有一个正确答案和四个干扰答案。\n",
                            "en": "\nCommonsenseQA is a multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers. \n"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/commonsenseqa'. Error: Path opencompass/commonsenseqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_513",
                    "name": "NQ",
                    "version": "1.0.0",
                    "description": "NQ (NaturalQuestion) corpus contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question. The inclusion of real user questions, and the requirement that solutions should read an entire page to find the answer, cause NQ to be a more realistic and challenging task than prior QA datasets.",
                    "url": "opencompass/opencompass_513.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_513",
                    "sample_count": 1000,
                    "traits": [
                        "knowledge"
                    ],
                    "eval_type": {
                        "gen": "NQ_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "513",
                        "name": "NQ",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/google-research-datasets/natural-questions",
                        "paperLink": "https://storage.googleapis.com/pub-tools-public-publication-data/pdf/b8c26e4347adc3453c15d96a09e6f7f102293f71.pdf",
                        "officialWebsiteLink": "https://ai.google.com/research/NaturalQuestions/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1543",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": true,
                        "updateDate": "2024-08-02 09:48:17",
                        "createDate": "2024-01-11 14:10:47",
                        "desc": {
                            "cn": "NQ 数据集来自于真实用户的问题，它要求 QA 系统阅读和理解整个维基百科文章，这些文章可能包含也可能不包含问题的答案。由真实用户问题构成，以及需要阅读整个页面才能找到答案的要求，比以往的 QA 数据集更现实和更具挑战性的任务。",
                            "en": "NQ (NaturalQuestion) corpus contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question. The inclusion of real user questions, and the requirement that solutions should read an entire page to find the answer, cause NQ to be a more realistic and challenging task than prior QA datasets."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/nq'. Error: Path opencompass/nq is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_514",
                    "name": "C3",
                    "version": "1.0.0",
                    "description": "A free-form multiple-Choice Chinese machine reading Comprehension dataset (C3), containing 13,369 documents (dialogues or more formally written mixed-genre texts) and their associated 19,577 multiple-choice free-form questions collected from Chinese-as-a-second-language examinations",
                    "url": "opencompass/opencompass_514.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_514",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {
                        "gen": "C3_gen",
                        "ppl": "C3_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "514",
                        "name": "C3",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "understanding",
                                "en": "understanding"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/nlpdata/c3",
                        "paperLink": "https://arxiv.org/abs/1904.09679",
                        "officialWebsiteLink": "https://github.com/nlpdata/c3",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1473",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:31:23",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-12 19:31:23",
                        "createDate": "2024-09-12 19:29:14",
                        "desc": {
                            "cn": "一个自由形式的多项选择中文机器阅读理解数据集（C3），包含13369篇文献（对话或更正式的混合体裁文本）及其相关的19577道自由选择题，这些问题都是从汉语作为第二语言的考试中收集到的",
                            "en": "A free-form multiple-Choice Chinese machine reading Comprehension dataset (C3), containing 13,369 documents (dialogues or more formally written mixed-genre texts) and their associated 19,577 multiple-choice free-form questions collected from Chinese-as-a-second-language examinations"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/c3'. Error: Path opencompass/c3 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_510",
                    "name": "BoolQ",
                    "version": "1.0.0",
                    "description": "BoolQ is a question answering dataset for yes/no questions containing 15942 examples. These questions are naturally occurring ---they are generated in unprompted and unconstrained settings. Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context.",
                    "url": "opencompass/opencompass_510.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_510",
                    "sample_count": 3270,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {
                        "gen": "BoolQ_gen",
                        "ppl": "BoolQ_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 9,
                        "estimated_output_tokens": 1
                    },
                    "original_data": {
                        "id": "510",
                        "name": "BoolQ",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "knowledge",
                                "en": "knowledge"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/google-research-datasets/boolean-questions",
                        "paperLink": "https://arxiv.org/abs/1905.10044",
                        "officialWebsiteLink": "https://github.com/google-research-datasets/boolean-questions",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1410",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:31:18",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-12 19:31:18",
                        "createDate": "2024-09-12 19:29:42",
                        "desc": {
                            "cn": "BoolQ是一个包含15942个示例的是/否问题的问答数据集。这些问题是自然生成的——即在无prompt和无约束的环境中产生的。每个例子都是一个三元组(问题、段落、答案)，页面标题是可选的附加上下文。",
                            "en": "BoolQ is a question answering dataset for yes/no questions containing 15942 examples. These questions are naturally occurring ---they are generated in unprompted and unconstrained settings. Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "is ncis new orleans over for the season",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "qa"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "information recall",
                                        "basic research"
                                    ],
                                    "concepts": [
                                        "TV series",
                                        "season",
                                        "cancellation"
                                    ],
                                    "qualification": [
                                        "None"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 35,
                                        "max": 80
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "can i have a beard in the military",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "qa"
                                    ],
                                    "domains": [
                                        "Military Studies"
                                    ],
                                    "skills": [
                                        "Regulatory knowledge",
                                        "Policy interpretation",
                                        "Communication"
                                    ],
                                    "concepts": [
                                        "Grooming regulations",
                                        "Facial hair policies",
                                        "Uniform standards",
                                        "Rank eligibility"
                                    ],
                                    "qualification": [
                                        "Active Duty Military",
                                        "Military Recruiter",
                                        "Enlisted Personnel"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 18,
                                        "avg": 25,
                                        "max": 50
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "can you contest a scrum in rugby league",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "qa"
                                    ],
                                    "domains": [
                                        "Rugby League"
                                    ],
                                    "skills": [
                                        "general knowledge",
                                        "sports knowledge"
                                    ],
                                    "concepts": [
                                        "scrum",
                                        "contest"
                                    ],
                                    "qualification": [],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 10,
                                        "avg": 15,
                                        "max": 80
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "will there be another now you see me movie",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "question_answering"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "information_retrieval",
                                        "basic_research"
                                    ],
                                    "concepts": [],
                                    "qualification": [],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 18,
                                        "avg": 35,
                                        "max": 85
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "is it possible to have a triangle with two obtuse angles",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "question_answering"
                                    ],
                                    "domains": [
                                        "Mathematics",
                                        "Geometry"
                                    ],
                                    "skills": [
                                        "basic_math",
                                        "critical_thinking",
                                        "problem_solving"
                                    ],
                                    "concepts": [
                                        "Triangle",
                                        "Obtuse angle",
                                        "Angle sum theorem",
                                        "Feasibility"
                                    ],
                                    "qualification": [
                                        "high_school"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 10,
                                        "avg": 13,
                                        "max": 80
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "is st george's chapel in westminster abbey",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "question_answering"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "general_knowledge",
                                        "memory_recall"
                                    ],
                                    "concepts": [
                                        "historical landmarks",
                                        "UK architecture",
                                        "Westminster Abbey",
                                        "St George's Chapel",
                                        "Windsor Castle"
                                    ],
                                    "qualification": [],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 10,
                                        "avg": 18,
                                        "max": 90
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "is san juan puerto rico in the caribbean",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "question_answering"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "knowledge_recall"
                                    ],
                                    "concepts": [],
                                    "qualification": [
                                        "1st_grade"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 6,
                                        "avg": 9,
                                        "max": 120
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "is the toyota highlander on a truck frame",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "question_answering"
                                    ],
                                    "domains": [
                                        "Automotive Engineering",
                                        "Mechanical Engineering",
                                        "Vehicle Design"
                                    ],
                                    "skills": [
                                        "domain knowledge",
                                        "critical thinking",
                                        "research",
                                        "problem solving"
                                    ],
                                    "concepts": [
                                        "Vehicle Architecture",
                                        "Unibody",
                                        "Body-on-Frame",
                                        "Crossover SUV",
                                        "Toyota TNGA Platform"
                                    ],
                                    "qualification": [
                                        "High School Diploma (Automotive Technology)",
                                        "Associate Degree in Automotive Technology",
                                        "Bachelor's Degree in Mechanical Engineering",
                                        "Automotive Technician",
                                        "Mechanical Engineer"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 17,
                                        "avg": 30,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "was integration the rule in the northern states",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "question_answering"
                                    ],
                                    "domains": [
                                        "US_History",
                                        "Civil_Rights",
                                        "19th_Century"
                                    ],
                                    "skills": [
                                        "knowledge_recall",
                                        "reading_comprehension",
                                        "historical_analytical_thinking"
                                    ],
                                    "concepts": [
                                        "Reconstruction",
                                        "Integration",
                                        "Northern_States",
                                        "13th_Amendment"
                                    ],
                                    "qualification": [
                                        "High_School",
                                        "Undergraduate"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 14,
                                        "avg": 16,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "do all rhombuses have 2 pairs of parallel sides",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "question_answering"
                                    ],
                                    "domains": [
                                        "Mathematics",
                                        "Geometry"
                                    ],
                                    "skills": [
                                        "basic_geometry_reasoning",
                                        "conceptual_understanding"
                                    ],
                                    "concepts": [
                                        "Quadrilateral",
                                        "Parallel Lines",
                                        "Rhombus",
                                        "Parallelogram"
                                    ],
                                    "qualification": [
                                        "middle_school_math",
                                        "high_school_math"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 13,
                                        "max": 80
                                    },
                                    "domain_expertise_required": true
                                }
                            ],
                            "total_questions": 3270,
                            "question_format": "Legal_information, QA, Question Answering, Question-answering, QuestionAnswering, Question_answering, conceptual_reasoning, explanation, fact_checking, fact_verification, factual_question, general_knowledge, information retrieval, information_retrieval, knowledge_explanation, knowledge_lookup, knowledge_question, knowledge_retrieval, legal_reasoning, legal_research, medical_information, medical_surgery, qa, question_answer, question_answering",
                            "difficulty_levels": [
                                "Beginner",
                                "Expert",
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Easy to interpret binary metric (accuracy).",
                                "Compact dataset: only ~3,300 examples—fast to evaluate.",
                                "Variety of real‑world topics ensures broad coverage of everyday knowledge.",
                                "Clear task structure (passage + yes/no question) reduces experimental noise.",
                                "Widely used benchmark – facilitates comparison across systems."
                            ],
                            "disadvantages": [
                                "Small dataset size may not fully test edge‑case reasoning.",
                                "Binary answers do not capture nuanced or ambiguous situations.",
                                "Lacks multi‑turn dialogue or long‑form answering, limiting evaluation of conversational depth.",
                                "Possible topic skew: certain domains may dominate and bias results.",
                                "Answers are pre‑generated, so the model cannot demonstrate open‑ended generation or elaboration."
                            ]
                        },
                        "age_distribution": {
                            "data": [
                                11,
                                30,
                                29,
                                37,
                                23,
                                7,
                                4,
                                29,
                                1,
                                6,
                                10,
                                1,
                                11,
                                0,
                                0,
                                1,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0
                            ],
                            "categories": [
                                "10",
                                "12",
                                "14",
                                "16",
                                "18",
                                "20",
                                "22",
                                "24",
                                "26",
                                "28",
                                "30",
                                "32",
                                "34",
                                "36",
                                "38",
                                "40",
                                "42",
                                "44",
                                "46",
                                "48",
                                "50",
                                "52",
                                "54",
                                "56",
                                "58",
                                "60"
                            ]
                        },
                        "why_run_this_eval": [
                            "Benches a model’s core conversational skills:\n- **Reading comprehension** – can the model extract the truth from the text?\n- **Fact‑based reasoning** – does it correctly differentiate between true and false statements?\n- **Domain adaptability** – does the model handle a range of topics (history, sports, engineering, regulations)?\n- **Binary decision making** – is the model capable of choosing a clear yes/no answer with minimal ambiguity?",
                            "Running BoolQ helps stakeholders quickly gauge these capabilities in a realistic, lightweight setup."
                        ],
                        "what_to_expect": [
                            "Running a model on BoolQ provides a single **accuracy score** – the proportion of questions the model answers correctly.",
                            "An accuracy close to 1 indicates the model can read the passage, understand the question, and retrieve or infer the correct binary answer.",
                            "Lower scores signal gaps in reading comprehension or knowledge recall.",
                            "Because each question tests a simple yes/no decision, the score directly reflects the model’s ability to perform quick, factual inference from a small context.",
                            "Relevance: A high accuracy suggests strong grounding in general knowledge and good paragraph comprehension, while a low score can highlight weaknesses in understanding nuanced policy language or domain‑specific terminology."
                        ],
                        "evaluation_description": "BoolQ is a question‑answering dataset that contains 3,270 short natural‑language questions. Each question is paired with a short passage (≈50 tokens) and a binary answer (yes/no). The questions cover a variety of topics such as U.S. history, sports, engineering, and everyday policy. The dataset is organized into two difficulty levels – Beginner and Intermediate – and reflects a wide range of skills including reading comprehension, information recall, basic reasoning, and domain knowledge.\n\nKey facts:\n- Number of rows: 3,270\n- Question‑type: Yes/No (binary) answers\n- Difficulty: Beginner & Intermediate\n- Domains: History, Sports, Engineering, Policy/Regulations, etc.\n- Each row includes a short context passage and a question about that passage.\n\nThe data has been compiled from the Stanford Natural Language Inference (SNLI) repository and reused for practical evaluation of LLMs on binary QA tasks.",
                        "top_5_task_types": [
                            "question_answering",
                            "qa"
                        ],
                        "top_5_domains": [
                            "Mathematics",
                            "Geometry",
                            "Military Studies",
                            "Rugby League",
                            "Automotive Engineering"
                        ],
                        "top_5_skills": [
                            "knowledge_recall",
                            "information recall",
                            "basic research",
                            "Regulatory knowledge",
                            "Policy interpretation"
                        ],
                        "top_5_concepts": [
                            "TV series",
                            "season",
                            "cancellation",
                            "Grooming regulations",
                            "Facial hair policies"
                        ],
                        "top_5_qualifications": [
                            "None",
                            "Active Duty Military",
                            "Military Recruiter",
                            "Enlisted Personnel",
                            "high_school"
                        ],
                        "top_5_languages": [
                            "English"
                        ]
                    },
                    "analysis_file": "analysis/opencompass_510_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 200,
                        "successful": 200,
                        "failed": 0
                    }
                },
                {
                    "id": "opencompass_500",
                    "name": "GAOKAO-Bench",
                    "version": "1.0.0",
                    "description": "GAOKAO-bench is an evaluation framework that utilizes Chinese high school entrance examination (GAOKAO) questions as a dataset to evaluate the language understanding and logical reasoning abilities of large language models.",
                    "url": "opencompass/opencompass_500.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_500",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "500",
                        "name": "GAOKAO-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "学科",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/OpenLMLab/GAOKAO-Bench",
                        "paperLink": "https://arxiv.org/abs/2305.12474",
                        "officialWebsiteLink": "https://github.com/OpenLMLab/GAOKAO-Bench",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1375",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": true,
                        "updateDate": "2024-08-02 09:49:06",
                        "createDate": "2024-01-11 14:09:41",
                        "desc": {
                            "cn": "GAOKAO-bench是一个以中国高考题目为数据集，旨在提供和人类对齐的，直观，高效地测评大模型语言理解能力、逻辑推理能力的测评框架",
                            "en": "GAOKAO-bench is an evaluation framework that utilizes Chinese high school entrance examination (GAOKAO) questions as a dataset to evaluate the language understanding and logical reasoning abilities of large language models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gaokao_bench'. Error: Path opencompass/gaokao_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_504",
                    "name": "WiC",
                    "version": "1.0.0",
                    "description": "WiC is a benchmark for the evaluation of context-sensitive word embeddings. WiC is framed as a binary classification task. Each instance in WiC has a target word w, either a verb or a noun, for which two contexts are provided. Each of these contexts triggers a specific meaning of w. The task is to identify if the occurrences of w in the two contexts correspond to the same meaning or not. In fact, the dataset can also be viewed as an application of Word Sense Disambiguation in practise.",
                    "url": "opencompass/opencompass_504.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_504",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {
                        "gen": "WiC_gen",
                        "ppl": "WiC_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "504",
                        "name": "WiC",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://pilehvar.github.io/wic/",
                        "paperLink": "https://arxiv.org/abs/1808.09121",
                        "officialWebsiteLink": "https://pilehvar.github.io/wic/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1257",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:48:51",
                        "createDate": "2024-01-11 14:09:58",
                        "desc": {
                            "cn": "Word-in-Context是一个词义消歧任务，被视为句子对的二元分类。给定两个文本片段和一个在两个句子中都出现的多义词，任务是确定该词在两个句子中是否具有相同的含义。",
                            "en": "WiC is a benchmark for the evaluation of context-sensitive word embeddings. WiC is framed as a binary classification task. Each instance in WiC has a target word w, either a verb or a noun, for which two contexts are provided. Each of these contexts triggers a specific meaning of w. The task is to identify if the occurrences of w in the two contexts correspond to the same meaning or not. In fact, the dataset can also be viewed as an application of Word Sense Disambiguation in practise."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/wic'. Error: Path opencompass/wic is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_509",
                    "name": "Flores",
                    "version": "1.0.0",
                    "description": "Flores is a benchmark dataset for machine translation between English and low-resource languages, which consists of sentences translated from Wikipedia, involving English and four low-resource languages, namely Nepali, Sinhala, Khmer and Pashto. Flores has two versions, we use the Flores-101 version here, which is the second version including 101 languages besides english. ",
                    "url": "opencompass/opencompass_509.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_509",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "509",
                        "name": "Flores",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/facebookresearch/flores/",
                        "paperLink": "https://arxiv.org/abs/2106.03193",
                        "officialWebsiteLink": "https://github.com/facebookresearch/flores/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1156",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:50:59",
                        "createDate": "2024-01-11 14:10:32",
                        "desc": {
                            "cn": "Flores是一个用于评估低资源语言机器翻译的基准数据集，它包含了从维基百科翻译的句子，涉及英语和四种低资源语言，分别是尼泊尔语、僧伽罗语、高棉语和普什图语。Flores有两个版本，我们这里使用的是第一个版本Flores-101，它包含有除英语外的101种语言。",
                            "en": "Flores is a benchmark dataset for machine translation between English and low-resource languages, which consists of sentences translated from Wikipedia, involving English and four low-resource languages, namely Nepali, Sinhala, Khmer and Pashto. Flores has two versions, we use the Flores-101 version here, which is the second version including 101 languages besides english. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "No valid url for ./data/flores_first100!\nPlease make sure  `./data/flores_first100` is correct"
                    }
                },
                {
                    "id": "opencompass_524",
                    "name": "CMNLI",
                    "version": "1.0.0",
                    "description": "CMNLI  is a Chinese natural language inference task, which requires to determine the logical relation between two sentences, with three relations: entailment, contradiction and neutral.",
                    "url": "opencompass/opencompass_524.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_524",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {
                        "gen": "cmnli_gen",
                        "ppl": "cmnli_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "524",
                        "name": "CMNLI",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/CLUEbenchmark/CLUE",
                        "paperLink": "https://arxiv.org/abs/2004.05986",
                        "officialWebsiteLink": "https://www.cluebenchmarks.com/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1139",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:49:37",
                        "createDate": "2024-01-11 14:11:34",
                        "desc": {
                            "cn": "CMNLI是一个中文自然语言推理任务，要求根据两个句子判断它们之间的逻辑关系，有三种关系：蕴含、矛盾和中立。",
                            "en": "CMNLI  is a Chinese natural language inference task, which requires to determine the logical relation between two sentences, with three relations: entailment, contradiction and neutral."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cmnli'. Error: Path opencompass/cmnli is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_520",
                    "name": "LCSTS",
                    "version": "1.0.0",
                    "description": "LCSTS is a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text.",
                    "url": "opencompass/opencompass_520.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_520",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {
                        "gen": "LCSTS_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "520",
                        "name": "LCSTS",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "http://icrc.hitsz.edu.cn/Article/show/139.html",
                        "paperLink": "https://arxiv.org/abs/1506.05865",
                        "officialWebsiteLink": "http://icrc.hitsz.edu.cn/Article/show/139.html",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1131",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:49:18",
                        "createDate": "2024-01-11 14:11:16",
                        "desc": {
                            "cn": "LCSTS是一个大规模的中文短文本摘要数据集，从中国微博网站新浪微博中构建而成，并已开源。该数据集包含超过 200 万条真实的中文短文本，每个文本都提供了一个简短的摘要。",
                            "en": "LCSTS is a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/lcsts'. Error: Path opencompass/lcsts is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_543",
                    "name": "HumanEval-X",
                    "version": "1.0.0",
                    "description": "HumanEval-X is a benchmark for evaluating the multilingual ability of code generative models. It consists of 820 high-quality human-crafted data samples (each with test cases) in Python, C++, Java, JavaScript, and Go, and can be used for various tasks, such as code generation and translation.",
                    "url": "opencompass/opencompass_543.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_543",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "543",
                        "name": "HumanEval-X",
                        "emoji": "🗽",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "code",
                                "en": "code"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/THUDM/CodeGeeX",
                        "paperLink": "https://arxiv.org/abs/2303.17568",
                        "officialWebsiteLink": "https://huggingface.co/datasets/THUDM/humaneval-x",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1123",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-12 19:32:11",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-12 19:32:11",
                        "createDate": "2024-09-12 19:25:43",
                        "desc": {
                            "cn": "HumanEval-X 是一个用于评估代码生成模型的多语言能力的基准测试。它包含了820个高质量的人工制作的数据样本（每个都有测试案例），包括Python、C++、Java、JavaScript和Go语言，可用于各种任务，如代码生成和翻译。",
                            "en": "HumanEval-X is a benchmark for evaluating the multilingual ability of code generative models. It consists of 820 high-quality human-crafted data samples (each with test cases) in Python, C++, Java, JavaScript, and Go, and can be used for various tasks, such as code generation and translation."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/humaneval_x'. Error: Path opencompass/humaneval_x is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1073",
                    "name": "SafetyBench",
                    "version": "1.0.0",
                    "description": " SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. Notably, SafetyBench also incorporates both Chinese and English data.",
                    "url": "opencompass/opencompass_1073.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1073",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1073",
                        "name": "SafetyBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/thu-coai/SafetyBench",
                        "paperLink": "https://arxiv.org/pdf/2309.07045",
                        "officialWebsiteLink": "https://llmbench.ai/safety",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "5018933",
                            "name": "thu-coai",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/5018933-17d059c5-d271-410a-82a9-ef8966927c24.png",
                            "nickname": "thu-coai"
                        },
                        "lookNum": "1086",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-27 17:50:17",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-27 17:50:17",
                        "createDate": "2024-09-27 12:52:37",
                        "desc": {
                            "cn": "SafetyBench 是一个全面的基准，用于评估大型语言模型（LLMs）的安全性，包含 11,435 道多样化的选择题，涵盖 7 个不同的安全关注类别。SafetyBench 还包含中文和英文的数据，方便双语评估。",
                            "en": " SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. Notably, SafetyBench also incorporates both Chinese and English data."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/safetybench'. Error: Path opencompass/safetybench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1052",
                    "name": "CaLM",
                    "version": "1.0.0",
                    "description": "CaLM is the first comprehensive benchmark for evaluating the causal reasoning capabilities of language models. The CaLM framework establishes a foundational taxonomy consisting of four modules: causal target, adaptation, metric, and error.\n",
                    "url": "opencompass/opencompass_1052.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1052",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1052",
                        "name": "CaLM",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/OpenCausaLab/CaLM",
                        "paperLink": "https://arxiv.org/abs/2405.00622",
                        "officialWebsiteLink": "https://opencausalab.github.io/CaLM/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50204289",
                            "name": "OpenCausaLab",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50204289-b2e0a1f4-5d24-4237-b62a-ed8731d8834d.png",
                            "nickname": "OpenCausaLab"
                        },
                        "lookNum": "1053",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-23 16:42:32",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-23 16:42:32",
                        "createDate": "2024-09-13 13:55:09",
                        "desc": {
                            "cn": "CaLM是上海人工智能实验室联合同济大学、上海交通大学、北京大学及商汤科技发布首个大模型因果推理开放评测体系。首次从因果推理角度提出评估框架，为AI研究者打造可靠评测工具，从而为推进大模型认知能力向人类水平看齐提供指标参考。",
                            "en": "CaLM is the first comprehensive benchmark for evaluating the causal reasoning capabilities of language models. The CaLM framework establishes a foundational taxonomy consisting of four modules: causal target, adaptation, metric, and error.\n"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/calm'. Error: Path opencompass/calm is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_532",
                    "name": "PIQA",
                    "version": "1.0.0",
                    "description": "PIQA is a physical interaction question answering task, which requires to select the most reasonable solution based on the given scenario and two possible solutions. This task is designed to test the model's knowledge in physical commonsense. This dataset consists of 16k training samples, 800 development samples and 2k test samples, all on English text.",
                    "url": "opencompass/opencompass_532.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_532",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {
                        "gen": "piqa_gen",
                        "ppl": "piqa_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "532",
                        "name": "PIQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/francois-rozet/piqa",
                        "paperLink": "https://arxiv.org/abs/1911.11641",
                        "officialWebsiteLink": "https://yonatanbisk.com/piqa/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1041",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:49:25",
                        "createDate": "2024-01-11 14:12:04",
                        "desc": {
                            "cn": "PIQA是一个物理交互问答任务，要求根据给定的场景和两个可能的解决方案，选择最合理的方案。这个任务是为了测试模型在物理常识方面的知识。这个数据集包含了16000个训练样本，800个开发样本和2000个测试样本，所有的文本都是英文文本。",
                            "en": "PIQA is a physical interaction question answering task, which requires to select the most reasonable solution based on the given scenario and two possible solutions. This task is designed to test the model's knowledge in physical commonsense. This dataset consists of 16k training samples, 800 development samples and 2k test samples, all on English text."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "No valid url for ./data/piqa!\nPlease make sure  `./data/piqa` is correct"
                    }
                },
                {
                    "id": "opencompass_519",
                    "name": "CSL",
                    "version": "1.0.0",
                    "description": "CSL is a large-scale Chinese Scientific Literature dataset, which contains the titles, abstracts, keywords and academic fields of 396k papers.",
                    "url": "opencompass/opencompass_519.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_519",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "519",
                        "name": "CSL",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/ydli-ai/CSL",
                        "paperLink": "https://arxiv.org/abs/2209.05034",
                        "officialWebsiteLink": "https://github.com/ydli-ai/CSL",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1038",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:50:33",
                        "createDate": "2024-01-11 14:11:11",
                        "desc": {
                            "cn": "CSL是一个大规模的中文科技文献数据集，包含 39.6 万篇论文的标题、摘要、关键词和学术领域信息。",
                            "en": "CSL is a large-scale Chinese Scientific Literature dataset, which contains the titles, abstracts, keywords and academic fields of 396k papers."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/csl'. Error: Path opencompass/csl is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_536",
                    "name": "DROP",
                    "version": "1.0.0",
                    "description": "DROP is a QA dataset which tests comprehensive understanding of paragraphs. In this crowdsourced, adversarially-created, 96k question-answering benchmark, a system must resolve multiple references in a question, map them onto a paragraph, and perform discrete operations over them (such as addition, counting, or sorting).",
                    "url": "opencompass/opencompass_536.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_536",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {
                        "gen": "DROP_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "536",
                        "name": "DROP",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/allenai/allennlp-reading-comprehension/blob/master/allennlp_rc/eval/drop_eval.py",
                        "paperLink": "https://arxiv.org/abs/1903.00161",
                        "officialWebsiteLink": "https://allennlp.org/drop",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1031",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:50:10",
                        "createDate": "2024-01-11 14:12:19",
                        "desc": {
                            "cn": "DROP 是一个测试段落综合理解能力的 QA 数据集。在这个众包、对抗性创建的 96K 问题解答基准中，系统必须解析问题中的多个引用，将它们映射到段落中，并对它们执行离散操作（如加法、计数或排序）。",
                            "en": "DROP is a QA dataset which tests comprehensive understanding of paragraphs. In this crowdsourced, adversarially-created, 96k question-answering benchmark, a system must resolve multiple references in a question, map them onto a paragraph, and perform discrete operations over them (such as addition, counting, or sorting)."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/drop'. Error: Path opencompass/drop is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_533",
                    "name": "SIQA",
                    "version": "1.0.0",
                    "description": "SIQA is a social interaction question answering task, which requires to select the most reasonable behavior based on the given scenario and three possible subsequent behaviors. This task is designed to test the model's knowledge in social commonsense. This dataset consists of 38,963 training samples, 1,951 development samples and 1,960 test samples, all on English text.",
                    "url": "opencompass/opencompass_533.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_533",
                    "sample_count": 1000,
                    "traits": [
                        "reasoning"
                    ],
                    "eval_type": {
                        "gen": "siqa_gen",
                        "ppl": "siqa_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "533",
                        "name": "SIQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/pdf/1904.09728.pdf",
                        "officialWebsiteLink": "https://leaderboard.allenai.org/socialiqa/submissions/public",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1029",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:49:13",
                        "createDate": "2024-01-11 14:12:08",
                        "desc": {
                            "cn": "SIQA 是一个社会交互问答任务，要求根据给定的场景和三个可能的后续行为，选择最合理的行为。这个任务是为了测试模型在社会常识方面的知识。这个数据集包含了 38963 个训练样本，1951 个开发样本和 1960 个测试样本，所有的文本都是英文文本。",
                            "en": "SIQA is a social interaction question answering task, which requires to select the most reasonable behavior based on the given scenario and three possible subsequent behaviors. This task is designed to test the model's knowledge in social commonsense. This dataset consists of 38,963 training samples, 1,951 development samples and 1,960 test samples, all on English text."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "No valid url for ./data/siqa!\nPlease make sure  `./data/siqa` is correct"
                    }
                },
                {
                    "id": "opencompass_503",
                    "name": "ARC-e",
                    "version": "1.0.0",
                    "description": "The AI2’s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. ",
                    "url": "opencompass/opencompass_503.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_503",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {
                        "gen": "ARC-e_gen",
                        "ppl": "ARC-e_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "503",
                        "name": "ARC-e",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "学科",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://allenai.org/data/arc",
                        "paperLink": "https://arxiv.org/pdf/1803.05457.pdf",
                        "officialWebsiteLink": "https://allenai.org/data/arc",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "1017",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:49:30",
                        "createDate": "2024-01-11 14:09:54",
                        "desc": {
                            "cn": "AI2的推理挑战（ARC）数据集是一个多项选择问题回答数据集，包含了从三年级到九年级的科学考试中提取的问题。该数据集分为两个部分：简单和挑战，其中后者包含了需要推理能力的更难的问题。大多数问题有4个答案选项，仅有不到1％的问题有3个或5个答案选项。",
                            "en": "The AI2’s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/arc_e'. Error: Path opencompass/arc_e is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_521",
                    "name": "XSum",
                    "version": "1.0.0",
                    "description": "XSum is a single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question “What is the article about?”. The dataset collects real-world, large scale data by harvesting online articles from the British Broadcasting Corporation (BBC).",
                    "url": "opencompass/opencompass_521.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_521",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {
                        "gen": "XSum_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "521",
                        "name": "XSum",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/EdinburghNLP/XSum",
                        "paperLink": "https://arxiv.org/abs/1808.08745",
                        "officialWebsiteLink": "https://github.com/EdinburghNLP/XSum",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "955",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:50:47",
                        "createDate": "2024-01-11 14:11:21",
                        "desc": {
                            "cn": "XSum是一个单文档摘要任务，不支持抽取式策略，需要采用抽象建模方法。其思想是创建一个简短的一句话新闻摘要，回答“这篇文章是关于什么的？”的问题。该数据集通过从英国广播公司（BBC）收集在线文章，得到了大量的现实数据。",
                            "en": "XSum is a single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question “What is the article about?”. The dataset collects real-world, large scale data by harvesting online articles from the British Broadcasting Corporation (BBC)."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "No valid url for ./data/Xsum/dev.jsonl!\nPlease make sure  `./data/Xsum/dev.jsonl` is correct"
                    }
                },
                {
                    "id": "opencompass_506",
                    "name": "AFQMC",
                    "version": "1.0.0",
                    "description": " AFQMC is an Ant Financial chinese semantic similarity task, which requires to judge whether two sentences have the same meaning or not.",
                    "url": "opencompass/opencompass_506.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_506",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "506",
                        "name": "AFQMC",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/IDEA-CCNL/Fengshenbang-LM/",
                        "paperLink": "https://arxiv.org/abs/2209.02970",
                        "officialWebsiteLink": "https://tianchi.aliyun.com/dataset/106411",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "945",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:50:04",
                        "createDate": "2024-01-11 14:10:07",
                        "desc": {
                            "cn": "AFQMC一个蚂蚁金服中文语义相似度任务，要求判断两个句子是否具有相同的语义。",
                            "en": " AFQMC is an Ant Financial chinese semantic similarity task, which requires to judge whether two sentences have the same meaning or not."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/afqmc'. Error: Path opencompass/afqmc is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1097",
                    "name": "HelloBench",
                    "version": "1.0.0",
                    "description": "HelloBench is a hierarchical long text generation benchmark to evaluate LLMs' performance in generating long text.  Based on Bloom's Taxonomy, HelloBench categorizes long text generation tasks into five subtasks: open-ended QA, summarization, chat, text completion, and heuristic text generation.",
                    "url": "opencompass/opencompass_1097.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1097",
                    "sample_count": 1000,
                    "traits": [
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1097",
                        "name": "HelloBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "长文本",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 3,
                        "githubLink": "https://github.com/Quehry/HelloBench",
                        "paperLink": "https://arxiv.org/pdf/2409.16191",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "934",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-30 15:32:45",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-30 15:32:45",
                        "createDate": "2024-09-30 15:06:26",
                        "desc": {
                            "cn": "HelloBench为长文本生成基准，这是一个全面的、开放式的基准，用于评估LLM在生成长文本方面的性能。基于Bloom的分类法，HelloBench将长文本生成任务分为五个子任务：开放式QA、摘要、聊天、文本完成和启发式文本生成。",
                            "en": "HelloBench is a hierarchical long text generation benchmark to evaluate LLMs' performance in generating long text.  Based on Bloom's Taxonomy, HelloBench categorizes long text generation tasks into five subtasks: open-ended QA, summarization, chat, text completion, and heuristic text generation."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/hellobench'. Error: Path opencompass/hellobench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1253",
                    "name": "BigCodeBench",
                    "version": "1.0.0",
                    "description": "BigCodeBench is a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained tasks. ",
                    "url": "opencompass/opencompass_1253.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1253",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1253",
                        "name": "BigCodeBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/bigcode-project/bigcodebench/",
                        "paperLink": "https://arxiv.org/abs/2406.15877",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "929",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-30 16:30:39",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-30 16:30:39",
                        "createDate": "2024-12-30 16:24:34",
                        "desc": {
                            "cn": "BigCodeBench用于评估LLM的代码生成能力，包含1140个可以调用139个库和7个域的多个函数来完成的细粒度任务。",
                            "en": "BigCodeBench is a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/bigcodebench'. Error: No data found in /home/budadmin/.cache/opencompass/./data/bigcodebench/ for split 'test'. Please check if the split exists and contains data files.\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_507",
                    "name": "WSC",
                    "version": "1.0.0",
                    "description": "WSC is a pronoun disambiguation task, which requires to determine which noun the pronoun refers to according to the context.",
                    "url": "opencompass/opencompass_507.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_507",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {
                        "gen": "WSC_gen",
                        "ppl": "WSC_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "507",
                        "name": "WSC",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html",
                        "paperLink": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.729.9814&rep=rep1&type=pdf",
                        "officialWebsiteLink": "https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "917",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:51:25",
                        "createDate": "2024-01-11 14:10:14",
                        "desc": {
                            "cn": "WSC是一个代词消歧任务，要求根据上下文判断代词指代的是哪个名词。",
                            "en": "WSC is a pronoun disambiguation task, which requires to determine which noun the pronoun refers to according to the context."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/wsc'. Error: Path opencompass/wsc is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_516",
                    "name": "RACE(High)",
                    "version": "1.0.0",
                    "description": "RACE is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle school and high school students. ",
                    "url": "opencompass/opencompass_516.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_516",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "516",
                        "name": "RACE(High)",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/qizhex/RACE_AR_baselines",
                        "paperLink": "https://arxiv.org/abs/1704.04683",
                        "officialWebsiteLink": "https://www.cs.cmu.edu/~glai1/data/race/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "916",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:49:43",
                        "createDate": "2024-01-11 14:10:58",
                        "desc": {
                            "cn": "RACE 是一个大规模的阅读理解数据集，包含超过 28,000 个段落和近 100,000 个问题。该数据集是从中国的英语考试中收集而来，这些考试是为中学和高中学生设计的。\n",
                            "en": "RACE is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle school and high school students. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/race(high)'. Error: Path opencompass/race(high) is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_517",
                    "name": "RACE(Middle)",
                    "version": "1.0.0",
                    "description": "RACE is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle school and high school students. ",
                    "url": "opencompass/opencompass_517.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_517",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "517",
                        "name": "RACE(Middle)",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/qizhex/RACE_AR_baselines",
                        "paperLink": "https://arxiv.org/abs/1704.04683",
                        "officialWebsiteLink": "https://www.cs.cmu.edu/~glai1/data/race/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "906",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:52:17",
                        "createDate": "2024-01-11 14:11:03",
                        "desc": {
                            "cn": "RACE 是一个大规模的阅读理解数据集，包含超过 28,000 个段落和近 100,000 个问题。该数据集是从中国的英语考试中收集而来，这些考试是为中学和高中学生设计的。\n",
                            "en": "RACE is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle school and high school students. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/race(middle)'. Error: Path opencompass/race(middle) is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_525",
                    "name": "OCNLI",
                    "version": "1.0.0",
                    "description": "OCNLI is a Chinese natural language inference task, which requires to determine the logical relation between two sentences, with three relations: entailment, contradiction and neutral.",
                    "url": "opencompass/opencompass_525.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_525",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {
                        "gen": "ocnli_gen",
                        "ppl": "ocnli_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "525",
                        "name": "OCNLI",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/cluebenchmark/OCNLI",
                        "paperLink": "https://arxiv.org/abs/2010.05444",
                        "officialWebsiteLink": "https://github.com/cluebenchmark/OCNLI",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "885",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:50:54",
                        "createDate": "2024-01-11 14:11:37",
                        "desc": {
                            "cn": "OCNLI是一个中文自然语言推理任务，要求根据两个句子判断它们之间的逻辑关系，有三种关系：蕴含、矛盾和中立。",
                            "en": "OCNLI is a Chinese natural language inference task, which requires to determine the logical relation between two sentences, with three relations: entailment, contradiction and neutral."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ocnli'. Error: Path opencompass/ocnli is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_924",
                    "name": "CS-Bench",
                    "version": "1.0.0",
                    "description": "CS-Bench, the first bilingual (Chinese-English) benchmark dedicated to evaluating the performance of LLMs in computer science. CS-Bench comprises approximately 5K meticulously curated test samples, covering 26 subfields across 4 key areas of computer science, encompassing various task forms and divisions of knowledge and reasoning.",
                    "url": "opencompass/opencompass_924.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_924",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning",
                        "Code",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "924",
                        "name": "CS-Bench",
                        "emoji": "😜",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "代码",
                                "en": "Code"
                            },
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/csbench/csbench",
                        "paperLink": "https://arxiv.org/pdf/2406.08587",
                        "officialWebsiteLink": "https://csbench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50103486",
                            "name": null,
                            "avatar": null,
                            "nickname": "LeonDiao0427"
                        },
                        "lookNum": "883",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-07-11 09:52:59",
                        "createDate": "2024-07-11 09:48:45",
                        "desc": {
                            "cn": "CS-Bench 第一个专门用于评估LLMs在计算机科学中表现的双语（中英文）基准。CS-Bench包括约5,000个精心策划的测试样本，涵盖了计算机科学4个关键领域中的26个子领域，并包括各种任务形式和知识推理的划分。",
                            "en": "CS-Bench, the first bilingual (Chinese-English) benchmark dedicated to evaluating the performance of LLMs in computer science. CS-Bench comprises approximately 5K meticulously curated test samples, covering 26 subfields across 4 key areas of computer science, encompassing various task forms and divisions of knowledge and reasoning."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cs_bench'. Error: Path opencompass/cs_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1172",
                    "name": "MT-Bench-101",
                    "version": "1.0.0",
                    "description": "MT-Bench-101 is specifically designed to evaluate the finegrained abilities of LLMs in multi-turn dialogues. ",
                    "url": "opencompass/opencompass_1172.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1172",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1172",
                        "name": "MT-Bench-101",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/mtbench101/mt-bench-101",
                        "paperLink": "https://arxiv.org/pdf/2402.14762",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "874",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-27 17:23:10",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-27 17:23:10",
                        "createDate": "2024-10-21 14:33:55",
                        "desc": {
                            "cn": "MT-Bench-101 专门设计用于评估 LLMs 在多轮对话中的细粒度能力。通过对真实多轮对话数据的详细分析，构建了一个三层级的能力分类法，涵盖 1388 个多轮对话中的 4208 个轮次，涉及 13 种不同的任务。",
                            "en": "MT-Bench-101 is specifically designed to evaluate the finegrained abilities of LLMs in multi-turn dialogues. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mt_bench_101'. Error: Path opencompass/mt_bench_101 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_508",
                    "name": "TyDiQA",
                    "version": "1.0.0",
                    "description": "TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language expresses.",
                    "url": "opencompass/opencompass_508.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_508",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "508",
                        "name": "TyDiQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/google-research-datasets/tydiqa",
                        "paperLink": "https://arxiv.org/abs/2003.05002",
                        "officialWebsiteLink": "https://ai.google.com/research/tydiqa",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "869",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-01-11 14:10:23",
                        "createDate": "2024-01-11 14:10:23",
                        "desc": {
                            "cn": "TyDi QA 是一个涵盖 11 种不同语言的问题回答数据集，包含 20.4 万个问题-答案对。TyDi QA 的语言种类多样，涵盖了语言学特征的各种类型。",
                            "en": "TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language expresses."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "No valid url for ./data/tydiqa/!\nPlease make sure  `./data/tydiqa/` is correct"
                    }
                },
                {
                    "id": "opencompass_523",
                    "name": "LAMBADA",
                    "version": "1.0.0",
                    "description": "The LAMBADA evaluates the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broad",
                    "url": "opencompass/opencompass_523.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_523",
                    "sample_count": 5153,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {
                        "gen": "LAMBADA_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 74,
                        "estimated_output_tokens": 1
                    },
                    "original_data": {
                        "id": "523",
                        "name": "LAMBADA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://zenodo.org/record/2630551",
                        "paperLink": "https://arxiv.org/abs/1606.06031",
                        "officialWebsiteLink": "https://zenodo.org/record/2630551",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "849",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:51:18",
                        "createDate": "2024-01-11 14:11:30",
                        "desc": {
                            "cn": "LAMBADA 通过一个单词预测任务来评估计算模型对文本理解的能力。LAMBADA 是有如下特点的一组叙述性文章：如果面对整篇文章，人们可以猜测它们的最后一个单词，但如果他们只看到目标单词前面的最后一句话，就无法猜测。为了在 LAMBADA 上由好的效果，模型不能仅仅依赖于局部上下文，而必须能够跟踪更广泛的话语信息。\nLAMBADA 数据集是从 BookCorpus 中提取的，包括 10,022 段落，分为 4,869 个开发段落和 5,153 个测试段落，共计 2.03 亿个单词。",
                            "en": "The LAMBADA evaluates the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse.\nThe LAMBADA dataset is extracted from BookCorpus and consists of 10'022 passages, divided into 4'869 development and 5'153 test passages, comprising 203 million words."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "“Carlos Rafael Wilson.”\nThe man smiled at him. Carlos didn’t have a clue what was going on. He looked to his manager.\n“Tom here’s just moved into the house at the bottom of the hill.”\n“Oh right.”\n“Abo",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "creative_writing",
                                        "story_continuation"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "creative_thinking",
                                        "plot_building",
                                        "character_development",
                                        "dialogue_creativity"
                                    ],
                                    "concepts": [
                                        "narrative structure",
                                        "character motivation",
                                        "dialogue continuity"
                                    ],
                                    "qualification": [
                                        "high_school",
                                        "undergraduate"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 15,
                                        "avg": 25,
                                        "max": 100
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "“You wouldn’t dare,” Elijah growled. I squeaked as he was all of a sudden an inch or two behind the rogue. “Tell me, rogue, where is Delilah?” The rogue stiffened. “On behalf of the Vampiric Council,”",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "Creative Writing",
                                        "Narrative Generation"
                                    ],
                                    "domains": [
                                        "Creative Writing",
                                        "Fantasy Literature"
                                    ],
                                    "skills": [
                                        "Creativity",
                                        "Storytelling",
                                        "Plot Development",
                                        "Character Development",
                                        "Writing Fluency"
                                    ],
                                    "concepts": [
                                        "Narrative Structure",
                                        "Character Voice",
                                        "Dialogue",
                                        "Suspense & Tension",
                                        "World Building",
                                        "Plot Continuation"
                                    ],
                                    "qualification": [
                                        "High School Diploma",
                                        "College-Level Creative Writing",
                                        "Literary Club Membership",
                                        "Experienced Writer"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 18,
                                        "max": 80
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "Bob was pensive for the rest of the evening, talking openly and realistically about his work on the railroad to the four students who appeared enthused over the prospect of high speed rail in their li",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "creative_writing",
                                        "story_completion"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "creativity",
                                        "imagination",
                                        "storytelling",
                                        "prose_writing"
                                    ],
                                    "concepts": [
                                        "narrative structure",
                                        "character development",
                                        "dialogue",
                                        "story arc"
                                    ],
                                    "qualification": [
                                        "high_school_graduate"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 15,
                                        "avg": 22,
                                        "max": 70
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "For the hungrier set there seemed to be every kind of fowl, from quail to goose, neatly carved into small pieces so a dancer could eat quickly and return to the floor.\n\nOnce again I wished I was hungr",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "reading_comprehension",
                                        "analysis",
                                        "summary"
                                    ],
                                    "domains": [
                                        "Literature"
                                    ],
                                    "skills": [
                                        "reading_comprehension",
                                        "critical_thinking",
                                        "analysis",
                                        "literary_knowledge"
                                    ],
                                    "concepts": [
                                        "narrative",
                                        "character_description",
                                        "setting_description",
                                        "dialogue"
                                    ],
                                    "qualification": [
                                        "High school diploma"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 16,
                                        "max": 80
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "Every few hours, one of them wheeled in the portable ultrasound machine -- Josh during the day, and Nellie in the evenings.  Lashonda couldn't understand the monochrome squiggles on its greenish-black",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "reading_comprehension",
                                        "contextual_inference"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "reading",
                                        "critical_thinking",
                                        "inference",
                                        "contextual_understanding"
                                    ],
                                    "concepts": [
                                        "ultrasound",
                                        "medical imaging",
                                        "patient assessment",
                                        "task sequencing"
                                    ],
                                    "qualification": [
                                        "high_school"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 7,
                                        "avg": 13,
                                        "max": 70
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "\"Erin talked to a reporter,\" Liz told him. Erin was Brady's ex-girlfriend. They had broken up after he had visited Liz in October.\n\n\"What makes you think that? She's not the type to get involved with ",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "reading_comprehension",
                                        "qa"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "reading_comprehension",
                                        "inference",
                                        "contextual_understanding"
                                    ],
                                    "concepts": [
                                        "dialogue",
                                        "ex-girlfriend",
                                        "reporter",
                                        "press",
                                        "anchor",
                                        "relationship"
                                    ],
                                    "qualification": [
                                        "high_school_graduate"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 13,
                                        "avg": 15,
                                        "max": 90
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "But then in the end she didn't have to.\n\n\"This is the daddy I want,\" Jessie said in a voice as clear as crystal. If Meg had wanted to be discreet about the drawing those hopes were dashed immediately.",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "creative_writing",
                                        "story_continuation"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "creative_writing",
                                        "dialogue_creation",
                                        "plot_consistency",
                                        "character_descriptive_skills"
                                    ],
                                    "concepts": [
                                        "narrative_continuation",
                                        "dialogue",
                                        "character_development"
                                    ],
                                    "qualification": [
                                        "high_school"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 10,
                                        "avg": 16,
                                        "max": 70
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "A small scream escaped her, but she landed in a shallow stream of frigid water. The fall had only been a meter or so.\n\n\"Watch out!\" Gray called.\n\nRachel rolled clear as the others slid, skidded, and d",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "creative_writing",
                                        "story_continuation"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "creative_writing",
                                        "imagination",
                                        "reading_comprehension",
                                        "context_understanding",
                                        "syntactical_proficiency"
                                    ],
                                    "concepts": [
                                        "narrative_construction",
                                        "character_action_description",
                                        "scene_narration"
                                    ],
                                    "qualification": [
                                        "high_school_graduate",
                                        "college_student"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 16,
                                        "max": 80
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "Not only did he not fully trust the constable, but he also doubted the little man’s abilities to comprehend it. “We shall see what turns up.” \n“Where are you going, Inspector?”\nHe was turning off the ",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "qa"
                                    ],
                                    "domains": [
                                        "English Literature",
                                        "Reading Comprehension"
                                    ],
                                    "skills": [
                                        "reading_comprehension",
                                        "critical_thinking",
                                        "inference",
                                        "attention_to_detail"
                                    ],
                                    "concepts": [
                                        "dialogue",
                                        "trust",
                                        "suspicion",
                                        "narrative structure"
                                    ],
                                    "qualification": [
                                        "High School Diploma",
                                        "Literacy"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 11,
                                        "avg": 14,
                                        "max": 80
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "Never did I have a daughter.  Now, I would appreciate it if you would change me.\"  Dad continued to order me.\nI rested my hand on Dad's head and rubbed his clammy skin.  \"Daddy, it's me, Stacey your d",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "creative_writing",
                                        "dialogue_generation"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "creative_writing",
                                        "storytelling",
                                        "empathy",
                                        "reading_comprehension"
                                    ],
                                    "concepts": [
                                        "dialogue",
                                        "character_development",
                                        "emotional_expression",
                                        "narrative_perspective"
                                    ],
                                    "qualification": [
                                        "High School Diploma"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 12,
                                        "avg": 16,
                                        "max": 80
                                    },
                                    "domain_expertise_required": false
                                }
                            ],
                            "total_questions": 5153,
                            "question_format": "Creative Writing, CreativeWriting, Creative_Writing, Dialogue Generation, Inference, Narrative Completion, NarrativeGeneration, Narrative_Analysis, Narrative_Composition, Narrative_Synthesis, Question_answering, Reading Comprehension, Reading_Comprehension, Reasoning, RolePlaying, Story Continuation, StoryCompletion, Story_Continuation, Summarisation, Textual Analysis, analysis, argumentation, comprehension, content_generation, conversation, counseling, creative text generation, creative writing, creative-writing, creative_narration, creative_prose, creative_story_completion, creative_story_prompt, creative_text_generation, creative_write, creative_writing, critical_thinking, deduction, dialogue, dialogue generation, dialogue_analysis, dialogue_completion, dialogue_continuation, dialogue_generation, empathetic_response, explanatory_conversation, fiction, fictional_narrative, forensic report writing, generative_text, inference, inferential_analysis, interpretation, investigation, language_understanding, literary_analysis, literary_inference, narrative_analysis, narrative_completion, narrative_extension, narrative_generation, narrative_response, natural language understanding, natural_language_generation, plot_development, problem solving, problem_solving, prompt_completion, prompt_response, puzzle_solving, qa, qualitative analysis, question answering, question_answering, reading comprehension, reading_comprehension, research, response_generation, role_play, self-reflection, self‑assessment, social_interaction, story continuation, story_completion, story_continuation, story_generation, storytelling, strategic_planning, summarisation, summarization, summary, text_analysis, text_continuation, text_generation, text_interpretation, textual analysis, textual_analysis, therapy, translation, troubleshooting, writing, writing_prompt",
                            "difficulty_levels": [
                                "Advanced",
                                "Beginner",
                                "Expert",
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Measures a model’s capacity to use extended context, a critical skill for realistic story generation.",
                                "Provides a clear, objective accuracy metric that can be easily compared across models.",
                                "Includes a diverse set of genres (fantasy, literary, reportage), training models to generalize beyond narrow domains.",
                                "The size (5k+ samples) is large enough to give statistically robust scores while still being quick to run."
                            ],
                            "disadvantages": [
                                "The evaluation focuses narrowly on the last‑word prediction task, which may not reflect all aspects of creative writing (e.g., dialogue style, pacing).",
                                "Success depends heavily on the model’s tokenization; different vocabularies can affect accuracy unfairly.",
                                "Because only narratives are included, models that excel at dialogue or expository writing may appear weaker.",
                                "Some stories contain ambiguous or multi‑word endings that can inflate error rates if the target word is tokenized differently."
                            ]
                        },
                        "age_distribution": {
                            "data": [
                                2,
                                11,
                                35,
                                50,
                                47,
                                8,
                                4,
                                24,
                                1,
                                3,
                                5,
                                2,
                                4,
                                0,
                                1,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0
                            ],
                            "categories": [
                                "10",
                                "12",
                                "14",
                                "16",
                                "18",
                                "20",
                                "22",
                                "24",
                                "26",
                                "28",
                                "30",
                                "32",
                                "34",
                                "36",
                                "38",
                                "40",
                                "42",
                                "44",
                                "46",
                                "48",
                                "50",
                                "52",
                                "54",
                                "56",
                                "58",
                                "60"
                            ]
                        },
                        "why_run_this_eval": [
                            "Running the LAMBADA evaluation lets you probe key generative competencies: *contextual understanding*, *long‑term memory*, *coherence, and narrative fluency*.",
                            "It is particularly useful when you want to."
                        ],
                        "what_to_expect": [
                            "When a language model is evaluated on LAMBADA, its performance is reported as a *accuracy* metric: the proportion of samples where the model’s top‑1 prediction matches the human‑written last word.",
                            "High accuracy indicates that the model can derive meaning from large stretches of text, infer narrative intent, and produce context‑appropriate completions.",
                            "A low score typically signals that the model struggles with long‑range dependencies or fails to capture subtle plot or character cues, which may translate into generative errors in larger‑scale storytelling tasks."
                        ],
                        "evaluation_description": "LAMBADA is a large story‑completion evaluation set with 5,153 samples. Each row contains a short narrative context (a few sentences) and the last word that must be predicted. The stories come from varied creative‑writing and literary domains such as Fantasy Literature, English Literature, and general Reading Comprehension, making the data ideal for testing a model’s ability to weave together plot, character, and setting. The dataset is labeled as Intermediate difficulty – it challenges models to keep track of long contextual cues rather than simple surface patterns.",
                        "top_5_task_types": [
                            "creative_writing",
                            "story_continuation",
                            "reading_comprehension",
                            "qa",
                            "Creative Writing"
                        ],
                        "top_5_domains": [
                            "Creative Writing",
                            "Fantasy Literature",
                            "Literature",
                            "English Literature",
                            "Reading Comprehension"
                        ],
                        "top_5_skills": [
                            "reading_comprehension",
                            "critical_thinking",
                            "inference",
                            "creative_writing",
                            "imagination"
                        ],
                        "top_5_concepts": [
                            "dialogue",
                            "narrative structure",
                            "character_development",
                            "character motivation",
                            "dialogue continuity"
                        ],
                        "top_5_qualifications": [
                            "high_school",
                            "High School Diploma",
                            "high_school_graduate",
                            "undergraduate",
                            "College-Level Creative Writing"
                        ],
                        "top_5_languages": [
                            "English"
                        ]
                    },
                    "analysis_file": "analysis/opencompass_523_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 200,
                        "successful": 197,
                        "failed": 3
                    }
                },
                {
                    "id": "opencompass_1085",
                    "name": "InfoBench",
                    "version": "1.0.0",
                    "description": "InfoBench is a benchmark comprising 500 diverse instructions and 2,250 decomposed questions across multiple constraint categories.\n",
                    "url": "opencompass/opencompass_1085.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1085",
                    "sample_count": 1000,
                    "traits": [
                        "Instruct"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1085",
                        "name": "InfoBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "指令跟随",
                                "en": "Instruct"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/qinyiwei/InfoBench",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.772.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50186103",
                            "name": "Tencent-AI-Lab",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186103-0a763557-5cd0-4d44-b53a-aa07caf31eb7.png",
                            "nickname": "Tencent-AI-Lab"
                        },
                        "lookNum": "804",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-29 16:24:48",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-29 16:24:48",
                        "createDate": "2024-09-29 13:22:54",
                        "desc": {
                            "cn": "InfoBench 是一个指令追随评测基准，包含 500 条多样化的指令和 2,250 个分解问题，涵盖多个约束类别。",
                            "en": "InfoBench is a benchmark comprising 500 diverse instructions and 2,250 decomposed questions across multiple constraint categories.\n"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/infobench'. Error: Path opencompass/infobench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_530",
                    "name": "ReCoRD",
                    "version": "1.0.0",
                    "description": "ReCoRD is a reading comprehension task, which requires to extract the answer from the article based on the given news article and question.",
                    "url": "opencompass/opencompass_530.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_530",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {
                        "gen": "ReCoRD_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "530",
                        "name": "ReCoRD",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://sheng-z.github.io/ReCoRD-explorer/",
                        "paperLink": "https://arxiv.org/abs/1905.00537",
                        "officialWebsiteLink": "https://sheng-z.github.io/ReCoRD-explorer/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "773",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 11:35:25",
                        "createDate": "2024-01-11 14:11:57",
                        "desc": {
                            "cn": "ReCoRD是一个阅读理解任务，要求根据给定的新闻文章和问题，从文章中抽取出答案。\n",
                            "en": "ReCoRD is a reading comprehension task, which requires to extract the answer from the article based on the given news article and question."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/record'. Error: Path opencompass/record is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1136",
                    "name": "IFEval",
                    "version": "1.0.0",
                    "description": "IFEval is a straightforward and easy-to reproduce evaluation benchmark. It focuses on a set of “verifiable instructions” such as “write in more than 400 words” and “mention the keyword of AI at least 3 times”.",
                    "url": "opencompass/opencompass_1136.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1136",
                    "sample_count": 1000,
                    "traits": [
                        "Instruct"
                    ],
                    "eval_type": {
                        "gen": "IFEval_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1136",
                        "name": "IFEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "指令跟随",
                                "en": "Instruct"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/google-research/google-research/tree/master/instruction_following_eval",
                        "paperLink": "https://arxiv.org/pdf/2311.07911",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "767",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-14 20:15:14",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-14 20:15:14",
                        "createDate": "2024-10-12 16:05:28",
                        "desc": {
                            "cn": "IFEval 是一个简单且易于复现的评估基准。它关注一组“可验证的指令”，例如“写超过 400 个单词”和“至少提到关键词 AI 3 次”。",
                            "en": "IFEval is a straightforward and easy-to reproduce evaluation benchmark. It focuses on a set of “verifiable instructions” such as “write in more than 400 words” and “mention the keyword of AI at least 3 times”."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ifeval'. Error: Path opencompass/ifeval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1155",
                    "name": "Ada-LEval",
                    "version": "1.0.0",
                    "description": "Ada-LEval is a length-adaptable benchmark for evaluating the long-context understanding\nof LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable\na more reliable evaluation of LLMs’ long context capabilities.",
                    "url": "opencompass/opencompass_1155.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1155",
                    "sample_count": 1000,
                    "traits": [
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1155",
                        "name": "Ada-LEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "长文本",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NAACL 2024",
                                "en": "NAACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 3,
                        "githubLink": "https://github.com/open-compass/Ada-LEval",
                        "paperLink": "https://aclanthology.org/2024.naacl-long.205.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "756",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-27 17:23:13",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-27 17:23:13",
                        "createDate": "2024-10-15 16:58:09",
                        "desc": {
                            "cn": "Ada-LEval 用于评估大型语言模型（LLMs）对长上下文的理解能力。Ada-LEval 包含两个具有挑战性的子集，TSort 和 BestAnswer，能够更可靠地评估 LLMs 的长上下文能力。",
                            "en": "Ada-LEval is a length-adaptable benchmark for evaluating the long-context understanding\nof LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable\na more reliable evaluation of LLMs’ long context capabilities."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ada_leval'. Error: Path opencompass/ada_leval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1096",
                    "name": "TruthfulQA",
                    "version": "1.0.0",
                    "description": "TruthfulQA is  a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics.",
                    "url": "opencompass/opencompass_1096.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1096",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1096",
                        "name": "TruthfulQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/sylinrl/TruthfulQA",
                        "paperLink": "https://arxiv.org/pdf/2109.07958",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "734",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-30 15:32:49",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-30 15:32:49",
                        "createDate": "2024-09-29 18:03:55",
                        "desc": {
                            "cn": "TruthfulQA 用于测量语言模型在回答问题时的真实度。该基准包含 817 个问题，涵盖 38 个类别，包括健康、法律、金融和政治。",
                            "en": "TruthfulQA is  a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/truthfulqa'. Error: Path opencompass/truthfulqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_557",
                    "name": "OCRBench",
                    "version": "1.0.0",
                    "description": "OCRBench provides a comprehensive evaluation of Large Multimodal Models, such as GPT4V and Gemini, in various text-related visual tasks including Text Recognition, Scene Text-Centric Visual Question Answering (VQA), Document-Oriented VQA, Key Information Extraction (KIE), and Handwritten Mathematical Expression Recognition (HMER). ",
                    "url": "opencompass/opencompass_557.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_557",
                    "sample_count": 1000,
                    "traits": [
                        "Language",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "557",
                        "name": "OCRBench",
                        "emoji": "👹",
                        "dimensions": [
                            {
                                "cn": "语言",
                                "en": "Language"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "vision-language",
                                "en": "vision-language"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Yuliang-Liu/MultimodalOCR",
                        "paperLink": "https://arxiv.org/abs/2305.07895",
                        "officialWebsiteLink": "https://huggingface.co/spaces/echo840/ocrbench-leaderboard",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "40041912",
                            "name": "echo840",
                            "avatar": null,
                            "nickname": "echo840"
                        },
                        "lookNum": "733",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-01-30 10:43:27",
                        "createDate": "2024-01-30 10:43:27",
                        "desc": {
                            "cn": "OCRBench对 GPT4V 和 Gemini 等大型多模态模型在各种文本相关的视觉任务中的表现进行了全面的评估，包括文本识别、场景文本为中心的视觉问答 (VQA)、面向文档的 VQA、关键信息提取 (KIE) 和手写数学表达式识别 (HMER)。",
                            "en": "OCRBench provides a comprehensive evaluation of Large Multimodal Models, such as GPT4V and Gemini, in various text-related visual tasks including Text Recognition, Scene Text-Centric Visual Question Answering (VQA), Document-Oriented VQA, Key Information Extraction (KIE), and Handwritten Mathematical Expression Recognition (HMER). "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ocrbench'. Error: Path opencompass/ocrbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_544",
                    "name": "DS-1000",
                    "version": "1.0.0",
                    "description": "DS-1000 is a code generation benchmark with a thousand data science questions spanning seven Python libraries that (1) reflects diverse, realistic, and practical use cases, (2) has a reliable metric, (3) defends against memorization by perturbing questions.",
                    "url": "opencompass/opencompass_544.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_544",
                    "sample_count": 1000,
                    "traits": [
                        "code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "544",
                        "name": "DS-1000",
                        "emoji": "🗽",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/xlang-ai/DS-1000",
                        "paperLink": "https://arxiv.org/pdf/2211.11501.pdf",
                        "officialWebsiteLink": "https://ds1000-code-gen.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "726",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-01-26 15:43:03",
                        "createDate": "2024-01-26 15:43:03",
                        "desc": {
                            "cn": "DS-1000 是一个代码生成基准测试，包含一千个数据科学问题，涵盖七个Python库，其特点是（1）反映多样化、现实且实用的用例，（2）具有可靠的度量标准，（3）通过扰乱问题来防止记忆化。",
                            "en": "DS-1000 is a code generation benchmark with a thousand data science questions spanning seven Python libraries that (1) reflects diverse, realistic, and practical use cases, (2) has a reliable metric, (3) defends against memorization by perturbing questions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ds_1000'. Error: Path opencompass/ds_1000 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_522",
                    "name": "EPRSTMT",
                    "version": "1.0.0",
                    "description": "(E-commerce Product Review Dataset for Sentiment Analysis), also known as EPRSTMT, is a binary sentiment analysis dataset based on product reviews on e-commerce platform. Each sample is labelled as Positive or Negative. It collect by ICIP Lab of Beijing Normal University.",
                    "url": "opencompass/opencompass_522.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_522",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "522",
                        "name": "EPRSTMT",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/CLUEbenchmark/FewCLUE",
                        "paperLink": "https://arxiv.org/abs/2107.07498",
                        "officialWebsiteLink": "https://github.com/CLUEbenchmark/FewCLUE",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "705",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:52:03",
                        "createDate": "2024-01-11 14:11:26",
                        "desc": {
                            "cn": "EPRSTMT，也称作电子商务产品评论情感分析数据集，是一个基于电子商务平台上的产品评论的二元情感分析数据集。每个样本都被标记为积极或消极。该数据集由北京师范大学 ICIP 实验室收集。",
                            "en": "(E-commerce Product Review Dataset for Sentiment Analysis), also known as EPRSTMT, is a binary sentiment analysis dataset based on product reviews on e-commerce platform. Each sample is labelled as Positive or Negative. It collect by ICIP Lab of Beijing Normal University."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/eprstmt'. Error: Path opencompass/eprstmt is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_527",
                    "name": "AX-g",
                    "version": "1.0.0",
                    "description": "AX-g is a Winogender diagnostic task, which requires to determine which noun the pronoun refers to according to the given sentence and pronoun. This task is selected from a subset of the Winogender dataset, mainly used to test the model's ability in dealing with gender bias and discrimination.",
                    "url": "opencompass/opencompass_527.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_527",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "527",
                        "name": "AX-g",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/rudinger/winogender-schemas",
                        "paperLink": "https://arxiv.org/abs/1905.00537",
                        "officialWebsiteLink": "https://github.com/rudinger/winogender-schemas",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "684",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 11:37:55",
                        "createDate": "2024-01-11 14:11:44",
                        "desc": {
                            "cn": "AX-g是一个Winogender诊断任务，要求根据给定的句子和代词，判断代词指代的是哪个名词。这个任务是从Winogender数据集中选取了一部分数据，主要用来测试模型在处理性别偏见和性别歧视方面的能力。",
                            "en": "AX-g is a Winogender diagnostic task, which requires to determine which noun the pronoun refers to according to the given sentence and pronoun. This task is selected from a subset of the Winogender dataset, mainly used to test the model's ability in dealing with gender bias and discrimination."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ax_g'. Error: Path opencompass/ax_g is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_528",
                    "name": "RTE",
                    "version": "1.0.0",
                    "description": "RTE is a natural language inference task, which requires to determine the logical relation between the given sentence pair, with three relations: entailment, contradiction and neutral.",
                    "url": "opencompass/opencompass_528.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_528",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {
                        "gen": "RTE_gen",
                        "ppl": "RTE_ppl"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "528",
                        "name": "RTE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://tac.nist.gov//2011/RTE/index.html",
                        "paperLink": "https://arxiv.org/abs/1905.00537",
                        "officialWebsiteLink": "https://tac.nist.gov//2011/RTE/index.html",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "668",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 09:52:48",
                        "createDate": "2024-01-11 14:11:48",
                        "desc": {
                            "cn": "RTE是一个自然语言推理任务，要求根据给定的句子对，判断它们之间的逻辑关系，有三种关系：蕴含、矛盾和中立。",
                            "en": "RTE is a natural language inference task, which requires to determine the logical relation between the given sentence pair, with three relations: entailment, contradiction and neutral."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/rte'. Error: Path opencompass/rte is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1074",
                    "name": "NewsBench",
                    "version": "1.0.0",
                    "description": "NewsBench is a novel evaluation framework to systematically assess the capabilities of Large Language Models (LLMs) for editorial capabilities in Chinese journalism. ",
                    "url": "opencompass/opencompass_1074.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1074",
                    "sample_count": 1000,
                    "traits": [
                        "Creation"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1074",
                        "name": "NewsBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "创作",
                                "en": "Creation"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/IAAR-Shanghai/NewsBench",
                        "paperLink": "https://arxiv.org/pdf/2403.00862",
                        "officialWebsiteLink": "https://iaar-shanghai.github.io/NewsBench/#/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50187574",
                            "name": "IAAR-Shanghai",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50187574-4cf5b8da-55c3-4c75-aa9b-9c2d053b5b6d.png",
                            "nickname": "IAAR-Shanghai"
                        },
                        "lookNum": "645",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-27 17:50:27",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-27 17:50:27",
                        "createDate": "2024-09-27 13:08:42",
                        "desc": {
                            "cn": "NewsBench 是一个新颖的评估框架，旨在系统性地评估大型语言模型在中文新闻编辑能力上的表现。构建的基准数据集聚焦于写作能力的四个方面和安全遵循的六个方面，包含 1,267 个手动精心设计的测试样本，类型包括选择题和简答题，涵盖 24 个新闻领域的五项编辑任务。",
                            "en": "NewsBench is a novel evaluation framework to systematically assess the capabilities of Large Language Models (LLMs) for editorial capabilities in Chinese journalism. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/newsbench'. Error: Path opencompass/newsbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1121",
                    "name": "HaluEval",
                    "version": "1.0.0",
                    "description": "HaluEval evaluates the performance of LLMs in recognizing hallucination. It includes 5,000 general user queries with ChatGPT responses and 30,000 task-specific examples from three tasks, i.e., question answering, knowledge-grounded dialogue, and text summarization.",
                    "url": "opencompass/opencompass_1121.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1121",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1121",
                        "name": "HaluEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/RUCAIBox/HaluEval",
                        "paperLink": "https://arxiv.org/pdf/2305.11747",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "642",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-13 15:31:34",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-13 15:31:34",
                        "createDate": "2025-02-13 18:15:01",
                        "desc": {
                            "cn": "HaluEval用于评估大语言模型识别幻觉的能力，包含 5,000 条普通用户查询及 ChatGPT 的回答，以及来自三个任务的 30,000 个特定任务示例，即问答、基于知识的对话和文本摘要。",
                            "en": "HaluEval evaluates the performance of LLMs in recognizing hallucination. It includes 5,000 general user queries with ChatGPT responses and 30,000 task-specific examples from three tasks, i.e., question answering, knowledge-grounded dialogue, and text summarization."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/halueval'. Error: Path opencompass/halueval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_526",
                    "name": "AX-b",
                    "version": "1.0.0",
                    "description": "AX-b is a broad-coverage diagnostic task, which requires to determine the logical relation between the given sentence pair, with three relations: entailment, contradiction and neutral. This task is selected from a subset of the GLUE broad-coverage diagnostic dataset, mainly used to test the model's understanding ability in grammar, semantics, world knowledge and so on.",
                    "url": "opencompass/opencompass_526.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_526",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "526",
                        "name": "AX-b",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://gluebenchmark.com/diagnostics",
                        "paperLink": "https://arxiv.org/abs/1905.00537",
                        "officialWebsiteLink": "https://gluebenchmark.com/diagnostics",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "622",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-02 11:38:03",
                        "createDate": "2024-01-11 14:11:41",
                        "desc": {
                            "cn": "AX-b是一个广覆盖诊断任务，要求根据给定的句子对，判断它们之间的逻辑关系，有三种关系：蕴含、矛盾和中立。这个任务是从GLUE的广覆盖诊断数据集中选取了一部分数据，主要用来测试模型在语法、语义、世界知识等方面的理解能力。",
                            "en": "AX-b is a broad-coverage diagnostic task, which requires to determine the logical relation between the given sentence pair, with three relations: entailment, contradiction and neutral. This task is selected from a subset of the GLUE broad-coverage diagnostic dataset, mainly used to test the model's understanding ability in grammar, semantics, world knowledge and so on."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ax_b'. Error: Path opencompass/ax_b is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1175",
                    "name": "MMStar",
                    "version": "1.0.0",
                    "description": "MMStar is an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMs’ multi-modal capacities with carefully balanced and purified samples.",
                    "url": "opencompass/opencompass_1175.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1175",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1175",
                        "name": "MMStar",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MMStar-Benchmark/MMStar",
                        "paperLink": "https://arxiv.org/pdf/2403.20330",
                        "officialWebsiteLink": "https://mmstar-benchmark.github.io/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "617",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 11:27:36",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 11:27:36",
                        "createDate": "2024-10-21 16:21:15",
                        "desc": {
                            "cn": "MMStar 是一个多模态基准，包含 1,500 个经过人工精心挑选的样本。MMStar 评估 6 项核心能力和 18 个详细维度，旨在通过精心平衡和净化的样本，评估 LVLM 的多模态能力。",
                            "en": "MMStar is an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMs’ multi-modal capacities with carefully balanced and purified samples."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmstar'. Error: Path opencompass/mmstar is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1089",
                    "name": "MathBench",
                    "version": "1.0.0",
                    "description": "MathBench, a new benchmark that rigorously assesses the mathematical capabilities of large\nlanguage models. MathBench spans a wide range of mathematical disciplines, offering a\ndetailed evaluation of both theoretical understanding and practical problem-solving skills.",
                    "url": "opencompass/opencompass_1089.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1089",
                    "sample_count": 1000,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1089",
                        "name": "MathBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 3,
                        "githubLink": "https://github.com/open-compass/MathBench",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.411.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "605",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-30 15:32:54",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-30 15:32:54",
                        "createDate": "2024-09-29 15:21:55",
                        "desc": {
                            "cn": "MathBench 严格评估大型语言模型的数学能力。MathBench 涉及广泛的数学学科，提供对理论理解和实际问题解决技能的详细评估。",
                            "en": "MathBench, a new benchmark that rigorously assesses the mathematical capabilities of large\nlanguage models. MathBench spans a wide range of mathematical disciplines, offering a\ndetailed evaluation of both theoretical understanding and practical problem-solving skills."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mathbench'. Error: Path opencompass/mathbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1961",
                    "name": "SFE",
                    "version": "1.0.0",
                    "description": "The Scientists' First Exam (SFE) benchmark, designed to comprehensively evaluate the scientific cognitive capabilities of MLLMs through three cognitive levels (cog-levels):Scientific Signal Perception、Scientific Attribute Understanding 、Scientific Comparative Reasoning.",
                    "url": "opencompass/opencompass_1961.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1961",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1961",
                        "name": "SFE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            },
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://huggingface.co/datasets/PrismaX/SFE",
                        "paperLink": "https://arxiv.org/abs/2506.10521",
                        "officialWebsiteLink": "https://prismax.opencompass.org.cn/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "585",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-24 15:55:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-24 15:55:33",
                        "createDate": "2025-06-24 15:55:24",
                        "desc": {
                            "cn": "The Scientists' First Exam (SFE) 基准测试旨在通过三个认知层级——科学信号感知、科学属性理解 和 科学对比推理，全面评估多模态大语言模型（MLLMs）的科学认知能力。",
                            "en": "The Scientists' First Exam (SFE) benchmark, designed to comprehensively evaluate the scientific cognitive capabilities of MLLMs through three cognitive levels (cog-levels):Scientific Signal Perception、Scientific Attribute Understanding 、Scientific Comparative Reasoning."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/sfe'. Error: Path opencompass/sfe is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1075",
                    "name": "AlignBench",
                    "version": "1.0.0",
                    "description": "ALIGNBENCH is a comprehensive multidimensional benchmark for evaluating LLMs’ alignment in Chinese. We tailor a humanin-the-loop data curation pipeline, containing 8 main categories, 683 real-scenario rooted queries and corresponding human verified references.",
                    "url": "opencompass/opencompass_1075.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1075",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1075",
                        "name": "AlignBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/THUDM/AlignBench",
                        "paperLink": "https://aclanthology.org/2024.acl-long.624.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "043910",
                            "name": "THUDM",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/043910-ad5eb16e-0c3f-4e1b-b1d6-e9dc8ab355a7.png",
                            "nickname": "智谱.AI"
                        },
                        "lookNum": "584",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-11-01 13:39:39",
                        "supportOnlineEval": false,
                        "updateDate": "2024-11-01 13:39:39",
                        "createDate": "2024-10-29 17:11:05",
                        "desc": {
                            "cn": "AlignBench 是一个用于评估中文大语言模型对齐性能的全面、多维度的评测基准。AlignBench 构建了人类参与的数据构建流程，来保证评测数据的动态更新。AlignBench 采用多维度、规则校准的模型评价方法（LLM-as-Judge），并且结合思维链（Chain-of-Thought）生成对模型回复的多维度分析和最终的综合评分，增强了评测的高可靠性和可解释性。",
                            "en": "ALIGNBENCH is a comprehensive multidimensional benchmark for evaluating LLMs’ alignment in Chinese. We tailor a humanin-the-loop data curation pipeline, containing 8 main categories, 683 real-scenario rooted queries and corresponding human verified references."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/alignbench'. Error: Path opencompass/alignbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_930",
                    "name": "MR-Ben-Meta-Reasoning-Benchmark",
                    "version": "1.0.0",
                    "description": "Welcome to the dataset page for the Meta-Reasoning Benchmark associated with our recent publication `Mr-Ben: A Comprehensive Meta-Reasoning Benchmark for Large Language Models`. We have provided a demo evaluate script for you to try out benchmark in mere two steps. We encourage everyone to try out our benchmark in the SOTA models and return its results to us. We would be happy to include it in the eval_results and update the evaluation tables below for you.",
                    "url": "opencompass/opencompass_930.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_930",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "930",
                        "name": "MR-Ben-Meta-Reasoning-Benchmark",
                        "emoji": "🗽",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "long-context",
                                "en": "long-context"
                            },
                            {
                                "cn": "understanding",
                                "en": "understanding"
                            },
                            {
                                "cn": "knowledge",
                                "en": "knowledge"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/dvlab-research/Mr-Ben",
                        "paperLink": "https://arxiv.org/abs/2406.13975",
                        "officialWebsiteLink": "https://randolph-zeng.github.io/Mr-Ben.github.io/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50104209",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-lFJ83HfdP"
                        },
                        "lookNum": "578",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-07-12 13:38:55",
                        "createDate": "2024-07-12 11:20:05",
                        "desc": {
                            "cn": "本工作联合MIT,清华,剑桥等知名院校, 提出了一个评测大语言模型对复杂问题的推理过程的“阅卷”批改能力的评测数据集，有别于以前的以结果匹配为评测模式的数据集MR-Ben，我们的数据集基于GSM8K[1], MMLU[2], LogiQA[3], MHPP[4]等数据集经由细致的高水平人工标注构建而成，显著地增加了难度及区分度。我们细致地分析了包括claude3.5, GPT4-Turbo, Kimi, Zhipu, Yi-Large, Qwen2, DeepseekCoderv2 等国内外一线的大语言模型，发现开源的模型在复杂推理的场景下有望追上顶尖的闭源模型。该评测数据集的所有数据均已开源，并且支持一键评测。欢迎所有做大模型训练的小伙伴向我们分享你的评测结果，我们会及时更新榜单。",
                            "en": "Welcome to the dataset page for the Meta-Reasoning Benchmark associated with our recent publication `Mr-Ben: A Comprehensive Meta-Reasoning Benchmark for Large Language Models`. We have provided a demo evaluate script for you to try out benchmark in mere two steps. We encourage everyone to try out our benchmark in the SOTA models and return its results to us. We would be happy to include it in the eval_results and update the evaluation tables below for you."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mr_ben_meta_reasoning_benchmark'. Error: Path opencompass/mr_ben_meta_reasoning_benchmark is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1077",
                    "name": "SALAD-Bench",
                    "version": "1.0.0",
                    "description": "SALAD-Bench, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy\nspanning three levels, and versatile functionalities.",
                    "url": "opencompass/opencompass_1077.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1077",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1077",
                        "name": "SALAD-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OpenSafetyLab/SALAD-BENCH",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.235.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50186740",
                            "name": "OpenSafetyLab",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186740-5e27e10c-3894-4bfd-b0c9-2079f251882d.png",
                            "nickname": "OpenSafetyLab"
                        },
                        "lookNum": "563",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-29 16:25:08",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-29 16:25:08",
                        "createDate": "2024-09-27 16:14:48",
                        "desc": {
                            "cn": "SALAD-Bench 是一个专门用于评估大型语言模型（LLMs）、攻击和防御方法的安全基准。SALAD-Bench 的特点在于其广泛性，超越了传统基准，具有大规模、丰富的多样性、复杂的三层分类法以及多功能性。",
                            "en": "SALAD-Bench, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy\nspanning three levels, and versatile functionalities."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/salad_bench'. Error: Path opencompass/salad_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1128",
                    "name": "Gorilla",
                    "version": "1.0.0",
                    "description": "Gorilla enables LLMs to use tools by invoking APIs. Given a natural language query, Gorilla comes up with the semantically- and syntactically- correct API to invoke. ",
                    "url": "opencompass/opencompass_1128.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1128",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1128",
                        "name": "Gorilla",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ShishirPatil/gorilla",
                        "paperLink": "https://arxiv.org/pdf/2305.15334",
                        "officialWebsiteLink": "https://gorilla.cs.berkeley.edu/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "550",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-16 11:03:02",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-16 11:03:02",
                        "createDate": "2024-10-11 15:12:19",
                        "desc": {
                            "cn": "Gorilla 使大语言模型能够通过调用 API 使用工具。针对自然语言查询，Gorilla 能够生成语义和语法上正确的 API 调用。",
                            "en": "Gorilla enables LLMs to use tools by invoking APIs. Given a natural language query, Gorilla comes up with the semantically- and syntactically- correct API to invoke. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gorilla'. Error: Path opencompass/gorilla is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1080",
                    "name": "E-EVAL",
                    "version": "1.0.0",
                    "description": "E-EVAL is the first comprehensive evaluation benchmark specifically tailored for Chinese K-12 education. E-EVAL comprises 4,351 multiple-choice questions spanning primary, middle, and high school levels, covering a diverse array of subjects.\n",
                    "url": "opencompass/opencompass_1080.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1080",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1080",
                        "name": "E-EVAL",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "学科",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AI-EDU-LAB/E-EVAL",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.462.pdf",
                        "officialWebsiteLink": "https://eevalbenchmark.com/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50186339",
                            "name": "AI-EDU-LAB",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186339-e3a36811-6e48-440b-a243-b7d59c6d4bd7.png",
                            "nickname": "AI-EDU-LAB"
                        },
                        "lookNum": "545",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-29 16:25:01",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-29 16:25:01",
                        "createDate": "2024-09-27 18:25:46",
                        "desc": {
                            "cn": "E-EVAL 是首个专门针对中国 K-12 教育的综合评估基准。E-EVAL 包含 4,351 道选择题，涵盖小学、初中和高中各个年级，涉及多种学科。",
                            "en": "E-EVAL is the first comprehensive evaluation benchmark specifically tailored for Chinese K-12 education. E-EVAL comprises 4,351 multiple-choice questions spanning primary, middle, and high school levels, covering a diverse array of subjects.\n"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/e_eval'. Error: Path opencompass/e_eval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1141",
                    "name": "CHARM",
                    "version": "1.0.0",
                    "description": "CHARM is the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense.",
                    "url": "opencompass/opencompass_1141.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1141",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1141",
                        "name": "CHARM",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/opendatalab/CHARM",
                        "paperLink": "https://aclanthology.org/2024.acl-long.604.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "528",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-19 18:04:11",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-19 18:04:11",
                        "createDate": "2024-10-14 14:20:06",
                        "desc": {
                            "cn": "CHARM 是首个全面深入评估大语言模型（LLMs）在中文中的常识推理能力的基准，涵盖了全球通用的常识和特定于中国的常识。",
                            "en": "CHARM is the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/charm'. Error: Path opencompass/charm is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1108",
                    "name": "HotpotQA",
                    "version": "1.0.0",
                    "description": "HOTPOTQA is a dataset with 113k Wikipedia-based question-answer pairs.",
                    "url": "opencompass/opencompass_1108.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1108",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1108",
                        "name": "HotpotQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/hotpotqa/hotpot",
                        "paperLink": "https://arxiv.org/pdf/1809.09600",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "504",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-10 18:36:24",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-10 18:36:24",
                        "createDate": "2025-01-10 18:28:24",
                        "desc": {
                            "cn": "HotpotQA 用于评估大语言模型的推理能力，包含 113,000 个基于维基百科的问题和答案。",
                            "en": "HOTPOTQA is a dataset with 113k Wikipedia-based question-answer pairs."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/hotpotqa'. Error: Path opencompass/hotpotqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1070",
                    "name": "OlympiadBench",
                    "version": "1.0.0",
                    "description": "OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring\n8,476 problems from Olympiad-level mathematics and physics competitions, including the\nChinese college entrance exam. Each problem is detailed with expert-level annotations\nfor step-by-step reasoning. ",
                    "url": "opencompass/opencompass_1070.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1070",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1070",
                        "name": "OlympiadBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "学科",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OpenBMB/OlympiadBench",
                        "paperLink": "https://arxiv.org/pdf/2402.14008",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "044167",
                            "name": "OpenBMB",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/044167-87da8649-04d0-47ea-90e1-a77a1d2e9585.png",
                            "nickname": "OpenBMB"
                        },
                        "lookNum": "489",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-27 16:53:44",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-27 16:53:44",
                        "createDate": "2024-09-26 17:51:57",
                        "desc": {
                            "cn": "OlympiadBench 是一个奥林匹克级别的双语多模态科学基准，包含来自奥林匹克级数学和物理竞赛的8,476道题目，包括中国高考。每道题目都配有专家级别的注释，提供逐步推理的详细说明。",
                            "en": "OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring\n8,476 problems from Olympiad-level mathematics and physics competitions, including the\nChinese college entrance exam. Each problem is detailed with expert-level annotations\nfor step-by-step reasoning. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/olympiadbench'. Error: Path opencompass/olympiadbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1137",
                    "name": "CMB",
                    "version": "1.0.0",
                    "description": "CMB is Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework.",
                    "url": "opencompass/opencompass_1137.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1137",
                    "sample_count": 11200,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {
                        "gen": "CMB_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 36,
                        "estimated_output_tokens": 50
                    },
                    "original_data": {
                        "id": "1137",
                        "name": "CMB",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NAACL 2024",
                                "en": "NAACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/FreedomIntelligence/CMB",
                        "paperLink": "https://arxiv.org/pdf/2308.08833",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "478",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-16 11:02:56",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-16 11:02:56",
                        "createDate": "2024-10-15 16:31:07",
                        "desc": {
                            "cn": "CMB 是一个综合医学基准，专为中文而设计，并完全依赖于本土的中文语言和文化框架中。",
                            "en": "CMB is Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "30岁男性，反复双眼睑浮肿伴夜尿增多2年，血压160/100mmHg，尿蛋白（+)，红细胞5～10个/HP，颗粒管型1～2/HP，血肌酐145μmol/L，血红蛋白85g/L，血清白蛋白32g/L。下列哪种药物本例不宜应用",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "Question answering",
                                        "Medical decision making"
                                    ],
                                    "domains": [
                                        "Medicine",
                                        "Internal Medicine",
                                        "Nephrology"
                                    ],
                                    "skills": [
                                        "Clinical reasoning",
                                        "Clinical pharmacology",
                                        "Differential diagnosis",
                                        "Interpretation of laboratory data"
                                    ],
                                    "concepts": [
                                        "Hypertension",
                                        "Proteinuria",
                                        "Hematuria",
                                        "Granular casts",
                                        "Renal impairment",
                                        "Hypoalbuminemia",
                                        "Anemia of chronic disease",
                                        "Pharmacotherapy in renal disease"
                                    ],
                                    "qualification": [
                                        "Undergraduate medical degree (MD/MBBS)",
                                        "Residency in internal medicine or nephrology",
                                        "Board certification in internal medicine or nephrology (optional but beneficial)"
                                    ],
                                    "language": [
                                        "Chinese"
                                    ],
                                    "age_range": {
                                        "min": 25,
                                        "avg": 32,
                                        "max": 65
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "尊重病人自主性意味着在病人坚持己见对自己的健康有害时,可能要求医生",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "question_answering",
                                        "ethics_discussion"
                                    ],
                                    "domains": [
                                        "Medicine",
                                        "Medical Ethics",
                                        "Philosophy"
                                    ],
                                    "skills": [
                                        "critical thinking",
                                        "ethical reasoning",
                                        "medical knowledge",
                                        "communication",
                                        "analysis"
                                    ],
                                    "concepts": [
                                        "Patient autonomy",
                                        "Beneficence",
                                        "Paternalism",
                                        "Informed consent",
                                        "Doctor‑patient relationship",
                                        "Ethical decision‑making"
                                    ],
                                    "qualification": [
                                        "Undergraduate degree in health sciences",
                                        "Medical school",
                                        "MD",
                                        "PhD in bioethics",
                                        "Healthcare practitioner"
                                    ],
                                    "language": [
                                        "Chinese",
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 18,
                                        "avg": 30,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "初产妇，孕41+5周，规律宫缩14小时，宫口开全1小时，先露下降停滞，阴道检查：宫口开10cm，S+3，胎头有一小产瘤，胎头矢状缝在左斜径上，小囟门在5点处，请问此时胎方位是",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "question_answering",
                                        "medical_domain"
                                    ],
                                    "domains": [
                                        "Obstetrics",
                                        "Midwifery",
                                        "Human Anatomy",
                                        "Prenatal Care"
                                    ],
                                    "skills": [
                                        "medical knowledge",
                                        "obstetrics competency",
                                        "anatomical orientation",
                                        "clinical reasoning",
                                        "past medical history interpretation"
                                    ],
                                    "concepts": [
                                        "胎方位",
                                        "宫口扩张",
                                        "宫颈宫口扩张",
                                        "胎头矢状缝定位",
                                        "小囟门定位",
                                        "胎位辨识"
                                    ],
                                    "qualification": [
                                        "Bachelor of Medicine (MBBS)",
                                        "Nursing degree with Midwifery specialization",
                                        "Certified Midwife",
                                        "Obstetrics & Gynecology Residency"
                                    ],
                                    "language": [
                                        "Chinese"
                                    ],
                                    "age_range": {
                                        "min": 18,
                                        "avg": 32,
                                        "max": 65
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "立即的外文缩写是",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "qa"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "knowledge_lookup",
                                        "language_understanding",
                                        "reading_comprehension"
                                    ],
                                    "concepts": [
                                        "abbreviation",
                                        "translation"
                                    ],
                                    "qualification": [
                                        "none"
                                    ],
                                    "language": [
                                        "Chinese",
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 10,
                                        "avg": 20,
                                        "max": 70
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "在确定的条件下，一定时间内分析物在给定介质中的化学稳定性为",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "question_answering",
                                        "scientific_reasoning"
                                    ],
                                    "domains": [
                                        "Chemistry",
                                        "Analytical Chemistry",
                                        "Physical Chemistry"
                                    ],
                                    "skills": [
                                        "analytical_thinking",
                                        "critical_thinking",
                                        "data_interpretation",
                                        "problem_solving",
                                        "domain_knowledge"
                                    ],
                                    "concepts": [
                                        "Chemical Stability",
                                        "Reaction Kinetics",
                                        "Solvent Effects",
                                        "pH",
                                        "Temperature Dependence",
                                        "Time-dependent Degradation",
                                        "Stability Protocols",
                                        "Analytical Methods"
                                    ],
                                    "qualification": [
                                        "Bachelor's in Chemistry",
                                        "Master's in Chemistry",
                                        "PhD in Chemistry",
                                        "Chemist",
                                        "Research Scientist"
                                    ],
                                    "language": [
                                        "Chinese",
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 18,
                                        "avg": 25,
                                        "max": 65
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "细菌形成的芽胞是其",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "qa",
                                        "knowledge_extraction"
                                    ],
                                    "domains": [
                                        "Biology",
                                        "Microbiology",
                                        "Cell Biology"
                                    ],
                                    "skills": [
                                        "cognitive comprehension",
                                        "basic microbiology knowledge",
                                        "critical thinking"
                                    ],
                                    "concepts": [
                                        "spore formation",
                                        "bacterial endospores",
                                        "dormancy",
                                        "protective mechanisms",
                                        "environmental stress"
                                    ],
                                    "qualification": [
                                        "High School Biology",
                                        "Undergraduate Biology",
                                        "Microbiology",
                                        "Bacteriology"
                                    ],
                                    "language": [
                                        "Chinese"
                                    ],
                                    "age_range": {
                                        "min": 14,
                                        "avg": 17,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "正常小儿前囟闭合的时间最迟为",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "question_answering"
                                    ],
                                    "domains": [
                                        "Pediatrics",
                                        "Human Development"
                                    ],
                                    "skills": [
                                        "medical knowledge recall",
                                        "reading comprehension",
                                        "specialized vocabulary"
                                    ],
                                    "concepts": [
                                        "fontanelle closure",
                                        "neonatal development",
                                        "anterior fontanelle",
                                        "growth milestones"
                                    ],
                                    "qualification": [
                                        "Bachelor's in Biology",
                                        "Medical Degree (MD)",
                                        "Pediatric Residency"
                                    ],
                                    "language": [
                                        "Chinese"
                                    ],
                                    "age_range": {
                                        "min": 25,
                                        "avg": 30,
                                        "max": 80
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "女性患者，45岁，因体检发现血压升高就诊，临床诊断为高血压病2级，中危组，遵医嘱规律服用降压药物两周后自觉心慌，并出现脚踝部水肿，考虑为药物所致不良反应，则最可能出现此反应的药物为",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "qa"
                                    ],
                                    "domains": [
                                        "Medicine",
                                        "Pharmacology"
                                    ],
                                    "skills": [
                                        "medical knowledge",
                                        "pharmacology understanding",
                                        "critical reasoning",
                                        "clinical decision making"
                                    ],
                                    "concepts": [
                                        "Hypertension",
                                        "Antihypertensive drugs",
                                        "Drug side effects",
                                        "Peripheral edema",
                                        "Palpitations"
                                    ],
                                    "qualification": [
                                        "Medical student",
                                        "Physician (MD)",
                                        "Pharmacist (PharmD)",
                                        "Clinical pharmacology specialist"
                                    ],
                                    "language": [
                                        "Mandarin Chinese"
                                    ],
                                    "age_range": {
                                        "min": 18,
                                        "avg": 24,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "吞咽障碍患者应以较常人缓慢的速度进行摄食，一般每餐适宜的进食时间应控制在",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "clinical_knowledge_retrieval",
                                        "healthcare_advice"
                                    ],
                                    "domains": [
                                        "Medicine",
                                        "Geriatrics",
                                        "Speech-language Pathology",
                                        "Dietetics"
                                    ],
                                    "skills": [
                                        "clinical reasoning",
                                        "domain knowledge",
                                        "reading comprehension",
                                        "communication",
                                        "critical thinking"
                                    ],
                                    "concepts": [
                                        "dysphagia",
                                        "feeding speed",
                                        "meal duration",
                                        "swallowing safety",
                                        "patient education"
                                    ],
                                    "qualification": [
                                        "Medical Doctor (MD)",
                                        "Speech‑Language Pathologist (SLP)",
                                        "Registered Dietitian (RD)",
                                        "Registered Nurse (RN)",
                                        "Gerontologist"
                                    ],
                                    "language": [
                                        "Chinese"
                                    ],
                                    "age_range": {
                                        "min": 22,
                                        "avg": 38,
                                        "max": 70
                                    },
                                    "domain_expertise_required": true
                                },
                                {
                                    "question": "与第二次世界大战之前的资本主义相比，当代资本主义生产关系中的社会阶层、阶级结构发生了许多新的变化，主要表现在",
                                    "difficulty": "moderate",
                                    "task_type": [
                                        "content_generation",
                                        "explanation",
                                        "essay_writing"
                                    ],
                                    "domains": [
                                        "Economics",
                                        "Sociology",
                                        "Political Science",
                                        "History",
                                        "Cultural Studies"
                                    ],
                                    "skills": [
                                        "critical thinking",
                                        "analytical reasoning",
                                        "research skills",
                                        "synthesis of information",
                                        "writing ability",
                                        "knowledge of social theory"
                                    ],
                                    "concepts": [
                                        "class structure",
                                        "capitalism",
                                        "social stratification",
                                        "labor relations",
                                        "globalization",
                                        "technology impact",
                                        "income inequality",
                                        "digital economy"
                                    ],
                                    "qualification": [
                                        "Bachelor's degree in Economics, Sociology, Political Science, or History",
                                        "Master's degree (optional)",
                                        "PhD (optional)",
                                        "Professional economist",
                                        "Sociologist",
                                        "Historian"
                                    ],
                                    "language": [
                                        "Mandarin Chinese"
                                    ],
                                    "age_range": {
                                        "min": 18,
                                        "avg": 26,
                                        "max": 60
                                    },
                                    "domain_expertise_required": true
                                }
                            ],
                            "total_questions": 11200,
                            "question_format": "Clinical Decision-Making, Clinical decision support, Clinical_QA, Ethical Analysis, Evidence‑Based Recommendation, Knowledge Retrieval, Medical Knowledge Retrieval, Medical advice interpretation, Medical_Advice, Medical_MCQ_Answering, Q&A, QA, Qualitative Reasoning, Question_Answering, Question_answering, Summarization, Technical Explanation, Text_Interpretation, analysis, application, assessment, calculations, case_analysis, classification, clinical reasoning, clinical_advice, clinical_decision, clinical_decision_support, clinical_diagnosis, clinical_guidance, clinical_guideline, clinical_reasoning, critical_reasoning, cultural knowledge, definition, diagnosis, diagnostic_reasoning, domain_knowledge, domain_specific_extraction, essay writing, examination_answering, explanation, fill_blank, information_explanation, information_retrieval, instruction, interpretation, knowledge, knowledge retrieval, knowledge_answering, knowledge_based, knowledge_question, knowledge_recall, knowledge_recognition, knowledge_retrieval, medical diagnosis, medical-qa, medical_QA, medical_advice, medical_diagnosis, medical_diagnosis_classification, medical_qa, medical_question, medical_question_answering, medical_recommendation, medical_research, medicine, multiple-choice question, multiple_choice, multiple_choice_question, pattern recognition, pharmacology, physics_problem, policy_evaluation, policy_knowledge, political_essay, prosthetic_planning, qa, question_answer, question_answering, quiz, radiation_shielding, reading_comprehension, recipe_interpretation, riddle, summarisation, summarization, therapeutic_decision, translation, treatment_planning",
                            "difficulty_levels": [
                                "Advanced",
                                "Beginner",
                                "Expert",
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Large size (11,200 questions) gives statistically robust results.",
                                "Multi‑disciplinary coverage ensures comprehensive testing of domain knowledge.",
                                "Real‑world clinical scenarios increase ecological validity.",
                                "Balanced difficulty levels allow staged benchmarking and skill‑progression analysis.",
                                "Chinese language focus makes it unique for evaluating models on non‑English medical data."
                            ],
                            "disadvantages": [
                                "Primarily Chinese; insights may not transfer to English medical tasks without translation.",
                                "No explicit annotation of correct answers listed in the prompt—requires separate key file.",
                                "Skill tags overlap, potentially complicating attribution of errors to specific shortcomings.",
                                "Intermediate difficulty only; lacks advanced‑level problem‑sets needed for specialist evaluation.",
                                "Dataset does not include demographic or outcome data beyond age, limiting analysis of bias or real‑world impact."
                            ]
                        },
                        "age_distribution": {
                            "data": [
                                0,
                                2,
                                9,
                                10,
                                6,
                                7,
                                13,
                                28,
                                10,
                                11,
                                38,
                                4,
                                53,
                                0,
                                0,
                                7,
                                0,
                                2,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0
                            ],
                            "categories": [
                                "10",
                                "12",
                                "14",
                                "16",
                                "18",
                                "20",
                                "22",
                                "24",
                                "26",
                                "28",
                                "30",
                                "32",
                                "34",
                                "36",
                                "38",
                                "40",
                                "42",
                                "44",
                                "46",
                                "48",
                                "50",
                                "52",
                                "54",
                                "56",
                                "58",
                                "60"
                            ]
                        },
                        "why_run_this_eval": [
                            "Running CMB evaluation helps assess the following model traits:\n- Medical domain expertise (clinical, pharmacological, laboratory, ethical)\n- Clinical reasoning and differential diagnosis capabilities\n- Ability to interpret Chinese medical terminology and context\n- Decision‑making under uncertainty\n- Knowledge retrieval and synthesis across multiple disciplines."
                        ],
                        "what_to_expect": [
                            "When an LLM is evaluated against CMB, the resulting score reflects its ability to comprehend Chinese medical text, retrieve domain‑specific facts, perform clinical reasoning, and provide accurate, context‑appropriate responses.",
                            "A high score indicates that the model can navigate complex decision‑making scenarios, interpret lab data, and articulate ethical positions.",
                            "Conversely, a low score signals gaps in clinical knowledge, poor reasoning, or language‑specific misunderstandings."
                        ],
                        "evaluation_description": "The CMB dataset is a curated collection of 11,200 Chinese-language medical‑question pairs. Each row contains a clinical scenario and a multi‑choice or short‑answer prompt. The dataset spans 30 distinct domains (e.g., Internal Medicine, Pharmacology, Obstetrics, Microbiology, Ethics, Sociology, etc.) and incorporates 32 core skill categories (clinical reasoning, pharmacology, lab interpretation, ethics, communication, critical thinking, knowledge retrieval, etc.). Questions are split into two difficulty tiers – Beginner and Intermediate – designed to test both foundational knowledge and intermediate clinical decision‑making. The age distribution of the respondents (ages 16‑38) and diverse skill tags illustrate that the scenarios target a broad audience of medical students, residents, and practicing clinicians.",
                        "top_5_task_types": [
                            "question_answering",
                            "qa",
                            "Question answering",
                            "Medical decision making",
                            "ethics_discussion"
                        ],
                        "top_5_domains": [
                            "Medicine",
                            "Internal Medicine",
                            "Nephrology",
                            "Medical Ethics",
                            "Philosophy"
                        ],
                        "top_5_skills": [
                            "critical thinking",
                            "medical knowledge",
                            "communication",
                            "clinical reasoning",
                            "reading comprehension"
                        ],
                        "top_5_concepts": [
                            "Hypertension",
                            "Proteinuria",
                            "Hematuria",
                            "Granular casts",
                            "Renal impairment"
                        ],
                        "top_5_qualifications": [
                            "Undergraduate medical degree (MD/MBBS)",
                            "Residency in internal medicine or nephrology",
                            "Board certification in internal medicine or nephrology (optional but beneficial)",
                            "Undergraduate degree in health sciences",
                            "Medical school"
                        ],
                        "top_5_languages": [
                            "Chinese",
                            "English",
                            "Mandarin Chinese"
                        ]
                    },
                    "analysis_file": "analysis/opencompass_1137_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 200,
                        "successful": 200,
                        "failed": 0
                    }
                },
                {
                    "id": "opencompass_1178",
                    "name": "MathVista",
                    "version": "1.0.0",
                    "description": "MathVista is a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets.",
                    "url": "opencompass/opencompass_1178.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1178",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Multimodal",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1178",
                        "name": "MathVista",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/lupantech/MathVista",
                        "paperLink": "https://arxiv.org/pdf/2310.02255",
                        "officialWebsiteLink": "https://mathvista.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "470",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-10 18:23:44",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-10 18:23:44",
                        "createDate": "2025-01-10 18:23:16",
                        "desc": {
                            "cn": "MathVista用于评估多模态大模型的数学能力，结合了丰富的数学和视觉任务挑战。它由6141个示例组成，来自28个涉及数学的现有多模态数据集和3个新创建的数据集。",
                            "en": "MathVista is a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mathvista'. Error: Path opencompass/mathvista is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1093",
                    "name": "APPS",
                    "version": "1.0.0",
                    "description": "APPS is a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code.",
                    "url": "opencompass/opencompass_1093.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1093",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {
                        "gen": "APPS_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1093",
                        "name": "APPS",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/hendrycks/apps",
                        "paperLink": "https://arxiv.org/pdf/2105.09938",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "460",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-30 15:32:52",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-30 15:32:52",
                        "createDate": "2024-09-29 17:01:36",
                        "desc": {
                            "cn": "APPS 是一个代码生成评测基准，该评测基准测量模型根据任意自然语言规范生成令人满意的 Python 代码的能力。",
                            "en": "APPS is a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/apps'. Error: Path opencompass/apps is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1083",
                    "name": "GAOKAO-MM",
                    "version": "1.0.0",
                    "description": "GAOKAO-MM is a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. ",
                    "url": "opencompass/opencompass_1083.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1083",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1083",
                        "name": "GAOKAO-MM",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "学科",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OpenMOSS/GAOKAO-MM",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.521.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50175431",
                            "name": "OpenMOSS",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50175431-71e98faf-7e15-46ce-8c84-56eb373cf63a.png",
                            "nickname": "OpenMOSS"
                        },
                        "lookNum": "455",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-29 16:24:53",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-29 16:24:53",
                        "createDate": "2024-09-29 11:48:20",
                        "desc": {
                            "cn": "GAOKAO-MM 是一个基于中国高考的多模态基准，包含 8 个科目和 12 种图像类型，例如图表、函数图、地图和照片。GAOKAO-MM 源自本土中文语境，并对模型的能力设置了人类水平的要求，包括感知、理解、知识和推理。",
                            "en": "GAOKAO-MM is a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gaokao_mm'. Error: Path opencompass/gaokao_mm is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1082",
                    "name": "StudentEval",
                    "version": "1.0.0",
                    "description": "STUDENTEVAL contains 1,749 prompts written by 80 students who have only completed one introductory Python course. STUDENTEVAL contains numerous non-expert prompts describing the same problem, enabling exploration of key factors in prompt success. ",
                    "url": "opencompass/opencompass_1082.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1082",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1082",
                        "name": "StudentEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Wellesley-EASEL-lab/StudentEval",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.501.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50186257",
                            "name": "Wellesley-EASEL-lab",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186257-118d14f5-a2d6-40c7-b3c5-712031ca526e.png",
                            "nickname": "Wellesley-EASEL-lab"
                        },
                        "lookNum": "454",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-29 16:24:55",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-29 16:24:55",
                        "createDate": "2024-09-29 11:16:00",
                        "desc": {
                            "cn": "StudentEval 包含 1,749 个由 80 名仅完成一门入门 Python 课程的学生撰写的提示。StudentEval 中包含许多非专家提示，描述相同的问题，使得探索提示成功的关键因素成为可能。",
                            "en": "STUDENTEVAL contains 1,749 prompts written by 80 students who have only completed one introductory Python course. STUDENTEVAL contains numerous non-expert prompts describing the same problem, enabling exploration of key factors in prompt success. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/studenteval'. Error: Path opencompass/studenteval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1079",
                    "name": "CFLUE",
                    "version": "1.0.0",
                    "description": "CFLUE is the Chinese Financial Language Understanding Evaluation benchmark, designed to assess the capability of LLMs across various dimensions. ",
                    "url": "opencompass/opencompass_1079.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1079",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1079",
                        "name": "CFLUE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/aliyun/cflue",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.337.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50186956",
                            "name": "Alibaba_Cloud",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186956-08003a88-f404-4b02-a282-c0c377547acd.png",
                            "nickname": "Alibaba_Cloud"
                        },
                        "lookNum": "453",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-29 16:25:03",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-29 16:25:03",
                        "createDate": "2024-09-27 17:54:49",
                        "desc": {
                            "cn": "CFLUE 是中国金融语言理解评估基准，旨在评估大型语言模型（LLMs）在各个维度上的能力。具体而言，CFLUE 提供了针对知识评估和应用评估量身定制的数据集。在知识评估方面，它包含超过 38,000 道选择题及相关的解决方案解释。",
                            "en": "CFLUE is the Chinese Financial Language Understanding Evaluation benchmark, designed to assess the capability of LLMs across various dimensions. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cflue'. Error: Path opencompass/cflue is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1135",
                    "name": "GPQA",
                    "version": "1.0.0",
                    "description": "GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry.",
                    "url": "opencompass/opencompass_1135.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1135",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1135",
                        "name": "GPQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/idavidrein/gpqa",
                        "paperLink": "https://arxiv.org/pdf/2311.12022",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "452",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-14 20:26:40",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-14 20:26:40",
                        "createDate": "2024-10-12 16:00:16",
                        "desc": {
                            "cn": "GPQA 包含 448 道由生物学、物理学和化学领域专家撰写的多项选择题。",
                            "en": "GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gpqa'. Error: Path opencompass/gpqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1081",
                    "name": "NaturalCodeBench",
                    "version": "1.0.0",
                    "description": "NaturalCodeBench is challenging code benchmark designed to mirror the complexity and variety of scenarios in real coding tasks. NCB comprises 402 high-quality problems in Python and Java, meticulously selected from natural user queries from online coding services, covering 6 different domains.",
                    "url": "opencompass/opencompass_1081.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1081",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1081",
                        "name": "NaturalCodeBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/THUDM/NaturalCodeBench",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.471.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "043910",
                            "name": "THUDM",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/043910-ad5eb16e-0c3f-4e1b-b1d6-e9dc8ab355a7.png",
                            "nickname": "智谱.AI"
                        },
                        "lookNum": "449",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-29 16:24:58",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-29 16:24:58",
                        "createDate": "2024-09-29 10:05:45",
                        "desc": {
                            "cn": "NaturalCodeBench 是一个具有挑战性的代码基准，旨在反映真实编码任务中的复杂性和多样性。NaturalCodeBench 包含 402 个高质量的 Python 和 Java 问题，这些问题是从在线编码服务的自然用户查询中精心挑选的，涵盖了 6 个不同的领域。",
                            "en": "NaturalCodeBench is challenging code benchmark designed to mirror the complexity and variety of scenarios in real coding tasks. NCB comprises 402 high-quality problems in Python and Java, meticulously selected from natural user queries from online coding services, covering 6 different domains."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/naturalcodebench'. Error: Path opencompass/naturalcodebench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1124",
                    "name": "RealToxicityPrompts",
                    "version": "1.0.0",
                    "description": "RealToxicityPrompts is a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely used toxicity classiﬁer. ",
                    "url": "opencompass/opencompass_1124.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1124",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1124",
                        "name": "RealToxicityPrompts",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/allenai/real-toxicity-prompts",
                        "paperLink": "https://aclanthology.org/2020.findings-emnlp.301.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "448",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-11 20:24:44",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-11 20:24:44",
                        "createDate": "2024-10-11 13:38:08",
                        "desc": {
                            "cn": "RealToxicityPrompts 是一个包含 100,000 个自然出现的、句子级提示的数据集，这些提示来自于大量的英语网络文本，并配有来自广泛使用的毒性分类器的毒性评分。",
                            "en": "RealToxicityPrompts is a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely used toxicity classiﬁer. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/realtoxicityprompts'. Error: Path opencompass/realtoxicityprompts is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1276",
                    "name": "MMLU-Pro",
                    "version": "1.0.0",
                    "description": "MMLU-Pro is the extension of MMLU, integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options.",
                    "url": "opencompass/opencompass_1276.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1276",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1276",
                        "name": "MMLU-Pro",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "学科",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/TIGER-AI-Lab/MMLU-Pro",
                        "paperLink": "https://arxiv.org/abs/2406.01574",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "447",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 14:44:23",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 14:44:23",
                        "createDate": "2024-12-24 19:48:37",
                        "desc": {
                            "cn": "MMLU-Pro是MMLU的扩展版本，涵盖了更具挑战性、以推理为重点的问题，并将选择集从4个选项扩展到10个选项。",
                            "en": "MMLU-Pro is the extension of MMLU, integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmlu_pro'. Error: No data found in /home/budadmin/.cache/opencompass/./data/mmlu_pro for split 'test'. Please check if the split exists and contains data files.\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1134",
                    "name": "TheoremQA",
                    "version": "1.0.0",
                    "description": "TheoremQA is the first theorem-driven question-answering dataset designed to evaluate AI models’ capabilities to apply theorems to solve challenging science problems. It is curated by domain experts containing 800 high-quality questions covering 350 theorems from Math, Physics, EE&CS, and Finance.",
                    "url": "opencompass/opencompass_1134.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1134",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {
                        "gen": "TheoremQA_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1134",
                        "name": "TheoremQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/TIGER-AI-Lab/TheoremQA",
                        "paperLink": "https://arxiv.org/pdf/2305.12524",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "442",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-14 20:15:11",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-14 20:15:11",
                        "createDate": "2024-10-12 13:48:08",
                        "desc": {
                            "cn": "TheoremQA 是第一个基于定理的问题回答数据集，旨在评估 AI 模型应用定理解决复杂科学问题的能力。该数据集由领域专家精心策划，包含 800 个高质量问题，涵盖来自数学、物理、电气与计算机科学以及金融的 350 个定理。",
                            "en": "TheoremQA is the first theorem-driven question-answering dataset designed to evaluate AI models’ capabilities to apply theorems to solve challenging science problems. It is curated by domain experts containing 800 high-quality questions covering 350 theorems from Math, Physics, EE&CS, and Finance."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/theoremqa'. Error: Path opencompass/theoremqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1100",
                    "name": "ScienceQA",
                    "version": "1.0.0",
                    "description": "SCIENCEQA is a new benchmark that consists of ∼21k multimodal multiple choice questions with diverse science topics and annotations of their answers with corresponding lectures and explanations.",
                    "url": "opencompass/opencompass_1100.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1100",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1100",
                        "name": "ScienceQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://scienceqa.github.io/",
                        "paperLink": "https://lupantech.github.io/papers/neurips22_scienceqa.pdf",
                        "officialWebsiteLink": "https://scienceqa.github.io/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "437",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-09 20:03:37",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-09 20:03:37",
                        "createDate": "2024-10-09 14:29:43",
                        "desc": {
                            "cn": "SCIENCEQA 包含约 21,000 道多模态选择题，涵盖多种科学主题，并附有相应的讲座和解释的答案注释。",
                            "en": "SCIENCEQA is a new benchmark that consists of ∼21k multimodal multiple choice questions with diverse science topics and annotations of their answers with corresponding lectures and explanations."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/scienceqa'. Error: Path opencompass/scienceqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1088",
                    "name": "UHGEval",
                    "version": "1.0.0",
                    "description": "UHGEval(Unconstrained Hallucination Generation Evaluation) benchmark contains hallucinations generated by LLMs with minimal restrictions.",
                    "url": "opencompass/opencompass_1088.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1088",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1088",
                        "name": "UHGEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/IAAR-Shanghai/UHGEval",
                        "paperLink": "https://arxiv.org/pdf/2311.15296",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "432",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-10 10:34:42",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-10 10:34:42",
                        "createDate": "2024-10-09 20:11:08",
                        "desc": {
                            "cn": "UHGEval 基准，包含由限制条件最小的大语言模型（LLMs）生成的幻觉。",
                            "en": "UHGEval(Unconstrained Hallucination Generation Evaluation) benchmark contains hallucinations generated by LLMs with minimal restrictions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/uhgeval'. Error: Path opencompass/uhgeval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1164",
                    "name": "TaskBench",
                    "version": "1.0.0",
                    "description": "TaskBench aims to evaluate the capability of LLMs in task automation, containing 28,271 samples spanning 3 critical stages: task decomposition, tool invocation, and parameter prediction.",
                    "url": "opencompass/opencompass_1164.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1164",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1164",
                        "name": "TaskBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/microsoft/JARVIS/tree/main/taskbench",
                        "paperLink": "https://arxiv.org/pdf/2311.18760",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "429",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 16:32:37",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 16:32:37",
                        "createDate": "2024-10-18 15:47:56",
                        "desc": {
                            "cn": "TaskBench旨在评估LLM在任务自动化方面的能力，包含面向任务分解、工具调用和参数预测三个关键阶段的28271个样本。",
                            "en": "TaskBench aims to evaluate the capability of LLMs in task automation, containing 28,271 samples spanning 3 critical stages: task decomposition, tool invocation, and parameter prediction."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/taskbench'. Error: Path opencompass/taskbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1086",
                    "name": "Belebele",
                    "version": "1.0.0",
                    "description": "BELEBELE, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in\nhigh-, medium-, and low-resource languages.",
                    "url": "opencompass/opencompass_1086.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1086",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1086",
                        "name": "Belebele",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/facebookresearch/belebele",
                        "paperLink": "https://arxiv.org/pdf/2308.16884",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "426",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-30 15:33:02",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-30 15:33:02",
                        "createDate": "2024-09-29 13:42:49",
                        "desc": {
                            "cn": "BELEBELE 是一个多项选择机器阅读理解（MRC）数据集，涵盖 122 种语言变体。该数据集显著扩展了自然语言理解（NLU）基准的语言覆盖范围，使得可以在高、中、低资源语言中评估文本模型。",
                            "en": "BELEBELE, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in\nhigh-, medium-, and low-resource languages."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/belebele'. Error: Path opencompass/belebele is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1078",
                    "name": "DebugBench",
                    "version": "1.0.0",
                    "description": "DebugBench is an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. ",
                    "url": "opencompass/opencompass_1078.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1078",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1078",
                        "name": "DebugBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/thunlp/DebugBench",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.247.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50188779",
                            "name": "THUNLP",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50188779-1d9e7f11-f6dd-4c37-a464-f75de27aa50a.png",
                            "nickname": "THUNLP"
                        },
                        "lookNum": "425",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-29 16:25:05",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-29 16:25:05",
                        "createDate": "2024-09-27 17:44:25",
                        "desc": {
                            "cn": "DebugBench 是一个包含 4,253 个实例的 LLM 调试基准。它涵盖了 C++、Java 和 Python 中的四个主要错误类别和 18 种次要类型。",
                            "en": "DebugBench is an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/debugbench'. Error: Path opencompass/debugbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_568",
                    "name": "CriticBench",
                    "version": "1.0.0",
                    "description": "CriticBench, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. CriticBench encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity.",
                    "url": "opencompass/opencompass_568.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_568",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "568",
                        "name": "CriticBench",
                        "emoji": "😂",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "critique",
                                "en": "critique"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/open-compass/CriticBench",
                        "paperLink": "https://arxiv.org/abs/2402.13764",
                        "officialWebsiteLink": "https://open-compass.github.io/CriticBench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "40025311",
                            "name": "Tian-Lan",
                            "avatar": null,
                            "nickname": "Tian-Lan"
                        },
                        "lookNum": "425",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-02-23 19:41:43",
                        "createDate": "2024-02-23 18:01:59",
                        "desc": {
                            "cn": "CriticBench是一个新颖的基准，旨在全面可靠地评估LLM的四个关键批判能力维度。CriticBench包括九项不同的任务，每项任务都评估LLM在不同质量粒度水平上对响应进行批评的能力。",
                            "en": "CriticBench, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. CriticBench encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/criticbench'. Error: Path opencompass/criticbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1148",
                    "name": "AbsPyramid",
                    "version": "1.0.0",
                    "description": "ABSPYRAMID is a unified entailment graph of 221K textual descriptions of abstraction knowledge. ABSPYRAMID collects abstract knowledge for three components\nof diverse events to comprehensively evaluate the abstraction ability of language\nmodels in the open domain.",
                    "url": "opencompass/opencompass_1148.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1148",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1148",
                        "name": "AbsPyramid",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NAACL 2024",
                                "en": "NAACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/HKUST-KnowComp/AbsPyramid",
                        "paperLink": "https://aclanthology.org/2024.findings-naacl.252.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50109576",
                            "name": "HKUST-KnowComp",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50109576-bd7795e6-da4b-40f0-a4e6-979ddb3230c2.png",
                            "nickname": "OpenXLab-dJloo3kUv"
                        },
                        "lookNum": "415",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-27 17:22:48",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-27 17:22:48",
                        "createDate": "2024-10-15 14:04:22",
                        "desc": {
                            "cn": "ABSPYRAMID 是包含 221,000 条文本描述的抽象知识,收集了多种事件的三个组成部分的抽象知识，以全面评估语言模型在开放域中的抽象能力。",
                            "en": "ABSPYRAMID is a unified entailment graph of 221K textual descriptions of abstraction knowledge. ABSPYRAMID collects abstract knowledge for three components\nof diverse events to comprehensively evaluate the abstraction ability of language\nmodels in the open domain."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/abspyramid'. Error: Path opencompass/abspyramid is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1538",
                    "name": "SuperGPQA",
                    "version": "1.0.0",
                    "description": "SuperGPQA, a comprehensive benchmark designed to evaluate the knowledge and reasoning abilities of Large Language Models (LLMs) across 285 graduate-level disciplines. SuperGPQA features at least 50 questions per discipline, covering a broad spectrum of graduate-level topics.",
                    "url": "opencompass/opencompass_1538.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1538",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {
                        "gen": "SuperGPQA_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1538",
                        "name": "SuperGPQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "学科",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SuperGPQA/SuperGPQA/",
                        "paperLink": "https://arxiv.org/abs/2502.14739",
                        "officialWebsiteLink": "https://supergpqa.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "412",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-02-26 18:52:28",
                        "supportOnlineEval": false,
                        "updateDate": "2025-02-26 18:52:28",
                        "createDate": "2025-02-26 09:51:42",
                        "desc": {
                            "cn": "SuperGPQA，这是一个旨在评估大型语言模型在 285 个研究生学科领域的知识和推理能力的全面基准。SuperGPQA 每个学科至少包含 50 个问题，涵盖广泛的硕士研究生学科主题。",
                            "en": "SuperGPQA, a comprehensive benchmark designed to evaluate the knowledge and reasoning abilities of Large Language Models (LLMs) across 285 graduate-level disciplines. SuperGPQA features at least 50 questions per discipline, covering a broad spectrum of graduate-level topics."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/supergpqa'. Error: Path opencompass/supergpqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1069",
                    "name": "AIR-Bench",
                    "version": "1.0.0",
                    "description": "AIR-Bench is the first benchmark designed to evaluate the\nability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format.",
                    "url": "opencompass/opencompass_1069.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1069",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1069",
                        "name": "AIR-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OFA-Sys/AIR-Bench",
                        "paperLink": "https://arxiv.org/pdf/2402.07729",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50186884",
                            "name": "OFA-Sys",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186884-c56d3538-0c00-4d32-86ed-c28d0817e429.jfif",
                            "nickname": "OFA-Sys"
                        },
                        "lookNum": "401",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-27 16:53:47",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-27 16:53:47",
                        "createDate": "2024-09-26 16:07:49",
                        "desc": {
                            "cn": "AIR-Bench 是第一个旨在评估 LALMs 理解各种音频信号（包括人类语言、自然声音和音乐）能力的基准，并进一步评估其以文本形式与人类互动的能力。AIR-Bench 包括两个维度：基础基准和聊天基准。前者由19个任务组成，包含约19,000个单选题，旨在检查LALMs的基本单任务能力。后者包含2,000个开放式问答数据实例，直接评估模型对复杂音频的理解及其遵循指令的能力。",
                            "en": "AIR-Bench is the first benchmark designed to evaluate the\nability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/air_bench'. Error: Path opencompass/air_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1087",
                    "name": "Reveal",
                    "version": "1.0.0",
                    "description": "REVEAL(ReasoningVerification Evaluation) is a new dataset to benchmark automatic verifiers of complex Chain-ofThought reasoning in open-domain question answering settings. ",
                    "url": "opencompass/opencompass_1087.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1087",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1087",
                        "name": "Reveal",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://reveal-dataset.github.io/",
                        "paperLink": "https://arxiv.org/pdf/2402.00559",
                        "officialWebsiteLink": "https://reveal-dataset.github.io/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "400",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-30 15:32:59",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-30 15:32:59",
                        "createDate": "2024-09-29 14:26:40",
                        "desc": {
                            "cn": "Reveal 是一个用于基准测试开放域问答环境中复杂链式推理自动验证器的新数据集。Reveal 包含关于语言模型答案中每个推理步骤的相关性、证据段落的归因和逻辑正确性的全面标签，涵盖多种数据集和最先进的语言模型。",
                            "en": "REVEAL(ReasoningVerification Evaluation) is a new dataset to benchmark automatic verifiers of complex Chain-ofThought reasoning in open-domain question answering settings. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/reveal'. Error: Path opencompass/reveal is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1129",
                    "name": "WikiSQL",
                    "version": "1.0.0",
                    "description": "WikiSQL is a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia that is an order of magnitude larger than comparable datasets.",
                    "url": "opencompass/opencompass_1129.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1129",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1129",
                        "name": "WikiSQL",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/salesforce/WikiSQL",
                        "paperLink": "https://arxiv.org/pdf/1709.00103v7",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "393",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-16 11:03:06",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-16 11:03:06",
                        "createDate": "2024-10-11 15:27:39",
                        "desc": {
                            "cn": "WikiSQL 是一个包含 80,654 个手动标注示例的问题和 SQL 查询的数据集，分布在来自维基百科的 24,241 个表格中。",
                            "en": "WikiSQL is a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia that is an order of magnitude larger than comparable datasets."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/wikisql'. Error: Path opencompass/wikisql is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1351",
                    "name": "AgentHarm",
                    "version": "1.0.0",
                    "description": "AgentHarm tests the robustness of LLMs to jailbreak attacks. It includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment.",
                    "url": "opencompass/opencompass_1351.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1351",
                    "sample_count": 1000,
                    "traits": [
                        "Safety",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1351",
                        "name": "AgentHarm",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "安全",
                                "en": "Safety"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/UKGovernmentBEIS/inspect_evals/tree/main/src/inspect_evals/agentharm",
                        "paperLink": "https://arxiv.org/abs/2410.09024",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "392",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:28",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:28",
                        "createDate": "2025-01-09 16:22:26",
                        "desc": {
                            "cn": "AgentHarm用于评估LLM智能体对越狱攻击的鲁棒性，包括110套恶意智能体任务（其中有440个强化任务），涵盖欺诈、网络犯罪和骚扰等11个危害类别。",
                            "en": "AgentHarm tests the robustness of LLMs to jailbreak attacks. It includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/agentharm'. Error: Path opencompass/agentharm is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1076",
                    "name": "PCA-Bench",
                    "version": "1.0.0",
                    "description": "PCA-Bench is a multimodal decisionmaking benchmark for evaluating the integrated capabilities of Multimodal Large Language Models (MLLMs). Departing from previous\nbenchmarks focusing on simplistic tasks and individual model capability.",
                    "url": "opencompass/opencompass_1076.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1076",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1076",
                        "name": "PCA-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/pkunlp-icler/PCA-EVAL",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.64.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50188713",
                            "name": "PKUNLP-ICLER",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50188713-85c05c22-aa65-44f2-b06f-aa859ed819eb.png",
                            "nickname": "PKUNLP-ICLER"
                        },
                        "lookNum": "383",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-27 17:50:33",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-27 17:50:33",
                        "createDate": "2024-09-27 15:11:18",
                        "desc": {
                            "cn": "PCA-Bench 是一个多模态决策基准，用于评估多模态大型语言模型（MLLMs）的综合能力。与之前专注于简单任务和单个模型能力的基准不同，PCA-Bench 引入了三个复杂场景：自动驾驶、家庭机器人和开放世界游戏。",
                            "en": "PCA-Bench is a multimodal decisionmaking benchmark for evaluating the integrated capabilities of Multimodal Large Language Models (MLLMs). Departing from previous\nbenchmarks focusing on simplistic tasks and individual model capability."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/pca_bench'. Error: Path opencompass/pca_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1098",
                    "name": "GrailQA",
                    "version": "1.0.0",
                    "description": "GrailQA is a new large-scale, high-quality dataset for question answering on knowledge bases (KBQA) on Freebase with 64,331 questions annotated with both answers and corresponding logical forms in different syntax (i.e., SPARQL, S-expression, etc.). ",
                    "url": "opencompass/opencompass_1098.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1098",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1098",
                        "name": "GrailQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/dki-lab/GrailQA",
                        "paperLink": "https://arxiv.org/pdf/2011.07743",
                        "officialWebsiteLink": "https://dki-lab.github.io/GrailQA/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "383",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-09 20:03:43",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-09 20:03:43",
                        "createDate": "2024-10-09 11:14:58",
                        "desc": {
                            "cn": "GrailQA 是一个大规模高质量数据集，用于知识库问答，包含 64,331 个问题，并附有答案和不同语法的相应逻辑形式。",
                            "en": "GrailQA is a new large-scale, high-quality dataset for question answering on knowledge bases (KBQA) on Freebase with 64,331 questions annotated with both answers and corresponding logical forms in different syntax (i.e., SPARQL, S-expression, etc.). "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/grailqa'. Error: Path opencompass/grailqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1219",
                    "name": "CS-Eval",
                    "version": "1.0.0",
                    "description": "CS-Eval is a large language model cybersecurity capability evaluation suite jointly established by Alibaba Security, Fudan University, and the University of Chinese Academy of Sciences. The dataset encompasses 11 major cybersecurity categories and 42 subcategories, offering comprehensive assessment ",
                    "url": "opencompass/opencompass_1219.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1219",
                    "sample_count": 1000,
                    "traits": [
                        "Examination",
                        "Knowledge",
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1219",
                        "name": "CS-Eval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "学科",
                                "en": "Examination"
                            },
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            },
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/CS-EVAL/CS-Eval",
                        "paperLink": "https://arxiv.org/pdf/2411.16239",
                        "officialWebsiteLink": "https://cs-eval.com",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "41601314",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-JdyYs9Oa3"
                        },
                        "lookNum": "383",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-11-27 16:53:44",
                        "supportOnlineEval": false,
                        "updateDate": "2024-11-27 16:53:44",
                        "createDate": "2024-11-26 10:42:49",
                        "desc": {
                            "cn": "CS-Eval 是由阿里安全、复旦大学和中国科学院大学联合建立的大模型网络安全能力评测集。数据集覆盖11个网络安全大类领域、42个子类领域，提供知识型和实战型的综合评估任务，支持用户自主评测，同时为大模型落地网络安全提供参考和启发。\n",
                            "en": "CS-Eval is a large language model cybersecurity capability evaluation suite jointly established by Alibaba Security, Fudan University, and the University of Chinese Academy of Sciences. The dataset encompasses 11 major cybersecurity categories and 42 subcategories, offering comprehensive assessment "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cs_eval'. Error: Path opencompass/cs_eval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1084",
                    "name": "StableToolBench",
                    "version": "1.0.0",
                    "description": "StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status.",
                    "url": "opencompass/opencompass_1084.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1084",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1084",
                        "name": "StableToolBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/THUNLP-MT/StableToolBench",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.664.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "50186303",
                            "name": "THUNLP-MT",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186303-a2701a3a-208f-4cc9-88b7-e09515c70865.png",
                            "nickname": "THUNLP-MT"
                        },
                        "lookNum": "377",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-29 16:24:50",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-29 16:24:50",
                        "createDate": "2024-09-29 13:09:10",
                        "desc": {
                            "cn": "StableToolBench 是一个从 ToolBench 发展而来的基准，提出了一个虚拟 API 服务器和稳定的评估系统。虚拟 API 服务器包含一个缓存系统和 API 模拟器，这些组件相辅相成，以缓解 API 状态变化带来的影响。",
                            "en": "StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/stabletoolbench'. Error: Path opencompass/stabletoolbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1152",
                    "name": "SportQA",
                    "version": "1.0.0",
                    "description": "SportQA aims to evaluate LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenariobased reasoning tasks.",
                    "url": "opencompass/opencompass_1152.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1152",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1152",
                        "name": "SportQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NAACL 2024",
                                "en": "NAACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/haotianxia/SportQA",
                        "paperLink": "https://aclanthology.org/2024.naacl-long.283.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "372",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-27 17:23:16",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-27 17:23:16",
                        "createDate": "2024-10-15 15:51:00",
                        "desc": {
                            "cn": "SportQA 专门用于评估大型语言模型（LLMs）在体育理解方面的能力。SportQA 包含超过 70,000 道多项选择题，分为三个不同的难度级别，针对从基本历史事实到复杂情境推理任务的各种体育知识。",
                            "en": "SportQA aims to evaluate LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenariobased reasoning tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/sportqa'. Error: Path opencompass/sportqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1287",
                    "name": "MedBench",
                    "version": "1.0.0",
                    "description": "MedBench is committed to building a scientific, fair and rigorous Chinese medical model evaluation system and open platform. Based on authoritative standards, we constantly update and maintain high-quality datasets, and comprehensively quantify capabilities of models in various medical dimensions.",
                    "url": "opencompass/opencompass_1287.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1287",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1287",
                        "name": "MedBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Medical",
                                "en": "Medical"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/open-compass/opencompass/tree/main/opencompass/datasets/medbench/",
                        "paperLink": "https://www.sciopen.com/article/10.26599/BDMA.2024.9020044",
                        "officialWebsiteLink": "https://medbench.opencompass.org.cn/home",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "368",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-27 14:45:32",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-27 14:45:32",
                        "createDate": "2024-12-27 14:01:19",
                        "desc": {
                            "cn": "MedBench致力于打造一个科学、公平且严谨的中文医疗大模型评测体系及开放平台。我们基于医学权威标准，不断更新维护高质量的医学数据集，全方位多维度量化模型在各个医学维度的能力。",
                            "en": "MedBench is committed to building a scientific, fair and rigorous Chinese medical model evaluation system and open platform. Based on authoritative standards, we constantly update and maintain high-quality datasets, and comprehensively quantify capabilities of models in various medical dimensions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medbench'. Error: Path opencompass/medbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1125",
                    "name": "Mind2Web",
                    "version": "1.0.0",
                    "description": "MIND2WEB is the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website.",
                    "url": "opencompass/opencompass_1125.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1125",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1125",
                        "name": "Mind2Web",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OSU-NLP-Group/Mind2Web",
                        "paperLink": "https://arxiv.org/pdf/2306.06070",
                        "officialWebsiteLink": "https://osu-nlp-group.github.io/Mind2Web/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "361",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-11 20:24:41",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-11 20:24:41",
                        "createDate": "2024-10-11 13:51:20",
                        "desc": {
                            "cn": "MIND2WEB 是首个用于开发和评估通用网页代理的数据集，能够根据语言指令在任何网站上完成复杂任务。",
                            "en": "MIND2WEB is the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mind2web'. Error: Path opencompass/mind2web is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1328",
                    "name": "GTA",
                    "version": "1.0.0",
                    "description": "GTA is meant for LLM's tool-use evaluations under real-world scenarios, including 229 real-world tasks and executable tool chains.",
                    "url": "opencompass/opencompass_1328.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1328",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1328",
                        "name": "GTA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/open-compass/GTA",
                        "paperLink": "https://arxiv.org/abs/2407.08713",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "355",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 16:32:41",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 16:32:41",
                        "createDate": "2025-01-03 11:35:50",
                        "desc": {
                            "cn": "GTA用于评估LLM调用工具解决实际问题的能力，由229个真实任务和可执行工具链组成。",
                            "en": "GTA is meant for LLM's tool-use evaluations under real-world scenarios, including 229 real-world tasks and executable tool chains."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gta'. Error: Path opencompass/gta is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1142",
                    "name": "MIRAGE",
                    "version": "1.0.0",
                    "description": "MIRAGE is a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. ",
                    "url": "opencompass/opencompass_1142.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1142",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1142",
                        "name": "MIRAGE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://teddy-xionggz.github.io/benchmark-medical-rag/",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.372.pdf",
                        "officialWebsiteLink": "https://teddy-xionggz.github.io/benchmark-medical-rag/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "351",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-19 18:04:07",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-19 18:04:07",
                        "createDate": "2024-10-14 18:05:57",
                        "desc": {
                            "cn": "MIRAGE 是首个此类基准，包含来自五个医学问答数据集的 7,663 个问题。",
                            "en": "MIRAGE is a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mirage'. Error: Path opencompass/mirage is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1245",
                    "name": "KOR-Bench",
                    "version": "1.0.0",
                    "description": "Knowledge-Orthogonal Reasoning Benchmark (KOR-Bench) encompasses five task categories: Operation, Logic, Cipher, Puzzle, and Counterfactual. KOR-Bench emphasizes the effectiveness of models in applying new rule descriptions to solve novel rule-driven questions. ",
                    "url": "opencompass/opencompass_1245.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1245",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1245",
                        "name": "KOR-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/KOR-Bench/KOR-Bench",
                        "paperLink": "https://arxiv.org/abs/2410.06526",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "332",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 11:12:43",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 11:12:43",
                        "createDate": "2025-01-10 19:42:40",
                        "desc": {
                            "cn": "KOR-Bench用于评估大语言模型的推理能力，包括五个任务类别：操作、逻辑、密码、拼图和反事实。",
                            "en": "Knowledge-Orthogonal Reasoning Benchmark (KOR-Bench) encompasses five task categories: Operation, Logic, Cipher, Puzzle, and Counterfactual. KOR-Bench emphasizes the effectiveness of models in applying new rule descriptions to solve novel rule-driven questions. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/kor_bench'. Error: Path opencompass/kor_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1359",
                    "name": "SEED-Bench",
                    "version": "1.0.0",
                    "description": "SEED-Bench aims at the evaluation of generative comprehension in MLLMs, consisting of 19K multiple choice questions, which spans 12 evaluation dimensions including the comprehension of both the image and video modality. ",
                    "url": "opencompass/opencompass_1359.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1359",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1359",
                        "name": "SEED-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AILab-CVC/SEED-Bench",
                        "paperLink": "https://arxiv.org/abs/2307.16125",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "323",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:55",
                        "createDate": "2025-01-09 21:34:54",
                        "desc": {
                            "cn": "SEED-Bench用于评估多模态大模型的理解能力，包括对图像和视频的理解，由跨越12个评估维度的19K道多项选择题组成。",
                            "en": "SEED-Bench aims at the evaluation of generative comprehension in MLLMs, consisting of 19K multiple choice questions, which spans 12 evaluation dimensions including the comprehension of both the image and video modality. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/seed_bench'. Error: Path opencompass/seed_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1248",
                    "name": "MMMU",
                    "version": "1.0.0",
                    "description": "MMMU is a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks.",
                    "url": "opencompass/opencompass_1248.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1248",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1248",
                        "name": "MMMU",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MMMU-Benchmark/MMMU",
                        "paperLink": "https://arxiv.org/abs/2311.16502",
                        "officialWebsiteLink": "https://mmmu-benchmark.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "319",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-22 18:09:32",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-22 18:09:32",
                        "createDate": "2024-12-20 19:45:07",
                        "desc": {
                            "cn": "MMMU用于评估多模态大模型在复杂多学科任务中的表现，包括从大学考试和教科书中精心收集的11.5K多模态问题，涵盖六个核心学科：艺术与设计、商业、科学、健康与医学、人文与社会科学以及技术与工程。",
                            "en": "MMMU is a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmmu'. Error: Path opencompass/mmmu is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1115",
                    "name": "MathQA",
                    "version": "1.0.0",
                    "description": "MathQA is a new large-scale, diverse dataset of 37k English multiple-choice math word problems covering multiple math domain categories by modeling operation programs\ncorresponding to word problems in the AQuA dataset.",
                    "url": "opencompass/opencompass_1115.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1115",
                    "sample_count": 1000,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1115",
                        "name": "MathQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://math-qa.github.io/math-QA/",
                        "paperLink": "https://arxiv.org/pdf/1905.13319v1",
                        "officialWebsiteLink": "https://math-qa.github.io/math-QA/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "317",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-11 19:59:27",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-11 19:59:27",
                        "createDate": "2024-10-10 14:14:12",
                        "desc": {
                            "cn": "MathQA 是一个大规模、多样化的数据集，包含 37,000 道英语多项选择数学文字问题，涵盖多个数学领域类别。",
                            "en": "MathQA is a new large-scale, diverse dataset of 37k English multiple-choice math word problems covering multiple math domain categories by modeling operation programs\ncorresponding to word problems in the AQuA dataset."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mathqa'. Error: Path opencompass/mathqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1244",
                    "name": "Omni-MATH",
                    "version": "1.0.0",
                    "description": "Omni-MATH focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels",
                    "url": "opencompass/opencompass_1244.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1244",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1244",
                        "name": "Omni-MATH",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/KbsdJames/Omni-MATH",
                        "paperLink": "https://arxiv.org/abs/2410.07985",
                        "officialWebsiteLink": "https://omni-math.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "313",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-30 16:29:59",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-30 16:29:59",
                        "createDate": "2024-12-30 16:20:58",
                        "desc": {
                            "cn": "Omni-MATH用于评估LLM在奥林匹克水平上的数学推理能力，包括4428道竞赛级问题，并带有严格的人工注释。这些问题被精心分类为超过33个子领域，涵盖10多个不同的难度级别。",
                            "en": "Omni-MATH focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/omni_math'. Error: Path opencompass/omni_math is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1091",
                    "name": "RoleLLM",
                    "version": "1.0.0",
                    "description": "RoleLLM is a role-playing framework of data construction and evaluation (RoleBench), as well as solutions for both closed-source and open-source models (RoleGPT, RoleLLaMA, RoleGLM). We also propose Context-Instruct for long-text knowledge extraction and role-specific knowledge injection.",
                    "url": "opencompass/opencompass_1091.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1091",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1091",
                        "name": "RoleLLM",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/InteractiveNLP-Team/RoleLLM-public",
                        "paperLink": "https://aclanthology.org/2024.findings-acl.878.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50187516",
                            "name": "InteractiveNLP-Team",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50187516-87c7afab-16ed-48c4-8147-1298485c7231.png",
                            "nickname": "InteractiveNLP-Team"
                        },
                        "lookNum": "310",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-30 15:54:49",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-30 15:54:49",
                        "createDate": "2024-09-29 16:37:55",
                        "desc": {
                            "cn": "RoleLLM 是一个角色扮演的数据构建和评估框架，同时提供闭源和开源模型的解决方案（RoleGPT、RoleLLaMA、RoleGLM）。",
                            "en": "RoleLLM is a role-playing framework of data construction and evaluation (RoleBench), as well as solutions for both closed-source and open-source models (RoleGPT, RoleLLaMA, RoleGLM). We also propose Context-Instruct for long-text knowledge extraction and role-specific knowledge injection."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/rolellm'. Error: Path opencompass/rolellm is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1413",
                    "name": "LiveCodeBench",
                    "version": "1.0.0",
                    "description": "LiveCodeBench evaluates LLMs' coding abilities. It continuously collects new problems over time from contests across LeetCode, AtCoder, and CodeForces. Notably, it also focuses on a broader range of code related capabilities besides code generation.",
                    "url": "opencompass/opencompass_1413.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1413",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1413",
                        "name": "LiveCodeBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/LiveCodeBench/LiveCodeBench",
                        "paperLink": "https://arxiv.org/abs/2403.07974",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "309",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:47:04",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:47:04",
                        "createDate": "2025-01-17 20:40:50",
                        "desc": {
                            "cn": "LiveCodeBench用于评估大语言模型的代码能力，包含来自LeetCode、AtCoder和CodeForces的动态更新的问题，并在代码生成能力的基础上将更广泛的相关能力纳入考量。",
                            "en": "LiveCodeBench evaluates LLMs' coding abilities. It continuously collects new problems over time from contests across LeetCode, AtCoder, and CodeForces. Notably, it also focuses on a broader range of code related capabilities besides code generation."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/livecodebench'. Error: Path opencompass/livecodebench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1109",
                    "name": "WinoGrande",
                    "version": "1.0.0",
                    "description": "WINOGRANDE is a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset.",
                    "url": "opencompass/opencompass_1109.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1109",
                    "sample_count": 1767,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {
                        "gen": "winogrande_gen",
                        "ppl": "winogrande_ppl",
                        "ll": "winogrande_ll"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 100,
                        "estimated_output_tokens": 50
                    },
                    "original_data": {
                        "id": "1109",
                        "name": "WinoGrande",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/allenai/winogrande",
                        "paperLink": "https://arxiv.org/pdf/1907.10641",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "305",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-09 20:03:23",
                        "supportOnlineEval": true,
                        "updateDate": "2024-10-09 20:03:23",
                        "createDate": "2024-10-09 18:41:18",
                        "desc": {
                            "cn": "WINOGRANDE 包含 44,000 个问题，受到 WSC 设计的启发，但进行了调整，以提高数据集的规模和难度。",
                            "en": "WINOGRANDE is a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "{\"qID\": \"3087LXLJ6OUC8YHY6A67DCJQIOA0FY-1\", \"sentence\": \"Eric was disappointed to see that Ryan had formica countertops, because _ thought formica was tacky.\", \"option1\": \"Eric\", \"option2\": \"Ryan\"}",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "reading_comprehension",
                                        "fill_in_the_blank"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "reading_comprehension",
                                        "inference",
                                        "grammar",
                                        "language_understanding"
                                    ],
                                    "concepts": [
                                        "pronouns",
                                        "inference",
                                        "subject-verb agreement",
                                        "reading comprehension"
                                    ],
                                    "qualification": [
                                        "middle_school_education"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 7,
                                        "avg": 12,
                                        "max": 99
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "{\"qID\": \"3TFJJUELSHNX771VAX8KWW3TOWSC24-2\", \"sentence\": \"Rachel was the victim of an assault by Elena. _ had was taken to the jail by the police.\", \"option1\": \"Rachel\", \"option2\": \"Elena\"}",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "reading_comprehension",
                                        "multiple_choice_fill_in_the_blank"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "reading comprehension",
                                        "basic grammar knowledge"
                                    ],
                                    "concepts": [
                                        "subject-verb agreement",
                                        "reading comprehension"
                                    ],
                                    "qualification": [
                                        "9th grade student"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 9,
                                        "avg": 13,
                                        "max": 18
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "{\"qID\": \"3DQYSJDTYNPSZANDEBB848L0FTSXEF-1\", \"sentence\": \"The cute little dog was the one Craig gave Donald after _ found out he was allergic to dogs.\", \"option1\": \"Craig\", \"option2\": \"Donald\"}",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "qa"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "reading_comprehension",
                                        "grammar_usage",
                                        "logical_inference"
                                    ],
                                    "concepts": [
                                        "pronoun_reference",
                                        "subject_verb_agreement",
                                        "logical_inference"
                                    ],
                                    "qualification": [
                                        "middle_school"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 10,
                                        "avg": 14,
                                        "max": 80
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "{\"qID\": \"3JTPR5MTZSA7P2W6KTRGIMJDI0T5KN-1\", \"sentence\": \"Megan washed Jessica's hat with the load of laundry, so _ felt guilty that the hat was ruined.\", \"option1\": \"Megan\", \"option2\": \"Jessica\"}",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "reading_comprehension"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "reading_comprehension",
                                        "basic_language_proficiency"
                                    ],
                                    "concepts": [
                                        "pronoun resolution",
                                        "causal inference",
                                        "guilt"
                                    ],
                                    "qualification": [
                                        "1st grade"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 5,
                                        "avg": 7,
                                        "max": 70
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "{\"qID\": \"3AFT28WXLHGHL6B793E36ASZVCYIOW-2\", \"sentence\": \"The house we rented had bed bugs which was very disgusting.  So we refused to sleep near the _ .\", \"option1\": \"house\", \"option2\": \"bed bugs\"}",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "comprehension",
                                        "multiple_choice"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "reading_comprehension",
                                        "vocabulary",
                                        "basic_inference"
                                    ],
                                    "concepts": [],
                                    "qualification": [
                                        "Kindergarten",
                                        "Elementary school"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 7,
                                        "avg": 12,
                                        "max": 90
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "{\"qID\": \"3X878VYTIGWQQ9HIR9RGSRTCPL67FU-1\", \"sentence\": \"Justin has ants in his kitchen, so he gives a call to Kevin. _ is likely the apartment owner.\", \"option1\": \"Justin\", \"option2\": \"Kevin\"}",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "reading_comprehension"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "inference",
                                        "pronoun_resolution",
                                        "reading_comprehension"
                                    ],
                                    "concepts": [
                                        "pronoun_resolution",
                                        "inference",
                                        "ownership"
                                    ],
                                    "qualification": [
                                        "middle_school"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 7,
                                        "avg": 10,
                                        "max": 18
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "{\"qID\": \"3QD8LUVX4XWD92VDJC658GRFE3XX5H-1\", \"sentence\": \"He wore his prescription glasses but he still couldn't read the letters because the _ were too weak.\", \"option1\": \"glasses\", \"option2\": \"letter",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "qa"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "vocabulary",
                                        "reading_comprehension",
                                        "contextual_inference"
                                    ],
                                    "concepts": [
                                        "vocabulary",
                                        "reading_comprehension",
                                        "context_clues"
                                    ],
                                    "qualification": [
                                        "elementary_school"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 7,
                                        "avg": 10,
                                        "max": 17
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "{\"qID\": \"3OREP8RUT4PJ6LG3FA9EFEFK94VBG9-1\", \"sentence\": \"Leslie has really pissed off Jeffrey by drawing with crayons on a wallpaper, so _ feels bad.\", \"option1\": \"Leslie\", \"option2\": \"Jeffrey\"}",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "question_answering"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "reading_comprehension",
                                        "grammar",
                                        "inference"
                                    ],
                                    "concepts": [
                                        "cause_and_effect",
                                        "subject_pronoun_reference"
                                    ],
                                    "qualification": [
                                        "5th_grade"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 8,
                                        "avg": 13,
                                        "max": 25
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "{\"qID\": \"3FSEU3P2NTEXBI66RIPGEXYWU2KRRI-1\", \"sentence\": \"Carrie installed Megan's dental crown today so _ spent time after answering how to take of it.\", \"option1\": \"Carrie\", \"option2\": \"Megan\"}",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "reading_comprehension",
                                        "question_answering",
                                        "fill_in_blank"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "reading_comprehension",
                                        "grammar_analysis",
                                        "inference"
                                    ],
                                    "concepts": [
                                        "pronoun reference",
                                        "grammatical inference",
                                        "reading comprehension"
                                    ],
                                    "qualification": [
                                        "middle_school",
                                        "high_school"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 7,
                                        "avg": 10,
                                        "max": 25
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "{\"qID\": \"3TTPFEFXCVYR05UJ3PAOPY5TA506HP-2\", \"sentence\": \"After getting a fever, Kevin spent the day in bed getting help from Jason because _ was sick before.\", \"option1\": \"Kevin\", \"option2\": \"Jason\"}",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "question_answering",
                                        "reading_comprehension"
                                    ],
                                    "domains": [
                                        "English_Language",
                                        "Grammar"
                                    ],
                                    "skills": [
                                        "basic_grammar",
                                        "inferential_reading",
                                        "natural_language_understanding"
                                    ],
                                    "concepts": [
                                        "pronoun_reference",
                                        "cause_and_effect",
                                        "basic_comprehension"
                                    ],
                                    "qualification": [
                                        "6th_grade",
                                        "English_Comprehension"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 7,
                                        "avg": 10,
                                        "max": 25
                                    },
                                    "domain_expertise_required": true
                                }
                            ],
                            "total_questions": 1767,
                            "question_format": "cloze, cloze_question, cloze_task, cloze_test, comprehension, fill_in_blank, fill_in_the_blank, gap_filling, grammar, inference, knowledge_check, language_comprehension, language_question, language_understanding, logic_inference, multiple_choice, multiple_choice_grading, multiple_choice_qa, multiple_choice_question, multiple_choice_selection, qa, question_answering, quiz, read_comprehension, reading_comprehension, reasoning, selecting_answer, single_choice, text_comprehension, word_choice, word_selection",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Large number of instances (1,767) provides a robust statistical assessment.",
                                "Tasks are simple enough for quick benchmarking yet require genuine inference, avoiding trivial memorization.",
                                "Clear, well‑structured format (sentence, two options) facilitates automated scoring.",
                                "Targets a core language skill (pronoun resolution) that is critical for many applications."
                            ],
                            "disadvantages": [
                                "All tasks are beginner‑level; top‑tier models may score near perfect, limiting differentiation.",
                                "Dataset is limited to English and includes only a narrow age range, reducing cross‑linguistic or age‑diverse applicability.",
                                "The focus on pronoun resolution means it does not test other aspects of language generation like creativity or stylistic nuance.",
                                "Potential annotation bias: the dataset may over‑represent certain syntactic constructions, leading models to overfit to those patterns."
                            ]
                        },
                        "age_distribution": {
                            "data": [
                                88,
                                68,
                                35,
                                8,
                                1,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0
                            ],
                            "categories": [
                                "10",
                                "12",
                                "14",
                                "16",
                                "18",
                                "20",
                                "22",
                                "24",
                                "26",
                                "28",
                                "30",
                                "32",
                                "34",
                                "36",
                                "38",
                                "40",
                                "42",
                                "44",
                                "46",
                                "48",
                                "50",
                                "52",
                                "54",
                                "56",
                                "58",
                                "60"
                            ]
                        },
                        "why_run_this_eval": [
                            "WinoGrande is valuable for testing a model’s ability to perform pronoun resolution, a fundamental skill for many downstream tasks such as question answering, summarization, and dialogue systems.",
                            "Running this evaluation signals that the model can handle basic syntactic parsing and contextual inference, which are essential for clear and coherent language generation."
                        ],
                        "what_to_expect": [
                            "When a language model is evaluated on WinoGrande, the resulting score reflects how well the model can resolve ambiguous pronouns by using contextual and inferential cues.",
                            "A higher accuracy indicates that the model is proficient at linking pronouns to the correct referent, which is a key component of reading comprehension and natural language understanding.",
                            "Conversely, a low score suggests weaknesses in reasoning about context, inference, and syntactic structures."
                        ],
                        "evaluation_description": "WinoGrande is a pronoun resolution benchmark consisting of 1,767 English sentences. Each instance presents a sentence with a missing pronoun and two candidate antecedents. The dataset focuses on beginner‑level reading comprehension and grammar skills, covering domains such as English Language and Grammar. The age distribution of the example sentences is skewed toward older teens and young adults (ages 10, 12, and 14). The labeled skills include basic grammar knowledge, inference, contextual inference, logical inference, and natural language understanding.",
                        "top_5_task_types": [
                            "reading_comprehension",
                            "question_answering",
                            "qa",
                            "fill_in_the_blank",
                            "multiple_choice_fill_in_the_blank"
                        ],
                        "top_5_domains": [
                            "English_Language",
                            "Grammar"
                        ],
                        "top_5_skills": [
                            "reading_comprehension",
                            "inference",
                            "grammar",
                            "vocabulary",
                            "language_understanding"
                        ],
                        "top_5_concepts": [
                            "reading comprehension",
                            "inference",
                            "subject-verb agreement",
                            "pronoun_reference",
                            "cause_and_effect"
                        ],
                        "top_5_qualifications": [
                            "middle_school",
                            "middle_school_education",
                            "9th grade student",
                            "1st grade",
                            "Kindergarten"
                        ],
                        "top_5_languages": [
                            "English"
                        ]
                    },
                    "analysis_file": "analysis/opencompass_1109_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 200,
                        "successful": 200,
                        "failed": 0
                    }
                },
                {
                    "id": "opencompass_1133",
                    "name": "Spider",
                    "version": "1.0.0",
                    "description": "Spider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students. The goal of the Spider challenge is to develop natural language interfaces to cross-domain databases. ",
                    "url": "opencompass/opencompass_1133.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1133",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1133",
                        "name": "Spider",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/taoyds/spider/tree/master/evaluation_examples",
                        "paperLink": "https://arxiv.org/pdf/1809.08887v5",
                        "officialWebsiteLink": "https://yale-lily.github.io/spider",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "305",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-16 11:03:12",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-16 11:03:12",
                        "createDate": "2024-10-12 13:37:46",
                        "desc": {
                            "cn": "Spider 是一个大规模、复杂且跨领域的语义解析和文本到 SQL 数据集。Spider 挑战的目标是开发自然语言接口以访问跨领域数据库。该数据集包含 10,181 个问题和 5,693 个独特的复杂 SQL 查询，涵盖 200 个包含多个表的数据库，涉及 138 个不同的领域。",
                            "en": "Spider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students. The goal of the Spider challenge is to develop natural language interfaces to cross-domain databases. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/spider'. Error: Path opencompass/spider is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1207",
                    "name": "MMBench-Video",
                    "version": "1.0.0",
                    "description": "MMBench-Video is a comprehensive video understanding evaluation benchmark that covers long videos, multiple shots, and evaluates the temporal understanding ability of MLLMs. Contains over 600 videos, 16 categories, and manually annotated Q&A pairs.",
                    "url": "opencompass/opencompass_1207.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1207",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1207",
                        "name": "MMBench-Video",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 3,
                        "githubLink": "https://github.com/open-compass/VLMEvalKit",
                        "paperLink": "https://arxiv.org/abs/2406.14515",
                        "officialWebsiteLink": "https://mmbench-video.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "305",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 17:04:03",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 17:04:03",
                        "createDate": "2025-01-21 14:14:14",
                        "desc": {
                            "cn": "MMBench-Video是全面视频理解评测基准，覆盖长视频、多镜头，评估MLLMs时序理解能力。包含16类共600+视频以及人工标注问答对。",
                            "en": "MMBench-Video is a comprehensive video understanding evaluation benchmark that covers long videos, multiple shots, and evaluates the temporal understanding ability of MLLMs. Contains over 600 videos, 16 categories, and manually annotated Q&A pairs."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmbench_video'. Error: Path opencompass/mmbench_video is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1772",
                    "name": "S1-Bench",
                    "version": "1.0.0",
                    "description": "S1-Bench is a novel benchmark designed to evaluate the performance of LRMs on simple tasks that are more aligned with intuitive System 1 thinking, rather than deliberate System 2 reasoning. S1-Bench offers a set of simple, diverse, and naturally clear questions across multiple domains and languages.",
                    "url": "opencompass/opencompass_1772.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1772",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Understanding",
                        "Instruct"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1772",
                        "name": "S1-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            },
                            {
                                "cn": "指令跟随",
                                "en": "Instruct"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "系统1",
                                "en": "系统1"
                            },
                            {
                                "cn": "快思考",
                                "en": "快思考"
                            },
                            {
                                "cn": "LRM",
                                "en": "LRM"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/WYRipple/S1_Bench",
                        "paperLink": "https://arxiv.org/abs/2504.10368",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "86305501",
                            "name": "WYRipple",
                            "avatar": "https://thirdwx.qlogo.cn/mmopen/vi_32/skbupBhJ87rXicbT3SlhqHwQCLK486tTT6yJribPhbj5sMzbyU6F17SBErBljjNXtRqGcCd8ZBt0MzIPsEFz0hdWNg7JRf5EL1X5FDkvqC3xU/132",
                            "nickname": "WYRipple"
                        },
                        "lookNum": "303",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-25 17:00:35",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-25 17:00:35",
                        "createDate": "2025-04-24 17:48:23",
                        "desc": {
                            "cn": "S1-Bench是一个新颖的基准，旨在评估大模型在简单任务中的表现，这些任务更倾向于直观的系统1思维，而非深思熟虑的系统2推理。尽管大模型在复杂推理任务中通过明确的思维链取得了显著突破，但它们对深度分析思维的依赖可能限制了其系统1思维能力。此外，目前缺乏评估大模型在需要此类能力的任务中表现的基准。为了填补这一空白，S1-Bench提供了一组简单、多样且自然清晰的问题，涵盖多个领域和语言，专门设计用于评估大模型在此类任务中的表现。",
                            "en": "S1-Bench is a novel benchmark designed to evaluate the performance of LRMs on simple tasks that are more aligned with intuitive System 1 thinking, rather than deliberate System 2 reasoning. S1-Bench offers a set of simple, diverse, and naturally clear questions across multiple domains and languages."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/s1_bench'. Error: Path opencompass/s1_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1599",
                    "name": "GAIA",
                    "version": "1.0.0",
                    "description": "GAIA is a benchmark which aims at evaluating next-generation LLMs (LLMs with augmented capabilities due to added tooling, efficient prompting, access to search, etc), mading of more than 450 non-trivial question with an unambiguous answer, requiring different levels of tooling and autonomy to solve.",
                    "url": "opencompass/opencompass_1599.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1599",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1599",
                        "name": "GAIA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2311.12983",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "302",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 11:17:15",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 11:17:15",
                        "createDate": "2025-03-06 11:15:47",
                        "desc": {
                            "cn": "GAIA 是一个旨在评估下一代LLMs（由于增加了工具、高效的提示、访问搜索等功能而具有增强能力的LLMs）的基准，由超过 450 个非平凡问题组成，这些问题有明确的答案，需要不同层次的工具和自主性来解决。",
                            "en": "GAIA is a benchmark which aims at evaluating next-generation LLMs (LLMs with augmented capabilities due to added tooling, efficient prompting, access to search, etc), mading of more than 450 non-trivial question with an unambiguous answer, requiring different levels of tooling and autonomy to solve."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gaia'. Error: Path opencompass/gaia is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1266",
                    "name": "BABILong",
                    "version": "1.0.0",
                    "description": "BABILong is designed to test language models' ability to reason across facts distributed in extremely long documents. It contains a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. ",
                    "url": "opencompass/opencompass_1266.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1266",
                    "sample_count": 1000,
                    "traits": [
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1266",
                        "name": "BABILong",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "长文本",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/booydar/babilong",
                        "paperLink": "https://arxiv.org/abs/2406.10149",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "302",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 14:14:54",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 14:14:54",
                        "createDate": "2024-12-25 14:14:32",
                        "desc": {
                            "cn": "BABILong旨在测试语言模型对分布在极长文档中的事实进行推理的能力，涵盖事实链接、简单归纳、推导、计数和处理列表/集合等20种各类推理任务。",
                            "en": "BABILong is designed to test language models' ability to reason across facts distributed in extremely long documents. It contains a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/babilong'. Error: No data found in /home/budadmin/.cache/opencompass/./data/babilong/data/ for split 'test'. Please check if the split exists and contains data files.\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1123",
                    "name": "Crows-Pairs",
                    "version": "1.0.0",
                    "description": "CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping.",
                    "url": "opencompass/opencompass_1123.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1123",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1123",
                        "name": "Crows-Pairs",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/nyu-mll/crows-pairs",
                        "paperLink": "https://arxiv.org/pdf/2010.00133v1",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "271",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-11 20:24:47",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-11 20:24:47",
                        "createDate": "2024-10-11 13:33:41",
                        "desc": {
                            "cn": "CrowS-Pairs 包含 1508 个示例，涵盖与九种偏见类型相关的刻板印象，例如种族、宗教和年龄。在 CrowS-Pairs 中，模型会接收到两句话：一句是更具刻板印象的，另一句则是较少刻板印象的。",
                            "en": "CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/crows_pairs'. Error: Path opencompass/crows_pairs is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1542",
                    "name": "StructFlowBench",
                    "version": "1.0.0",
                    "description": "StructFlowBench is a structurally annotated multi-turn benchmark that leverages a structure-driven generation paradigm to enhance the simulation of complex dialogue scenarios.",
                    "url": "opencompass/opencompass_1542.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1542",
                    "sample_count": 1000,
                    "traits": [
                        "Instruct"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1542",
                        "name": "StructFlowBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "指令跟随",
                                "en": "Instruct"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MLGroupJLU/StructFlowBench",
                        "paperLink": "https://arxiv.org/abs/2502.14494",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "252",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-02-25 19:09:02",
                        "supportOnlineEval": false,
                        "updateDate": "2025-02-25 19:09:02",
                        "createDate": "2025-02-25 12:46:25",
                        "desc": {
                            "cn": "StructFlowBench，这是一个包含155条数据的结构化标注多轮基准，它利用结构驱动生成范式来增强复杂对话场景的模拟。",
                            "en": "StructFlowBench is a structurally annotated multi-turn benchmark that leverages a structure-driven generation paradigm to enhance the simulation of complex dialogue scenarios."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/structflowbench'. Error: Path opencompass/structflowbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1701",
                    "name": "WritingBench",
                    "version": "1.0.0",
                    "description": "WritingBench: A Comprehensive Benchmark for Generative Writing",
                    "url": "opencompass/opencompass_1701.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1701",
                    "sample_count": 1000,
                    "traits": [
                        "Long-Context",
                        "Creation"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1701",
                        "name": "WritingBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "长文本",
                                "en": "Long-Context"
                            },
                            {
                                "cn": "创作",
                                "en": "Creation"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/X-PLUG/WritingBench",
                        "paperLink": "https://arxiv.org/pdf/2503.05244",
                        "officialWebsiteLink": "https://modelscope.cn/studios/iic/DeepWriting",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "52302232",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-qSFSXYARf"
                        },
                        "lookNum": "246",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-03 19:45:43",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-03 19:45:43",
                        "createDate": "2025-04-02 15:22:21",
                        "desc": {
                            "cn": "WritingBench: A Comprehensive Benchmark for Generative Writing",
                            "en": "WritingBench: A Comprehensive Benchmark for Generative Writing"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/writingbench'. Error: Path opencompass/writingbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1132",
                    "name": "TabFact",
                    "version": "1.0.0",
                    "description": "TabFac consists of 117,854 manually annotated statements with regard to 16,573 Wikipedia tables, their relations are classified as ENTAILED and REFUTED. ",
                    "url": "opencompass/opencompass_1132.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1132",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1132",
                        "name": "TabFact",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/wenhuchen/Table-Fact-Checking",
                        "paperLink": "https://arxiv.org/pdf/1909.02164v5",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "245",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-16 11:03:09",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-16 11:03:09",
                        "createDate": "2024-10-12 11:35:24",
                        "desc": {
                            "cn": "TabFac 包含 117,854 条手动标注的语句，涉及 16,573 个维基百科表格，是第一个评估结构化数据上语言推理的数据集，涉及在符号和语言两个方面的混合推理能力。",
                            "en": "TabFac consists of 117,854 manually annotated statements with regard to 16,573 Wikipedia tables, their relations are classified as ENTAILED and REFUTED. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/tabfact'. Error: Path opencompass/tabfact is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1835",
                    "name": "AudioJailbreak",
                    "version": "1.0.0",
                    "description": "LAMs face jailbreak risks. AJailBench, our new benchmark, reveals leading LAMs lack robustness. Subtle audio perturbations significantly degrade their safety. We release AJailBench for research.",
                    "url": "opencompass/opencompass_1835.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1835",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Language",
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1835",
                        "name": "AudioJailbreak",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "语言",
                                "en": "Language"
                            },
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "audio",
                                "en": "audio"
                            },
                            {
                                "cn": "jailbreak",
                                "en": "jailbreak"
                            },
                            {
                                "cn": "LAM",
                                "en": "LAM"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/mbzuai-nlp/AudioJailbreak",
                        "paperLink": "https://arxiv.org/abs/2505.15406",
                        "officialWebsiteLink": "https://huggingface.co/datasets/MBZUAI/AudioJailbreak",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "53309115",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-2Fm1XxTnD"
                        },
                        "lookNum": "239",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-05-27 12:51:28",
                        "supportOnlineEval": false,
                        "updateDate": "2025-05-27 12:51:28",
                        "createDate": "2025-05-24 12:28:39",
                        "desc": {
                            "cn": "LAM 面临越狱风险。我们新的基准测试 AJailBench 揭示，领先的 LAM 缺乏稳健性。细微的音频干扰会显著降低其安全性。我们发布 AJailBench 进行研究。",
                            "en": "LAMs face jailbreak risks. AJailBench, our new benchmark, reveals leading LAMs lack robustness. Subtle audio perturbations significantly degrade their safety. We release AJailBench for research."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/audiojailbreak'. Error: Path opencompass/audiojailbreak is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1836",
                    "name": "PashtoOCR",
                    "version": "1.0.0",
                    "description": "PsOCR is a large-scale synthetic dataset for Optical Character Recognition in low-resource Pashto language.",
                    "url": "opencompass/opencompass_1836.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1836",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding",
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1836",
                        "name": "PashtoOCR",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            },
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "OCR",
                                "en": "OCR"
                            },
                            {
                                "cn": "NLP",
                                "en": "NLP"
                            },
                            {
                                "cn": "Computer Vision",
                                "en": "Computer Vision"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/zirak-ai/PashtoOCR",
                        "paperLink": "https://arxiv.org/abs/2505.10055",
                        "officialWebsiteLink": "https://zirak.ai/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "53006610",
                            "name": "ijazulhaq",
                            "avatar": null,
                            "nickname": "ijazulhaq"
                        },
                        "lookNum": "225",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-05-27 12:52:04",
                        "supportOnlineEval": false,
                        "updateDate": "2025-05-27 12:52:04",
                        "createDate": "2025-05-26 09:43:14",
                        "desc": {
                            "cn": "PsOCR is a large-scale synthetic dataset for Optical Character Recognition in low-resource Pashto language.",
                            "en": "PsOCR is a large-scale synthetic dataset for Optical Character Recognition in low-resource Pashto language."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/pashtoocr'. Error: Path opencompass/pashtoocr is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1335",
                    "name": "LTMbenchmark",
                    "version": "1.0.0",
                    "description": "LTMbenchmark assess the long-term memory, continual learning, and information integration capabilities of the agents via dynamic conversational tasks. ",
                    "url": "opencompass/opencompass_1335.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1335",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1335",
                        "name": "LTMbenchmark",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/GoodAI/goodai-ltm-benchmark",
                        "paperLink": "https://arxiv.org/abs/2409.20222",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "225",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:05:39",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:05:39",
                        "createDate": "2025-01-03 16:10:04",
                        "desc": {
                            "cn": "LTMbenchmark通过动态对话任务评估智能体的长期记忆、持续学习和信息集成能力。",
                            "en": "LTMbenchmark assess the long-term memory, continual learning, and information integration capabilities of the agents via dynamic conversational tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ltmbenchmark'. Error: Path opencompass/ltmbenchmark is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1246",
                    "name": "LiveBench",
                    "version": "1.0.0",
                    "description": "LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-free versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. ",
                    "url": "opencompass/opencompass_1246.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1246",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1246",
                        "name": "LiveBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/livebench/livebench",
                        "paperLink": "https://arxiv.org/abs/2406.19314",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "224",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-22 18:09:35",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-22 18:09:35",
                        "createDate": "2024-12-20 19:35:51",
                        "desc": {
                            "cn": "LiveBench是一个LLM基准测试，涵盖数学、编码、推理、语言、指令遵循和数据分析，包含基于最近发布的数学竞赛、arXiv 论文、新闻文章和数据集的问题，及经典基准测试（如 Big-Bench Hard、AMPS 和 IFEval）的更难、无污染的任务。",
                            "en": "LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-free versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/livebench'. Error: Path opencompass/livebench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1252",
                    "name": "RE-Bench",
                    "version": "1.0.0",
                    "description": "RE-Bench (Research Engineering Benchmark, v1) consists of 7 challenging, open-ended ML research engineering environments and data from 71 8-hour attempts by 61 distinct human experts.",
                    "url": "opencompass/opencompass_1252.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1252",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Code",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1252",
                        "name": "RE-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "代码",
                                "en": "Code"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/METR/ai-rd-tasks/tree/main",
                        "paperLink": "https://arxiv.org/abs/2411.15114",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "223",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-30 16:30:41",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-30 16:30:41",
                        "createDate": "2024-12-30 16:24:14",
                        "desc": {
                            "cn": "RE-Bench用于评估AI智能体研发的自动化能力，它由61位人类专家71次在7个具有挑战性的开放式ML研究工程环境中的8小时尝试的数据组成。",
                            "en": "RE-Bench (Research Engineering Benchmark, v1) consists of 7 challenging, open-ended ML research engineering environments and data from 71 8-hour attempts by 61 distinct human experts."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/re_bench'. Error: Path opencompass/re_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1322",
                    "name": "InfiBench",
                    "version": "1.0.0",
                    "description": "InfiBench is a large-scale freeform question-answering (QA) benchmark for code to our knowledge, comprising 234 carefully selected high-quality Stack Overflow questions that span across 15 programming languages.",
                    "url": "opencompass/opencompass_1322.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1322",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1322",
                        "name": "InfiBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/infi-coder/infibench-evaluation-harness/",
                        "paperLink": "https://arxiv.org/abs/2404.07940",
                        "officialWebsiteLink": "https://infi-coder.github.io/infibench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "219",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:05:42",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:05:42",
                        "createDate": "2024-12-31 15:59:24",
                        "desc": {
                            "cn": "InfiBench用于评测LLM回答代码相关问题的能力，包括涵盖15种编程语言的234个精心挑选的高质量Stack Overflow问题。",
                            "en": "InfiBench is a large-scale freeform question-answering (QA) benchmark for code to our knowledge, comprising 234 carefully selected high-quality Stack Overflow questions that span across 15 programming languages."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/infibench'. Error: Path opencompass/infibench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1641",
                    "name": "MedAgents-Bench",
                    "version": "1.0.0",
                    "description": "MedAgentsBench features 862 challenging medical questions from seven datasets, focusing on cases where models struggle. It emphasizes multi-step clinical reasoning and addresses limitations in existing benchmarks by eliminating simple questions and standardizing evaluation protocols.",
                    "url": "opencompass/opencompass_1641.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1641",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning",
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1641",
                        "name": "MedAgents-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Medical",
                                "en": "Medical"
                            },
                            {
                                "cn": "Agent",
                                "en": "Agent"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/gersteinlab/medagents-benchmark",
                        "paperLink": "https://arxiv.org/abs/2503.07459",
                        "officialWebsiteLink": "https://github.com/gersteinlab/medagents-benchmark",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "26204315",
                            "name": null,
                            "avatar": null,
                            "nickname": "super-dainiu"
                        },
                        "lookNum": "215",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-17 13:59:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-17 13:59:33",
                        "createDate": "2025-03-17 12:44:07",
                        "desc": {
                            "cn": "MedAgentsBench是一个专注于复杂医学推理的基准测试，从七个医学数据集中精选了862个挑战性问题。这些数据集包括MedQA、PubMedQA、MedMCQA、MedBullets、MedExQA、MedXpertQA和MMLU/MMLU-Pro，涵盖了从医学执照考试到研究文献的多种医学问题。\n该基准选择少于50%模型能正确回答的问题，确保医学知识领域全面覆盖，并优先选择需要多步临床推理的问题。这解决了现有评估中简单问题普遍存在、评估协议不一致，以及缺乏性能-成本-时间分析的局限。",
                            "en": "MedAgentsBench features 862 challenging medical questions from seven datasets, focusing on cases where models struggle. It emphasizes multi-step clinical reasoning and addresses limitations in existing benchmarks by eliminating simple questions and standardizing evaluation protocols."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medagents_bench'. Error: Path opencompass/medagents_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2217",
                    "name": "SmartBench",
                    "version": "1.0.0",
                    "description": "SmartBench is the first benchmark specifically designed to evaluate the capabilities of on-device large language models in smartphone scenarios.",
                    "url": "opencompass/opencompass_2217.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2217",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2217",
                        "name": "SmartBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": " Capabilities of on-device LLMs",
                                "en": " Capabilities of on-device LLMs"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/vivo-ai-lab/SmartBench",
                        "paperLink": "https://arxiv.org/abs/2503.06029",
                        "officialWebsiteLink": "",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "92500767",
                            "name": null,
                            "avatar": null,
                            "nickname": "浩屿💖楠"
                        },
                        "lookNum": "215",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-10-15 16:26:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-10-15 16:26:55",
                        "createDate": "2025-10-10 17:50:51",
                        "desc": {
                            "cn": "SmartBench 是首个面向智能手机终端大模型能力评估的基准，基于手机厂商提供的功能将其划分为五类共20项任务，涵盖文本摘要、问答、信息抽取、内容创作和通知管理等场景。它提供高质量数据集与定制化评估标准，旨在推动终端大模型在移动应用中的标准化评估与发展。",
                            "en": "SmartBench is the first benchmark specifically designed to evaluate the capabilities of on-device large language models in smartphone scenarios."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/smartbench'. Error: Path opencompass/smartbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1116",
                    "name": "AQUA-RAT",
                    "version": "1.0.0",
                    "description": "AQUA-RAT contains the algebraic word problems. The dataset consists of about 100,000 algebraic word problems with natural language rationales.",
                    "url": "opencompass/opencompass_1116.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1116",
                    "sample_count": 1000,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1116",
                        "name": "AQUA-RAT",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/google-deepmind/AQuA",
                        "paperLink": "https://arxiv.org/pdf/1705.04146",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "214",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-11 19:59:29",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-11 19:59:29",
                        "createDate": "2024-10-10 14:20:23",
                        "desc": {
                            "cn": "AQUA-RAT 包含代数文字问题。该数据集由约 100,000 道带有自然语言推理的代数文字问题组成。",
                            "en": "AQUA-RAT contains the algebraic word problems. The dataset consists of about 100,000 algebraic word problems with natural language rationales."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/aqua_rat'. Error: Path opencompass/aqua_rat is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1562",
                    "name": "BenchMAX",
                    "version": "1.0.0",
                    "description": "BenchMAX is a comprehensive, high-quality, and multiway parallel multilingual benchmark comprising 10 tasks designed to assess crucial capabilities across 17 diverse language.",
                    "url": "opencompass/opencompass_1562.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1562",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1562",
                        "name": "BenchMAX",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/CONE-MT/BenchMAX",
                        "paperLink": "https://arxiv.org/abs/2502.07346",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "213",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-03 10:56:52",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-03 10:56:52",
                        "createDate": "2025-02-27 11:05:13",
                        "desc": {
                            "cn": "BenchMAX 是一个全面、高质量的多向并行多语言基准，包含 10 个任务，旨在评估 17 种不同语言的关键能力。",
                            "en": "BenchMAX is a comprehensive, high-quality, and multiway parallel multilingual benchmark comprising 10 tasks designed to assess crucial capabilities across 17 diverse language."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/benchmax'. Error: Path opencompass/benchmax is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1576",
                    "name": "PhysReason",
                    "version": "1.0.0",
                    "description": "PhysReason is a comprehensive physics-based reasoning benchmark consisting of 1,200 physics problems spanning multiple domains, with a focus on both knowledge-based (25%) and reasoning-based (75%) questions.",
                    "url": "opencompass/opencompass_1576.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1576",
                    "sample_count": 1000,
                    "traits": [
                        "Examination",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1576",
                        "name": "PhysReason",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "学科",
                                "en": "Examination"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2502.12054",
                        "officialWebsiteLink": "https://dxzxy12138.github.io/PhysReason/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "212",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-04 16:59:45",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-04 16:59:45",
                        "createDate": "2025-03-03 16:02:34",
                        "desc": {
                            "cn": "PhysReason 是一个包含 1,200 个物理问题的综合物理推理基准，涵盖多个领域，重点关注基于知识（25%）和推理（75%）的问题。",
                            "en": "PhysReason is a comprehensive physics-based reasoning benchmark consisting of 1,200 physics problems spanning multiple domains, with a focus on both knowledge-based (25%) and reasoning-based (75%) questions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/physreason'. Error: Path opencompass/physreason is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1283",
                    "name": "P-MMEval",
                    "version": "1.0.0",
                    "description": "P-MMEval is a comprehensive multilingual multitask benchmark, covering effective fundamental and capability-specialized datasets.",
                    "url": "opencompass/opencompass_1283.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1283",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1283",
                        "name": "P-MMEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2411.09116",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "43209637",
                            "name": "Qwen",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/43209637-2eaaac15-94a9-4382-9047-16ae007f33f6.png",
                            "nickname": "Qwen"
                        },
                        "lookNum": "210",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 19:59:39",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 19:59:39",
                        "createDate": "2024-12-25 19:57:03",
                        "desc": {
                            "cn": "P-MMEval是一个全面的多语言多任务基准，涵盖了高效的基础和专项能力数据集。",
                            "en": "P-MMEval is a comprehensive multilingual multitask benchmark, covering effective fundamental and capability-specialized datasets."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/p_mmeval'. Error: Path opencompass/p_mmeval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1072",
                    "name": "xCodeEval",
                    "version": "1.0.0",
                    "description": "xCodeEval is the largest executable multilingual multitask benchmark to date consisting of 25M document-level coding examples (16.5B tokens) from about 7.5K unique problems covering up to 11 programming languages with execution-level parallelism.",
                    "url": "opencompass/opencompass_1072.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1072",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1072",
                        "name": "xCodeEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "ACL 2024",
                                "en": "ACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ntunlp/xCodeEval",
                        "paperLink": "https://arxiv.org/pdf/2303.03004",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50187454",
                            "name": "NTU-NLP",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50187454-16b303e1-3675-4b47-b9ec-83f20eef9b46.png",
                            "nickname": "NTU-NLP"
                        },
                        "lookNum": "210",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-09-27 16:53:40",
                        "supportOnlineEval": false,
                        "updateDate": "2024-09-27 16:53:40",
                        "createDate": "2024-09-27 12:40:47",
                        "desc": {
                            "cn": "xCodeEval 是迄今为止最大的可执行多语言多任务基准，包含 2500 万个文档级编码示例（165 亿个标记），来自约 7500 个独特问题，涵盖多达 11 种编程语言。它包括 7 个任务，涉及代码理解、生成、翻译和检索。",
                            "en": "xCodeEval is the largest executable multilingual multitask benchmark to date consisting of 25M document-level coding examples (16.5B tokens) from about 7.5K unique problems covering up to 11 programming languages with execution-level parallelism."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/xcodeeval'. Error: Path opencompass/xcodeeval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1251",
                    "name": "PlanBench",
                    "version": "1.0.0",
                    "description": "PlanBench is an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. ",
                    "url": "opencompass/opencompass_1251.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1251",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1251",
                        "name": "PlanBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/karthikv792/LLMs-Planning",
                        "paperLink": "https://arxiv.org/abs/2206.10498",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "207",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-30 16:29:45",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-30 16:29:45",
                        "createDate": "2024-12-30 16:23:47",
                        "desc": {
                            "cn": "PlanBench用于评估LLM的规划能力，基于自动化规划社区（尤其是在国际规划竞赛）中涉及的各种领域来测试大模型在规划或推理行动和变更方面的能力。",
                            "en": "PlanBench is an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/planbench'. Error: Path opencompass/planbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1748",
                    "name": "VisualPuzzles",
                    "version": "1.0.0",
                    "description": "VisualPuzzles is a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of 1168 diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. ",
                    "url": "opencompass/opencompass_1748.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1748",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1748",
                        "name": "VisualPuzzles",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/neulab/VisualPuzzles",
                        "paperLink": "https://arxiv.org/abs/2504.10342",
                        "officialWebsiteLink": "https://neulab.github.io/VisualPuzzles/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "53009543",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-oErCdNaBY"
                        },
                        "lookNum": "204",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-18 17:11:40",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-18 17:11:40",
                        "createDate": "2025-04-17 21:40:00",
                        "desc": {
                            "cn": "LLM 能考公务员吗？我们做了个测试…\n\n近年来，大模型（LLM）的能力突飞猛进，似乎“越来越聪明”了。但有一个关键问题仍然摆在眼前：\n🤔 它们真的会“推理”吗？\n\n🚀 我们设计了一个名为 VisualPuzzles 🧩 的全新数据集，专门用来回答这个问题：\n脱离专业知识的支持，大模型能靠逻辑本身解题吗？\n我们从多个来源精心挑选或改编了 1168 道图文逻辑题，其中一个重要来源便是中国国家公务员考试行测中的逻辑推理题（没错，真·考公难度）🎯",
                            "en": "VisualPuzzles is a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of 1168 diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/visualpuzzles'. Error: Path opencompass/visualpuzzles is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1017",
                    "name": "Yue_Benchmark",
                    "version": "1.0.0",
                    "description": "The benchmarks introduced for evaluating large language models (LLMs) on Cantonese include Yue-TruthfulQA, Yue-GSM8K, Yue-ARC-C, Yue-MMLU, and Yue-TRANS. Each of these benchmarks focuses on different aspects of language understanding and generation in Cantonese, offering a comprehensive means of assessing the capabilities of LLMs in handling this language.",
                    "url": "opencompass/opencompass_1017.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1017",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1017",
                        "name": "Yue_Benchmark",
                        "emoji": "🐲",
                        "dimensions": [
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "factual generation",
                                "en": "factual generation"
                            },
                            {
                                "cn": "complex reasoning",
                                "en": "complex reasoning"
                            },
                            {
                                "cn": "general knowledge",
                                "en": "general knowledge"
                            },
                            {
                                "cn": "mathematical logic",
                                "en": "mathematical logic"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/jiangjyjy/Yue-Benchmark",
                        "paperLink": "https://arxiv.org/abs/2408.16756",
                        "officialWebsiteLink": "https://huggingface.co/datasets/BillBao/Yue-Benchmark",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50157836",
                            "name": null,
                            "avatar": null,
                            "nickname": "enemy"
                        },
                        "lookNum": "204",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-30 11:30:05",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-30 11:30:05",
                        "createDate": "2024-12-28 23:41:20",
                        "desc": {
                            "cn": "Yue_Benchmark用于评估粤语大型语言模型（LLMs）。该评测集包含：Yue TruthtyQA、Yue-GSM8K、Yue-ARC-C、Yue MMLU和Yue TRANS，侧重于粤语语言理解和生成的不同方面，为评估LLM粤语能力提供了一种全面的方法。",
                            "en": "The benchmarks introduced for evaluating large language models (LLMs) on Cantonese include Yue-TruthfulQA, Yue-GSM8K, Yue-ARC-C, Yue-MMLU, and Yue-TRANS. Each of these benchmarks focuses on different aspects of language understanding and generation in Cantonese, offering a comprehensive means of assessing the capabilities of LLMs in handling this language."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/yue_benchmark'. Error: Path opencompass/yue_benchmark is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1238",
                    "name": "LINGOLY",
                    "version": "1.0.0",
                    "description": "Aiming at evaluating LLM's advanced reasoning abilities, LingOly is composed of  olympiad-level linguistic reasoning puzzles in low-resource and extinct languages. It covers more than 90 mostly low-resource languages, and contains 1,133 problems across 6 formats and 5 levels of human difficulty.",
                    "url": "opencompass/opencompass_1238.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1238",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1238",
                        "name": "LINGOLY",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/am-bean/lingOly",
                        "paperLink": "https://arxiv.org/abs/2406.06196",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "193",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 11:46:22",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 11:46:22",
                        "createDate": "2024-12-20 14:03:59",
                        "desc": {
                            "cn": "LingOly由低资源和已灭绝语言的奥林匹克级别语言推理谜题组成，用于评估大语言模型的高级推理能力。其中涵盖了90多种语言，共有1133个涉及6种格式和5个人工难度级别的问题。",
                            "en": "Aiming at evaluating LLM's advanced reasoning abilities, LingOly is composed of  olympiad-level linguistic reasoning puzzles in low-resource and extinct languages. It covers more than 90 mostly low-resource languages, and contains 1,133 problems across 6 formats and 5 levels of human difficulty."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/lingoly'. Error: Path opencompass/lingoly is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1355",
                    "name": "HallusionBench",
                    "version": "1.0.0",
                    "description": "HallusionBench is a comprehensive benchmark designed for the evaluation of image-context reasoning, comprising 346 images paired with 1129 questions.",
                    "url": "opencompass/opencompass_1355.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1355",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1355",
                        "name": "HallusionBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/tianyi-lab/HallusionBench",
                        "paperLink": "https://arxiv.org/abs/2310.14566",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "192",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 11:27:39",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 11:27:39",
                        "createDate": "2025-01-09 20:08:24",
                        "desc": {
                            "cn": "HallusionBench是一个专为评估图像上下文推理而设计的综合基准测试，包括346张图像和1129个问题。",
                            "en": "HallusionBench is a comprehensive benchmark designed for the evaluation of image-context reasoning, comprising 346 images paired with 1129 questions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/hallusionbench'. Error: Path opencompass/hallusionbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1643",
                    "name": "Creation-MMBench",
                    "version": "1.0.0",
                    "description": "A multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs. It features three main aspects: 1. Comprehensive Creation Benchmark for MLLM and LLM. 2. Robust Evaluation Methodology. 3. Attractive Experiment Insight. ",
                    "url": "opencompass/opencompass_1643.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1643",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Long-Context",
                        "Creation"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1643",
                        "name": "Creation-MMBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "长文本",
                                "en": "Long-Context"
                            },
                            {
                                "cn": "创作",
                                "en": "Creation"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 3,
                        "githubLink": "https://github.com/open-compass/Creation-MMBench",
                        "paperLink": "",
                        "officialWebsiteLink": "https://open-compass.github.io/Creation-MMBench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "40047277",
                            "name": "fangxy-09",
                            "avatar": "https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKFASuxOhldeP1BsgbQewWr71yNBNAehdVp5Qqrxq0D6hQ0libbrYZs9n5GVtoicy4uOvtNrmebicSKQ/132",
                            "nickname": "FangXinyu-0913"
                        },
                        "lookNum": "188",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-05-06 11:00:38",
                        "supportOnlineEval": false,
                        "updateDate": "2025-05-06 11:00:38",
                        "createDate": "2025-04-30 12:29:15",
                        "desc": {
                            "cn": "专为评估 多模态大模型 的创作能力而设计的多模态基准。采用两个不同指标对模型的基础感知能力和深层次视觉创作能力进行评估，采用GPT-4o作为评判模型进行评估。",
                            "en": "A multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs. It features three main aspects: 1. Comprehensive Creation Benchmark for MLLM and LLM. 2. Robust Evaluation Methodology. 3. Attractive Experiment Insight. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/creation_mmbench'. Error: Path opencompass/creation_mmbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1349",
                    "name": "CyberSecEval",
                    "version": "1.0.0",
                    "description": "CyberSecEval is a comprehensive benchmark developed to help bolster the cybersecurity of LLMs. It provides a thorough evaluation in two crucial security domains: the propensity to generate insecure code and the level of compliance when asked to assist in cyberattacks.",
                    "url": "opencompass/opencompass_1349.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1349",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1349",
                        "name": "CyberSecEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks",
                        "paperLink": "https://arxiv.org/abs/2312.04724",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "186",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:31",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:31",
                        "createDate": "2025-01-09 12:17:41",
                        "desc": {
                            "cn": "CyberSecEval旨在评估LLM的安全性，聚焦于大模型生成不安全代码的倾向以及当被要求协助网络攻击时的合规性水平。",
                            "en": "CyberSecEval is a comprehensive benchmark developed to help bolster the cybersecurity of LLMs. It provides a thorough evaluation in two crucial security domains: the propensity to generate insecure code and the level of compliance when asked to assist in cyberattacks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cyberseceval'. Error: Path opencompass/cyberseceval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1546",
                    "name": "CodeCriticBench",
                    "version": "1.0.0",
                    "description": "CodeCriticBench assesses LLMs' critiquing ability in code generation and QA tasks. Covering 10 criteria, it features a 4.3k-samples dataset with three difficulty levels and balanced distribution.",
                    "url": "opencompass/opencompass_1546.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1546",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1546",
                        "name": "CodeCriticBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/multimodal-art-projection/CodeCriticBench",
                        "paperLink": "https://arxiv.org/abs/2502.16614",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "183",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-02-26 18:41:24",
                        "supportOnlineEval": false,
                        "updateDate": "2025-02-26 18:41:24",
                        "createDate": "2025-02-26 11:00:09",
                        "desc": {
                            "cn": "CodeCriticBench ，旨在系统地评估LLMs在代码生成和代码问答任务中的批评能力。其涵盖 10 个不同的标准，数据集根据难度分为三个等级，共包含4.3k个样本，确保了难度级别的平衡分布。",
                            "en": "CodeCriticBench assesses LLMs' critiquing ability in code generation and QA tasks. Covering 10 criteria, it features a 4.3k-samples dataset with three difficulty levels and balanced distribution."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/codecriticbench'. Error: Path opencompass/codecriticbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1564",
                    "name": "EmbodiedBench",
                    "version": "1.0.0",
                    "description": "EmbodiedBench is a comprehensive benchmark designed to evaluate Multi-modal Large Language Models (MLLMs) as embodied agents.",
                    "url": "opencompass/opencompass_1564.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1564",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1564",
                        "name": "EmbodiedBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/EmbodiedBench/EmbodiedBench",
                        "paperLink": "https://arxiv.org/abs/2502.09560",
                        "officialWebsiteLink": "https://embodiedbench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "183",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-03 11:00:23",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-03 11:00:23",
                        "createDate": "2025-02-27 11:25:40",
                        "desc": {
                            "cn": "EmbodiedBench，这是一个旨在评估多模态大型语言模型作为具身智能体的全面基准。",
                            "en": "EmbodiedBench is a comprehensive benchmark designed to evaluate Multi-modal Large Language Models (MLLMs) as embodied agents."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/embodiedbench'. Error: Path opencompass/embodiedbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1736",
                    "name": "MMTB",
                    "version": "1.0.0",
                    "description": "Large language models (LLMs) demonstrate strong potential as agents for tool invocation due to their advanced comprehension and planning capabilities. Users increasingly rely on LLM-based agents to solve complex missions through iterative interactions. However, existing benchmarks predominantly acce",
                    "url": "opencompass/opencompass_1736.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1736",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1736",
                        "name": "MMTB",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/yupeijei1997/MMTB",
                        "paperLink": "https://arxiv.org/abs/2504.02623",
                        "officialWebsiteLink": "https://harrywgcn.github.io/mmtb-leaderboard/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "90306731",
                            "name": null,
                            "avatar": null,
                            "nickname": "yupeijei1997"
                        },
                        "lookNum": "182",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:08:39",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:08:39",
                        "createDate": "2025-04-11 15:30:25",
                        "desc": {
                            "cn": "大型语言模型（LLM）凭借其先进的理解能力和规划能力，在作为工具调用的智能体方面展现出了巨大的潜力。用户越来越依赖基于大型语言模型的智能体，通过迭代交互来解决复杂任务。 然而，现有的基准测试主要是在单任务场景中评估智能体，无法体现现实世界的复杂性。为了填补这一空白，我们提出了 Multi-Mission Tool Bench 测试。在这个基准测试中，每个测试用例都包含多个相互关联的任务。这种设计要求智能体能够动态适应不断变化的需求。此外，所提出的基准测试探索了在固定任务数量下所有可能的任务切换模式。具体而言，我们提出了一个多智能体数据生成框架来构建这个基准测试。我们还提出了一种利用动态决策树来",
                            "en": "Large language models (LLMs) demonstrate strong potential as agents for tool invocation due to their advanced comprehension and planning capabilities. Users increasingly rely on LLM-based agents to solve complex missions through iterative interactions. However, existing benchmarks predominantly acce"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmtb'. Error: Path opencompass/mmtb is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1117",
                    "name": "NaturalProofs",
                    "version": "1.0.0",
                    "description": "NATURALPROOFS is a multi-domain corpus of mathematical statements and their proofs, written in natural mathematical language. NATURALPROOFS unifies broad coverage, deep coverage, and low-resource mathematical sources, allowing for evaluating both in-distribution and zero-shot generalization.",
                    "url": "opencompass/opencompass_1117.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1117",
                    "sample_count": 1000,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1117",
                        "name": "NaturalProofs",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/wellecks/naturalproofs#naturalproofs-dataset",
                        "paperLink": "https://arxiv.org/pdf/2104.01112v2",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "181",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-11 19:59:32",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-11 19:59:32",
                        "createDate": "2024-10-10 14:39:51",
                        "desc": {
                            "cn": "NATURALPROOFS 是一个多领域的数学语句及其证明的语料库，采用自然数学语言编写。整合了广泛覆盖、深入覆盖和低资源数学来源，便于评估分布内和零样本泛化的能力。",
                            "en": "NATURALPROOFS is a multi-domain corpus of mathematical statements and their proofs, written in natural mathematical language. NATURALPROOFS unifies broad coverage, deep coverage, and low-resource mathematical sources, allowing for evaluating both in-distribution and zero-shot generalization."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/naturalproofs'. Error: Path opencompass/naturalproofs is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1334",
                    "name": "MMDU",
                    "version": "1.0.0",
                    "description": "MMDU is intended for evaluating the multi-image multi-turn dialogue capabilities. It comprises 110 high-quality multi-image multi-turn dialogues with more than 1600 questions, each accompanied by detailed long-form answers. ",
                    "url": "opencompass/opencompass_1334.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1334",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1334",
                        "name": "MMDU",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Liuziyu77/MMDU",
                        "paperLink": "https://arxiv.org/abs/2406.11833",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "179",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 16:19:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 16:19:55",
                        "createDate": "2025-01-03 14:51:38",
                        "desc": {
                            "cn": "MMDU用于评估大型视觉语言模型的多图像多轮对话能力，包含110个高质量的多图像多轮对话，由1600多个附有详细的长篇答案的问题组成。",
                            "en": "MMDU is intended for evaluating the multi-image multi-turn dialogue capabilities. It comprises 110 high-quality multi-image multi-turn dialogues with more than 1600 questions, each accompanied by detailed long-form answers. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmdu'. Error: Path opencompass/mmdu is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1146",
                    "name": "BUST",
                    "version": "1.0.0",
                    "description": "BUST is a comprehensive benchmark for evaluating synthetic text detectors, focusing on their effectiveness against outputs from various Large Language Models (LLMs). ",
                    "url": "opencompass/opencompass_1146.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1146",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1146",
                        "name": "BUST",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NAACL 2024",
                                "en": "NAACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/IDSIA-NLP/BUST",
                        "paperLink": "https://aclanthology.org/2024.naacl-long.444.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50126080",
                            "name": "IDSIA-NLP",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50126080-9bda56f6-aaad-43e3-86d5-1278eb268d20.png",
                            "nickname": "OpenXLab-rR7CGPrkS"
                        },
                        "lookNum": "179",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-27 17:23:25",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-27 17:23:25",
                        "createDate": "2024-10-15 13:46:56",
                        "desc": {
                            "cn": "BUST 是一个综合基准，旨在评估合成文本检测器，BUST 使用多种指标来评估检测器，包括语言特征、可读性和作者态度。",
                            "en": "BUST is a comprehensive benchmark for evaluating synthetic text detectors, focusing on their effectiveness against outputs from various Large Language Models (LLMs). "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/bust'. Error: Path opencompass/bust is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1239",
                    "name": "CVQA",
                    "version": "1.0.0",
                    "description": "CVQA is a new culturally-diverse multilingual Visual Question Answering benchmark, designed to cover a rich set of languages and cultures. It includes culturally-driven images and 10k questions from across 30 countries on 4 continents.",
                    "url": "opencompass/opencompass_1239.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1239",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1239",
                        "name": "CVQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2406.05967",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "178",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-30 16:30:03",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-30 16:30:03",
                        "createDate": "2024-12-30 16:18:03",
                        "desc": {
                            "cn": "CVQA是一种新的文化多元化多语言视觉问答基准，旨在涵盖丰富的语言和文化，包括来自四大洲30个国家/地区的文化向图像和10000个问题。",
                            "en": "CVQA is a new culturally-diverse multilingual Visual Question Answering benchmark, designed to cover a rich set of languages and cultures. It includes culturally-driven images and 10k questions from across 30 countries on 4 continents."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cvqa'. Error: Path opencompass/cvqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1582",
                    "name": "CodeMMLU",
                    "version": "1.0.0",
                    "description": "CodeMMLU is a comprehensive benchmark designed to evaluate the capabilities of large language models (LLMs) in coding and software knowledge. It builds upon the structure of multiple-choice question answering (MCQA) to cover a wide range of programming tasks and domains.",
                    "url": "opencompass/opencompass_1582.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1582",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1582",
                        "name": "CodeMMLU",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/FSoft-AI4Code/CodeMMLU",
                        "paperLink": "https://arxiv.org/abs/2406.15877",
                        "officialWebsiteLink": "https://fsoft-ai4code.github.io/codemmlu/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "175",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:46:54",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:46:54",
                        "createDate": "2025-03-04 11:00:59",
                        "desc": {
                            "cn": "CodeMMLU 是一个旨在评估大型语言模型（LLMs）在编码和软件知识方面能力的全面基准。它基于多项选择题回答（MCQA）的结构，涵盖了广泛的编程任务和领域，包括代码生成、缺陷检测、软件工程原则等。",
                            "en": "CodeMMLU is a comprehensive benchmark designed to evaluate the capabilities of large language models (LLMs) in coding and software knowledge. It builds upon the structure of multiple-choice question answering (MCQA) to cover a wide range of programming tasks and domains."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/codemmlu'. Error: Path opencompass/codemmlu is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1357",
                    "name": "MME",
                    "version": "1.0.0",
                    "description": "MME is a comprehensive MLLM evaluation benchmark. It measures both perception and cognition abilities on a total of 14 subtasks.",
                    "url": "opencompass/opencompass_1357.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1357",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1357",
                        "name": "MME",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation",
                        "paperLink": "https://arxiv.org/abs/2306.13394",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "174",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 16:03:56",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 16:03:56",
                        "createDate": "2025-01-09 20:56:27",
                        "desc": {
                            "cn": "MME是一个全面的多模态大模型评估基准，涵盖14 个考察感知和认知能力的子任务。",
                            "en": "MME is a comprehensive MLLM evaluation benchmark. It measures both perception and cognition abilities on a total of 14 subtasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mme'. Error: Path opencompass/mme is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1107",
                    "name": "StrategyQA",
                    "version": "1.0.0",
                    "description": "STRATEGYQA is a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy",
                    "url": "opencompass/opencompass_1107.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1107",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {
                        "gen": "StrategyQA_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1107",
                        "name": "StrategyQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/eladsegal/strategyqa",
                        "paperLink": "https://arxiv.org/pdf/2101.02235",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "173",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-09 20:03:28",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-09 20:03:28",
                        "createDate": "2024-10-09 17:54:13",
                        "desc": {
                            "cn": "STRATEGYQA 是一个问答基准，其中所需的推理步骤在问题中是隐含的，可以通过策略进行推断。",
                            "en": "STRATEGYQA is a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/strategyqa'. Error: Path opencompass/strategyqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2073",
                    "name": "J1-Bench",
                    "version": "1.0.0",
                    "description": "J1-Bench is an interactive and comprehensive legal benchmark where LLM agents engage in diverse legal scenarios, completing tasks through interactions with various participants under procedural rules.",
                    "url": "opencompass/opencompass_2073.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2073",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2073",
                        "name": "J1-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "legal agent",
                                "en": "legal agent"
                            },
                            {
                                "cn": "interactive benchmark",
                                "en": "interactive benchmark"
                            },
                            {
                                "cn": "legal benchmark",
                                "en": "legal benchmark"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/FudanDISC/J1Bench",
                        "paperLink": "https://arxiv.org/abs/2507.04037",
                        "officialWebsiteLink": "https://j1bench.github.io/",
                        "leaderboardLink": true,
                        "creatorInfo": {
                            "uid": "18905101",
                            "name": "ShengbinYue",
                            "avatar": null,
                            "nickname": "yueshengbin"
                        },
                        "lookNum": "171",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-21 17:12:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-21 17:12:55",
                        "createDate": "2025-07-21 15:07:28",
                        "desc": {
                            "cn": "J1-Bench 是一个交互式的综合法律基准，法律智能体在此参与各种法律情景，根据程序规则通过与不同参与者的互动完成指定的法律任务。",
                            "en": "J1-Bench is an interactive and comprehensive legal benchmark where LLM agents engage in diverse legal scenarios, completing tasks through interactions with various participants under procedural rules."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/j1_bench'. Error: Path opencompass/j1_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1617",
                    "name": "IFIR",
                    "version": "1.0.0",
                    "description": "IFIR is the first comprehensive benchmark designed to evaluate instruction-following information retrieval (IR) in expert domains. IFIR includes 2,426 high-quality examples and covers eight subsets across four specialized domains: finance, law, healthcare, and science literature.",
                    "url": "opencompass/opencompass_1617.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1617",
                    "sample_count": 1000,
                    "traits": [
                        "Instruct"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1617",
                        "name": "IFIR",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "指令跟随",
                                "en": "Instruct"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SighingSnow/IFIR",
                        "paperLink": "https://arxiv.org/abs/2503.04644",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "169",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-10 17:11:56",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-10 17:11:56",
                        "createDate": "2025-03-10 11:01:15",
                        "desc": {
                            "cn": " IFIR，这是第一个旨在评估专家领域指令跟随信息检索（IR）的综合基准。IFIR 包含 2,426 个高质量示例，涵盖四个专业领域（金融、法律、医疗保健和科学文献）的八个子集。",
                            "en": "IFIR is the first comprehensive benchmark designed to evaluate instruction-following information retrieval (IR) in expert domains. IFIR includes 2,426 high-quality examples and covers eight subsets across four specialized domains: finance, law, healthcare, and science literature."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ifir'. Error: Path opencompass/ifir is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1242",
                    "name": "AgentBoard",
                    "version": "1.0.0",
                    "description": "AgentBoard is tailored to analytical evaluation of LLM agents. It offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assessment of agents for multi-faceted analysis through interactive visualization.",
                    "url": "opencompass/opencompass_1242.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1242",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1242",
                        "name": "AgentBoard",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/hkust-nlp/AgentBoard/tree/main",
                        "paperLink": "https://arxiv.org/abs/2401.13178",
                        "officialWebsiteLink": "https://hkust-nlp.github.io/agentboard/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "164",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 11:46:29",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 11:46:29",
                        "createDate": "2024-12-20 17:19:44",
                        "desc": {
                            "cn": "AgentBoard专用于LLM Agent的分析评估，它提供了一个精细指标用于捕获增量进步，以及一个全面的评估工具包，能基于交互式可视化评估进行多方面分析。",
                            "en": "AgentBoard is tailored to analytical evaluation of LLM agents. It offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assessment of agents for multi-faceted analysis through interactive visualization."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/agentboard'. Error: Path opencompass/agentboard is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1113",
                    "name": "SVAMP",
                    "version": "1.0.0",
                    "description": "SVAMP includes one-unknown arithmetic word problems with grade level up to 4 by applying simple variations over word problems in an existing dataset. SVAMP further highlights the brittle nature of existing models when trained on these benchmark datasets.",
                    "url": "opencompass/opencompass_1113.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1113",
                    "sample_count": 1000,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {
                        "gen": "SVAMP_gen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 100,
                        "estimated_output_tokens": 50
                    },
                    "original_data": {
                        "id": "1113",
                        "name": "SVAMP",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 1,
                        "githubLink": "https://github.com/arkilpatel/SVAMP",
                        "paperLink": "https://arxiv.org/pdf/2103.07191v2",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "163",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-11 19:59:21",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-11 19:59:21",
                        "createDate": "2024-10-10 13:41:55",
                        "desc": {
                            "cn": "SVAMP 是一个包含算术文字问题的数据集，最高适用于四年级的学生，是通过对现有数据集中的文字问题应用简单变体而生成。",
                            "en": "SVAMP includes one-unknown arithmetic word problems with grade level up to 4 by applying simple variations over word problems in an existing dataset. SVAMP further highlights the brittle nature of existing models when trained on these benchmark datasets."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [
                                {
                                    "question": "{\"Answer\": 337.0, \"Question\": \"How many boys are there in that school?\", \"Equation\": \"( 739.0 - 402.0 )\", \"Body\": \"In a school there are 402 more girls than boys. If there are 739 girls\", \"Type\": \"Sub",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "subtraction",
                                        "math_problem"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic_arithmetic",
                                        "reading_comprehension",
                                        "problem_solving"
                                    ],
                                    "concepts": [
                                        "Subtraction",
                                        "Word problem",
                                        "Basic algebra"
                                    ],
                                    "qualification": [
                                        "4th grade"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 6,
                                        "avg": 8,
                                        "max": 18
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "{\"Answer\": 10.0, \"Question\": \"How many pots are there in all?\", \"Equation\": \"( 400.0 / 40.0 )\", \"Body\": \"Each pot has 40 flowers in it. There are 400 flowers in total.\", \"Type\": \"Common-Division\", \"ID",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "qa",
                                        "arithmetic",
                                        "equation_solve"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic_arithmetic",
                                        "number_sense"
                                    ],
                                    "concepts": [
                                        "division",
                                        "basic_arithmetic"
                                    ],
                                    "qualification": [
                                        "primary_school"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 6,
                                        "avg": 8,
                                        "max": 70
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "{\"Answer\": 75.0, \"Question\": \"How many cookies did Paco have left?\", \"Equation\": \"( ( 40.0 - 2.0 ) + 37.0 )\", \"Body\": \"Paco had 40 cookies. He ate 2 of them. Then he bought 37 more cookies\", \"Type\": \"",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "math",
                                        "addition",
                                        "arithmetic"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic arithmetic",
                                        "reading comprehension",
                                        "numeracy"
                                    ],
                                    "concepts": [
                                        "addition",
                                        "subtraction",
                                        "basic arithmetic"
                                    ],
                                    "qualification": [
                                        "Elementary school"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 5,
                                        "avg": 7,
                                        "max": 120
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "{\"Answer\": 268627.0, \"Question\": \"How many kids stay home?\", \"Equation\": \"( 898051.0 - 629424.0 )\", \"Body\": \"Lawrence county has 898051 kids. During summer break 629424 kids go to camp and the rest st",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "subtraction"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic_arithmetic",
                                        "numeric_reasoning"
                                    ],
                                    "concepts": [
                                        "Subtraction",
                                        "Basic Arithmetic"
                                    ],
                                    "qualification": [
                                        "4th grade"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 9,
                                        "avg": 12,
                                        "max": 80
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "{\"Answer\": 12.0, \"Question\": \"How many more bottle caps than wrappers does danny have now?\", \"Equation\": \"( 28.0 - 16.0 )\", \"Body\": \"Danny collects bottle caps and wrappers. He found 71 bottle caps an",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "arithmetic",
                                        "subtraction",
                                        "simple_multiplication_if_needed"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic_arithmetic",
                                        "reading_comprehension",
                                        "number_sense"
                                    ],
                                    "concepts": [
                                        "Subtraction",
                                        "Number comparison",
                                        "Difference calculation"
                                    ],
                                    "qualification": [
                                        "1st_primary",
                                        "Elementary_school_math"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 5,
                                        "avg": 7,
                                        "max": 90
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "{\"Answer\": 58.0, \"Question\": \"How many more pieces of candy than chocolate did Bobby eat?\", \"Equation\": \"( ( 38.0 + 36.0 ) - 16.0 )\", \"Body\": \"Bobby ate 38 pieces of candy. Then he ate 36 more. He als",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "qa"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic arithmetic",
                                        "calculation",
                                        "numerical reasoning"
                                    ],
                                    "concepts": [
                                        "addition",
                                        "subtraction",
                                        "basic arithmetic"
                                    ],
                                    "qualification": [
                                        "2nd grade",
                                        "3rd grade"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 5,
                                        "avg": 8,
                                        "max": 90
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "{\"Answer\": 29.0, \"Question\": \"How big is each group of bananas?\", \"Equation\": \"( 203.0 / 7.0 )\", \"Body\": \"There are 203 bananas and 63 oranges in Philip's collection. If the bananas are organized into",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "math",
                                        "question_answering"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic arithmetic",
                                        "division",
                                        "numerical reasoning"
                                    ],
                                    "concepts": [
                                        "division"
                                    ],
                                    "qualification": [
                                        "elementary school"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 6,
                                        "avg": 10,
                                        "max": 90
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "{\"Answer\": 460.0, \"Question\": \"How many magazines does he have in total?\", \"Equation\": \"( 46.0 * 10.0 )\", \"Body\": \"Bryan took a look at his books and magazines. If he has 9 books and 46 magazines in e",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "math"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic arithmetic",
                                        "multiplication",
                                        "reading comprehension"
                                    ],
                                    "concepts": [
                                        "multiplication",
                                        "basic arithmetic"
                                    ],
                                    "qualification": [
                                        "elementary school"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 6,
                                        "avg": 10,
                                        "max": 90
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "{\"Answer\": 997.0, \"Question\": \"How many pieces of gum does Robin have?\", \"Equation\": \"( ( 43.0 * 23.0 ) + 8.0 )\", \"Body\": \"Robin has 43 packages of gum. There are 23 pieces in each package. Robin has ",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "arithmetic",
                                        "qa"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic arithmetic",
                                        "problem solving",
                                        "mental math"
                                    ],
                                    "concepts": [
                                        "multiplication",
                                        "addition"
                                    ],
                                    "qualification": [
                                        "elementary school"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 8,
                                        "avg": 10,
                                        "max": 80
                                    },
                                    "domain_expertise_required": false
                                },
                                {
                                    "question": "{\"Answer\": 1.0, \"Question\": \"How many more bags did she have on monday than she found on the next day?\", \"Equation\": \"( 8.0 - 7.0 )\", \"Body\": \"Tiffany was collecting cans for recycling. On monday she ",
                                    "difficulty": "easy",
                                    "task_type": [
                                        "question_answering",
                                        "mathematics"
                                    ],
                                    "domains": [],
                                    "skills": [
                                        "basic_arithmetic",
                                        "counting",
                                        "problem_solving"
                                    ],
                                    "concepts": [
                                        "subtraction",
                                        "counting",
                                        "difference",
                                        "basic arithmetic"
                                    ],
                                    "qualification": [
                                        "1st grade math"
                                    ],
                                    "language": [
                                        "English"
                                    ],
                                    "age_range": {
                                        "min": 6,
                                        "avg": 8,
                                        "max": 90
                                    },
                                    "domain_expertise_required": false
                                }
                            ],
                            "total_questions": 1000,
                            "question_format": "Addition, Arithmetic, Math, Math Problem, Multiplication, QA, Subtraction Problem, addition, arithmetic, arithmetic_subtraction, arithmetics, basic arithmetic, basic_arithmetic, basic_calculation, basic_computation, basic_division, basic_math, basic_maths, basic_multiplication, calculation, calculations, division, math, math_basic, math_comprehension, math_problem, math_problem_solving, math_problems, math_question, math_quiz, math_understanding, math_word_problem, mathematical_computation, mathematics, multiplication, number_problem, problem_solving, q_and_a, qa, question answering, question_and_answer, question_answer, question_answering, question_response, reading_comprehension, reasoning, simple arithmetic, simple_math, simple_multiplication, subtraction, word problem, word_problem",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [
                                "Clarity & Simplicity – Beginner‑level questions make results easy to interpret.",
                                "Comprehensive Operation Mix – covers all four primary arithmetic operations.",
                                "Embedded Equations – each answer comes with a human‑readable equation, aiding debugging.",
                                "Compact Size – 1,000 samples allow quick evaluation and reproducibility.",
                                "Domain‑agnostic – questions are generic enough to test pure numeric reasoning without domain bias."
                            ],
                            "disadvantages": [
                                "Limited Complexity – no advanced topics such as algebra, geometry, or multi‑step reasoning.",
                                "Small Sample Size – may not capture full variability of real‑world math problems.",
                                "Unspecified Domain Distribution – without known domains, it’s hard to assess domain‑specific performance.",
                                "Water‑Blob Scoring – accuracy on such simple tasks may not correlate with performance on more complex benchmarks.",
                                "No Language Variance – all problems are written in the same style, potentially over‑fitting language‑to‑math strategies."
                            ]
                        },
                        "age_distribution": {
                            "data": [
                                192,
                                7,
                                1,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0,
                                0
                            ],
                            "categories": [
                                "10",
                                "12",
                                "14",
                                "16",
                                "18",
                                "20",
                                "22",
                                "24",
                                "26",
                                "28",
                                "30",
                                "32",
                                "34",
                                "36",
                                "38",
                                "40",
                                "42",
                                "44",
                                "46",
                                "48",
                                "50",
                                "52",
                                "54",
                                "56",
                                "58",
                                "60"
                            ]
                        },
                        "why_run_this_eval": [
                            "Run this evaluation when you want to gauge the following model traits:\n- **Basic Arithmetic Proficiency** – can the model add, subtract, multiply, and divide correctly?\n- **Reading‑to‑Equation Mapping** – does the model understand the narrative and translate it into the proper mathematical expression?\n- **Numeracy / Number Sense** – does the model apply numbers in a realistic context?\n- **Early‑Stage Problem‑Solving** – is the model able to solve simple word problems as a baseline before tackling more advanced reasoning tasks?"
                        ],
                        "what_to_expect": [
                            "When an LLM is evaluated on SVAMP, the model’s accuracy indicates how reliably it can parse a short story, extract the relevant numbers, carry out the required arithmetic, and produce the correct numeric answer.",
                            "A higher score demonstrates stronger numeric reasoning, the ability to interpret simple textual constraints, and precise calculation capabilities.",
                            "Conversely, a low score reveals gaps in the model’s handling of basic math or its ability to translate language into equations."
                        ],
                        "evaluation_description": "SVAMP is a lightweight benchmark for elementary mathematics. It contains 1,000 question‑answer pairs that mix arithmetic operations (addition, subtraction, multiplication, division) with simple word problems. Each example provides a short text (the problem body), a question, the correct numerical answer, and an equation in a human‑readable form. The dataset is purposely beginner‑level, targeting basic numeracy and reading comprehension skills suitable for students in late primary school.",
                        "top_5_task_types": [
                            "arithmetic",
                            "subtraction",
                            "qa",
                            "math",
                            "question_answering"
                        ],
                        "top_5_domains": [],
                        "top_5_skills": [
                            "basic_arithmetic",
                            "basic arithmetic",
                            "reading_comprehension",
                            "problem_solving",
                            "number_sense"
                        ],
                        "top_5_concepts": [
                            "basic arithmetic",
                            "Subtraction",
                            "addition",
                            "subtraction",
                            "division"
                        ],
                        "top_5_qualifications": [
                            "elementary school",
                            "4th grade",
                            "primary_school",
                            "Elementary school",
                            "1st_primary"
                        ],
                        "top_5_languages": [
                            "English"
                        ]
                    },
                    "analysis_file": "analysis/opencompass_1113_analysis.json",
                    "analysis_summary": {
                        "total_analyzed": 200,
                        "successful": 200,
                        "failed": 0
                    }
                },
                {
                    "id": "opencompass_1541",
                    "name": "VLM2-Bench",
                    "version": "1.0.0",
                    "description": "VLM²-Bench is the first comprehensive benchmark that evaluates vision-language models' (VLMs) ability to visually link matching cues across multi-image sequences and videos. The benchmark consists of 9 subtasks with over 3,000 test cases.",
                    "url": "opencompass/opencompass_1541.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1541",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1541",
                        "name": "VLM2-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/vlm2-bench/VLM2-Bench",
                        "paperLink": "https://arxiv.org/abs/2502.12084",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "162",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-02-26 18:53:47",
                        "supportOnlineEval": false,
                        "updateDate": "2025-02-26 18:53:47",
                        "createDate": "2025-02-25 20:23:33",
                        "desc": {
                            "cn": "VLM²-Bench 是第一个全面评估视觉语言模型（VLMs）在多图像序列和视频中视觉链接匹配线索能力的基准。该基准包括 9 个子任务，超过 3000 个测试案例，旨在评估人类日常使用的根本视觉链接能力。",
                            "en": "VLM²-Bench is the first comprehensive benchmark that evaluates vision-language models' (VLMs) ability to visually link matching cues across multi-image sequences and videos. The benchmark consists of 9 subtasks with over 3,000 test cases."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/vlm2_bench'. Error: Path opencompass/vlm2_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1523",
                    "name": "MMIE",
                    "version": "1.0.0",
                    "description": " MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs).",
                    "url": "opencompass/opencompass_1523.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1523",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1523",
                        "name": "MMIE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Lillianwei-h/MMIE",
                        "paperLink": "https://arxiv.org/abs/2410.10139",
                        "officialWebsiteLink": "https://mmie-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "161",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:58:15",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:58:15",
                        "createDate": "2025-02-21 11:23:50",
                        "desc": {
                            "cn": " MMIE，这是一个大规模知识密集型基准，用于评估大型视觉-语言模型（LVLMs）中的交错多模态理解和生成。",
                            "en": " MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs)."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmie'. Error: Path opencompass/mmie is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1103",
                    "name": "QASC",
                    "version": "1.0.0",
                    "description": "QASC is a question-answering dataset with a focus on sentence composition. It consists of 9,980 8-way multiple-choice questions about grade school science (8,134 train, 926 dev, 920 test), and comes with a corpus of 17M sentences.",
                    "url": "opencompass/opencompass_1103.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1103",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1103",
                        "name": "QASC",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/allenai/qasc",
                        "paperLink": "https://arxiv.org/pdf/1910.11473",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "160",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-09 20:03:32",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-09 20:03:32",
                        "createDate": "2024-10-09 16:45:32",
                        "desc": {
                            "cn": "QASC 是一个专注于句子组合的问答数据集。它包含 9,980 道小学科学的多项选择题（8,134 道用于训练，926 道用于开发，920 道用于测试），并配有一个包含 1,700 万个句子的语料库。",
                            "en": "QASC is a question-answering dataset with a focus on sentence composition. It consists of 9,980 8-way multiple-choice questions about grade school science (8,134 train, 926 dev, 920 test), and comes with a corpus of 17M sentences."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/qasc'. Error: Path opencompass/qasc is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1337",
                    "name": "JailTrickBench",
                    "version": "1.0.0",
                    "description": "JailTrickBench can evaluate the impact of various attack settings on LLM performance, including 8 key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives.",
                    "url": "opencompass/opencompass_1337.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1337",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1337",
                        "name": "JailTrickBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/usail-hkust/JailTrickBench",
                        "paperLink": "https://arxiv.org/abs/2406.09324",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "158",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:05:47",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:05:47",
                        "createDate": "2025-01-03 16:33:01",
                        "desc": {
                            "cn": "JailTrickBench用于评估LLM应对各种越狱攻击的能力，涵盖从目标级和攻击级2个角度实施越狱攻击的8个关键因素。",
                            "en": "JailTrickBench can evaluate the impact of various attack settings on LLM performance, including 8 key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/jailtrickbench'. Error: Path opencompass/jailtrickbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1572",
                    "name": "LOKI",
                    "version": "1.0.0",
                    "description": "LOKI, a multimodal synthetic data detection benchmark, designed specifically to comprehensively assess the capabilities of LMMs in detecting synthetic data.",
                    "url": "opencompass/opencompass_1572.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1572",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1572",
                        "name": "LOKI",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/opendatalab/LOKI",
                        "paperLink": "https://arxiv.org/abs/2410.09732",
                        "officialWebsiteLink": "https://opendatalab.github.io/LOKI/#fingding",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "157",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-18 15:43:49",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-18 15:43:49",
                        "createDate": "2025-03-18 15:43:15",
                        "desc": {
                            "cn": "LOKI是一个多模态合成数据检测基准，专门设计用于全面评估 LMMs 在检测合成数据方面的能力。",
                            "en": "LOKI, a multimodal synthetic data detection benchmark, designed specifically to comprehensively assess the capabilities of LMMs in detecting synthetic data."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/loki'. Error: Path opencompass/loki is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1250",
                    "name": "Collie",
                    "version": "1.0.0",
                    "description": "COLLIE is a grammar-based framework that allows the specification of rich, compositional constraints with diverse generation levels (word, sentence, paragraph, passage) and modeling challenges (e.g.,language understanding, logical reasoning, counting, semantic planning).",
                    "url": "opencompass/opencompass_1250.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1250",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1250",
                        "name": "Collie",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/princeton-nlp/Collie",
                        "paperLink": "https://arxiv.org/abs/2307.08689",
                        "officialWebsiteLink": "https://collie-benchmark.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "157",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-30 16:29:49",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-30 16:29:49",
                        "createDate": "2024-12-30 16:23:20",
                        "desc": {
                            "cn": "COLLIE用于评估大模型在约束性文本生成任务中的表现，可指定具有不同生成级别（单词、句子、段落、段落）和建模挑战（例如，语言理解、逻辑推理、计数、语义规划）的丰富组合。",
                            "en": "COLLIE is a grammar-based framework that allows the specification of rich, compositional constraints with diverse generation levels (word, sentence, paragraph, passage) and modeling challenges (e.g.,language understanding, logical reasoning, counting, semantic planning)."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/collie'. Error: Path opencompass/collie is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1543",
                    "name": "KITAB-Bench",
                    "version": "1.0.0",
                    "description": "KITAB-Bench is a Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding,  and spans 36 sub-domains with over 8,809 samples, carefully curated to rigorously evaluate essential skills required for Arabic OCR and document analysis.",
                    "url": "opencompass/opencompass_1543.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1543",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1543",
                        "name": "KITAB-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/mbzuai-oryx/KITAB-Bench",
                        "paperLink": "https://arxiv.org/abs/2502.14949",
                        "officialWebsiteLink": "https://mbzuai-oryx.github.io/KITAB-Bench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "156",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-05 13:36:30",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-05 13:36:30",
                        "createDate": "2025-03-05 13:36:03",
                        "desc": {
                            "cn": "KITAB-Bench是一个全面多领域阿拉伯文 OCR 和文档理解基准，包含 36 个子领域，超过 8,809 个样本，经过精心挑选，以严格评估阿拉伯 OCR 和文档分析所需的基本技能。",
                            "en": "KITAB-Bench is a Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding,  and spans 36 sub-domains with over 8,809 samples, carefully curated to rigorously evaluate essential skills required for Arabic OCR and document analysis."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/kitab_bench'. Error: Path opencompass/kitab_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1332",
                    "name": "UniBench",
                    "version": "1.0.0",
                    "description": "UniBench is meant for evaluating VLMs' reasoning abilities. It is a unified implementation of 50+ VLM benchmarks spanning a comprehensive range of carefully categorized capabilities from object recognition to spatial awareness, counting, and much more. ",
                    "url": "opencompass/opencompass_1332.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1332",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1332",
                        "name": "UniBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/facebookresearch/unibench",
                        "paperLink": "https://arxiv.org/abs/2408.04810",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "156",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-08 11:56:31",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-08 11:56:31",
                        "createDate": "2025-01-03 14:22:05",
                        "desc": {
                            "cn": "UniBench旨在评估VLM的推理能力，包括50个基准测试，涵盖对象识别、空间感知、计数等任务。",
                            "en": "UniBench is meant for evaluating VLMs' reasoning abilities. It is a unified implementation of 50+ VLM benchmarks spanning a comprehensive range of carefully categorized capabilities from object recognition to spatial awareness, counting, and much more. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/unibench'. Error: Path opencompass/unibench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1333",
                    "name": "RedCode",
                    "version": "1.0.0",
                    "description": "RedCode provides comprehensive and practical evaluations on the safety of code agents, including 4,050 risky test cases covering 25 types of critical vulnerabilities spanning 8 domains and 160 prompts aiming to generate harmful code or software.",
                    "url": "opencompass/opencompass_1333.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1333",
                    "sample_count": 1000,
                    "traits": [
                        "Safety",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1333",
                        "name": "RedCode",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "安全",
                                "en": "Safety"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AI-secure/RedCode",
                        "paperLink": "https://arxiv.org/abs/2411.07781",
                        "officialWebsiteLink": "https://redcode-agent.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "155",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 11:12:36",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 11:12:36",
                        "createDate": "2025-01-10 19:41:07",
                        "desc": {
                            "cn": "RedCode旨在为LLM代码智能体的安全性提供全面实用的评估，包括来自8个领域25种关键漏洞的4050个风险测试用例，以及160个生成有害代码的提示。",
                            "en": "RedCode provides comprehensive and practical evaluations on the safety of code agents, including 4,050 risky test cases covering 25 types of critical vulnerabilities spanning 8 domains and 160 prompts aiming to generate harmful code or software."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/redcode'. Error: Path opencompass/redcode is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1367",
                    "name": "A-OKVQA",
                    "version": "1.0.0",
                    "description": "A-OKVQA assesses commonsense reasoning abilities. It is a crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer.",
                    "url": "opencompass/opencompass_1367.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1367",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1367",
                        "name": "A-OKVQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/allenai/aokvqa",
                        "paperLink": "https://arxiv.org/abs/2206.01718",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "155",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:39",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:39",
                        "createDate": "2025-01-10 12:28:01",
                        "desc": {
                            "cn": "A-OKVQA用于评估多模态大模型的常识及推理能力，由25K个不同的问题组成，需要对图像中描述的场景进行某种形式的常识性推理来回答。",
                            "en": "A-OKVQA assesses commonsense reasoning abilities. It is a crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/a_okvqa'. Error: Path opencompass/a_okvqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1275",
                    "name": "MMLongBench-Doc",
                    "version": "1.0.0",
                    "description": "MMLongBench-Doc is a long-context multi-modal benchmark comprising 1,062 expert-annotated questions.",
                    "url": "opencompass/opencompass_1275.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1275",
                    "sample_count": 1000,
                    "traits": [
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1275",
                        "name": "MMLongBench-Doc",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "长文本",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/mayubo2333/MMLongBench-Doc",
                        "paperLink": "https://arxiv.org/abs/2407.01523",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "154",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:14:42",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:14:42",
                        "createDate": "2025-01-07 14:12:34",
                        "desc": {
                            "cn": "MMLONGBENCH-DOC是一个长上下文的多模态基准，由1062个专家注释的问题组成。",
                            "en": "MMLongBench-Doc is a long-context multi-modal benchmark comprising 1,062 expert-annotated questions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmlongbench_doc'. Error: Path opencompass/mmlongbench_doc is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1548",
                    "name": "MedHallu",
                    "version": "1.0.0",
                    "description": "MedHallu is a comprehensive benchmark dataset designed to evaluate the ability of large language models to detect hallucinations in medical question-answering tasks. ",
                    "url": "opencompass/opencompass_1548.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1548",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1548",
                        "name": "MedHallu",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Medical",
                                "en": "Medical"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MedHallu/MedHalu",
                        "paperLink": "https://arxiv.org/abs/2502.14302",
                        "officialWebsiteLink": "https://medhallu.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "153",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-02-26 19:00:05",
                        "supportOnlineEval": false,
                        "updateDate": "2025-02-26 19:00:05",
                        "createDate": "2025-02-26 13:33:02",
                        "desc": {
                            "cn": "MedHallu 旨在评估大型语言模型在医学问题解答任务中检测幻觉的能力。",
                            "en": "MedHallu is a comprehensive benchmark dataset designed to evaluate the ability of large language models to detect hallucinations in medical question-answering tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medhallu'. Error: Path opencompass/medhallu is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1331",
                    "name": "MedSafetyBench",
                    "version": "1.0.0",
                    "description": "MedSafetyBench is designed to measure the medical safety of LLMs. It includes 1,800 medical safety demonstrations, where each safety demonstration consists of a harmful medical request and a corresponding safe response. ",
                    "url": "opencompass/opencompass_1331.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1331",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1331",
                        "name": "MedSafetyBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AI4LIFE-GROUP/med-safety-bench",
                        "paperLink": "https://arxiv.org/abs/2403.03744",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "150",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 16:32:34",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 16:32:34",
                        "createDate": "2025-01-03 14:14:58",
                        "desc": {
                            "cn": "MedSafetyBench用于评估LLM在医疗安全上的表现，包含1800个由有害请求和安全响应组成的医疗安全场景。",
                            "en": "MedSafetyBench is designed to measure the medical safety of LLMs. It includes 1,800 medical safety demonstrations, where each safety demonstration consists of a harmful medical request and a corresponding safe response. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medsafetybench'. Error: Path opencompass/medsafetybench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1705",
                    "name": "OlymMATH",
                    "version": "1.0.0",
                    "description": "A benchmark of 200 Olympiad math problems across algebra, geometry, number theory, and combinatorics. Available in English and Chinese, it features two difficulty levels: EASY (AIME-level) to test standard reasoning, and HARD to challenge advanced models.",
                    "url": "opencompass/opencompass_1705.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1705",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Reasoning",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1705",
                        "name": "OlymMATH",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/RUCAIBox/OlymMATH",
                        "paperLink": "https://arxiv.org/abs/2503.21380",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "52300476",
                            "name": "CoderBak",
                            "avatar": null,
                            "nickname": "CoderBak"
                        },
                        "lookNum": "150",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-03 19:46:03",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-03 19:46:03",
                        "createDate": "2025-04-02 23:55:49",
                        "desc": {
                            "cn": "一个包含 200 道奥林匹克数学题的基准测试，涵盖代数、几何、数论和组合。我们提供英文和中文版本，并提供两个难度等级：EASY（AIME水平）用于测试标准推理能力，以及 HARD 用于挑战高级模型。",
                            "en": "A benchmark of 200 Olympiad math problems across algebra, geometry, number theory, and combinatorics. Available in English and Chinese, it features two difficulty levels: EASY (AIME-level) to test standard reasoning, and HARD to challenge advanced models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/olymmath'. Error: Path opencompass/olymmath is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1451",
                    "name": "MME-RealWorld",
                    "version": "1.0.0",
                    "description": "MME-RealWorld evaluates MLLMs' real-world recognition, featuring 13,366 high-resolution images averaging 2,000 × 1,500 pixels. ",
                    "url": "opencompass/opencompass_1451.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1451",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1451",
                        "name": "MME-RealWorld",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/yfzhang114/MME-RealWorld",
                        "paperLink": "https://arxiv.org/abs/2408.13257",
                        "officialWebsiteLink": "https://mme-realworld.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "150",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-05 15:26:47",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-05 15:26:47",
                        "createDate": "2025-01-27 12:05:30",
                        "desc": {
                            "cn": "MME-RealWorld用于评估多模态大模型对真实场景的理解能力，包含13366个平均2000*1500像素的高分辨率图像。",
                            "en": "MME-RealWorld evaluates MLLMs' real-world recognition, featuring 13,366 high-resolution images averaging 2,000 × 1,500 pixels. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mme_realworld'. Error: Path opencompass/mme_realworld is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1270",
                    "name": "Spider2-V",
                    "version": "1.0.0",
                    "description": "Spider2-V is the first multimodal agent benchmark focusing on professional data science and engineering workflows, featuring 494 real-world tasks in authentic computer environments and incorporating 20 enterprise-level professional applications.",
                    "url": "opencompass/opencompass_1270.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1270",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1270",
                        "name": "Spider2-V",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/xlang-ai/Spider2-V",
                        "paperLink": "https://arxiv.org/abs/2407.10956",
                        "officialWebsiteLink": "https://spider2-v.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "150",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:14:51",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:14:51",
                        "createDate": "2025-01-07 14:07:32",
                        "desc": {
                            "cn": "Spider2-V是第一个专注于专业数据科学和工程工作流程的多模态代理基准测试，整合了20 个企业级专业应用程序，包含来自真实计算机环境的494 个真实任务。",
                            "en": "Spider2-V is the first multimodal agent benchmark focusing on professional data science and engineering workflows, featuring 494 real-world tasks in authentic computer environments and incorporating 20 enterprise-level professional applications."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/spider2_v'. Error: Path opencompass/spider2_v is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1272",
                    "name": "GSM1k",
                    "version": "1.0.0",
                    "description": "GSM1k is meant for evaluating LLM's math reasoning ability. It mirrors the style and complexity of the established GSM8k benchmark while consider the problem of data-leaking and overfitting",
                    "url": "opencompass/opencompass_1272.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1272",
                    "sample_count": 1000,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1272",
                        "name": "GSM1k",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/scaleapi/gsm1k_eval",
                        "paperLink": "https://arxiv.org/abs/2405.00332",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "149",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 14:36:23",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 14:36:23",
                        "createDate": "2024-12-24 19:16:04",
                        "desc": {
                            "cn": "GSM1k可用于评估LLM的数学推理能力；它与GSM8k保持了风格和复杂性的一致，同时考虑了数据泄露和过拟合的问题。",
                            "en": "GSM1k is meant for evaluating LLM's math reasoning ability. It mirrors the style and complexity of the established GSM8k benchmark while consider the problem of data-leaking and overfitting"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gsm1k'. Error: Path opencompass/gsm1k is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1532",
                    "name": "ZeroBench",
                    "version": "1.0.0",
                    "description": "ZeroBench is a challenging visual reasoning benchmark for LMMs. It consists of a main set of 100 high-quality, manually curated questions covering numerous domains, reasoning types and image type. Questions have been designed and calibrated to be beyond the capabilities of current frontier models.\n",
                    "url": "opencompass/opencompass_1532.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1532",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1532",
                        "name": "ZeroBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/jonathan-roberts1/zerobench",
                        "paperLink": "https://arxiv.org/abs/2502.09696",
                        "officialWebsiteLink": "https://zerobench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "147",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-02-25 19:54:07",
                        "supportOnlineEval": false,
                        "updateDate": "2025-02-25 19:54:07",
                        "createDate": "2025-02-24 13:40:00",
                        "desc": {
                            "cn": "ZeroBench 是针对多模态模型（LMMs）的具有挑战性的视觉推理基准。它由一组主要的 100 个高质量人工问题组成，涵盖多个领域、推理类型和图像类型。ZeroBench 中的问题经过设计和校准，已经超出了当前前沿模型的能力范围。",
                            "en": "ZeroBench is a challenging visual reasoning benchmark for LMMs. It consists of a main set of 100 high-quality, manually curated questions covering numerous domains, reasoning types and image type. Questions have been designed and calibrated to be beyond the capabilities of current frontier models.\n"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/zerobench'. Error: Path opencompass/zerobench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1365",
                    "name": "BLINK",
                    "version": "1.0.0",
                    "description": "BLINK focuses on MLLMs' core visual perception abilities. It contains 3,807 multiple-choice questions spanning 14 classic computer vision tasks. ",
                    "url": "opencompass/opencompass_1365.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1365",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1365",
                        "name": "BLINK",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/zeyofu/BLINK_Benchmark",
                        "paperLink": "https://arxiv.org/abs/2404.12390",
                        "officialWebsiteLink": "https://zeyofu.github.io/blink/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "146",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:42",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:42",
                        "createDate": "2025-01-10 12:06:09",
                        "desc": {
                            "cn": "BLINK用于评估多模态大模型的视觉感知能力，包含来自14个经典计算机视觉任务的3807道多项选择题。",
                            "en": "BLINK focuses on MLLMs' core visual perception abilities. It contains 3,807 multiple-choice questions spanning 14 classic computer vision tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/blink'. Error: Path opencompass/blink is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1326",
                    "name": "MedJourney",
                    "version": "1.0.0",
                    "description": "MedJourney offers a comprehensive assessment of LLMs' effectiveness in real-world clinical settings. It includes multiple tasks from 4 stages of a typical patient's hospital visit journey and comprises 12 datasets.",
                    "url": "opencompass/opencompass_1326.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1326",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1326",
                        "name": "MedJourney",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Medical-AI-Learning/MedJourney",
                        "paperLink": "https://openreview.net/pdf?id=XXaIoJyYs7",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "145",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 16:32:46",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 16:32:46",
                        "createDate": "2025-01-02 20:04:00",
                        "desc": {
                            "cn": "MedJourney用于评估 LLM 在真实临床环境中的有效性，其中包含多个任务，涵盖来自患者就诊典型流程的4个阶段的12个数据集。",
                            "en": "MedJourney offers a comprehensive assessment of LLMs' effectiveness in real-world clinical settings. It includes multiple tasks from 4 stages of a typical patient's hospital visit journey and comprises 12 datasets."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medjourney'. Error: Path opencompass/medjourney is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1268",
                    "name": "CTIBench",
                    "version": "1.0.0",
                    "description": "CTIBench is a benchmark designed to assess LLMs' performance in CTI (Cyber threat intelligence) applications. It includes multiple datasets focused on evaluating knowledge acquired by LLMs in the cyber-threat landscape. ",
                    "url": "opencompass/opencompass_1268.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1268",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1268",
                        "name": "CTIBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/xashru/cti-bench",
                        "paperLink": "https://arxiv.org/abs/2406.07599",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "145",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 14:15:02",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 14:15:02",
                        "createDate": "2024-12-24 15:01:43",
                        "desc": {
                            "cn": "CTIBench旨在评估LLM在CTI（网络安全情报）场景下的能力，包含多个数据集，专注于评测LLM在网络威胁环境中获得的知识。",
                            "en": "CTIBench is a benchmark designed to assess LLMs' performance in CTI (Cyber threat intelligence) applications. It includes multiple datasets focused on evaluating knowledge acquired by LLMs in the cyber-threat landscape. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ctibench'. Error: Path opencompass/ctibench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1149",
                    "name": "InstruSum",
                    "version": "1.0.0",
                    "description": "InstruSum evaluates the task of instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for desired summary characteristics.\n",
                    "url": "opencompass/opencompass_1149.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1149",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1149",
                        "name": "InstruSum",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NAACL 2024",
                                "en": "NAACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/yale-nlp/InstruSum",
                        "paperLink": "https://aclanthology.org/2024.findings-naacl.280.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50107925",
                            "name": "yale-nlp",
                            "avatar": null,
                            "nickname": "OpenXLab-kJ37KHAvs"
                        },
                        "lookNum": "144",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-27 17:22:45",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-27 17:22:45",
                        "createDate": "2024-10-15 14:35:05",
                        "desc": {
                            "cn": "InstruSum 是评测指令可控的文本摘要任务，模型输入包括源文章和对所需摘要特征的自然语言要求。",
                            "en": "InstruSum evaluates the task of instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for desired summary characteristics.\n"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/instrusum'. Error: Path opencompass/instrusum is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1273",
                    "name": "MolPuzzle",
                    "version": "1.0.0",
                    "description": "MolPuzzle is a benchmark for evaluating MLM's reasoning ability, comprising 234 instances of structure elucidation, which feature over 18,000 QA samples.",
                    "url": "opencompass/opencompass_1273.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1273",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1273",
                        "name": "MolPuzzle",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/KehanGuo2/MolPuzzle",
                        "paperLink": "https://openreview.net/pdf?id=t1mAXb4Cop",
                        "officialWebsiteLink": "https://kehanguo2.github.io/Molpuzzle.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "144",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:14:38",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:14:38",
                        "createDate": "2025-01-07 14:07:05",
                        "desc": {
                            "cn": "MolPuzzle用于考察MLM的推理能力，包含234个结构解析实例以及超过18000个QA样本。",
                            "en": "MolPuzzle is a benchmark for evaluating MLM's reasoning ability, comprising 234 instances of structure elucidation, which feature over 18,000 QA samples."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/molpuzzle'. Error: Path opencompass/molpuzzle is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1323",
                    "name": "SciFIBench",
                    "version": "1.0.0",
                    "description": "SciFIBench is a scientific figure interpretation benchmark for LMMs, consisting of 2000 questions split between two tasks across 8 categories. ",
                    "url": "opencompass/opencompass_1323.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1323",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1323",
                        "name": "SciFIBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/jonathan-roberts1/SciFIBench",
                        "paperLink": "https://arxiv.org/abs/2405.08807",
                        "officialWebsiteLink": "https://scifibench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "143",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 11:12:46",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 11:12:46",
                        "createDate": "2025-01-13 11:57:33",
                        "desc": {
                            "cn": " SciFIBench用于评估多模态大模型的科学图表解释能力，由2000个问题组成，涵盖8个类别的2种任务。",
                            "en": "SciFIBench is a scientific figure interpretation benchmark for LMMs, consisting of 2000 questions split between two tasks across 8 categories. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/scifibench'. Error: Path opencompass/scifibench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1000",
                    "name": "Q-Bench",
                    "version": "1.0.0",
                    "description": "Q-Bench/Q-Bench+ is a benchmark for general-purpose foundation models on low-level vision.",
                    "url": "opencompass/opencompass_1000.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1000",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1000",
                        "name": "Q-Bench",
                        "emoji": "🐲",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [
                            {
                                "cn": "yes-or-no",
                                "en": "yes-or-no"
                            },
                            {
                                "cn": "what",
                                "en": "what"
                            },
                            {
                                "cn": "how",
                                "en": "how"
                            },
                            {
                                "cn": "distortion",
                                "en": "distortion"
                            },
                            {
                                "cn": "others",
                                "en": "others"
                            },
                            {
                                "cn": "in-context distortion",
                                "en": "in-context distortion"
                            },
                            {
                                "cn": "in-context others",
                                "en": "in-context others"
                            },
                            {
                                "cn": "overall",
                                "en": "overall"
                            }
                        ],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Q-Future/Q-Bench",
                        "paperLink": "https://arxiv.org/abs/2309.14181",
                        "officialWebsiteLink": "https://q-future.github.io/Q-Bench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50151145",
                            "name": null,
                            "avatar": null,
                            "nickname": "Orange"
                        },
                        "lookNum": "143",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": null,
                        "supportOnlineEval": false,
                        "updateDate": "2024-08-16 20:27:46",
                        "createDate": "2024-08-13 09:33:41",
                        "desc": {
                            "cn": "Q-Bench/Q-Bench+是一个面向多模态大模型底层视觉理解的数据集。此数据集从底层视觉的感知、描述、评价能力出发来对多模态大模型进行完整的测试，测试的对象既包括单张图像也包括图像对。",
                            "en": "Q-Bench/Q-Bench+ is a benchmark for general-purpose foundation models on low-level vision."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/q_bench'. Error: Path opencompass/q_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1330",
                    "name": "SG-Bench",
                    "version": "1.0.0",
                    "description": "SG-Bench assess LLM safety across various tasks and prompt types. It integrates both generative and discriminative evaluation tasks and includes extended data to examine the impact of prompt engineering and jailbreak. ",
                    "url": "opencompass/opencompass_1330.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1330",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1330",
                        "name": "SG-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MurrayTom/SG-Bench",
                        "paperLink": "https://arxiv.org/abs/2410.21965",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "142",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-08 11:56:40",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-08 11:56:40",
                        "createDate": "2025-01-03 14:07:00",
                        "desc": {
                            "cn": "SG-Bench用于评估LLM在不同任务和提示下的安全性，整合了生成性和判别性评估任务，并包含扩展数据以度量提示工程和越狱对安全性的影响。",
                            "en": "SG-Bench assess LLM safety across various tasks and prompt types. It integrates both generative and discriminative evaluation tasks and includes extended data to examine the impact of prompt engineering and jailbreak. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/sg_bench'. Error: Path opencompass/sg_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1336",
                    "name": "CompBench",
                    "version": "1.0.0",
                    "description": "CompBench is designed to evaluate the comparative reasoning capability of multimodal large language models, including a collection of around 40K image pairs and visually oriented questions covering 8 dimensions",
                    "url": "opencompass/opencompass_1336.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1336",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1336",
                        "name": "CompBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/RaptorMai/CompBench",
                        "paperLink": "https://arxiv.org/abs/2407.16837",
                        "officialWebsiteLink": "https://compbench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "142",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:05:36",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:05:36",
                        "createDate": "2025-01-03 16:21:25",
                        "desc": {
                            "cn": "CompBench旨在评估多模态大模型的比较推理能力，包含约4万个图像对及8个维度的视觉配对问题。",
                            "en": "CompBench is designed to evaluate the comparative reasoning capability of multimodal large language models, including a collection of around 40K image pairs and visually oriented questions covering 8 dimensions"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/compbench'. Error: Path opencompass/compbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1241",
                    "name": "MedCalc-Bench",
                    "version": "1.0.0",
                    "description": " MedCalc-Bench focuses on evaluating the medical calculation capability of LLMs. It contains an evaluation set of over 1000 manually reviewed instances from 55 different medical calculation tasks.",
                    "url": "opencompass/opencompass_1241.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1241",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1241",
                        "name": "MedCalc-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ncbi-nlp/MedCalc-Bench",
                        "paperLink": "https://arxiv.org/abs/2406.12036",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "140",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 11:46:27",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 11:46:27",
                        "createDate": "2024-12-20 17:03:18",
                        "desc": {
                            "cn": "MedCalc-Bench专注于评估LLM的医学计算能力，包含来自55个不同医学计算任务的1000个经过人工审查的实例。",
                            "en": " MedCalc-Bench focuses on evaluating the medical calculation capability of LLMs. It contains an evaluation set of over 1000 manually reviewed instances from 55 different medical calculation tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medcalc_bench'. Error: Path opencompass/medcalc_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1565",
                    "name": "MME-CoT",
                    "version": "1.0.0",
                    "description": "MME-CoT is a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes.",
                    "url": "opencompass/opencompass_1565.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1565",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1565",
                        "name": "MME-CoT",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/CaraJ7/MME-CoT",
                        "paperLink": "https://arxiv.org/abs/2502.09621",
                        "officialWebsiteLink": "https://mmecot.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "139",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-03 11:01:40",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-03 11:01:40",
                        "createDate": "2025-02-27 11:38:00",
                        "desc": {
                            "cn": "MME-CoT，一个专门用于评估 LMMs CoT 推理性能的基准，涵盖六个领域：数学、科学、OCR、逻辑、时空和一般场景。",
                            "en": "MME-CoT is a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mme_cot'. Error: Path opencompass/mme_cot is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1324",
                    "name": "FLUB",
                    "version": "1.0.0",
                    "description": "FLUB evaluates the reasoning and understanding abilities of LLMs. It includes three tasks with increasing difficulty, consisting of the tricky, humorous, and misleading texts collected from the real internet environment.",
                    "url": "opencompass/opencompass_1324.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1324",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1324",
                        "name": "FLUB",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/THUKElab/FLUB",
                        "paperLink": "https://arxiv.org/abs/2402.11100",
                        "officialWebsiteLink": "https://thukelab.github.io/FLUB/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "138",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:05:50",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:05:50",
                        "createDate": "2025-01-02 19:42:41",
                        "desc": {
                            "cn": "FLUB用于评估LLM的推理和理解能力，其中包含3个难度递进的任务，由从真实互联网环境中收集的狡猾、幽默和误导性的文本构成。",
                            "en": "FLUB evaluates the reasoning and understanding abilities of LLMs. It includes three tasks with increasing difficulty, consisting of the tricky, humorous, and misleading texts collected from the real internet environment."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/flub'. Error: Path opencompass/flub is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1102",
                    "name": "MS_MARCO",
                    "version": "1.0.0",
                    "description": "MS MARCO comprises of 1,010,916 anonymized questions—sampled from Bing’s search query logs—each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages.",
                    "url": "opencompass/opencompass_1102.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1102",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1102",
                        "name": "MS_MARCO",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/microsoft/MSMARCO-Question-Answering",
                        "paperLink": "https://arxiv.org/pdf/1611.09268",
                        "officialWebsiteLink": "https://microsoft.github.io/msmarco/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "138",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-09 20:03:35",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-09 20:03:35",
                        "createDate": "2024-10-09 16:38:58",
                        "desc": {
                            "cn": "MS MARCO 数据集包含 1,010,916 个来自 Bing 的搜索查询日志的匿名问题，每个问题都有一个人工生成的答案和 182,669 个完全由人重写的生成答案。此外，该数据集还包含从 3,563,535 个由 Bing 检索的网页文档中提取的 8,841,823 个段落，这些段落提供了策划自然语言答案所需的信息。",
                            "en": "MS MARCO comprises of 1,010,916 anonymized questions—sampled from Bing’s search query logs—each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ms_marco'. Error: Path opencompass/ms_marco is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1269",
                    "name": "ConvBench",
                    "version": "1.0.0",
                    "description": "ConvBench is a novel multi-turn conversation evaluation benchmark tailored for Large Vision-Language Models (LVLMs). It comprises 577 meticulously curated multi-turn conversations encompassing 215 tasks reflective of real-world demands.",
                    "url": "opencompass/opencompass_1269.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1269",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1269",
                        "name": "ConvBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/shirlyliu64/ConvBench",
                        "paperLink": "https://arxiv.org/abs/2403.20194",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "135",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:14:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:14:55",
                        "createDate": "2025-01-07 14:14:09",
                        "desc": {
                            "cn": "ConvBench是专用于大型视觉语言模型 （LVLM）的新型多轮对话评估基准，由基于215 个反映实际需求的任务的577个多轮次对话组成。 ",
                            "en": "ConvBench is a novel multi-turn conversation evaluation benchmark tailored for Large Vision-Language Models (LVLMs). It comprises 577 meticulously curated multi-turn conversations encompassing 215 tasks reflective of real-world demands."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/convbench'. Error: Path opencompass/convbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1566",
                    "name": "MM-IQ",
                    "version": "1.0.0",
                    "description": "MM-IQ is a comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms.",
                    "url": "opencompass/opencompass_1566.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1566",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1566",
                        "name": "MM-IQ",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AceCHQ/MMIQ",
                        "paperLink": "https://arxiv.org/abs/2502.00698",
                        "officialWebsiteLink": "https://acechq.github.io/MMIQ-benchmark/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "134",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-03 11:03:22",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-03 11:03:22",
                        "createDate": "2025-02-27 13:43:06",
                        "desc": {
                            "cn": "MM-IQ，这是一个包含 2,710 个精心挑选的测试项目的综合评估框架，涵盖了 8 种不同的推理范式。",
                            "en": "MM-IQ is a comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mm_iq'. Error: Path opencompass/mm_iq is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1534",
                    "name": "CHASE-Code",
                    "version": "1.0.0",
                    "description": "CHASE is a unified framework to synthetically generate challenging problems using LLMs without human involvement",
                    "url": "opencompass/opencompass_1534.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1534",
                    "sample_count": 1000,
                    "traits": [
                        "Code",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1534",
                        "name": "CHASE-Code",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            },
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/McGill-NLP/CHASE",
                        "paperLink": "https://arxiv.org/pdf/2502.14678",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "134",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 17:02:59",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 17:02:59",
                        "createDate": "2025-02-24 14:12:19",
                        "desc": {
                            "cn": "CHASE是一个无需人工参与的统一框架，用于合成生成具有挑战性的问题",
                            "en": "CHASE is a unified framework to synthetically generate challenging problems using LLMs without human involvement"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/chase_code'. Error: Path opencompass/chase_code is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1518",
                    "name": "miniCTX",
                    "version": "1.0.0",
                    "description": "A context-rich benchmark for evaluating neural theorem proving in realistic scenarios, providing premises, full context, multi-source benchmark, and temporal splits, enabling evaluation of a model's ability to work with context that evolves over time.",
                    "url": "opencompass/opencompass_1518.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1518",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1518",
                        "name": "miniCTX",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Strong Reasoning",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/cmu-l3/minictx-eval",
                        "paperLink": "https://www.arxiv.org/pdf/2408.03350",
                        "officialWebsiteLink": "https://cmu-l3.github.io/minictx/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "133",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:53:10",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:53:10",
                        "createDate": "2025-02-20 15:38:03",
                        "desc": {
                            "cn": "一个用于评估现实场景中神经定理证明的丰富上下文基准。通过提供前提、完整上下文、多源基准和时序分割，突出其评估模型处理随时间演变的上下文能力。",
                            "en": "A context-rich benchmark for evaluating neural theorem proving in realistic scenarios, providing premises, full context, multi-source benchmark, and temporal splits, enabling evaluation of a model's ability to work with context that evolves over time."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/minictx'. Error: Path opencompass/minictx is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1618",
                    "name": "FedMABench",
                    "version": "1.0.0",
                    "description": "FedMABench is an open-source benchmark for federated training and evaluation of mobile agents, specifically designed for heterogeneous scenarios.",
                    "url": "opencompass/opencompass_1618.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1618",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1618",
                        "name": "FedMABench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/wwh0411/FedMABench",
                        "paperLink": "https://arxiv.org/abs/2503.05143",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "132",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-10 17:12:07",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-10 17:12:07",
                        "createDate": "2025-03-10 17:06:15",
                        "desc": {
                            "cn": "FedMABench 是一个开源的联邦训练和评估移动代理的基准，特别为异构场景设计。",
                            "en": "FedMABench is an open-source benchmark for federated training and evaluation of mobile agents, specifically designed for heterogeneous scenarios."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/fedmabench'. Error: Path opencompass/fedmabench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1325",
                    "name": "LLM-Uncertainty-Bench",
                    "version": "1.0.0",
                    "description": "LLM-Uncertainty-Bench is a new benchmarking approach for LLMs that integrates uncertainty quantification. It spans 5 representative natural language processing tasks, each has a dataset with 10,000 instances.",
                    "url": "opencompass/opencompass_1325.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1325",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1325",
                        "name": "LLM-Uncertainty-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/smartyfh/LLM-Uncertainty-Bench",
                        "paperLink": "https://arxiv.org/abs/2401.12794",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "130",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 14:21:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 14:21:33",
                        "createDate": "2025-01-02 19:52:18",
                        "desc": {
                            "cn": "LLM-Uncertainty-Bench将不确定性纳入LLM评估，包含5个具有代表性的自然语言处理任务，每个任务都有包含10000个实例的数据集支撑。",
                            "en": "LLM-Uncertainty-Bench is a new benchmarking approach for LLMs that integrates uncertainty quantification. It spans 5 representative natural language processing tasks, each has a dataset with 10,000 instances."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/llm_uncertainty_bench'. Error: Path opencompass/llm_uncertainty_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1099",
                    "name": "MKQA",
                    "version": "1.0.0",
                    "description": "MKQA is an open-domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). ",
                    "url": "opencompass/opencompass_1099.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1099",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1099",
                        "name": "MKQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/apple/ml-mkqa",
                        "paperLink": "https://arxiv.org/pdf/2007.15207",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "130",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-09 20:03:40",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-09 20:03:40",
                        "createDate": "2024-10-09 14:12:42",
                        "desc": {
                            "cn": "MKQA 是一个开放域问答评估集，包含 10,000 对问题和答案，涵盖 26 种类型多样的语言（总计 260,000 对问题和答案）。",
                            "en": "MKQA is an open-domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mkqa'. Error: Path opencompass/mkqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1114",
                    "name": "ASDiv",
                    "version": "1.0.0",
                    "description": "ASDiv is a new MWP corpus that contains diverse lexicon patterns with wide problem type coverage. Each problem provides consistent equations and answers. It is further annotated with the corresponding problem type and grade level.",
                    "url": "opencompass/opencompass_1114.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1114",
                    "sample_count": 1000,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1114",
                        "name": "ASDiv",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/chaochun/nlu-asdiv-dataset/tree/master",
                        "paperLink": "https://aclanthology.org/2020.acl-main.92.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "130",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-11 19:59:24",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-11 19:59:24",
                        "createDate": "2024-10-10 14:06:57",
                        "desc": {
                            "cn": "ASDiv 是一个新的数学文字问题（MWP）语料库，包含多样的词汇模式，覆盖广泛的问题类型。每个问题提供对应的方程和答案。它进一步标注了相应的问题类型和年级水平，可用于测试系统的能力，并指明问题的难度等级。",
                            "en": "ASDiv is a new MWP corpus that contains diverse lexicon patterns with wide problem type coverage. Each problem provides consistent equations and answers. It is further annotated with the corresponding problem type and grade level."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/asdiv'. Error: Path opencompass/asdiv is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1606",
                    "name": "MCiteBench",
                    "version": "1.0.0",
                    "description": "MCiteBench is a benchmark to evaluate multimodal citation text generation in MLLMs. It consists of 3,000 samples from 1,749 academic papers, featuring 2,000 Explanation tasks and 1,000 Locating tasks, with balanced evidence across text, figures, tables, and mixed modalities.",
                    "url": "opencompass/opencompass_1606.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1606",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1606",
                        "name": "MCiteBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/caiyuhu/MCiteBench",
                        "paperLink": "https://arxiv.org/abs/2503.02589",
                        "officialWebsiteLink": "https://caiyuhu.github.io/MCiteBench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "129",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-20 14:11:57",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-20 14:11:57",
                        "createDate": "2025-03-20 14:11:46",
                        "desc": {
                            "cn": "MCiteBench 是一个用于评估多模态大模型中多模态引用文本生成的基准，由 1,749 篇学术论文中的 3,000 个样本组成，包括 2,000 个解释任务和 1,000 个定位任务，在文本、图表、表格和混合模态中平衡证据。",
                            "en": "MCiteBench is a benchmark to evaluate multimodal citation text generation in MLLMs. It consists of 3,000 samples from 1,749 academic papers, featuring 2,000 Explanation tasks and 1,000 Locating tasks, with balanced evidence across text, figures, tables, and mixed modalities."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mcitebench'. Error: Path opencompass/mcitebench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1891",
                    "name": "MedXpertQA",
                    "version": "1.0.0",
                    "description": "We introduce MedXpertQA, a highly challenging and comprehensive medical benchmark to evaluate expert-level medical knowledge and advanced reasoning. \nIt has been accepted by ICML 2025 and selected by Google DeepMind as the benchmark for MedGemma.",
                    "url": "opencompass/opencompass_1891.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1891",
                    "sample_count": 1000,
                    "traits": [
                        "Examination",
                        "Reasoning",
                        "Knowledge"
                    ],
                    "eval_type": {
                        "_gen": "MedXpertQAgen"
                    },
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1891",
                        "name": "MedXpertQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "学科",
                                "en": "Examination"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Medical",
                                "en": "Medical"
                            },
                            {
                                "cn": "ICML 2025",
                                "en": "ICML 2025"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/TsinghuaC3I/MedXpertQA",
                        "paperLink": "https://arxiv.org/abs/2501.18362",
                        "officialWebsiteLink": "https://medxpertqa.github.io",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "85505819",
                            "name": "pjlab-opencompass",
                            "avatar": null,
                            "nickname": "pjlab-opencompass"
                        },
                        "lookNum": "129",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 21:34:53",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 21:34:53",
                        "createDate": "2025-06-04 21:02:16",
                        "desc": {
                            "cn": "MedXpertQA是由清华大学和上海人工智能实验室构建的全面且具有高度挑战性的医学基准，用于评估专家级的医学知识和高级推理能力。论文已被ICML 2025接收，并被Google DeepMind使用作为MedGemma的评估基准。MedXpertQA 共包含 4,460 道题目，涵盖 17 个医学专科和 11 个身体系统。该基准包含两个子集：用于文本医学能力评估的 Text 子集，以及用于多模态医学能力评估的 MM 子集。MM 子集首次引入了带有多样化图像和丰富临床信息（如病历和检查结果）的专家级考试题，区别于传统多模态医学基准中基于图像描述生成的简单问答对。",
                            "en": "We introduce MedXpertQA, a highly challenging and comprehensive medical benchmark to evaluate expert-level medical knowledge and advanced reasoning. \nIt has been accepted by ICML 2025 and selected by Google DeepMind as the benchmark for MedGemma."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medxpertqa'. Error: Path opencompass/medxpertqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1607",
                    "name": "ToolRet",
                    "version": "1.0.0",
                    "description": "ToolRet is a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets.",
                    "url": "opencompass/opencompass_1607.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1607",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1607",
                        "name": "ToolRet",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/mangopy/tool-retrieval-benchmark",
                        "paperLink": "https://arxiv.org/abs/2503.01763",
                        "officialWebsiteLink": "https://mangopy.github.io/tool-retrieval-benchmark/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "128",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:43:53",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:43:53",
                        "createDate": "2025-03-06 14:40:00",
                        "desc": {
                            "cn": "ToolRet是一个包含 7.6k 个不同检索任务的异构工具检索基准，以及从现有数据集中收集的 43k 个工具语料库。",
                            "en": "ToolRet is a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/toolret'. Error: Path opencompass/toolret is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1361",
                    "name": "RealworldQA",
                    "version": "1.0.0",
                    "description": "RealWorldQA is a benchmark designed for real-world understanding, including 765 images, each accompanied by a question and a verifiable answer.",
                    "url": "opencompass/opencompass_1361.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1361",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1361",
                        "name": "RealworldQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "128",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 17:25:35",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 17:25:35",
                        "createDate": "2025-01-09 22:10:13",
                        "desc": {
                            "cn": "RealWorldQA用于评估多模态模型在现实世界中的空间理解能力，包含765张图像，每张图像都配有一个问题和易于验证的答案。",
                            "en": "RealWorldQA is a benchmark designed for real-world understanding, including 765 images, each accompanied by a question and a verifiable answer."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/realworldqa'. Error: Path opencompass/realworldqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1635",
                    "name": "KnowLogic",
                    "version": "1.0.0",
                    "description": "KnowLogic is a knowledge-driven synthetic benchmark designed to evaluate the reasoning abilities of large language models (LLMs). It includes 5400 bilingual (Chinese and English) questions across various domains, covering different aspects of commonsense knowledge and logical reasoning.",
                    "url": "opencompass/opencompass_1635.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1635",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1635",
                        "name": "KnowLogic",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/pokerwf/KnowLogic",
                        "paperLink": "https://arxiv.org/abs/2503.06218",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "128",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-13 15:31:21",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-13 15:31:21",
                        "createDate": "2025-03-13 14:36:03",
                        "desc": {
                            "cn": "KnowLogic 是一个以知识驱动的合成基准，旨在评估大型语言模型的推理能力（LLMs）。它包含涵盖各个领域、涵盖常识知识和逻辑推理不同方面的 5400 个中英双语问题。",
                            "en": "KnowLogic is a knowledge-driven synthetic benchmark designed to evaluate the reasoning abilities of large language models (LLMs). It includes 5400 bilingual (Chinese and English) questions across various domains, covering different aspects of commonsense knowledge and logical reasoning."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/knowlogic'. Error: Path opencompass/knowlogic is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1376",
                    "name": "GenAI-Bench",
                    "version": "1.0.0",
                    "description": "GenAI-Bench is a benchmark designed to benchmark MLLMs’s ability in judging the quality of AI generative contents, containing over 40,000 human ratings to evaluate the performance of MLLMs on aligning with human preferences.",
                    "url": "opencompass/opencompass_1376.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1376",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1376",
                        "name": "GenAI-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/TIGER-AI-Lab/GenAI-Bench",
                        "paperLink": "https://arxiv.org/abs/2406.13743",
                        "officialWebsiteLink": "https://linzhiqiu.github.io/papers/genai_bench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "127",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:33",
                        "createDate": "2025-01-10 17:02:53",
                        "desc": {
                            "cn": "GenAI-Bench用于衡量MLLM判断AI生成内容质量的能力，包含超过40000个人工评分用以评估大模型与人类偏好的一致性。",
                            "en": "GenAI-Bench is a benchmark designed to benchmark MLLMs’s ability in judging the quality of AI generative contents, containing over 40,000 human ratings to evaluate the performance of MLLMs on aligning with human preferences."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/genai_bench'. Error: Path opencompass/genai_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1453",
                    "name": "MMIU",
                    "version": "1.0.0",
                    "description": "MMIU test MLLMs' multi-image understanding capabilities, encompassesing 7 types of multi-image relationships, 52 tasks, 77K images, and 11K meticulously curated multiple-choice questions.",
                    "url": "opencompass/opencompass_1453.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1453",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1453",
                        "name": "MMIU",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OpenGVLab/MMIU",
                        "paperLink": "https://arxiv.org/abs/2408.02718",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "127",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-02-26 18:55:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-02-26 18:55:33",
                        "createDate": "2025-01-27 14:55:19",
                        "desc": {
                            "cn": "MMIU用于评估多模态大模型的多图理解能力，包含7种类型的多图像关系、52个任务、77K图像和11K精心策划的多选题。",
                            "en": "MMIU test MLLMs' multi-image understanding capabilities, encompassesing 7 types of multi-image relationships, 52 tasks, 77K images, and 11K meticulously curated multiple-choice questions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmiu'. Error: Path opencompass/mmiu is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1524",
                    "name": "RM-Bench",
                    "version": "1.0.0",
                    "description": "RM-Bench, a benchmark dataset for evaluating reward models of language modeling.",
                    "url": "opencompass/opencompass_1524.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1524",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1524",
                        "name": "RM-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/THU-KEG/RM-Bench",
                        "paperLink": "https://arxiv.org/abs/2410.16184",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "126",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 17:00:31",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 17:00:31",
                        "createDate": "2025-02-21 11:34:57",
                        "desc": {
                            "cn": "RM-Bench，一个用于评估语言模型奖励模型的基准数据集",
                            "en": "RM-Bench, a benchmark dataset for evaluating reward models of language modeling."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/rm_bench'. Error: Path opencompass/rm_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1648",
                    "name": "DME",
                    "version": "1.0.0",
                    "description": "VLB provides a robust and comprehensive assessment for LVLMs with reduced data contamination and flexible complexity. Based on LlavaBench and MMvet, we have curated two more challenging versions of the datasets: LlavaBench_hard and MMvet_hard. These are the hardest multimodal combinations.",
                    "url": "opencompass/opencompass_1648.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1648",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1648",
                        "name": "DME",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/yangyue5114/DME",
                        "paperLink": "https://arxiv.org/abs/2410.08695",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "125",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-18 17:39:57",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-18 17:39:57",
                        "createDate": "2025-03-18 17:37:18",
                        "desc": {
                            "cn": "VLB 为 LVLMs 提供了一种稳健且全面的评估，降低了数据污染并具有灵活的复杂性。基于 LlavaBench 和 MMvet，我们精心制作了两个更具挑战性的数据集版本：LlavaBench_hard 和 MMvet_hard。这些是我们动态策略中最具挑战性的多模态组合（V1+L4）。",
                            "en": "VLB provides a robust and comprehensive assessment for LVLMs with reduced data contamination and flexible complexity. Based on LlavaBench and MMvet, we have curated two more challenging versions of the datasets: LlavaBench_hard and MMvet_hard. These are the hardest multimodal combinations."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/dme'. Error: Path opencompass/dme is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1605",
                    "name": "Deepfake-Eval-2024",
                    "version": "1.0.0",
                    "description": "Deepfake-Eval-2024 is an in-the-wild deepfake dataset. Deepfake-Eval-2024 contains 44 hours of videos, 56.5 hours of audio, and 1,975 images, encompassing contemporary manipulation technologies, diverse media content, 88 different website sources, and 52 different languages. ",
                    "url": "opencompass/opencompass_1605.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1605",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1605",
                        "name": "Deepfake-Eval-2024",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/nuriachandra/Deepfake-Eval-2024",
                        "paperLink": "https://arxiv.org/abs/2503.02857",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "124",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:45:17",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:45:17",
                        "createDate": "2025-03-06 14:04:04",
                        "desc": {
                            "cn": "Deepfake-Eval-2024 是一个真实场景的深度伪造数据集。该数据集包含44小时的视频、56.5小时的音频和1,975张图像，涵盖当代篡改技术、多样化的媒体内容、来自88个不同网站来源的素材以及52种不同语言。",
                            "en": "Deepfake-Eval-2024 is an in-the-wild deepfake dataset. Deepfake-Eval-2024 contains 44 hours of videos, 56.5 hours of audio, and 1,975 images, encompassing contemporary manipulation technologies, diverse media content, 88 different website sources, and 52 different languages. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/deepfake_eval_2024'. Error: Path opencompass/deepfake_eval_2024 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1537",
                    "name": "MVL-SIB",
                    "version": "1.0.0",
                    "description": "MVL-SIB is a multilingual dataset that provides image-sentence pairs spanning 205 languages and 7 topical categories (entertainment, geography, health, politics, science, sports, travel). It was constructed by extending the SIB-200 benchmark.",
                    "url": "opencompass/opencompass_1537.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1537",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding",
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1537",
                        "name": "MVL-SIB",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            },
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2502.12852",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "123",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-03 11:48:04",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-03 11:48:04",
                        "createDate": "2025-02-24 18:19:14",
                        "desc": {
                            "cn": "MVL-SIB 是一个多语言数据集，提供了涵盖 205 种语言和 7 个主题类别的图像-句子对（ entertainment ， geography ， health ， politics ， science ， sports ， travel ）。它通过扩展 SIB-200 基准构建而成。",
                            "en": "MVL-SIB is a multilingual dataset that provides image-sentence pairs spanning 205 languages and 7 topical categories (entertainment, geography, health, politics, science, sports, travel). It was constructed by extending the SIB-200 benchmark."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mvl_sib'. Error: Path opencompass/mvl_sib is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1319",
                    "name": "ActionAtlas",
                    "version": "1.0.0",
                    "description": "ActionAtlas is a multiple-choice video question answering benchmark, including 934 videos showcasing 580 unique actions across 56 sports, with a total of 1896 actions within choices.",
                    "url": "opencompass/opencompass_1319.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1319",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1319",
                        "name": "ActionAtlas",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/mrsalehi/action-atlas",
                        "paperLink": "https://arxiv.org/abs/2410.05774",
                        "officialWebsiteLink": "https://mrsalehi.github.io/action-atlas/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "123",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 15:07:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 15:07:55",
                        "createDate": "2024-12-31 15:19:23",
                        "desc": {
                            "cn": "ActionAtlas是一个多项选择视频问答基准测试，包括934个视频，展示了56项运动中的580个独特动作，选项共包含1896个动作。",
                            "en": "ActionAtlas is a multiple-choice video question answering benchmark, including 934 videos showcasing 580 unique actions across 56 sports, with a total of 1896 actions within choices."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/actionatlas'. Error: Path opencompass/actionatlas is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1581",
                    "name": "HoloBench",
                    "version": "1.0.0",
                    "description": "HoloBench is a benchmark designed to evaluate the ability of long-context language models (LCLMs) to perform holistic reasoning over extended text contexts. ",
                    "url": "opencompass/opencompass_1581.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1581",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning",
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1581",
                        "name": "HoloBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "长文本",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/megagonlabs/holobench",
                        "paperLink": "https://arxiv.org/abs/2410.11996",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "123",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:46:42",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:46:42",
                        "createDate": "2025-03-04 10:53:51",
                        "desc": {
                            "cn": "HoloBench 是一个用于评估长上下文语言模型（LCLMs）在扩展文本上下文中进行整体推理能力的基准测试。",
                            "en": "HoloBench is a benchmark designed to evaluate the ability of long-context language models (LCLMs) to perform holistic reasoning over extended text contexts. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/holobench'. Error: Path opencompass/holobench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1671",
                    "name": "Forensics-bench",
                    "version": "1.0.0",
                    "description": "A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models",
                    "url": "opencompass/opencompass_1671.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1671",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1671",
                        "name": "Forensics-bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Forgery Detection",
                                "en": "Forgery Detection"
                            },
                            {
                                "cn": "Large Vision Language Models",
                                "en": "Large Vision Language Models"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Forensics-Bench/Forensics-Bench",
                        "paperLink": "https://arxiv.org/pdf/2503.15024",
                        "officialWebsiteLink": "https://forensics-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "5001578",
                            "name": null,
                            "avatar": null,
                            "nickname": "劲劲"
                        },
                        "lookNum": "123",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-22 12:30:46",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-22 12:30:46",
                        "createDate": "2025-03-24 22:19:51",
                        "desc": {
                            "cn": "A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models",
                            "en": "A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/forensics_bench'. Error: Path opencompass/forensics_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1843",
                    "name": "EWMBench",
                    "version": "1.0.0",
                    "description": "EWMBench is a benchmark for evaluating Embodied World Models, covering aspects such as scene consistency, motion correctness, and semantic alignment. It includes a diverse dataset and multi-dimensional metrics tailored for embodied manipulation tasks.",
                    "url": "opencompass/opencompass_1843.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1843",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Agent",
                        "Instruct"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1843",
                        "name": "EWMBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            },
                            {
                                "cn": "指令跟随",
                                "en": "Instruct"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "worldmodel",
                                "en": "worldmodel"
                            },
                            {
                                "cn": "embodied AI",
                                "en": "embodied AI"
                            },
                            {
                                "cn": "manipulate",
                                "en": "manipulate"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AgibotTech/EWMBench",
                        "paperLink": "https://arxiv.org/abs/2505.09694",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "80003662",
                            "name": null,
                            "avatar": null,
                            "nickname": "NoomiHu"
                        },
                        "lookNum": "122",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-05-27 12:53:24",
                        "supportOnlineEval": false,
                        "updateDate": "2025-05-27 12:53:24",
                        "createDate": "2025-05-27 10:49:31",
                        "desc": {
                            "cn": "EWMBench 是一个用于评估具身世界模型的基准，涵盖“场景一致性”、“动作正确性”和“语义对齐”等方面。它包含多样化的数据集和面向具身任务的多维评估指标，能够揭示现有模型的局限，并为生成具物理基础、任务导向的视频提供评价标准。\n",
                            "en": "EWMBench is a benchmark for evaluating Embodied World Models, covering aspects such as scene consistency, motion correctness, and semantic alignment. It includes a diverse dataset and multi-dimensional metrics tailored for embodied manipulation tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ewmbench'. Error: Path opencompass/ewmbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1329",
                    "name": "OlympicArena",
                    "version": "1.0.0",
                    "description": "OlympicArena evaluates cognitive reasoning abilities. It includes 11,163 bilingual problems spanning seven fields and 62 international Olympic competitions. ",
                    "url": "opencompass/opencompass_1329.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1329",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1329",
                        "name": "OlympicArena",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/GAIR-NLP/OlympicArena",
                        "paperLink": "https://arxiv.org/abs/2406.12753",
                        "officialWebsiteLink": "https://gair-nlp.github.io/OlympicArena/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "121",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 16:32:39",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 16:32:39",
                        "createDate": "2025-01-03 11:50:50",
                        "desc": {
                            "cn": "OlympicArena用于评估大模型的认知推理能力，包含来自7个领域、62项国际奥林匹克比赛的11163个双语问题。",
                            "en": "OlympicArena evaluates cognitive reasoning abilities. It includes 11,163 bilingual problems spanning seven fields and 62 international Olympic competitions. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/olympicarena'. Error: Path opencompass/olympicarena is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1853",
                    "name": "MiniLongBench",
                    "version": "1.0.0",
                    "description": "MiniLongBench is a low-cost benchmark for evaluating the Long Context Understanding (LCU) capabilities of LLMs, featuring a compact yet diverse test set of only 237 samples spanning 6 major task categories and 21 distinct tasks.",
                    "url": "opencompass/opencompass_1853.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1853",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding",
                        "Language",
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1853",
                        "name": "MiniLongBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            },
                            {
                                "cn": "语言",
                                "en": "Language"
                            },
                            {
                                "cn": "长文本",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MilkThink-Lab/MiniLongBench",
                        "paperLink": "https://arxiv.org/abs/2505.19959",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "43307103",
                            "name": null,
                            "avatar": null,
                            "nickname": "linggm3"
                        },
                        "lookNum": "121",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-05-30 10:20:05",
                        "supportOnlineEval": false,
                        "updateDate": "2025-05-30 10:20:05",
                        "createDate": "2025-05-27 21:12:49",
                        "desc": {
                            "cn": "MiniLongBench is a low-cost benchmark for evaluating the Long Context Understanding (LCU) capabilities of LLMs, featuring a compact yet diverse test set of only 237 samples spanning 6 major task categories and 21 distinct tasks.",
                            "en": "MiniLongBench is a low-cost benchmark for evaluating the Long Context Understanding (LCU) capabilities of LLMs, featuring a compact yet diverse test set of only 237 samples spanning 6 major task categories and 21 distinct tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/minilongbench'. Error: Path opencompass/minilongbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1500",
                    "name": "CRPE",
                    "version": "1.0.0",
                    "description": "CRPE is a benchmark designed to quantitatively evaluate the object recognition and relation comprehension ability of models. It consists of four splits, and the evaluation is formulated as single-choice questions. ",
                    "url": "opencompass/opencompass_1500.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1500",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1500",
                        "name": "CRPE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OpenGVLab/all-seeing",
                        "paperLink": "https://arxiv.org/abs/2402.19474",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "121",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:46:25",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:46:25",
                        "createDate": "2025-02-13 19:32:36",
                        "desc": {
                            "cn": "CRPE用于定量评估多模态大模型的对象识别和关系理解能力，分为四部分，以单选题形式呈现。",
                            "en": "CRPE is a benchmark designed to quantitatively evaluate the object recognition and relation comprehension ability of models. It consists of four splits, and the evaluation is formulated as single-choice questions. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/crpe'. Error: Path opencompass/crpe is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1327",
                    "name": "EHRNoteQA",
                    "version": "1.0.0",
                    "description": "EHRNoteQA evaluates LLM's ability to assist clinical decision-making based on electronic health records, comprising 962 different QA pairs each linked to distinct patients' discharge summaries. ",
                    "url": "opencompass/opencompass_1327.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1327",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1327",
                        "name": "EHRNoteQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ji-youn-kim/EHRNoteQA",
                        "paperLink": "https://arxiv.org/abs/2402.16040",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "120",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 16:32:43",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 16:32:43",
                        "createDate": "2025-01-02 20:15:09",
                        "desc": {
                            "cn": "EHRNoteQA用于评估LLM基于电子健康记录辅助临床决策的能力，由962个问答对组成，每个问答对都与不同患者的出院总结相关联。",
                            "en": "EHRNoteQA evaluates LLM's ability to assist clinical decision-making based on electronic health records, comprising 962 different QA pairs each linked to distinct patients' discharge summaries. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ehrnoteqa'. Error: Path opencompass/ehrnoteqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1855",
                    "name": "TransBench",
                    "version": "1.0.0",
                    "description": "TransBench is the first industry-oriented comprehensive multilingual translation evaluation system designed for industrial applications. It quantifies translation model performance across diverse industries and linguistic environments through meticulously curated datasets aligned with standards.",
                    "url": "opencompass/opencompass_1855.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1855",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1855",
                        "name": "TransBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AIDC-AI/TransBench",
                        "paperLink": "https://arxiv.org/pdf/2505.14244",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "120",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:20",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:20",
                        "createDate": "2025-06-04 11:17:40",
                        "desc": {
                            "cn": "TransBench is the first industry-oriented comprehensive multilingual translation evaluation system designed for industrial applications. It quantifies translation model performance across diverse industries and linguistic environments through meticulously curated datasets aligned with standards.",
                            "en": "TransBench is the first industry-oriented comprehensive multilingual translation evaluation system designed for industrial applications. It quantifies translation model performance across diverse industries and linguistic environments through meticulously curated datasets aligned with standards."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/transbench'. Error: Path opencompass/transbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1554",
                    "name": "JL1-CD",
                    "version": "1.0.0",
                    "description": "JL1-CD is a large-scale, sub-meter, all-inclusive open-source dataset for remote sensing image change detection (CD). It contains 5,000 pairs of 512×512 pixel satellite images with a resolution of 0.5 to 0.75 meters, covering various types of surface changes in multiple regions of China. ",
                    "url": "opencompass/opencompass_1554.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1554",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1554",
                        "name": "JL1-CD",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/circleLZY/MTKD-CD",
                        "paperLink": "https://arxiv.org/abs/2502.13407",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "119",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:46:12",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:46:12",
                        "createDate": "2025-03-05 18:54:32",
                        "desc": {
                            "cn": "JL1-CD 是一个大规模、亚米级、全包含的开源遥感影像变化检测（CD）数据集。它包含 5000 对 512×512 像素的卫星影像，分辨率为 0.5 至 0.75 米，覆盖中国多个地区的各种地表变化。",
                            "en": "JL1-CD is a large-scale, sub-meter, all-inclusive open-source dataset for remote sensing image change detection (CD). It contains 5,000 pairs of 512×512 pixel satellite images with a resolution of 0.5 to 0.75 meters, covering various types of surface changes in multiple regions of China. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/jl1_cd'. Error: Path opencompass/jl1_cd is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1120",
                    "name": "ProofNet",
                    "version": "1.0.0",
                    "description": "ProofNet is a benchmark for autoformalization and formal proving of undergraduate-level mathematics. The ProofNet benchmarks consists of 371 examples, each consisting of a formal theorem statement in Lean 3, a natural language theorem statement, and a natural language proof. ",
                    "url": "opencompass/opencompass_1120.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1120",
                    "sample_count": 1000,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1120",
                        "name": "ProofNet",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/zhangir-azerbayev/ProofNet",
                        "paperLink": "https://arxiv.org/pdf/2302.12433",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "118",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-10 20:21:42",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-10 20:21:42",
                        "createDate": "2024-10-10 14:54:13",
                        "desc": {
                            "cn": "ProofNet 是一个用于本科数学的自动形式化和形式证明的基准。包含 371 个示例，每个示例包括一个 Lean 3 中的形式定理陈述、一个自然语言定理陈述和一个自然语言证明。这些问题主要来自流行的本科纯数学教材，涵盖实分析、复分析、线性代数、抽象代数和拓扑等主题。",
                            "en": "ProofNet is a benchmark for autoformalization and formal proving of undergraduate-level mathematics. The ProofNet benchmarks consists of 371 examples, each consisting of a formal theorem statement in Lean 3, a natural language theorem statement, and a natural language proof. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/proofnet'. Error: Path opencompass/proofnet is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1145",
                    "name": "FREB-TQA",
                    "version": "1.0.0",
                    "description": "FREB-TQA is a Fine-grained Robustness Evaluation Benchmark for Table Question Answering. ",
                    "url": "opencompass/opencompass_1145.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1145",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1145",
                        "name": "FREB-TQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NAACL 2024",
                                "en": "NAACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/boschresearch/FREB-TQA",
                        "paperLink": "https://aclanthology.org/2024.naacl-long.137.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50109778",
                            "name": "Bosch_Research",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/50109778-dd360e6a-5366-4ab9-8d82-260b4e376600.png",
                            "nickname": "OpenXLab-XxvqMQwcb"
                        },
                        "lookNum": "118",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-27 17:23:19",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-27 17:23:19",
                        "createDate": "2024-10-15 13:24:34",
                        "desc": {
                            "cn": "FREB-TQA 是一个细粒度的稳健性评估基准，专注于表格问答（TQA）。",
                            "en": "FREB-TQA is a Fine-grained Robustness Evaluation Benchmark for Table Question Answering. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/freb_tqa'. Error: Path opencompass/freb_tqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1712",
                    "name": "ToolHop",
                    "version": "1.0.0",
                    "description": "ToolHop is a dataset specifically designed for rigorous evaluation of multi-hop tool use, which ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach.",
                    "url": "opencompass/opencompass_1712.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1712",
                    "sample_count": 1000,
                    "traits": [
                        "Agent",
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1712",
                        "name": "ToolHop",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            },
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://huggingface.co/datasets/bytedance-research/ToolHop",
                        "paperLink": "https://arxiv.org/abs/2501.02506",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "62407087",
                            "name": null,
                            "avatar": null,
                            "nickname": "尼摩"
                        },
                        "lookNum": "118",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-07 15:15:34",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-07 15:15:34",
                        "createDate": "2025-04-05 23:37:16",
                        "desc": {
                            "cn": "ToolHop是一个通过查询驱动构建的数据集，专门用于评测大模型的多跳工具使用能力，具备多样化的查询、有意义的相互依赖关系、本地可执行的工具、详细的反馈和可验证的答案五大特征。",
                            "en": "ToolHop is a dataset specifically designed for rigorous evaluation of multi-hop tool use, which ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/toolhop'. Error: Path opencompass/toolhop is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1279",
                    "name": "WhodunitBench",
                    "version": "1.0.0",
                    "description": "WhodunitBench is used to evaluate large multimodal agent under complex tasks and dynamic scenarios.",
                    "url": "opencompass/opencompass_1279.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1279",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1279",
                        "name": "WhodunitBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/jun0wanan/WhodunitBench-Murder_Mystery_Games",
                        "paperLink": "https://openreview.net/pdf?id=qmvtDIfbmS",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "117",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 18:10:25",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 18:10:25",
                        "createDate": "2024-12-24 20:24:49",
                        "desc": {
                            "cn": "WhodunitBench用于评估大型多模式代理在复杂任务场景下的动态评估。",
                            "en": "WhodunitBench is used to evaluate large multimodal agent under complex tasks and dynamic scenarios."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/whodunitbench'. Error: Path opencompass/whodunitbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1579",
                    "name": "PosterSum",
                    "version": "1.0.0",
                    "description": "The PosterSum dataset is a multimodal benchmark designed for the summarization of scientific posters into research paper abstracts. The dataset consists of 16,305 research posters collected from major machine learning conferences, including ICLR, ICML, and NeurIPS, spanning the years 2022-2024. ",
                    "url": "opencompass/opencompass_1579.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1579",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1579",
                        "name": "PosterSum",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/saxenarohit/postersum",
                        "paperLink": "https://arxiv.org/abs/2502.17540",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "116",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:46:01",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:46:01",
                        "createDate": "2025-03-05 18:56:15",
                        "desc": {
                            "cn": "PosterSum 数据集是一个多模态基准数据集，旨在将科学海报总结成研究论文摘要。该数据集包含从2022-2024年的主要机器学习会议（包括 ICLR、ICML 和 NeurIPS）收集的 16,305 篇研究海报。",
                            "en": "The PosterSum dataset is a multimodal benchmark designed for the summarization of scientific posters into research paper abstracts. The dataset consists of 16,305 research posters collected from major machine learning conferences, including ICLR, ICML, and NeurIPS, spanning the years 2022-2024. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/postersum'. Error: Path opencompass/postersum is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1625",
                    "name": "ubuntu_osworld",
                    "version": "1.0.0",
                    "description": "OSWorld is a first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across operating systems. ",
                    "url": "opencompass/opencompass_1625.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1625",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1625",
                        "name": "ubuntu_osworld",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/xlang-ai/OSWorld",
                        "paperLink": "https://arxiv.org/abs/2404.07972",
                        "officialWebsiteLink": "https://os-world.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "116",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-12 11:07:38",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-12 11:07:38",
                        "createDate": "2025-03-11 16:09:49",
                        "desc": {
                            "cn": "OSWorld 是一个首创的、可扩展的、真实计算机环境，用于多模态智能体，支持操作系统跨平台的任务设置、基于执行的评估和交互式学习。",
                            "en": "OSWorld is a first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across operating systems. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ubuntu_osworld'. Error: Path opencompass/ubuntu_osworld is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1243",
                    "name": "EmbodiedAgentInterface",
                    "version": "1.0.0",
                    "description": "Embodied Agent Interface supports the formalization of various types of tasks and input-output specifications of LLM-based modules, offering a comprehensive assessment of LLMs' performance for different subtasks and pinpointing the strengths and weaknesses in LLM-powered embodied AI systems.",
                    "url": "opencompass/opencompass_1243.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1243",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1243",
                        "name": "EmbodiedAgentInterface",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Embodied Decision Making",
                                "en": "Embodied Decision Making"
                            }
                        ],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/embodied-agent-interface/embodied-agent-interface",
                        "paperLink": "https://arxiv.org/abs/2410.07166",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "116",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 11:46:31",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 11:46:31",
                        "createDate": "2024-12-20 17:40:13",
                        "desc": {
                            "cn": "Embodied Agent Interface支持各种类型的任务和基于LLM模块的输入输出规范的形式化，对LLM在不同子任务中的性能进行了全面评估，指出了基于LLM的具身AI系统的优势和劣势。",
                            "en": "Embodied Agent Interface supports the formalization of various types of tasks and input-output specifications of LLM-based modules, offering a comprehensive assessment of LLMs' performance for different subtasks and pinpointing the strengths and weaknesses in LLM-powered embodied AI systems."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/embodiedagentinterface'. Error: Path opencompass/embodiedagentinterface is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1908",
                    "name": "AudioTrust",
                    "version": "1.0.0",
                    "description": "AudioTrust is a comprehensive trust evaluation framework for Audio Large Language Models (ALLMs) that effectively reveals potential risks in six dimensions: fairness, hallucination, security, privacy, robustness, and authentication. It aggregates over 4,420 real-world audio/text data samples, coveri",
                    "url": "opencompass/opencompass_1908.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1908",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1908",
                        "name": "AudioTrust",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/JusperLee/AudioTrust",
                        "paperLink": "https://arxiv.org/abs/2505.16211",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "21000833",
                            "name": null,
                            "avatar": null,
                            "nickname": "JusperLee"
                        },
                        "lookNum": "115",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-09 11:29:41",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-09 11:29:41",
                        "createDate": "2025-06-06 17:31:02",
                        "desc": {
                            "cn": "AudioTrust针对Audio Large Language Models（ALLMs）的全方位可信评估框架，有效揭示音频大模型在公平性、幻觉、安全、隐私、鲁棒性和身份验证六大维度的潜在风险。汇集4,420+条真实场景音频/文本数据，覆盖日常对话、紧急呼叫、语音助手等18种实验设置，设计9项音频特定评测指标，构建自动化评估流水线。主要发现：闭源模型在鲁棒性和安全防护上表现更佳，开源模型对隐私和公平性仍存盲区；多数ALLMs对性别、口音、年龄等敏感属性存在系统性偏见。期待研究者基于AudioTrust继续优化音频大模型，共同推动更安全、可信的AI音频生态发展！",
                            "en": "AudioTrust is a comprehensive trust evaluation framework for Audio Large Language Models (ALLMs) that effectively reveals potential risks in six dimensions: fairness, hallucination, security, privacy, robustness, and authentication. It aggregates over 4,420 real-world audio/text data samples, coveri"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/audiotrust'. Error: Path opencompass/audiotrust is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1587",
                    "name": "MR-GSM8K",
                    "version": "1.0.0",
                    "description": "MR-GSM8K is a challenging benchmark designed to evaluate the meta-reasoning capabilities of state-of-the-art Large Language Models (LLMs). It goes beyond traditional evaluation metrics by focusing on the reasoning process rather than just the final answer.",
                    "url": "opencompass/opencompass_1587.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1587",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1587",
                        "name": "MR-GSM8K",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/dvlab-research/MR-GSM8K",
                        "paperLink": "https://arxiv.org/abs/2312.17080",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "114",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:47:12",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:47:12",
                        "createDate": "2025-03-04 14:13:29",
                        "desc": {
                            "cn": "MR-GSM8K 是一个旨在评估最先进大型语言模型（LLMs）元推理能力的挑战性基准。它超越了传统的评估指标，专注于推理过程而非仅仅关注最终答案，从而对模型的认知能力进行更细致的评估。",
                            "en": "MR-GSM8K is a challenging benchmark designed to evaluate the meta-reasoning capabilities of state-of-the-art Large Language Models (LLMs). It goes beyond traditional evaluation metrics by focusing on the reasoning process rather than just the final answer."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mr_gsm8k'. Error: Path opencompass/mr_gsm8k is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1609",
                    "name": "MASK",
                    "version": "1.0.0",
                    "description": "The MASK evaluation provides a rigorous benchmark for evaluating honesty in large language models by measuring whether models remain truthful when incentivized to lie. The public set contains 1,028 high-quality human-labeled examples across six distinct archetypes.",
                    "url": "opencompass/opencompass_1609.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1609",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1609",
                        "name": "MASK",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/centerforaisafety/mask",
                        "paperLink": "https://arxiv.org/abs/2503.03750",
                        "officialWebsiteLink": "https://www.mask-benchmark.ai/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "114",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:43:36",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:43:36",
                        "createDate": "2025-03-06 16:39:16",
                        "desc": {
                            "cn": "MASK 评估提供了一个严格的基准，用于评估大型语言模型中的诚实度，通过测量模型在受到诱使说谎的激励时是否保持真实性。公共集包含 1,028 个高质量的人标注示例，涵盖六个不同的原型。",
                            "en": "The MASK evaluation provides a rigorous benchmark for evaluating honesty in large language models by measuring whether models remain truthful when incentivized to lie. The public set contains 1,028 high-quality human-labeled examples across six distinct archetypes."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mask'. Error: Path opencompass/mask is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1574",
                    "name": "Text2World",
                    "version": "1.0.0",
                    "description": "Text2World, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. ",
                    "url": "opencompass/opencompass_1574.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1574",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1574",
                        "name": "Text2World",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Aaron617/text2world",
                        "paperLink": "https://arxiv.org/abs/2502.13092",
                        "officialWebsiteLink": "https://text-to-world.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "113",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-04 16:58:03",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-04 16:58:03",
                        "createDate": "2025-03-03 14:38:08",
                        "desc": {
                            "cn": "Text2World 基于规划领域定义语言 (PDDL)，拥有数百个不同的领域，并采用多标准、基于执行的衡量标准来进行更稳健的评估。",
                            "en": "Text2World, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/text2world'. Error: Path opencompass/text2world is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1358",
                    "name": "Video-MME",
                    "version": "1.0.0",
                    "description": "Video-MME is an evaluation benchmark of multi-modal LLMs in video analysis, including 900 videos in various duration with a total of 254 hours which spans 6 primary visual domains with 30 subfields.",
                    "url": "opencompass/opencompass_1358.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1358",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1358",
                        "name": "Video-MME",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/BradyFU/Video-MME",
                        "paperLink": "https://github.com/BradyFU/Video-MME",
                        "officialWebsiteLink": "https://video-mme.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "113",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:57",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:57",
                        "createDate": "2025-01-09 21:09:31",
                        "desc": {
                            "cn": "Video-MME用于评估多模态大模型的视频分析能力，包含900个不同长度的视频，来自6个主要视觉领域和30个子领域，总时长达254小时。",
                            "en": "Video-MME is an evaluation benchmark of multi-modal LLMs in video analysis, including 900 videos in various duration with a total of 254 hours which spans 6 primary visual domains with 30 subfields."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/video_mme'. Error: Path opencompass/video_mme is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1364",
                    "name": "MMT-Bench",
                    "version": "1.0.0",
                    "description": "MMT-Bench is designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning. It comprises 31,325 multi-choice visual questions, covering 32 core meta-tasks and 162 subtasks in multimodal understanding.",
                    "url": "opencompass/opencompass_1364.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1364",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1364",
                        "name": "MMT-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OpenGVLab/MMT-Bench",
                        "paperLink": "https://arxiv.org/abs/2404.16006",
                        "officialWebsiteLink": "https://mmt-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "113",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:44",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:44",
                        "createDate": "2025-01-10 11:56:21",
                        "desc": {
                            "cn": "MMT-Bench考察多模态大模型的视觉识别、定位、推理和规划能力，包括31325个多选视觉问题，涵盖了32个核心元任务和162个多模态理解子任务。",
                            "en": "MMT-Bench is designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning. It comprises 31,325 multi-choice visual questions, covering 32 core meta-tasks and 162 subtasks in multimodal understanding."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmt_bench'. Error: Path opencompass/mmt_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1730",
                    "name": "PaperBench",
                    "version": "1.0.0",
                    "description": "PaperBench is a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research.",
                    "url": "opencompass/opencompass_1730.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1730",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1730",
                        "name": "PaperBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/openai/preparedness",
                        "paperLink": "https://arxiv.org/abs/2504.01848",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "113",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:12:27",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:12:27",
                        "createDate": "2025-04-11 14:10:05",
                        "desc": {
                            "cn": "PaperBench，这是一个评估AI代理复制最新AI研究能力的基准测试。",
                            "en": "PaperBench is a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/paperbench'. Error: Path opencompass/paperbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1580",
                    "name": "A-Bench",
                    "version": "1.0.0",
                    "description": " A-Bench is a benchmark designed to diagnose whether LMMs are masters at evaluating AIGIs. 2,864 AIGIs from 16 text-to-image models are sampled, each paired with question-answers annotated by human experts, and tested across 18 leading LMMs.",
                    "url": "opencompass/opencompass_1580.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1580",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1580",
                        "name": "A-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Q-Future/A-Bench",
                        "paperLink": "https://arxiv.org/abs/2406.03070",
                        "officialWebsiteLink": "https://a-bench-sjtu.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "112",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:46:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:46:33",
                        "createDate": "2025-03-04 10:43:10",
                        "desc": {
                            "cn": "A-Bench是一个旨在诊断 LMMs 是否擅长评估 AIGIs 的基准，从 16 个文本到图像模型中采样了 2,864 个 AIGIs，每个都与由人类专家标注的问题-答案配对。",
                            "en": " A-Bench is a benchmark designed to diagnose whether LMMs are masters at evaluating AIGIs. 2,864 AIGIs from 16 text-to-image models are sampled, each paired with question-answers annotated by human experts, and tested across 18 leading LMMs."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/a_bench'. Error: Path opencompass/a_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1278",
                    "name": "ChronoMagic-Bench",
                    "version": "1.0.0",
                    "description": "ChronoMagic-Bench can evaluate the temporal and metamorphic capabilities of the T2V (text-to-video) models in time-lapse video generation, introducing 1,649 prompts and real-world videos as references.",
                    "url": "opencompass/opencompass_1278.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1278",
                    "sample_count": 1000,
                    "traits": [
                        "Creation"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1278",
                        "name": "ChronoMagic-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "创作",
                                "en": "Creation"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/PKU-YuanGroup/ChronoMagic-Bench",
                        "paperLink": "https://arxiv.org/abs/2406.18522",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "112",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:14:44",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:14:44",
                        "createDate": "2025-01-07 14:13:34",
                        "desc": {
                            "cn": "ChronoMagic-Bench用来评估 T2V （文本到视频 ）模型在延时视频生成中的时间和变形能力，引入了1649个提示和真实世界的视频作为参考。",
                            "en": "ChronoMagic-Bench can evaluate the temporal and metamorphic capabilities of the T2V (text-to-video) models in time-lapse video generation, introducing 1,649 prompts and real-world videos as references."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/chronomagic_bench'. Error: Path opencompass/chronomagic_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1280",
                    "name": "AMBROSIA",
                    "version": "1.0.0",
                    "description": "AMBROSIA is a new benchmark for recognizing and interpreting ambiguous requests in text-to-SQL. It contains questions showcasing three different types of ambiguity (scope ambiguity, attachment ambiguity, and vagueness), their interpretations, and corresponding SQL queries.",
                    "url": "opencompass/opencompass_1280.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1280",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1280",
                        "name": "AMBROSIA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/saparina/ambrosia",
                        "paperLink": "https://arxiv.org/abs/2406.19073",
                        "officialWebsiteLink": "https://ambrosia-benchmark.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "110",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 14:31:36",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 14:31:36",
                        "createDate": "2024-12-24 20:31:47",
                        "desc": {
                            "cn": "AMBROSIA是识别和解释text-to-SQL中歧义请求的新基准，其中包含三种不同类型的歧义（范围歧义、附件歧义和模糊性）问题、它们的解释和相应的SQL查询。",
                            "en": "AMBROSIA is a new benchmark for recognizing and interpreting ambiguous requests in text-to-SQL. It contains questions showcasing three different types of ambiguity (scope ambiguity, attachment ambiguity, and vagueness), their interpretations, and corresponding SQL queries."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ambrosia'. Error: Path opencompass/ambrosia is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1608",
                    "name": "SwiLTra-Bench",
                    "version": "1.0.0",
                    "description": "SwiLTra-Bench is a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems.",
                    "url": "opencompass/opencompass_1608.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1608",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1608",
                        "name": "SwiLTra-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2503.01372",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "109",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:43:44",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:43:44",
                        "createDate": "2025-03-06 15:15:16",
                        "desc": {
                            "cn": "SwiLTra-Bench是一个包含超过 18 万对对齐的瑞士法律翻译语料库的全面多语言基准，包括所有瑞士语言以及英语的法律、摘要和新闻稿，旨在评估基于LLM的翻译系统。",
                            "en": "SwiLTra-Bench is a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/swiltra_bench'. Error: Path opencompass/swiltra_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1557",
                    "name": "AIRBench-2024",
                    "version": "1.0.0",
                    "description": "AIR-Bench 2024, the first policy-aligned AI safety benchmark, structures 8 government regulations and 16 corporate policies into four security tiers, with 5,694 diverse prompts spanning these categories.",
                    "url": "opencompass/opencompass_1557.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1557",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1557",
                        "name": "AIRBench-2024",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/stanford-crfm/air-bench-2024",
                        "paperLink": "https://arxiv.org/abs/2407.17436",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "108",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:50:05",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:50:05",
                        "createDate": "2025-02-26 17:30:51",
                        "desc": {
                            "cn": "AIR-Bench 2024是首个与新兴政府法规和企业政策相一致的 AI 安全基准， 将 8 项政府法规和 16 项企业政策分解为四级安全分类，涵盖了这些类别的 5,694 个多样化的提示。",
                            "en": "AIR-Bench 2024, the first policy-aligned AI safety benchmark, structures 8 government regulations and 16 corporate policies into four security tiers, with 5,694 diverse prompts spanning these categories."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/airbench_2024'. Error: Path opencompass/airbench_2024 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1360",
                    "name": "LLaVA-Bench",
                    "version": "1.0.0",
                    "description": "LLaVA-Bench evaluates the model's ability in more challenging tasks, including a diverse set of 24 images with 60 questions in total, including indoor and outdoor scenes, memes, paintings, sketches, etc.",
                    "url": "opencompass/opencompass_1360.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1360",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1360",
                        "name": "LLaVA-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/haotian-liu/LLaVA/blob/main/docs/LLaVA_Bench.md",
                        "paperLink": "",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "108",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:49",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:49",
                        "createDate": "2025-01-09 21:47:19",
                        "desc": {
                            "cn": "LLaVA-Bench用于评估多模态大模型应对复杂任务的能力，内含24 张图像及60个问题，包括室内和室外场景、模因、绘画、素描等。",
                            "en": "LLaVA-Bench evaluates the model's ability in more challenging tasks, including a diverse set of 24 images with 60 questions in total, including indoor and outdoor scenes, memes, paintings, sketches, etc."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/llava_bench'. Error: Path opencompass/llava_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1636",
                    "name": "UrbanVideo-Bench",
                    "version": "1.0.0",
                    "description": "The benchmark is designed to evaluate whether video-large language models (Video-LLMs) can naturally process continuous first-person visual observations like humans, enabling recall, perception, reasoning, and navigation.",
                    "url": "opencompass/opencompass_1636.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1636",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1636",
                        "name": "UrbanVideo-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/EmbodiedCity/UrbanVideo-Bench.code",
                        "paperLink": "https://arxiv.org/abs/2503.06157",
                        "officialWebsiteLink": "https://embodiedcity.github.io/UrbanVideo-Bench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "108",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-13 15:31:18",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-13 15:31:18",
                        "createDate": "2025-03-13 14:54:33",
                        "desc": {
                            "cn": "UrbanVideo-Bench旨在评估视频大型语言模型（Video-LLMs）是否能够像人类一样自然地处理连续的第一人称视觉观察，实现回忆、感知、推理和导航。",
                            "en": "The benchmark is designed to evaluate whether video-large language models (Video-LLMs) can naturally process continuous first-person visual observations like humans, enabling recall, perception, reasoning, and navigation."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/urbanvideo_bench'. Error: Path opencompass/urbanvideo_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1680",
                    "name": "BigOBench",
                    "version": "1.0.0",
                    "description": "BigO(Bench)是一个包含约 300 个需要用 Python 解决的代码问题的基准测试，以及 3,105 个编码问题和 1,190,250 个解决方案用于训练，以评估LLMs能否找到代码解决方案的时间-空间复杂度，或者生成符合时间-空间复杂度要求的代码解决方案。",
                    "url": "opencompass/opencompass_1680.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1680",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1680",
                        "name": "BigOBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/facebookresearch/bigobench",
                        "paperLink": "https://arxiv.org/abs/2503.15242",
                        "officialWebsiteLink": "https://facebookresearch.github.io/BigOBench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "108",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-26 15:56:21",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-26 15:56:21",
                        "createDate": "2025-03-26 14:44:38",
                        "desc": {
                            "cn": "BigO(Bench)是一个包含约 300 个需要用 Python 解决的代码问题的基准测试，以及 3,105 个编码问题和 1,190,250 个解决方案用于训练，以评估LLMs能否找到代码解决方案的时间-空间复杂度，或者生成符合时间-空间复杂度要求的代码解决方案。",
                            "en": "BigO(Bench)是一个包含约 300 个需要用 Python 解决的代码问题的基准测试，以及 3,105 个编码问题和 1,190,250 个解决方案用于训练，以评估LLMs能否找到代码解决方案的时间-空间复杂度，或者生成符合时间-空间复杂度要求的代码解决方案。"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/bigobench'. Error: Path opencompass/bigobench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1583",
                    "name": "JudgeBench",
                    "version": "1.0.0",
                    "description": "JudgeBench is a benchmark aimed at evaluating LLM-based judges for objective correctness on challenging response pairs.",
                    "url": "opencompass/opencompass_1583.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1583",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1583",
                        "name": "JudgeBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ScalerLab/JudgeBench",
                        "paperLink": "https://arxiv.org/abs/2410.12784",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "107",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:48:25",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:48:25",
                        "createDate": "2025-03-04 11:06:42",
                        "desc": {
                            "cn": "JudgeBench 是一个旨在评估基于LLM的裁判在具有挑战性的响应对上的客观正确性的基准。",
                            "en": "JudgeBench is a benchmark aimed at evaluating LLM-based judges for objective correctness on challenging response pairs."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/judgebench'. Error: Path opencompass/judgebench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1321",
                    "name": "ShoppingMMLU",
                    "version": "1.0.0",
                    "description": "Shopping MMLU is a diverse multi-task online shopping benchmark derived from real-world Amazon data. It consists of 57 tasks covering 4 major shopping skills: concept understanding, knowledge reasoning, user behavior alignment, and multi-linguality.",
                    "url": "opencompass/opencompass_1321.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1321",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1321",
                        "name": "ShoppingMMLU",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/KL4805/ShoppingMMLU",
                        "paperLink": "https://arxiv.org/abs/2410.20745",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "106",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 14:21:36",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 14:21:36",
                        "createDate": "2024-12-31 15:44:55",
                        "desc": {
                            "cn": "Shopping MMLU是一个基于真实亚马逊数据的多样化多任务在线购物基准测试，由57项任务组成，涵盖概念理解、知识推理、用户行为对齐和多语言4大购物场景技能。",
                            "en": "Shopping MMLU is a diverse multi-task online shopping benchmark derived from real-world Amazon data. It consists of 57 tasks covering 4 major shopping skills: concept understanding, knowledge reasoning, user behavior alignment, and multi-linguality."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/shoppingmmlu'. Error: Path opencompass/shoppingmmlu is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1267",
                    "name": "SpreadsheetBench",
                    "version": "1.0.0",
                    "description": "SpreadsheetBench is a challenging spreadsheet manipulation benchmark built from 912 real questions gathered from online Excel forums.",
                    "url": "opencompass/opencompass_1267.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1267",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1267",
                        "name": "SpreadsheetBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/RUCKBReasoning/SpreadsheetBench",
                        "paperLink": "https://arxiv.org/abs/2406.14991",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "106",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-25 14:11:06",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-25 14:11:06",
                        "createDate": "2024-12-24 14:50:13",
                        "desc": {
                            "cn": "SpreadsheetBench是一个具有挑战性的电子表格操作基准测试，包含912个来自在线Excel论坛的真实问题",
                            "en": "SpreadsheetBench is a challenging spreadsheet manipulation benchmark built from 912 real questions gathered from online Excel forums."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/spreadsheetbench'. Error: Path opencompass/spreadsheetbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1634",
                    "name": "VisualSimpleQA",
                    "version": "1.0.0",
                    "description": "VisualSimpleQA is a multimodal fact-seeking benchmark with two key features. ",
                    "url": "opencompass/opencompass_1634.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1634",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1634",
                        "name": "VisualSimpleQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2503.06492",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "105",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-13 15:31:25",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-13 15:31:25",
                        "createDate": "2025-03-13 14:29:04",
                        "desc": {
                            "cn": "VisualSimpleQA 是一个多模态事实寻求基准，具有两个关键特性。首先，它使视觉和语言模态中 LVLMs 的评估更加简化和解耦。其次，它纳入了明确的难度标准，以指导人工标注并促进提取具有挑战性的子集，即 VisualSimpleQA-hard。",
                            "en": "VisualSimpleQA is a multimodal fact-seeking benchmark with two key features. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/visualsimpleqa'. Error: Path opencompass/visualsimpleqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1654",
                    "name": "V-STaR",
                    "version": "1.0.0",
                    "description": "V-STaR is a spatio-temporal reasoning benchmark for Video-LLMs, evaluating Video-LLM’s spatio-temporal reasoning ability in answering questions explicitly in the context of “when”, “where”, and “what”.",
                    "url": "opencompass/opencompass_1654.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1654",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1654",
                        "name": "V-STaR",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/V-STaR-Bench/V-STaR",
                        "paperLink": "https://arxiv.org/abs/2503.11495",
                        "officialWebsiteLink": "https://v-star-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "105",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-20 14:27:09",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-20 14:27:09",
                        "createDate": "2025-03-20 13:44:36",
                        "desc": {
                            "cn": "V-STaR 是一个针对 Video-LLMs的空间时间推理基准，评估 Video-LLM在“何时”、“何地”和“何物”的上下文中明确回答问题的空间时间推理能力。",
                            "en": "V-STaR is a spatio-temporal reasoning benchmark for Video-LLMs, evaluating Video-LLM’s spatio-temporal reasoning ability in answering questions explicitly in the context of “when”, “where”, and “what”."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/v_star'. Error: Path opencompass/v_star is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1717",
                    "name": "ViLBench",
                    "version": "1.0.0",
                    "description": "ViLBench is a benchmark designed to evaluate vision-language models. It features 600 examples from 5 datasets, selected based on the criterion that process reward models offer greater improvements over output reward models in guiding generations.",
                    "url": "opencompass/opencompass_1717.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1717",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1717",
                        "name": "ViLBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "vision-language",
                                "en": "vision-language"
                            },
                            {
                                "cn": "math",
                                "en": "math"
                            },
                            {
                                "cn": "reasoning",
                                "en": "reasoning"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2503.20271",
                        "officialWebsiteLink": "https://ucsc-vlaa.github.io/ViLBench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "62408676",
                            "name": null,
                            "avatar": null,
                            "nickname": "ImKe"
                        },
                        "lookNum": "105",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-08 10:14:31",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-08 10:14:31",
                        "createDate": "2025-04-08 02:53:29",
                        "desc": {
                            "cn": "ViLBench 是一项旨在评估视觉-语言模型的数据集，其强调对模型进行细粒度的逐步推理能力测试。该基准共包含600个经过严格筛选的样本，来源于五个不同的视觉-语言数据集，筛选标准是在模型答案选择过程中，过程奖励模型（process-reward model）相较于输出奖励模型（output-reward model）具有更显著的性能提升效果。",
                            "en": "ViLBench is a benchmark designed to evaluate vision-language models. It features 600 examples from 5 datasets, selected based on the criterion that process reward models offer greater improvements over output reward models in guiding generations."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/vilbench'. Error: Path opencompass/vilbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1318",
                    "name": "WikiContradict",
                    "version": "1.0.0",
                    "description": "WikiContradict is a benchmark consisting of 253 high-quality, human-annotated instances designed to assess LLM performance when augmented with retrieved passages containing real-world knowledge conflicts.",
                    "url": "opencompass/opencompass_1318.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1318",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1318",
                        "name": "WikiContradict",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2406.13805",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "104",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 14:21:41",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 14:21:41",
                        "createDate": "2024-12-31 14:05:23",
                        "desc": {
                            "cn": "WikiContradict旨在评估LLM遇到包含真实世界知识冲突的段落检索增强时的性能，由253个高质量的人工注释实例组成。",
                            "en": "WikiContradict is a benchmark consisting of 253 high-quality, human-annotated instances designed to assess LLM performance when augmented with retrieved passages containing real-world knowledge conflicts."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/wikicontradict'. Error: Path opencompass/wikicontradict is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1147",
                    "name": "M3T",
                    "version": "1.0.0",
                    "description": "M3T is a novel benchmark dataset tailored to evaluate NMT systems on the comprehensive task of translating semi-structured documents. This dataset aims to bridge the evaluation gap in document-level NMT systems, acknowledging the challenges posed by rich text layouts in real-world applications.",
                    "url": "opencompass/opencompass_1147.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1147",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1147",
                        "name": "M3T",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NAACL 2024",
                                "en": "NAACL 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/amazon-science/m3t-multi-modal-translation-bench",
                        "paperLink": "https://aclanthology.org/2024.naacl-short.41.pdf",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "5010775",
                            "name": "amazon-science",
                            "avatar": null,
                            "nickname": "OpenXLab-hzYw4sVeC"
                        },
                        "lookNum": "104",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-10-27 17:23:28",
                        "supportOnlineEval": false,
                        "updateDate": "2024-10-27 17:23:28",
                        "createDate": "2024-10-15 13:57:09",
                        "desc": {
                            "cn": "M3T 是旨在评估神经机器翻译（NMT）系统在翻译半结构化文档的综合任务上的表现。",
                            "en": "M3T is a novel benchmark dataset tailored to evaluate NMT systems on the comprehensive task of translating semi-structured documents. This dataset aims to bridge the evaluation gap in document-level NMT systems, acknowledging the challenges posed by rich text layouts in real-world applications."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/m3t'. Error: Path opencompass/m3t is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1971",
                    "name": "DyCodeEval",
                    "version": "1.0.0",
                    "description": "DyCodeEval introduces methods to generate dynamic evaluation dataset and metric. Using multi-agent cooperation to rewrite benchmarks at evaluation time, it generates semantically equivalent, diverse, and non-deterministic problems—reducing data contamination and enabling more trustworthy evaluation.",
                    "url": "opencompass/opencompass_1971.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1971",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Reasoning",
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1971",
                        "name": "DyCodeEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Code",
                                "en": "Code"
                            },
                            {
                                "cn": "Dynamic Benchmarking",
                                "en": "Dynamic Benchmarking"
                            },
                            {
                                "cn": "ICML 2025",
                                "en": "ICML 2025"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SeekingDream/DyCodeEval",
                        "paperLink": "https://arxiv.org/pdf/2503.04149",
                        "officialWebsiteLink": "https://codekaleidoscope.github.io/dycodeeval.html",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "53004543",
                            "name": null,
                            "avatar": null,
                            "nickname": "CM"
                        },
                        "lookNum": "104",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-24 10:13:26",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-24 10:13:26",
                        "createDate": "2025-06-24 05:07:16",
                        "desc": {
                            "cn": "DyCodeEval 提出了一种动态生成评测数据集和评测指标的方法。通过多智能体协作，在评测时对基准题目进行重写，生成语义等价、多样化且非确定性的问题，从而减少数据污染，实现更可信的评测。\n",
                            "en": "DyCodeEval introduces methods to generate dynamic evaluation dataset and metric. Using multi-agent cooperation to rewrite benchmarks at evaluation time, it generates semantically equivalent, diverse, and non-deterministic problems—reducing data contamination and enabling more trustworthy evaluation."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/dycodeeval'. Error: Path opencompass/dycodeeval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1277",
                    "name": "VideoGUI",
                    "version": "1.0.0",
                    "description": "VideoGUI is designed to evaluate GUI assistants on visual-centric GUI tasks. Sourced from high-quality web instructional videos, it focuses on tasks involving professional and novel software and complex activities (e.g., video editing). ",
                    "url": "opencompass/opencompass_1277.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1277",
                    "sample_count": 1000,
                    "traits": [
                        "Creation"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1277",
                        "name": "VideoGUI",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "创作",
                                "en": "Creation"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/showlab/videogui",
                        "paperLink": "https://arxiv.org/abs/2406.10227",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "104",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-07 14:14:48",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-07 14:14:48",
                        "createDate": "2025-01-07 14:13:51",
                        "desc": {
                            "cn": "VideoGUI旨在评估以视觉为中心的GUI任务上的GUI助手，来自高质量的网络教学视频，侧重于涉及专业和新颖软件和复杂活动（例如视频编辑）的任务。",
                            "en": "VideoGUI is designed to evaluate GUI assistants on visual-centric GUI tasks. Sourced from high-quality web instructional videos, it focuses on tasks involving professional and novel software and complex activities (e.g., video editing). "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/videogui'. Error: Path opencompass/videogui is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1586",
                    "name": "MRAG-Bench",
                    "version": "1.0.0",
                    "description": "MRAG-Bench consists of 16,130 images and 1,353 human-annotated multiple-choice questions across 9 distinct scenarios, providing a robust and systematic evaluation of Large Vision Language Model (LVLM)’s vision-centric multimodal retrieval-augmented generation (RAG) abilities.",
                    "url": "opencompass/opencompass_1586.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1586",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1586",
                        "name": "MRAG-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/mragbench/MRAG-Bench",
                        "paperLink": "https://arxiv.org/abs/2410.08182",
                        "officialWebsiteLink": "https://mragbench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "102",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:47:20",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:47:20",
                        "createDate": "2025-03-04 13:56:04",
                        "desc": {
                            "cn": "MRAG-Bench 包含 16,130 张图片和 1,353 个跨越 9 个不同场景的人标注多选题，为大型视觉语言模型（LVLM）的视觉中心多模态检索增强生成（RAG）能力提供了稳健和系统的评估。",
                            "en": "MRAG-Bench consists of 16,130 images and 1,353 human-annotated multiple-choice questions across 9 distinct scenarios, providing a robust and systematic evaluation of Large Vision Language Model (LVLM)’s vision-centric multimodal retrieval-augmented generation (RAG) abilities."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mrag_bench'. Error: Path opencompass/mrag_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1356",
                    "name": "MM-Vet",
                    "version": "1.0.0",
                    "description": "MM-Vet evaluates the capabilities to deal with complicated multimodal tasks, defining 6 core VL capabilities and examining the 16 integrations of interest derived from the capability combination.",
                    "url": "opencompass/opencompass_1356.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1356",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1356",
                        "name": "MM-Vet",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/yuweihao/MM-Vet",
                        "paperLink": "https://arxiv.org/abs/2308.02490",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "102",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:24:00",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:24:00",
                        "createDate": "2025-01-09 20:41:34",
                        "desc": {
                            "cn": "MM-Vet用于评估复杂多模态任务能力，涵盖了6个核心视觉语言功能的16种功能组合。",
                            "en": "MM-Vet evaluates the capabilities to deal with complicated multimodal tasks, defining 6 core VL capabilities and examining the 16 integrations of interest derived from the capability combination."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mm_vet'. Error: Path opencompass/mm_vet is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1363",
                    "name": "SEED-Bench-2",
                    "version": "1.0.0",
                    "description": "SEED-Bench-2 assesses both text and image generation of MLLMs. It spans 27 evaluation dimensions, featuring 24K multiple-choice questions with precise human annotations.",
                    "url": "opencompass/opencompass_1363.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1363",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1363",
                        "name": "SEED-Bench-2",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AILab-CVC/SEED-Bench",
                        "paperLink": "https://arxiv.org/abs/2311.17092",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "102",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:52",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:52",
                        "createDate": "2025-01-10 11:31:59",
                        "desc": {
                            "cn": "SEED-Bench-2用于评估多模态大模型的文本和图像生成能力，包括跨越27个维度的24K道多选题及准确的人工注释。",
                            "en": "SEED-Bench-2 assesses both text and image generation of MLLMs. It spans 27 evaluation dimensions, featuring 24K multiple-choice questions with precise human annotations."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/seed_bench_2'. Error: Path opencompass/seed_bench_2 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1640",
                    "name": "EMMA",
                    "version": "1.0.0",
                    "description": "EMMA is composed of 2,788 problems, of which 1,796 are newly constructed, across four domains. Within each subject, we further provide fine-grained labels for each question based on the specific skills it measures.",
                    "url": "opencompass/opencompass_1640.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1640",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1640",
                        "name": "EMMA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/hychaochao/EMMA",
                        "paperLink": "https://www.arxiv.org/abs/2501.05444",
                        "officialWebsiteLink": "https://emma-benchmark.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "102",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-17 10:46:29",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-17 10:46:29",
                        "createDate": "2025-03-17 10:44:03",
                        "desc": {
                            "cn": "EMMA 由 2,788 个问题组成，其中 1,796 个是新构建的，涵盖四个领域。在每个主题中，我们根据所测量的具体技能为每个问题提供细粒度标签。",
                            "en": "EMMA is composed of 2,788 problems, of which 1,796 are newly constructed, across four domains. Within each subject, we further provide fine-grained labels for each question based on the specific skills it measures."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/emma'. Error: Path opencompass/emma is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1274",
                    "name": "IMDL-BenCo",
                    "version": "1.0.0",
                    "description": "IMDL-BenCo offers a comprehensive IMDL benchmark and modular codebase.",
                    "url": "opencompass/opencompass_1274.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1274",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1274",
                        "name": "IMDL-BenCo",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/scu-zjz/IMDLBenCo",
                        "paperLink": "https://arxiv.org/abs/2406.10580",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "102",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2024-12-30 16:30:17",
                        "supportOnlineEval": false,
                        "updateDate": "2024-12-30 16:30:17",
                        "createDate": "2024-12-30 16:26:51",
                        "desc": {
                            "cn": "IMDL-BenCo提供了全面的IMDL基准测试和模块化代码库。",
                            "en": "IMDL-BenCo offers a comprehensive IMDL benchmark and modular codebase."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/imdl_benco'. Error: Path opencompass/imdl_benco is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1320",
                    "name": "IaC-Eval",
                    "version": "1.0.0",
                    "description": "IaC-Eval is meant for quantitatively evaluating the capabilities of LLMs in cloud IaC code generation, containing 458 questions ranging from simple to difficult across various cloud services.",
                    "url": "opencompass/opencompass_1320.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1320",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1320",
                        "name": "IaC-Eval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/autoiac-project/iac-eval",
                        "paperLink": "https://openreview.net/pdf?id=7TCK0aBL1C",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "101",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 14:21:38",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 14:21:38",
                        "createDate": "2024-12-31 15:35:20",
                        "desc": {
                            "cn": "IaC-Eval用于定量评估LLM在云IaC代码生成中的功能，其中包含458个从易到难的问题，涵盖了各种云服务。",
                            "en": "IaC-Eval is meant for quantitatively evaluating the capabilities of LLMs in cloud IaC code generation, containing 458 questions ranging from simple to difficult across various cloud services."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/iac_eval'. Error: Path opencompass/iac_eval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1362",
                    "name": "POPE",
                    "version": "1.0.0",
                    "description": "POPE is an improved evaluation method for LVLMs' object hallucination by proposing a polling-based query method. It offers a more stable and flexible solution.",
                    "url": "opencompass/opencompass_1362.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1362",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1362",
                        "name": "POPE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AoiDragon/POPE",
                        "paperLink": "https://arxiv.org/abs/2305.10355",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "101",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-14 15:23:47",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-14 15:23:47",
                        "createDate": "2025-01-10 10:53:21",
                        "desc": {
                            "cn": "POPE用于评估视觉语言模型的物体幻觉，基于轮询的查询方法设计，提供了一种更稳定、更灵活的评估方案。",
                            "en": "POPE is an improved evaluation method for LVLMs' object hallucination by proposing a polling-based query method. It offers a more stable and flexible solution."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/pope'. Error: Path opencompass/pope is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1751",
                    "name": "MultiLoKo",
                    "version": "1.0.0",
                    "description": "MultiLoKo is a multilingual knowledge benchmark, covering 30 languages plus English.",
                    "url": "opencompass/opencompass_1751.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1751",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1751",
                        "name": "MultiLoKo",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/facebookresearch/multiloko/",
                        "paperLink": "https://arxiv.org/abs/2504.10356",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "101",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-21 12:34:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-21 12:34:33",
                        "createDate": "2025-04-21 11:55:37",
                        "desc": {
                            "cn": "MultiLoKo是一个多语言知识基准，涵盖30种语言及英语。",
                            "en": "MultiLoKo is a multilingual knowledge benchmark, covering 30 languages plus English."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/multiloko'. Error: Path opencompass/multiloko is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1539",
                    "name": "MM-RLHF",
                    "version": "1.0.0",
                    "description": " MM-RLHF, a comprehensive project for aligning Multimodal Large Language Models (MLLMs) with human preferences. The dataset and algorithms enable consistent performance improvements across 10 dimensions and 27 benchmarks for open-source MLLMs.",
                    "url": "opencompass/opencompass_1539.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1539",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1539",
                        "name": "MM-RLHF",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            },
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Kwai-YuanQi/MM-RLHF",
                        "paperLink": "https://arxiv.org/abs/2406.08487",
                        "officialWebsiteLink": "https://mm-rlhf.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "100",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:50:29",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:50:29",
                        "createDate": "2025-02-24 19:28:08",
                        "desc": {
                            "cn": "MM-RLHF，这是一个将多模态大型语言模型（MLLMs）与人类偏好对齐的全面项目，使开源多语言机器学习模型在 10 个维度和 27 个基准测试中实现持续的性能提升。",
                            "en": " MM-RLHF, a comprehensive project for aligning Multimodal Large Language Models (MLLMs) with human preferences. The dataset and algorithms enable consistent performance improvements across 10 dimensions and 27 benchmarks for open-source MLLMs."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mm_rlhf'. Error: Path opencompass/mm_rlhf is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1317",
                    "name": "RepLiQA",
                    "version": "1.0.0",
                    "description": "RepLiQA is suited for question-answering and topic retrieval tasks, including collection of five splits of test sets. Accurate answers can only be generated if a model can find relevant content within the provided document.",
                    "url": "opencompass/opencompass_1317.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1317",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1317",
                        "name": "RepLiQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [
                            {
                                "cn": "NeurIPS 2024",
                                "en": "NeurIPS 2024"
                            }
                        ],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ServiceNow/repliqa",
                        "paperLink": "https://arxiv.org/abs/2406.11811",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "100",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-06 14:21:43",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-06 14:21:43",
                        "createDate": "2024-12-31 13:56:35",
                        "desc": {
                            "cn": "RepLiQA适用于问答和主题检索任务，集合了是5个测试集；只有当模型可以在提供的文档中找到相关内容时，才能生成准确的答案。",
                            "en": "RepLiQA is suited for question-answering and topic retrieval tasks, including collection of five splits of test sets. Accurate answers can only be generated if a model can find relevant content within the provided document."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/repliqa'. Error: Path opencompass/repliqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1633",
                    "name": "ProJudge",
                    "version": "1.0.0",
                    "description": "ProJudge is a comprehensive, multi-modal, multi-discipline, and multi-difficulty benchmark specifically designed for evaluating abilities of MLLM-based process judges.It comprises 2,400 test cases and 50,118 step-level labels, spanning four scientific disciplines with diverse difficulty levels and",
                    "url": "opencompass/opencompass_1633.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1633",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1633",
                        "name": "ProJudge",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/jiaxin-ai/ProJudge",
                        "paperLink": "https://arxiv.org/abs/2503.06553",
                        "officialWebsiteLink": "https://projudge.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "100",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-13 15:31:28",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-13 15:31:28",
                        "createDate": "2025-03-13 14:14:35",
                        "desc": {
                            "cn": "ProJudge 是一个针对基于 MLLM 的过程裁判能力的全面、多模态、多学科和多难度的基准。它包含 2,400 个测试案例和 50,118 个步骤级标签，涵盖四个科学学科，难度级别和内容多样化。",
                            "en": "ProJudge is a comprehensive, multi-modal, multi-discipline, and multi-difficulty benchmark specifically designed for evaluating abilities of MLLM-based process judges.It comprises 2,400 test cases and 50,118 step-level labels, spanning four scientific disciplines with diverse difficulty levels and"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/projudge'. Error: Path opencompass/projudge is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1624",
                    "name": "CodeElo",
                    "version": "1.0.0",
                    "description": "Researchers introduce CodeElo, a standardized competition-level code generation benchmark that effectively addresses all these challenges for the first time. CodeElo benchmark is mainly based on the official CodeForces platform and tries to align with the platform as much as possible. ",
                    "url": "opencompass/opencompass_1624.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1624",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1624",
                        "name": "CodeElo",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/QwenLM/CodeElo",
                        "paperLink": "https://arxiv.org/abs/2501.01257",
                        "officialWebsiteLink": "https://codeelo-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "99",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-12 11:07:44",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-12 11:07:44",
                        "createDate": "2025-03-11 15:53:47",
                        "desc": {
                            "cn": "CodeElo，这是一个标准化的竞赛级代码生成基准测试，有效解决了所有这些挑战。CodeElo 基准测试主要基于官方 CodeForces 平台，并尽可能与该平台保持一致。",
                            "en": "Researchers introduce CodeElo, a standardized competition-level code generation benchmark that effectively addresses all these challenges for the first time. CodeElo benchmark is mainly based on the official CodeForces platform and tries to align with the platform as much as possible. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/codeelo'. Error: Path opencompass/codeelo is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1905",
                    "name": "MVPBench",
                    "version": "1.0.0",
                    "description": "MVPBench, a curated benchmark designed to rigorously evaluate visual physical reasoning through the lens of visual CoT. Each example features interleaved multi-image inputs and demands not only the correct final answer but also a coherent, step-by-step reasoning path grounded in evolving visual cues",
                    "url": "opencompass/opencompass_1905.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1905",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1905",
                        "name": "MVPBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/CSU-JPG/MVPBench",
                        "paperLink": "https://arxiv.org/pdf/2505.24182",
                        "officialWebsiteLink": "https://csu-jpg.github.io/MVPBench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "53302848",
                            "name": null,
                            "avatar": null,
                            "nickname": "LoneR"
                        },
                        "lookNum": "99",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-06 14:45:42",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-06 14:45:42",
                        "createDate": "2025-06-06 12:35:56",
                        "desc": {
                            "cn": "MVPBench专注于视觉物理推理中的视觉链式思维（CoT）能力评估。该基准涵盖真实图像、多步逻辑与多条可行思维路径，每个样例均配有图像证据，要求模型在剥离文本提示依赖的前提下，不仅得出正确答案，还需正确完成每一个中间推理步骤，模拟人类的逐步图像推理过程。",
                            "en": "MVPBench, a curated benchmark designed to rigorously evaluate visual physical reasoning through the lens of visual CoT. Each example features interleaved multi-image inputs and demands not only the correct final answer but also a coherent, step-by-step reasoning path grounded in evolving visual cues"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mvpbench'. Error: Path opencompass/mvpbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1658",
                    "name": "MiLiC-Eval",
                    "version": "1.0.0",
                    "description": "MiLiC-Eval is an NLP evaluation suite for Minority Languages in China, covering Tibetan (bo), Uyghur (ug), Kazakh (kk, in the Kazakh Arabic script), and Mongolian (mn, in the traditional Mongolian script).",
                    "url": "opencompass/opencompass_1658.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1658",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1658",
                        "name": "MiLiC-Eval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/luciusssss/MiLiC-Eval",
                        "paperLink": "https://arxiv.org/abs/2503.01150",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "99",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-20 14:26:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-20 14:26:55",
                        "createDate": "2025-03-20 14:21:29",
                        "desc": {
                            "cn": "MiLiC-Eval 是针对中国少数民族语言的 NLP 评估套件，涵盖藏语（bo）、维吾尔语（ug）、哈萨克语（kk，使用哈萨克阿拉伯文脚本）和蒙古语（mn，使用传统蒙古文脚本）。",
                            "en": "MiLiC-Eval is an NLP evaluation suite for Minority Languages in China, covering Tibetan (bo), Uyghur (ug), Kazakh (kk, in the Kazakh Arabic script), and Mongolian (mn, in the traditional Mongolian script)."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/milic_eval'. Error: Path opencompass/milic_eval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1670",
                    "name": "IndicMMLU-Pro",
                    "version": "1.0.0",
                    "description": "IndicMMLU-Pro provides a standardized evaluation framework to push the research boundaries in Indic language AI, facilitating the development of more accurate, efficient, and culturally sensitive models.",
                    "url": "opencompass/opencompass_1670.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1670",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1670",
                        "name": "IndicMMLU-Pro",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2501.15747",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "99",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-24 18:48:16",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-24 18:48:16",
                        "createDate": "2025-03-24 18:46:36",
                        "desc": {
                            "cn": "IndicMMLU-Pro 提供了一个标准化的评估框架，以推动印度语系语言 AI 的研究边界，促进更准确、高效和具有文化敏感性的模型的发展。",
                            "en": "IndicMMLU-Pro provides a standardized evaluation framework to push the research boundaries in Indic language AI, facilitating the development of more accurate, efficient, and culturally sensitive models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/indicmmlu_pro'. Error: Path opencompass/indicmmlu_pro is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1984",
                    "name": "SWE-bench-Live",
                    "version": "1.0.0",
                    "description": "SWE-bench-Live is a live-updatable benchmark designed for evaluating large language models (LLMs) and agents on real-world software issue resolution tasks.",
                    "url": "opencompass/opencompass_1984.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1984",
                    "sample_count": 1000,
                    "traits": [
                        "Code",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1984",
                        "name": "SWE-bench-Live",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SWE-bench-Live",
                        "paperLink": "https://arxiv.org/abs/2505.23419",
                        "officialWebsiteLink": "https://huggingface.co/SWE-bench-Live",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "99",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-27 18:12:47",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-27 18:12:47",
                        "createDate": "2025-06-26 14:18:04",
                        "desc": {
                            "cn": "SWE-bench-Live 是一个面向大语言模型（LLMs）和智能体的实时可更新评测基准，专注于真实世界软件开发中的问题修复任务。 该基准从 2024 年以来的 GitHub 活跃仓库中自动收集了 1,319 个问题修复任务，涵盖 93 个项目，并为每个任务提供可复现的 Docker 执行环境。 ",
                            "en": "SWE-bench-Live is a live-updatable benchmark designed for evaluating large language models (LLMs) and agents on real-world software issue resolution tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/swe_bench_live'. Error: Path opencompass/swe_bench_live is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1584",
                    "name": "MMSearch",
                    "version": "1.0.0",
                    "description": "Logo MMSearch is a multimodal search benchmark crafted to evaluate the potential of LMMs to function as a multimodal AI search engine. This benchmark encompasses a meticulously collected dataset of 300 queries spanning 14 subfields. ",
                    "url": "opencompass/opencompass_1584.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1584",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1584",
                        "name": "MMSearch",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/CaraJ7/MMSearch",
                        "paperLink": "https://arxiv.org/abs/2409.12959",
                        "officialWebsiteLink": "https://mmsearch.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "98",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:47:36",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:47:36",
                        "createDate": "2025-03-04 11:25:58",
                        "desc": {
                            "cn": "MMSearch 是一个多模态搜索基准，旨在评估大型语言模型（LMMs）作为多模态 AI 搜索引擎的潜力。该基准包含了一个精心收集的包含 300 个查询的数据集，涵盖 14 个子领域。",
                            "en": "Logo MMSearch is a multimodal search benchmark crafted to evaluate the potential of LMMs to function as a multimodal AI search engine. This benchmark encompasses a meticulously collected dataset of 300 queries spanning 14 subfields. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmsearch'. Error: Path opencompass/mmsearch is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1510",
                    "name": "LongVideoBench",
                    "version": "1.0.0",
                    "description": "LongVideoBench tests LMMs' understanding of long videos. It's a question-answering benchmark with video-language interleaved inputs up to an hour long and comprises 3,763 web-collected videos with subtitles across diverse themes,",
                    "url": "opencompass/opencompass_1510.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1510",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1510",
                        "name": "LongVideoBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/longvideobench/LongVideoBench",
                        "paperLink": "https://arxiv.org/abs/2407.15754",
                        "officialWebsiteLink": "https://longvideobench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "97",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:51:19",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:51:19",
                        "createDate": "2025-02-17 15:50:45",
                        "desc": {
                            "cn": "LongVideoBench用于评估多模态大模型的长视频理解能力，是基于交错长视频语料构建的问答集，包含3763个不同主题的带字幕的视频。",
                            "en": "LongVideoBench tests LMMs' understanding of long videos. It's a question-answering benchmark with video-language interleaved inputs up to an hour long and comprises 3,763 web-collected videos with subtitles across diverse themes,"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/longvideobench'. Error: Path opencompass/longvideobench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1553",
                    "name": "OmniAlign-V",
                    "version": "1.0.0",
                    "description": "OmniAlign-V datasets mainly focus on improving the alignment of Multi-modal Large Language Models(MLLMs) with human preference. It contains 205k high-quality Image-Quetion-Answer pairs with open-ended, creative quetions and long, knowledge-rich, comprehensive answers.",
                    "url": "opencompass/opencompass_1553.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1553",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1553",
                        "name": "OmniAlign-V",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/PhoenixZ810/OmniAlign-V",
                        "paperLink": "https://arxiv.org/abs/2502.18411",
                        "officialWebsiteLink": "https://phoenixz810.github.io/OmniAlign-V",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "96",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:49:29",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:49:29",
                        "createDate": "2025-02-26 14:41:09",
                        "desc": {
                            "cn": "OmniAlign-V 数据集主要关注提高多模态大型语言模型（MLLMs）与人类偏好的对齐。它包含 205k 个高质量的图像-问答对，包含开放式、创意性问题以及长篇、知识丰富、内容全面的答案。",
                            "en": "OmniAlign-V datasets mainly focus on improving the alignment of Multi-modal Large Language Models(MLLMs) with human preference. It contains 205k high-quality Image-Quetion-Answer pairs with open-ended, creative quetions and long, knowledge-rich, comprehensive answers."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/omnialign_v'. Error: Path opencompass/omnialign_v is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1645",
                    "name": "MastermindEval",
                    "version": "1.0.0",
                    "description": "Evaluating Reasoning Capabilities of LLMs Using the Mastermind Board Game.",
                    "url": "opencompass/opencompass_1645.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1645",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1645",
                        "name": "MastermindEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/flairNLP/mastermind",
                        "paperLink": "https://arxiv.org/abs/2503.05891",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "96",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-18 16:17:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-18 16:17:33",
                        "createDate": "2025-03-18 16:08:48",
                        "desc": {
                            "cn": "MastermindEval使用猜谜游戏棋盘评估大型语言模型的推理能力。\n",
                            "en": "Evaluating Reasoning Capabilities of LLMs Using the Mastermind Board Game."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mastermindeval'. Error: Path opencompass/mastermindeval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1396",
                    "name": "AV-Odyssey-Bench",
                    "version": "1.0.0",
                    "description": "AV-Odyssey Bench. This benchmark encompasses 26 different tasks and 4,555 carefully crafted problems, each incorporating text, visual, and audio components. All data are newly collected and annotated by humans, not from any existing audio-visual dataset.",
                    "url": "opencompass/opencompass_1396.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1396",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1396",
                        "name": "AV-Odyssey-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "audio-visual",
                                "en": "audio-visual"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AV-Odyssey/AV-Odyssey",
                        "paperLink": "https://arxiv.org/pdf/2412.02611",
                        "officialWebsiteLink": "https://huggingface.co/datasets/AV-Odyssey/AV_Odyssey_Bench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "40068663",
                            "name": null,
                            "avatar": null,
                            "nickname": "豪༙྇"
                        },
                        "lookNum": "96",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-15 18:34:14",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-15 18:34:14",
                        "createDate": "2025-01-15 17:26:48",
                        "desc": {
                            "cn": "AV-Odyssey Bench. This benchmark encompasses 26 different tasks and 4,555 carefully crafted problems, each incorporating text, visual, and audio components. All data are newly collected and annotated by humans, not from any existing audio-visual dataset.",
                            "en": "AV-Odyssey Bench. This benchmark encompasses 26 different tasks and 4,555 carefully crafted problems, each incorporating text, visual, and audio components. All data are newly collected and annotated by humans, not from any existing audio-visual dataset."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/av_odyssey_bench'. Error: Path opencompass/av_odyssey_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1571",
                    "name": "BRIGHT",
                    "version": "1.0.0",
                    "description": "BRIGHT is the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. ",
                    "url": "opencompass/opencompass_1571.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1571",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1571",
                        "name": "BRIGHT",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/xlang-ai/BRIGHT",
                        "paperLink": "https://arxiv.org/abs/2407.12883",
                        "officialWebsiteLink": "https://brightbenchmark.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "95",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:50:14",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:50:14",
                        "createDate": "2025-02-27 18:29:17",
                        "desc": {
                            "cn": "BRIGHT 是第一个需要大量推理来检索相关文档的文本检索基准。",
                            "en": "BRIGHT is the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/bright'. Error: Path opencompass/bright is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1679",
                    "name": "ContextualJudgeBench",
                    "version": "1.0.0",
                    "description": "ContextualJudgeBench is a pairwise benchmark with 2,000 samples for evaluating LLM-as-judge models in two contextual settings: Contextual QA and summarization. We propose a pairwise evaluation hierarchy and generate splits for our proposed hierarchy.",
                    "url": "opencompass/opencompass_1679.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1679",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1679",
                        "name": "ContextualJudgeBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SalesforceAIResearch/ContextualJudgeBench",
                        "paperLink": "https://arxiv.org/abs/2503.15620",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "95",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-26 15:56:25",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-26 15:56:25",
                        "createDate": "2025-03-26 14:40:00",
                        "desc": {
                            "cn": "ContextualJudgeBench 是一个包含 2,000 个样本的成对基准，用于评估在两个上下文环境下的LLM-as-judge 模型：上下文问答和摘要。我们提出一个成对评估层次结构，并为我们的层次结构生成分割。",
                            "en": "ContextualJudgeBench is a pairwise benchmark with 2,000 samples for evaluating LLM-as-judge models in two contextual settings: Contextual QA and summarization. We propose a pairwise evaluation hierarchy and generate splits for our proposed hierarchy."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/contextualjudgebench'. Error: Path opencompass/contextualjudgebench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1547",
                    "name": "MMIR",
                    "version": "1.0.0",
                    "description": "A benchmark for evaluating Multimodal Large Language Models (MLLMs) on detecting and reasoning about inconsistencies in layout-rich multimodal content. MMIR features 534 challenging samples across five reasoning-heavy inconsistency categories.",
                    "url": "opencompass/opencompass_1547.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1547",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1547",
                        "name": "MMIR",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/eric-ai-lab/MMIR",
                        "paperLink": "https://arxiv.org/abs/2502.16033",
                        "officialWebsiteLink": "https://jackie-2000.github.io/mmir.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "94",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:48:36",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:48:36",
                        "createDate": "2025-02-26 11:11:06",
                        "desc": {
                            "cn": "用于评估多模态模型（MLLM）检测和推理布局丰富的多模态内容中的不一致性的基准。MMIR 包含 534 个具有挑战性的样本，涉及五个推理能力较强的不一致类别。",
                            "en": "A benchmark for evaluating Multimodal Large Language Models (MLLMs) on detecting and reasoning about inconsistencies in layout-rich multimodal content. MMIR features 534 challenging samples across five reasoning-heavy inconsistency categories."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmir'. Error: Path opencompass/mmir is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1585",
                    "name": "MMAD",
                    "version": "1.0.0",
                    "description": "MMAD is the first-ever full-spectrum MLLMs benchmark in industrial Anomaly Detection. \nResearchers defined seven key subtasks of MLLMs in industrial inspection and designed a novel pipeline to generate the MMAD dataset with 39,672 questions for 8,366 industrial images. ",
                    "url": "opencompass/opencompass_1585.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1585",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1585",
                        "name": "MMAD",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/jam-cc/MMAD",
                        "paperLink": "https://arxiv.org/abs/2410.09453",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "93",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:47:27",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:47:27",
                        "createDate": "2025-03-04 13:48:12",
                        "desc": {
                            "cn": "MMAD是第一个工业异常检测领域的全谱 MLLMs 基准，研究人员定义了工业检测中 MLLMs 的七个关键子任务，并设计了一个新颖的流程来生成包含 39,672 个问题以及 8,366 个工业图像的 MMAD 数据集。",
                            "en": "MMAD is the first-ever full-spectrum MLLMs benchmark in industrial Anomaly Detection. \nResearchers defined seven key subtasks of MLLMs in industrial inspection and designed a novel pipeline to generate the MMAD dataset with 39,672 questions for 8,366 industrial images. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmad'. Error: Path opencompass/mmad is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1621",
                    "name": "ProcessBench",
                    "version": "1.0.0",
                    "description": "ProcessBench can measure the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. ",
                    "url": "opencompass/opencompass_1621.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1621",
                    "sample_count": 1000,
                    "traits": [
                        "Examination",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1621",
                        "name": "ProcessBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "学科",
                                "en": "Examination"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/QwenLM/ProcessBench",
                        "paperLink": "https://arxiv.org/abs/2412.06559",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "93",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-12 11:08:00",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-12 11:08:00",
                        "createDate": "2025-03-11 15:28:49",
                        "desc": {
                            "cn": "ProcessBench，用于衡量识别数学推理中错误步骤的能力。它包含 3,400 个测试案例，主要关注竞赛和奥林匹克级别的数学问题。",
                            "en": "ProcessBench can measure the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/processbench'. Error: Path opencompass/processbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1395",
                    "name": "SEED-Bench-2-Plus",
                    "version": "1.0.0",
                    "description": "SEED-Bench-2-Plus, a benchmark specifically designed for evaluating text-rich visual comprehension of MLLMs. The benchmark comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad categories: Charts, Maps, and Webs.",
                    "url": "opencompass/opencompass_1395.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1395",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1395",
                        "name": "SEED-Bench-2-Plus",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "text-rich image",
                                "en": "text-rich image"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AILab-CVC/SEED-Bench",
                        "paperLink": "https://arxiv.org/pdf/2404.16790",
                        "officialWebsiteLink": "https://huggingface.co/datasets/AILab-CVC/SEED-Bench-2-plus",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "40068663",
                            "name": null,
                            "avatar": null,
                            "nickname": "豪༙྇"
                        },
                        "lookNum": "93",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-01-15 18:33:41",
                        "supportOnlineEval": false,
                        "updateDate": "2025-01-15 18:33:41",
                        "createDate": "2025-01-15 17:20:44",
                        "desc": {
                            "cn": "SEED-Bench-2-Plus, a benchmark specifically designed for evaluating text-rich visual comprehension of MLLMs. The benchmark comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad categories: Charts, Maps, and Webs.",
                            "en": "SEED-Bench-2-Plus, a benchmark specifically designed for evaluating text-rich visual comprehension of MLLMs. The benchmark comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad categories: Charts, Maps, and Webs."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/seed_bench_2_plus'. Error: Path opencompass/seed_bench_2_plus is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1513",
                    "name": "CG-Bench",
                    "version": "1.0.0",
                    "description": "CG-Bench is meant for evaluating MLLMs' long video understanding, including 12,129 QA pairs from 1219 videos in 3 major question types: perception, reasoning, and hallucination.",
                    "url": "opencompass/opencompass_1513.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1513",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1513",
                        "name": "CG-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/CG-Bench/CG-Bench",
                        "paperLink": "https://arxiv.org/abs/2412.12075v1",
                        "officialWebsiteLink": "https://cg-bench.github.io/leaderboard/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "92",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:51:42",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:51:42",
                        "createDate": "2025-02-17 16:29:23",
                        "desc": {
                            "cn": "CG-Bench用于评估多模态大模型的长视频理解能力，基于1219个视频设计了12129个涵盖感知、推理和幻觉三种问题类型的QA对。",
                            "en": "CG-Bench is meant for evaluating MLLMs' long video understanding, including 12,129 QA pairs from 1219 videos in 3 major question types: perception, reasoning, and hallucination."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cg_bench'. Error: Path opencompass/cg_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1555",
                    "name": "WildBench",
                    "version": "1.0.0",
                    "description": "Weintroduce WildBench, an automated evaluation framework designed to bench-mark large language models (LLMs) using challenging, real-world user queries, which consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs.",
                    "url": "opencompass/opencompass_1555.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1555",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1555",
                        "name": "WildBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/allenai/WildBench",
                        "paperLink": "https://arxiv.org/abs/2406.04770",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "90",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:49:39",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:49:39",
                        "createDate": "2025-02-26 17:05:44",
                        "desc": {
                            "cn": "WildBench推出自动评估框架和数据集，基于真实用户难题评测大语言模型，包含从逾百万人机对话日志中精选的1,024个任务样本。",
                            "en": "Weintroduce WildBench, an automated evaluation framework designed to bench-mark large language models (LLMs) using challenging, real-world user queries, which consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/wildbench'. Error: Path opencompass/wildbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1578",
                    "name": "MMKE-Bench",
                    "version": "1.0.0",
                    "description": " MMKE-Bench is a benchmark designed to evaluate the ability of LMMs to edit visual knowledge in real-world scenarios. It includes 2,940 pieces of knowledge and 8,363 images across 33 broad categories, with automatically generated, human-verified evaluation questions.",
                    "url": "opencompass/opencompass_1578.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1578",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1578",
                        "name": "MMKE-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MMKE-Bench-ICLR/MMKE-Bench",
                        "paperLink": "https://arxiv.org/abs/2502.19870",
                        "officialWebsiteLink": "https://mmke-bench-iclr.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "90",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-04 17:02:34",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-04 17:02:34",
                        "createDate": "2025-03-03 16:27:20",
                        "desc": {
                            "cn": "MMKE-Bench是一个旨在评估 LMM 在现实场景中编辑视觉知识能力的基准，包括 33 个广泛类别中的 2,940 条知识和 8,363 张图像，以及自动生成并由人工验证的评估问题。",
                            "en": " MMKE-Bench is a benchmark designed to evaluate the ability of LMMs to edit visual knowledge in real-world scenarios. It includes 2,940 pieces of knowledge and 8,363 images across 33 broad categories, with automatically generated, human-verified evaluation questions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmke_bench'. Error: Path opencompass/mmke_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1842",
                    "name": "HARDMath2",
                    "version": "1.0.0",
                    "description": "HARDMath2 is a benchmark for applied mathematics created by students in a graduate class at Harvard University, featuring 211 original problems covering core topics such as boundary-layer analysis, WKB methods, asymptotic solutions of nonlinear partial differential equations, and the asymptotics.",
                    "url": "opencompass/opencompass_1842.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1842",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning",
                        "Code",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1842",
                        "name": "HARDMath2",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "代码",
                                "en": "Code"
                            },
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Strong Reasoning",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/pdf/2505.11774",
                        "officialWebsiteLink": "https://huggingface.co/datasets/JVRoggeveen/HARDMath2",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "90",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 14:19:57",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 14:19:57",
                        "createDate": "2025-06-04 14:17:14",
                        "desc": {
                            "cn": "HARDMath2是由哈佛大学研究生课程的学生创建的一项应用数学基准测试，包含211道原创问题，涵盖边界层分析、WKB方法、非线性偏微分方程的渐近解以及振荡积分的渐近性等核心主题。该基准通过一种创新的协作方式构建，学生不仅设计并改进符合课程大纲的高难度问题，还对解决方案进行同行验证，同时测试不同模型的表现。最终，LLM生成的解答会与学生的答案以及数值真值进行自动对比，以评估模型的准确性和能力。",
                            "en": "HARDMath2 is a benchmark for applied mathematics created by students in a graduate class at Harvard University, featuring 211 original problems covering core topics such as boundary-layer analysis, WKB methods, asymptotic solutions of nonlinear partial differential equations, and the asymptotics."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/hardmath2'. Error: Path opencompass/hardmath2 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1959",
                    "name": "PersonaLens",
                    "version": "1.0.0",
                    "description": "PersonaLens  a large-scale benchmark specifically designed to evaluate personalization in task-oriented dialogues. The benchmark features 1,500 in-depth user profiles, each integrating real demographic data, detailed cross-domain preferences, and rich interaction histories.",
                    "url": "opencompass/opencompass_1959.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1959",
                    "sample_count": 1000,
                    "traits": [
                        "Language",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1959",
                        "name": "PersonaLens",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "语言",
                                "en": "Language"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "AI Assistant",
                                "en": "AI Assistant"
                            },
                            {
                                "cn": "personalization",
                                "en": "personalization"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/amazon-science/PersonaLens",
                        "paperLink": "https://arxiv.org/abs/2506.09902",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "82606592",
                            "name": "zsquaredz",
                            "avatar": null,
                            "nickname": "zsquaredz"
                        },
                        "lookNum": "89",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-20 15:03:36",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-20 15:03:36",
                        "createDate": "2025-06-19 18:58:32",
                        "desc": {
                            "cn": "PersonaLens是一个专为任务导向型对话设计的、大规模的个性化能力评测基准。它包含1,500个深度用户画像，每个画像都集成了真实的人口统计信息、详尽的个人偏好及历史互动记录。这些画像与覆盖20个领域的111项真实世界任务相结合，并辅以动态的“情景上下文”来模拟现实世界的复杂性。为了实现自动化、可扩展的评估，该基准引入了两个LLM驱动的智能体：一个“用户智能体”负责模拟真人与AI进行对话，另一个“评判智能体”则对个性化水平、任务成功率和对话质量进行系统性打分。PersonaLens旨在为研究社区提供一个强大可靠的工具，共同推动下一代更懂你、更智能的AI助手的研发。",
                            "en": "PersonaLens  a large-scale benchmark specifically designed to evaluate personalization in task-oriented dialogues. The benchmark features 1,500 in-depth user profiles, each integrating real demographic data, detailed cross-domain preferences, and rich interaction histories."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/personalens'. Error: Path opencompass/personalens is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1735",
                    "name": "WorldScore",
                    "version": "1.0.0",
                    "description": "WorldScore benchmark is the first unified benchmark for world generation.",
                    "url": "opencompass/opencompass_1735.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1735",
                    "sample_count": 1000,
                    "traits": [
                        "Creation",
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1735",
                        "name": "WorldScore",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "创作",
                                "en": "Creation"
                            },
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "视频生成",
                                "en": "视频生成"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/haoyi-duan/WorldScore",
                        "paperLink": "https://arxiv.org/abs/2504.00983",
                        "officialWebsiteLink": "https://haoyi-duan.github.io/WorldScore/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "89",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:14:14",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:14:14",
                        "createDate": "2025-04-11 15:23:05",
                        "desc": {
                            "cn": "WorldScore基准测试，这是首个用于世界生成的统一基准测试。",
                            "en": "WorldScore benchmark is the first unified benchmark for world generation."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/worldscore'. Error: Path opencompass/worldscore is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1999",
                    "name": "MORSE-500",
                    "version": "1.0.0",
                    "description": "MORSE-500 introduces 500 programmatically generated videos testing 6 reasoning types: abstract, physical, planning, spatial, temporal, mathematical. Its controllable generation (via Manim, Matplotlib, generative models etc.) enables scalable difficulty, designed to evolve as SOTA models improve.",
                    "url": "opencompass/opencompass_1999.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1999",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1999",
                        "name": "MORSE-500",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/morse-benchmark/morse-500",
                        "paperLink": "https://arxiv.org/abs/2506.05523",
                        "officialWebsiteLink": "https://morse-500.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "72009045",
                            "name": null,
                            "avatar": null,
                            "nickname": "Clarence"
                        },
                        "lookNum": "89",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-30 10:06:27",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-30 10:06:27",
                        "createDate": "2025-06-30 05:39:25",
                        "desc": {
                            "cn": "当前多模态推理基准存在三大不足：依赖静态图像、偏重数学解题、易饱和。为此，我们提出MORSE-500视频基准：包含500个脚本化视频片段，涵盖抽象、物理、规划、空间、时间、数学六类推理问题。其核心在于程序化生成（使用Manim、Matplotlib等），可精确控制视觉复杂度、干扰物密度和时序动态，从而系统化提升难度。与易过时的静态基准不同，MORSE-500具备可持续演进能力，其可控生成流程能无限创建新挑战实例。在顶尖模型（Gemini 2.5 Pro、OpenAI o3等）上的测试揭示了显著性能差距，尤其在抽象和规划任务上。我们开源数据集、生成脚本及评估工具，以促进透明、可复现的前沿研究。",
                            "en": "MORSE-500 introduces 500 programmatically generated videos testing 6 reasoning types: abstract, physical, planning, spatial, temporal, mathematical. Its controllable generation (via Manim, Matplotlib, generative models etc.) enables scalable difficulty, designed to evolve as SOTA models improve."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/morse_500'. Error: Path opencompass/morse_500 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1502",
                    "name": "MTVQA",
                    "version": "1.0.0",
                    "description": "MTVQA is designed to evaluate LMMs' multilingual text understanding, featuring high-quality human expert annotations across 9 diverse languages. ",
                    "url": "opencompass/opencompass_1502.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1502",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1502",
                        "name": "MTVQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/bytedance/MTVQA",
                        "paperLink": "https://arxiv.org/abs/2405.11985",
                        "officialWebsiteLink": "https://bytedance.github.io/MTVQA/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "89",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:50:52",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:50:52",
                        "createDate": "2025-02-14 20:41:37",
                        "desc": {
                            "cn": "MTVQA用于评估多模态大模型理解多语言文本的能力，包含来自9种语言的由人类专家注释的高质量数据。",
                            "en": "MTVQA is designed to evaluate LMMs' multilingual text understanding, featuring high-quality human expert annotations across 9 diverse languages. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mtvqa'. Error: Path opencompass/mtvqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1778",
                    "name": "AgentRewardBench",
                    "version": "1.0.0",
                    "description": "AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. ",
                    "url": "opencompass/opencompass_1778.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1778",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1778",
                        "name": "AgentRewardBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://agent-reward-bench.github.io/",
                        "paperLink": "https://arxiv.org/abs/2504.08942",
                        "officialWebsiteLink": "https://agent-reward-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "88",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-25 17:00:44",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-25 17:00:44",
                        "createDate": "2025-04-25 16:26:45",
                        "desc": {
                            "cn": "AgentRewardBench 是首个用于评估大型语言模型（LLM）评判者评估网络代理有效性的基准测试。AgentRewardBench 包含来自 5 个基准测试和 4 个大型语言模型的 1302 条轨迹。",
                            "en": "AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/agentrewardbench'. Error: Path opencompass/agentrewardbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1533",
                    "name": "NutritionQA",
                    "version": "1.0.0",
                    "description": "CoSyn-400K dataset contains 9 categories of synthetc text-rich images with 2.7M instruction-tuning data",
                    "url": "opencompass/opencompass_1533.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1533",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1533",
                        "name": "NutritionQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/allenai/pixmo-docs",
                        "paperLink": "https://arxiv.org/abs/2502.14846",
                        "officialWebsiteLink": "https://yueyang1996.github.io/cosyn/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "88",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 17:00:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 17:00:55",
                        "createDate": "2025-02-24 13:51:34",
                        "desc": {
                            "cn": "通过代码引导的合成多模态数据生成扩展文本丰富图像理解，CoSyn-400K 数据集包含 9 类合成文本丰富图像，以及 270 万条指令微调数据。",
                            "en": "CoSyn-400K dataset contains 9 categories of synthetc text-rich images with 2.7M instruction-tuning data"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/nutritionqa'. Error: Path opencompass/nutritionqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1982",
                    "name": "WebUI-Bench",
                    "version": "1.0.0",
                    "description": "WebUIBench is a comprehensive benchmark designed to evaluate Multimodal Large Language Models (MLLMs) in Web UI-to-code generation tasks across four key capabilities: UI perception, HTML programming, UI-code understanding, and end-to-end transformation.",
                    "url": "opencompass/opencompass_1982.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1982",
                    "sample_count": 1000,
                    "traits": [
                        "Code",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1982",
                        "name": "WebUI-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MAIL-Tele-AI/WebUIBench",
                        "paperLink": "https://arxiv.org/abs/2506.07818",
                        "officialWebsiteLink": "https://huggingface.co/datasets/Tele-AI-MAIL/WebUIBench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "87",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-01 10:18:37",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-01 10:18:37",
                        "createDate": "2025-07-01 10:18:13",
                        "desc": {
                            "cn": "WebUIBench 是一个面向多模态大语言模型（MLLMs）的综合性评测基准，旨在系统评估模型在 Web UI 到代码生成任务中的四个关键能力：界面感知、HTML 编程、界面-代码理解以及整体转换能力。该基准包含来自 700 多个真实网站的 21,000 个高质量问答对，支持对模型在各阶段的细粒度能力分析。",
                            "en": "WebUIBench is a comprehensive benchmark designed to evaluate Multimodal Large Language Models (MLLMs) in Web UI-to-code generation tasks across four key capabilities: UI perception, HTML programming, UI-code understanding, and end-to-end transformation."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/webui_bench'. Error: Path opencompass/webui_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1604",
                    "name": "EgoNormia",
                    "version": "1.0.0",
                    "description": "EgoNormia is a challenging QA benchmark that tests VLMs' ability to reason over norms in context. The datset consists of 1,853 physically grounded egocentric interaction clips from Ego4D and corresponding five-way multiple-choice questions tasks for each.",
                    "url": "opencompass/opencompass_1604.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1604",
                    "sample_count": 1000,
                    "traits": [
                        "Examination",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1604",
                        "name": "EgoNormia",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "学科",
                                "en": "Examination"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Open-Social-World/EgoNormia",
                        "paperLink": "https://arxiv.org/abs/2502.20490",
                        "officialWebsiteLink": "https://opensocial.world/leaderboard",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "86",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:45:43",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:45:43",
                        "createDate": "2025-03-06 13:51:13",
                        "desc": {
                            "cn": "EgoNormia 是一个具有挑战性的问答基准，用于测试 VLMs 在上下文中推理规范的能力。该数据集包含来自 Ego4D 的 1,853 个物理基础化的以自我为中心的交互剪辑，以及每个剪辑对应的五选一多项选择题任务。",
                            "en": "EgoNormia is a challenging QA benchmark that tests VLMs' ability to reason over norms in context. The datset consists of 1,853 physically grounded egocentric interaction clips from Ego4D and corresponding five-way multiple-choice questions tasks for each."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/egonormia'. Error: Path opencompass/egonormia is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1632",
                    "name": "ProBench",
                    "version": "1.0.0",
                    "description": "ProBench is a benchmark that contains open-ended multimodal queries that require intensive expert-level knowledge to solve. ",
                    "url": "opencompass/opencompass_1632.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1632",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1632",
                        "name": "ProBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Yan98/ProBench_eval",
                        "paperLink": "https://arxiv.org/abs/2503.06885",
                        "officialWebsiteLink": "https://yan98.github.io/ProBench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "86",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-13 15:31:13",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-13 15:31:13",
                        "createDate": "2025-03-13 14:00:05",
                        "desc": {
                            "cn": "ProBench是一个包含需要大量专家级知识来解决的开放式多模态查询的基准。ProBench 包含 10 个任务领域和 56 个子领域，支持 17 种语言，并支持最多 13 轮对话。",
                            "en": "ProBench is a benchmark that contains open-ended multimodal queries that require intensive expert-level knowledge to solve. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/probench'. Error: Path opencompass/probench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1512",
                    "name": "MLVU",
                    "version": "1.0.0",
                    "description": "MLVU test MLLMs' understanding of multi-task long videos, encompassing diverse tasks based on various long videos. ",
                    "url": "opencompass/opencompass_1512.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1512",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1512",
                        "name": "MLVU",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/JUNJIE99/MLVU",
                        "paperLink": "https://arxiv.org/abs/2406.04264",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "86",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:51:32",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:51:32",
                        "createDate": "2025-02-17 16:07:19",
                        "desc": {
                            "cn": "MLVU用于评估多模态大模型的长视频理解能力，包含面向各种类型长视频的多样化的评估任务。",
                            "en": "MLVU test MLLMs' understanding of multi-task long videos, encompassing diverse tasks based on various long videos. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mlvu'. Error: Path opencompass/mlvu is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1929",
                    "name": "MTCMB",
                    "version": "1.0.0",
                    "description": "We introduce MTCMB-a Multi-Task Benchmark for Evaluating LLMs on TCM Knowledge, Reasoning, and Safety. Developed in collaboration with certified TCM experts, MTCMB comprises 12 sub-datasets spanning five major categories: knowledge QA, language understanding, diagnostic reasoning, formula generation",
                    "url": "opencompass/opencompass_1929.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1929",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning",
                        "Knowledge",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1929",
                        "name": "MTCMB",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Chinese Medicine",
                                "en": "Chinese Medicine"
                            },
                            {
                                "cn": "Benchmark",
                                "en": "Benchmark"
                            },
                            {
                                "cn": "TCM",
                                "en": "TCM"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Wayyuanyuan/MTCMB",
                        "paperLink": "https://doi.org/10.48550/arXiv.2506.01252",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "17001784",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-g5il9RCGl"
                        },
                        "lookNum": "85",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-16 09:49:31",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-16 09:49:31",
                        "createDate": "2025-06-14 09:16:46",
                        "desc": {
                            "cn": "大语言模型在中医领域的应用日益增多，到底大语言模型在这一古老的学科表现如何？虽然有一些零碎的评测数据集，但大都以单选题、病案分析、粗糙的医患对话为主，很难全面评估大语言模型在中医领域的实际能力。近日，中山大学联合湖南中医药大学等团队推出全球首个中医多任务评测基准（Benchmark） —— MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine",
                            "en": "We introduce MTCMB-a Multi-Task Benchmark for Evaluating LLMs on TCM Knowledge, Reasoning, and Safety. Developed in collaboration with certified TCM experts, MTCMB comprises 12 sub-datasets spanning five major categories: knowledge QA, language understanding, diagnostic reasoning, formula generation"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mtcmb'. Error: Path opencompass/mtcmb is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1829",
                    "name": "MMS-VPR",
                    "version": "1.0.0",
                    "description": "MMS-VPR is a large-scale multimodal dataset for street-level place recognition in pedestrian areas. It contains 78,575 images and 2,512 videos from 207 locations in Chengdu, China, with rich metadata and spatial graph structure.",
                    "url": "opencompass/opencompass_1829.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1829",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding",
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1829",
                        "name": "MMS-VPR",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            },
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Visual Place Recognition",
                                "en": "Visual Place Recognition"
                            },
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "Dataset and Benchmark",
                                "en": "Dataset and Benchmark"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/yiasun/MMS-VPRlib",
                        "paperLink": "https://arxiv.org/abs/2505.12254",
                        "officialWebsiteLink": "https://huggingface.co/datasets/Yiwei-Ou/MMS-VPR",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "53005345",
                            "name": "Yiwei-Ou",
                            "avatar": null,
                            "nickname": "Yiwei-Ou"
                        },
                        "lookNum": "84",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-05-27 12:50:29",
                        "supportOnlineEval": false,
                        "updateDate": "2025-05-27 12:50:29",
                        "createDate": "2025-05-23 20:07:40",
                        "desc": {
                            "cn": "MMS-VPR 是一个面向复杂城市步行街区的 多模态街景视觉地点识别大规模数据集，采集自成都约 70,800 平方米的开放式商业街区，覆盖 207 个地点，包含 78,575 张图像和 2,512 段视频，每条数据均带有 GPS 坐标、时间戳和文本元信息。相较以车载视角和西方城市为主的传统数据集，MMS-VPR 更贴近真实、密集、多用途的街道空间，具备丰富的视角、时段和模态多样性。此外，该数据集构建了包含 81 个节点、125 条边的空间图结构，支持结构感知的地点识别方法。数据集还定义了两个子集（Edges 和Points），支持精细化和图结构评估任务，助力多模态与地理空间理解的交叉研究。",
                            "en": "MMS-VPR is a large-scale multimodal dataset for street-level place recognition in pedestrian areas. It contains 78,575 images and 2,512 videos from 207 locations in Chengdu, China, with rich metadata and spatial graph structure."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mms_vpr'. Error: Path opencompass/mms_vpr is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1839",
                    "name": "tiny_qa_benchmark_pp",
                    "version": "1.0.0",
                    "description": "TinyQA is a benchmark suite designed to evaluate the reasoning abilities of large language models (LLMs). It focuses on assessing LLMs through natural language question-answer pairs, covering various types of reasoning tasks such as causal, logical, and commonsense reasoning. ",
                    "url": "opencompass/opencompass_1839.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1839",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1839",
                        "name": "tiny_qa_benchmark_pp",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Strong Reasoning",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/vincentkoc/tiny_qa_benchmark_pp",
                        "paperLink": "https://arxiv.org/pdf/2505.12058",
                        "officialWebsiteLink": "https://huggingface.co/datasets/vincentkoc/tiny_qa_benchmark_pp",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "82",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:30",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:30",
                        "createDate": "2025-06-04 11:15:55",
                        "desc": {
                            "cn": "TinyQA是一个用于评估大语言模型（LLMs）推理能力的基准测试套件。该基准专注于通过自然语言问题和答案对来衡量LLMs的推理能力，涵盖了多种类型的推理任务，包括因果推理、逻辑推理和常识推理。TinyQA提供了多样化的数据集，旨在挑战LLMs的推理深度和广度。通过严格的评估，TinyQA能够帮助研究人员更好地理解LLMs在处理复杂语言任务时的表现，并为改进模型提供方向。",
                            "en": "TinyQA is a benchmark suite designed to evaluate the reasoning abilities of large language models (LLMs). It focuses on assessing LLMs through natural language question-answer pairs, covering various types of reasoning tasks such as causal, logical, and commonsense reasoning. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/tiny_qa_benchmark_pp'. Error: Path opencompass/tiny_qa_benchmark_pp is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1845",
                    "name": "STARK_10k",
                    "version": "1.0.0",
                    "description": "STARK is a comprehensive benchmark designed to systematically evaluate large language models (LLMs) and large reasoning models (LRMs) on spatiotemporal reasoning tasks, particularly for applications in cyber-physical systems (CPS) such as robotics, autonomous vehicles, and smart city infrastructure.",
                    "url": "opencompass/opencompass_1845.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1845",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1845",
                        "name": "STARK_10k",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/nesl/STARK_Benchmark/",
                        "paperLink": "https://arxiv.org/pdf/2505.11618",
                        "officialWebsiteLink": "https://huggingface.co/datasets/prquan/STARK_10k",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "82",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:41",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:41",
                        "createDate": "2025-06-04 11:15:01",
                        "desc": {
                            "cn": "STARK 是一个全面的基准测试套件，旨在系统评估大语言模型（LLMs）和大推理模型（LRMs）在时空推理任务中的表现，特别是在网络物理系统（CPS）中的应用，如机器人、自动驾驶和智能城市基础设施。该基准包含 26 种不同的时空任务，涵盖状态估计、时空关系推理和世界知识感知推理三个层次。",
                            "en": "STARK is a comprehensive benchmark designed to systematically evaluate large language models (LLMs) and large reasoning models (LRMs) on spatiotemporal reasoning tasks, particularly for applications in cyber-physical systems (CPS) such as robotics, autonomous vehicles, and smart city infrastructure."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/stark_10k'. Error: Path opencompass/stark_10k is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1656",
                    "name": "RFUAV",
                    "version": "1.0.0",
                    "description": "RFUAV offers a comprehensive benchmark dataset for Radio-Frequency (RF)-based drone detection and identification.",
                    "url": "opencompass/opencompass_1656.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1656",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1656",
                        "name": "RFUAV",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/kitoweeknd/RFUAV/",
                        "paperLink": "https://arxiv.org/pdf/2503.09033",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "82",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-20 14:27:02",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-20 14:27:02",
                        "createDate": "2025-03-20 14:05:07",
                        "desc": {
                            "cn": "RFUAV 提供了一个基于射频（RF）的无人机检测和识别的全面基准数据集。",
                            "en": "RFUAV offers a comprehensive benchmark dataset for Radio-Frequency (RF)-based drone detection and identification."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/rfuav'. Error: Path opencompass/rfuav is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1972",
                    "name": "CPRet",
                    "version": "1.0.0",
                    "description": "Programming contests are long used to evaluate algorithmic thinking and coding skills, and have recently become benchmarks for assessing large language models (LLMs). However, the rapid expansion of problem sets has led to a surge in duplicate or highly similar problems, compromising fairness ...",
                    "url": "opencompass/opencompass_1972.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1972",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1972",
                        "name": "CPRet",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Problem Similarity",
                                "en": "Problem Similarity"
                            },
                            {
                                "cn": "Competitive Programming",
                                "en": "Competitive Programming"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/coldchair/CPRet",
                        "paperLink": "https://arxiv.org/abs/2505.12925",
                        "officialWebsiteLink": "https://huggingface.co/collections/coldchair16/cpret-682451276f05c5988fcbdf34",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "38705824",
                            "name": null,
                            "avatar": null,
                            "nickname": "coldchair"
                        },
                        "lookNum": "82",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-24 15:44:30",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-24 15:44:30",
                        "createDate": "2025-06-24 14:41:53",
                        "desc": {
                            "cn": "编程竞赛长期用于评估算法与编程能力，近年来也被用于大语言模型的评测。但随着题库扩展，重复或相似题激增，影响竞赛公平性与模型评测效果。为此，本文提出“相似题目检索”任务，并构建统一检索基准数据集 CPRet，涵盖题目与代码的四类检索任务，包含自动爬取和人工标注的数据样本。同时，设计并训练了两种检索模型 CPRetriever-Code 与 CPRetriever-Prob，显著提升检索效果。实验还发现相似题会提高模型得分、减小模型差异，强调了评测中引入“相似性感知”的必要性。我们还发布了开源检索平台，支持重复题检测与相似题推荐。项目地址：https://github.com/coldchair/",
                            "en": "Programming contests are long used to evaluate algorithmic thinking and coding skills, and have recently become benchmarks for assessing large language models (LLMs). However, the rapid expansion of problem sets has led to a surge in duplicate or highly similar problems, compromising fairness ..."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cpret'. Error: Path opencompass/cpret is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1987",
                    "name": "DeepResearchBench",
                    "version": "1.0.0",
                    "description": "DeepResearch Bench is a comprehensive benchmark designed to evaluate large language model (LLM) agents on complex research tasks. ",
                    "url": "opencompass/opencompass_1987.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1987",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1987",
                        "name": "DeepResearchBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Ayanami0730/deep_research_bench",
                        "paperLink": "https://arxiv.org/abs/2506.11763",
                        "officialWebsiteLink": "https://deepresearch-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "81",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-30 11:37:54",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-30 11:37:54",
                        "createDate": "2025-06-30 11:37:46",
                        "desc": {
                            "cn": "DeepResearch Bench 是一个面向大型语言模型（LLM）智能体的综合性评测基准，专为评估其在复杂研究任务中的表现而设计。 该基准包含 100 个由 22 个领域的专家精心设计的博士级研究任务，涵盖多领域的深度研究需求。",
                            "en": "DeepResearch Bench is a comprehensive benchmark designed to evaluate large language model (LLM) agents on complex research tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/deepresearchbench'. Error: Path opencompass/deepresearchbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1752",
                    "name": "LLM-SRBench",
                    "version": "1.0.0",
                    "description": "LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization.",
                    "url": "opencompass/opencompass_1752.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1752",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1752",
                        "name": "LLM-SRBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Strong Reasoning",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/deep-symbolic-mathematics/llm-srbench",
                        "paperLink": "https://arxiv.org/abs/2504.10415",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "80",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-21 12:34:37",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-21 12:34:37",
                        "createDate": "2025-04-21 12:02:24",
                        "desc": {
                            "cn": "LLM-SRBench，这是一个包含239个挑战性问题的综合基准测试，涵盖四个科学领域，专门设计用于评估基于LLMs的科学方程式发现方法，同时防止简单记忆。",
                            "en": "LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/llm_srbench'. Error: Path opencompass/llm_srbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1622",
                    "name": "CORAL",
                    "version": "1.0.0",
                    "description": "Researchers present a large-scale conversational RAG benchmark named CORAL and propose a unified framework for standardizing and evaluating various conversational RAG baselines.",
                    "url": "opencompass/opencompass_1622.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1622",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1622",
                        "name": "CORAL",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "RAG",
                                "en": "RAG"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Ariya12138/CORAL",
                        "paperLink": "https://arxiv.org/abs/2410.23090",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "79",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-12 11:07:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-12 11:07:55",
                        "createDate": "2025-03-11 15:39:20",
                        "desc": {
                            "cn": "CORAL 是一个大规模对话 RAG 基准，包含一个统一框架，用于标准化和评估各种对话 RAG 基线。",
                            "en": "Researchers present a large-scale conversational RAG benchmark named CORAL and propose a unified framework for standardizing and evaluating various conversational RAG baselines."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/coral'. Error: Path opencompass/coral is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1733",
                    "name": "FEABench",
                    "version": "1.0.0",
                    "description": "FEABench is a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA). ",
                    "url": "opencompass/opencompass_1733.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1733",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1733",
                        "name": "FEABench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "FEA基准测试",
                                "en": "FEA基准测试"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/google/feabench/tree/main",
                        "paperLink": "https://arxiv.org/abs/2504.06260",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "79",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:13:05",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:13:05",
                        "createDate": "2025-04-11 14:42:38",
                        "desc": {
                            "cn": "FEABench，一个用于评估大型语言模型和LLM代理使用有限元分析（FEA）模拟和解决物理、数学及工程问题能力的基准测试。",
                            "en": "FEABench is a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA). "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/feabench'. Error: Path opencompass/feabench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1744",
                    "name": "StyleRec",
                    "version": "1.0.0",
                    "description": "A Benchmark Dataset for Prompt Recovery in Writing Style Transformation.",
                    "url": "opencompass/opencompass_1744.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1744",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding",
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1744",
                        "name": "StyleRec",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            },
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "NLP",
                                "en": "NLP"
                            },
                            {
                                "cn": "Writing Style Transformation",
                                "en": "Writing Style Transformation"
                            },
                            {
                                "cn": "Prompt Recovery",
                                "en": "Prompt Recovery"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/promptrecovery501/StyleRec",
                        "paperLink": "https://arxiv.org/abs/2504.04373",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "18903058",
                            "name": "qianertongre",
                            "avatar": null,
                            "nickname": "qianertongre"
                        },
                        "lookNum": "79",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-16 11:32:28",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-16 11:32:28",
                        "createDate": "2025-04-15 13:45:08",
                        "desc": {
                            "cn": "基于写作风格转换的提示词恢复的评测集",
                            "en": "A Benchmark Dataset for Prompt Recovery in Writing Style Transformation."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/stylerec'. Error: Path opencompass/stylerec is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1552",
                    "name": "CEB",
                    "version": "1.0.0",
                    "description": "CEB evaluates LLM bias compositionally, featuring 11k samples characterized across bias types, social groups, and tasks.",
                    "url": "opencompass/opencompass_1552.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1552",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1552",
                        "name": "CEB",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SongW-SW/CEB",
                        "paperLink": "https://arxiv.org/abs/2407.02408",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "78",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-06 16:49:21",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-06 16:49:21",
                        "createDate": "2025-02-26 14:20:34",
                        "desc": {
                            "cn": "CEB是一个用于大型语言模型偏差的组成评估基准，引入了包含 11,004 个样本的组成评估基准，从偏差类型、社会群体和任务三个维度描述每个数据集。",
                            "en": "CEB evaluates LLM bias compositionally, featuring 11k samples characterized across bias types, social groups, and tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ceb'. Error: Path opencompass/ceb is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1838",
                    "name": "LLM-BabyBench",
                    "version": "1.0.0",
                    "description": "LLM-BabyBench is a benchmark suite designed to evaluate Large Language Models (LLMs) on grounded planning and reasoning tasks.",
                    "url": "opencompass/opencompass_1838.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1838",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1838",
                        "name": "LLM-BabyBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Strong Reasoning",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/choukrani/llm-babybench",
                        "paperLink": "https://arxiv.org/pdf/2505.12135",
                        "officialWebsiteLink": "https://huggingface.co/datasets/salem-mbzuai/LLM-BabyBench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "78",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:49",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:49",
                        "createDate": "2025-06-04 11:16:10",
                        "desc": {
                            "cn": "LLM-BabyBench是一个专门评估大语言模型在交互环境中规划和推理能力的新基准测试套件。基于BabyAI网格世界的文本适配版本，该基准评估LLMs在三个核心方面的表现：预测动作对环境状态的影响（Predict任务）、生成低级动作序列以实现指定目标（Plan任务）、以及将高级指令分解为连贯的子目标序列（Decompose任务）。\n基准包含16个难度级别，提供三种文本格式（Narrative、Structured、JSON），并配备OmniBot专家代理用于生成基准数据。",
                            "en": "LLM-BabyBench is a benchmark suite designed to evaluate Large Language Models (LLMs) on grounded planning and reasoning tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/llm_babybench'. Error: Path opencompass/llm_babybench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1678",
                    "name": "RSMMVP",
                    "version": "1.0.0",
                    "description": "This dataset follows a similar procedure to the original MMVP benchmark on natural images but directed towards the remote sensing domain. Challenging visual patterns are identified based on CLIP blind pairs, accompanied with the correpsonding questions, options and ground-truth answer.",
                    "url": "opencompass/opencompass_1678.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1678",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1678",
                        "name": "RSMMVP",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2503.15816",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "77",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-26 15:56:27",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-26 15:56:27",
                        "createDate": "2025-03-26 14:35:19",
                        "desc": {
                            "cn": "RSMMVP遵循与原始 MMVP 基准在自然图像上的类似流程，但针对遥感领域。根据 CLIP 盲对识别具有挑战性的视觉模式，并附带相应的问题、选项和真实答案。",
                            "en": "This dataset follows a similar procedure to the original MMVP benchmark on natural images but directed towards the remote sensing domain. Challenging visual patterns are identified based on CLIP blind pairs, accompanied with the correpsonding questions, options and ground-truth answer."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/rsmmvp'. Error: Path opencompass/rsmmvp is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2005",
                    "name": "OSS-Bench",
                    "version": "1.0.0",
                    "description": "OSS-Bench replaces functions with LLM-generated code and evaluates them using three natural metrics: compilability, functional correctness, and memory safety, leveraging robust signals like compilation failures, test-suite violations, and sanitizer alerts as ground truth.",
                    "url": "opencompass/opencompass_2005.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2005",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2005",
                        "name": "OSS-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Memory Safety",
                                "en": "Memory Safety"
                            },
                            {
                                "cn": "Open-Source Software",
                                "en": "Open-Source Software"
                            },
                            {
                                "cn": "Living Benchmark",
                                "en": "Living Benchmark"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/oss-bench/oss-bench",
                        "paperLink": "https://arxiv.org/abs/2505.12331",
                        "officialWebsiteLink": "https://oss-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "85503313",
                            "name": null,
                            "avatar": null,
                            "nickname": "jiangyc"
                        },
                        "lookNum": "77",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-01 09:36:14",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-01 09:36:14",
                        "createDate": "2025-06-30 18:41:59",
                        "desc": {
                            "cn": "OSS-Bench，这是一个基准生成器，它可以从真实的开源软件中自动构建大规模的实时评估任务。OSS-Bench 将函数替换为 LLM 生成的代码，并使用三个自然指标（可编译性、功能正确性和内存安全性）对其进行评估，并利用编译失败、测试套件违规和Sanitizer警报等稳健信号作为基准事实。",
                            "en": "OSS-Bench replaces functions with LLM-generated code and evaluates them using three natural metrics: compilability, functional correctness, and memory safety, leveraging robust signals like compilation failures, test-suite violations, and sanitizer alerts as ground truth."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/oss_bench'. Error: Path opencompass/oss_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1848",
                    "name": "MAVOS-DD",
                    "version": "1.0.0",
                    "description": "MAVOS-DD is the first large-scale open-set benchmark for multilingual audio-video deepfake detection.",
                    "url": "opencompass/opencompass_1848.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1848",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1848",
                        "name": "MAVOS-DD",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            },
                            {
                                "cn": "AVQA",
                                "en": "AVQA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/pdf/2505.11109",
                        "officialWebsiteLink": "https://huggingface.co/datasets/unibuc-cs/MAVOS-DD",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "75",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:33",
                        "createDate": "2025-06-04 11:14:23",
                        "desc": {
                            "cn": "MAVOS-DD是一个大规模的多语言音视频深度伪造检测基准数据集，包含超过250小时的真实和伪造视频，涵盖八种语言。该数据集通过七种不同的深度伪造生成模型生成伪造视频，这些模型基于不同的生成方法，包括说话头像生成、表情转移和换脸。MAVOS-DD设计了多种开放集测试场景，包括开放集模型、开放集语言和全开放集，以评估深度伪造检测模型在未知模型和语言下的泛化能力。实验结果表明，现有的深度伪造检测模型在开放集场景下的性能显著下降，凸显了开发更鲁棒检测技术的必要性。",
                            "en": "MAVOS-DD is the first large-scale open-set benchmark for multilingual audio-video deepfake detection."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mavos_dd'. Error: Path opencompass/mavos_dd is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1742",
                    "name": "U-NIAH",
                    "version": "1.0.0",
                    "description": "U-NIAH is a framework unifying RAG and LLM for needle-in-a-haystack tasks, based on the fictional Starlight Academy dataset. It eliminates interference from pre-trained knowledge and supports diverse, complex scenarios (e.g., multi-needle, long-needle, \"needle-in-needle\").",
                    "url": "opencompass/opencompass_1742.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1742",
                    "sample_count": 1000,
                    "traits": [
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1742",
                        "name": "U-NIAH",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "长文本",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "RAG",
                                "en": "RAG"
                            },
                            {
                                "cn": "Long-Context",
                                "en": "Long-Context"
                            },
                            {
                                "cn": "Needle In A Haystack",
                                "en": "Needle In A Haystack"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Tongji-KGLLM/U-NIAH",
                        "paperLink": "https://arxiv.org/abs/2503.00353",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "85508164",
                            "name": "yunfan",
                            "avatar": null,
                            "nickname": "yunfan"
                        },
                        "lookNum": "74",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-16 11:32:40",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-16 11:32:40",
                        "createDate": "2025-04-13 10:56:15",
                        "desc": {
                            "cn": "U-NIAH是将 RAG 和 LLM 统一映射在大海捞针任务中的框架。所有任务基于一个虚构背景下的数据集Starlight Academy，涵盖了魔法系统、学术课程、校园生活、等多个方面，旨在消除预训练知识的干扰，从而能够独立于 LLMs 的先验知识。框架包含多种评估场景，支持多针（3、7、15个针）和长针（400-500 token）配置，还引入了“针中针”结构，进一步增加了复杂性。该数据集通过多样化的场景和合成生成的内容，能够从多个维度分析模型在长文本场景下的性能。同时通过模块化设计，U-NIAH可持续注入新的挑战场景。",
                            "en": "U-NIAH is a framework unifying RAG and LLM for needle-in-a-haystack tasks, based on the fictional Starlight Academy dataset. It eliminates interference from pre-trained knowledge and supports diverse, complex scenarios (e.g., multi-needle, long-needle, \"needle-in-needle\")."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/u_niah'. Error: Path opencompass/u_niah is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1780",
                    "name": "C-FAITH",
                    "version": "1.0.0",
                    "description": "C-FAITH, a Chinese QA hallucination benchmark created from 1,399 knowledge documents obtained from web scraping, totaling 60,702 entries.",
                    "url": "opencompass/opencompass_1780.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1780",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1780",
                        "name": "C-FAITH",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/pkulcwmzx/C-FAITH",
                        "paperLink": "https://arxiv.org/abs/2504.10167",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "74",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-25 17:00:49",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-25 17:00:49",
                        "createDate": "2025-04-25 16:42:33",
                        "desc": {
                            "cn": " C-FAITH，这是一个中国 QA 幻觉基准，由从网络抓取中获得的 1,399 份知识文档创建，总共 60,702 个条目。",
                            "en": "C-FAITH, a Chinese QA hallucination benchmark created from 1,399 knowledge documents obtained from web scraping, totaling 60,702 entries."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/c_faith'. Error: Path opencompass/c_faith is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1865",
                    "name": "GitGoodBench",
                    "version": "1.0.0",
                    "description": "GitGoodBench Lite is a subset of 900 samples for evaluating the performance of AI agents in resolving git tasks (see Supported Scenarios). ",
                    "url": "opencompass/opencompass_1865.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1865",
                    "sample_count": 1000,
                    "traits": [
                        "Code",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1865",
                        "name": "GitGoodBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/JetBrains-Research/git-good-bench",
                        "paperLink": "https://arxiv.org/pdf/2505.22583",
                        "officialWebsiteLink": "https://huggingface.co/datasets/JetBrains/git_good_bench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "73",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-03 15:06:02",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-03 15:06:02",
                        "createDate": "2025-06-03 12:15:09",
                        "desc": {
                            "cn": "GitGoodBench Lite是一个包含900个样本的子集，用于评估AI智能体在解决git任务方面的性能（参见支持的场景）。数据集中的样本在编程语言Python、Java和Kotlin以及样本类型合并冲突解决和文件提交语法之间均匀分布。因此，该数据集包含每种样本类型和编程语言各150个样本。",
                            "en": "GitGoodBench Lite is a subset of 900 samples for evaluating the performance of AI agents in resolving git tasks (see Supported Scenarios). "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gitgoodbench'. Error: Path opencompass/gitgoodbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1681",
                    "name": "PRMBench_Preview",
                    "version": "1.0.0",
                    "description": "PRMBench is a benchmark dataset for evaluating process-level reward models (PRMs). It consists of 6,216 data instances, each containing a question, a solution process, and a modified process with errors.",
                    "url": "opencompass/opencompass_1681.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1681",
                    "sample_count": 1000,
                    "traits": [
                        "Examination"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1681",
                        "name": "PRMBench_Preview",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "学科",
                                "en": "Examination"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ssmisya/PRMBench",
                        "paperLink": "https://arxiv.org/abs/2501.03124",
                        "officialWebsiteLink": "https://prmbench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "73",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-26 15:56:18",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-26 15:56:18",
                        "createDate": "2025-03-26 15:07:01",
                        "desc": {
                            "cn": "PRMBench 是一个用于评估过程级奖励模型（PRM）的基准数据集。它包含 6,216 个数据实例，每个实例包含一个问题、一个解决方案过程以及一个包含错误的修改过程。",
                            "en": "PRMBench is a benchmark dataset for evaluating process-level reward models (PRMs). It consists of 6,216 data instances, each containing a question, a solution process, and a modified process with errors."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/prmbench_preview'. Error: Path opencompass/prmbench_preview is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1849",
                    "name": "CleanPatrick",
                    "version": "1.0.0",
                    "description": "CleanPatrick is a large-scale, real-world image data-cleaning benchmark with 496,377 binary annotations from 933 medical crowd workers for ranking off-topic, near-duplicate, and label-error issues.",
                    "url": "opencompass/opencompass_1849.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1849",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding",
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1849",
                        "name": "CleanPatrick",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            },
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Digital-Dermatology/CleanPatrick",
                        "paperLink": "https://arxiv.org/pdf/2505.11034",
                        "officialWebsiteLink": "https://huggingface.co/datasets/Digital-Dermatology/CleanPatrick",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "72",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:38",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:38",
                        "createDate": "2025-06-04 11:14:42",
                        "desc": {
                            "cn": "CleanPatrick是首个大规模图像数据清洗基准，基于公开的Fitzpatrick17k皮肤科数据集构建。该基准包含超过50万条来自933名医学众包工人的二元注释，涵盖三种数据质量问题：离题样本、近似重复样本和标签错误。通过医学专家验证，CleanPatrick提供了高质量的基准数据，用于评估图像数据清洗策略。基准测试结果表明，现有的数据清洗方法在近似重复检测中表现出色，但在标签错误检测方面仍面临挑战。CleanPatrick为数据清洗方法提供了标准化的评估框架，推动了更可靠的数据中心人工智能的发展。",
                            "en": "CleanPatrick is a large-scale, real-world image data-cleaning benchmark with 496,377 binary annotations from 933 medical crowd workers for ranking off-topic, near-duplicate, and label-error issues."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cleanpatrick'. Error: Path opencompass/cleanpatrick is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1981",
                    "name": "OmniBench",
                    "version": "1.0.0",
                    "description": "OmniBench is a multi-dimensional benchmark for virtual agents, designed to systematically evaluate ten core capabilities such as planning, decision-making, and instruction comprehension through automatically generated task graphs with controllable complexity. ",
                    "url": "opencompass/opencompass_1981.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1981",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1981",
                        "name": "OmniBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/antgroup/OmniBench",
                        "paperLink": "https://arxiv.org/abs/2506.08933",
                        "officialWebsiteLink": "https://omni-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "72",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-27 18:12:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-27 18:12:33",
                        "createDate": "2025-06-26 13:55:08",
                        "desc": {
                            "cn": "OmniBench 是一个面向虚拟智能体的多维度评测基准，旨在通过自动化流程生成具有可控复杂度的任务图，系统评估智能体在计划、决策、指令理解等十个核心能力上的表现。该基准包含 36,000 个图结构任务，覆盖 20 个真实场景，并引入 OmniEval 框架，实现子任务级别的细粒度评估，显著提升了评测的效率和可扩展性。",
                            "en": "OmniBench is a multi-dimensional benchmark for virtual agents, designed to systematically evaluate ten core capabilities such as planning, decision-making, and instruction comprehension through automatically generated task graphs with controllable complexity. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/omnibench'. Error: Path opencompass/omnibench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1739",
                    "name": "CrossWordBench",
                    "version": "1.0.0",
                    "description": "CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles.",
                    "url": "opencompass/opencompass_1739.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1739",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1739",
                        "name": "CrossWordBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SeanLeng1/CrossWordBench",
                        "paperLink": "https://arxiv.org/abs/2504.00043",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "72",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:15:53",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:15:53",
                        "createDate": "2025-04-11 16:14:37",
                        "desc": {
                            "cn": "CrossWordBench，这是一个基准测试，旨在通过填字游戏的方式来评估LLMs和LVLMs的推理能力。",
                            "en": "CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/crosswordbench'. Error: Path opencompass/crosswordbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1910",
                    "name": "MMAR",
                    "version": "1.0.0",
                    "description": "We introduce MMAR, a new benchmark comprising 1,000 meticulously curated audio-question-answer triplets, designed to evaluate the deep reasoning capabilities of Audio-Language Models (ALMs) across massive multi-disciplinary tasks. ",
                    "url": "opencompass/opencompass_1910.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1910",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1910",
                        "name": "MMAR",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "MMAR",
                                "en": "MMAR"
                            },
                            {
                                "cn": "音频语言模型",
                                "en": "音频语言模型"
                            },
                            {
                                "cn": "深度推理",
                                "en": "深度推理"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ddlBoJack/MMAR",
                        "paperLink": "https://arxiv.org/abs/2505.13032",
                        "officialWebsiteLink": "https://huggingface.co/datasets/BoJack/MMAR",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "20908931",
                            "name": null,
                            "avatar": null,
                            "nickname": "ddlBoJack"
                        },
                        "lookNum": "71",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-09 11:31:06",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-09 11:31:06",
                        "createDate": "2025-06-06 19:32:53",
                        "desc": {
                            "cn": "MMAR是一个全新评测基准，旨在评估音频-语言模型（ALMs）的深度推理能力。该基准包含1,000个精心构建的音频与问答，并经过多轮纠错与质量校验以确保高标准。与现有局限于特定声音、音乐或语音领域的评测体系不同，MMAR覆盖现实场景中的混合模态，并采用四级分层分类体系（信号层、感知层、语义层与文化层）。该基准中的每个题目都需要超越表层理解的多层次深度推理，部分问题更要求研究生级别的专业领域知识与感知能力。我们在MMAR上评估了多类模型，测试结果表明该基准具有显著挑战性，分析结果进一步揭示了当前模型在理解与推理能力上的关键局限。我们期待MMAR能推动这个重要但尚未充分探索的研究领域的发展。",
                            "en": "We introduce MMAR, a new benchmark comprising 1,000 meticulously curated audio-question-answer triplets, designed to evaluate the deep reasoning capabilities of Audio-Language Models (ALMs) across massive multi-disciplinary tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmar'. Error: Path opencompass/mmar is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1740",
                    "name": "GPT-ImgEval",
                    "version": "1.0.0",
                    "description": "GPT-ImgEval, quantitatively and qualitatively diagnoses GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis.",
                    "url": "opencompass/opencompass_1740.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1740",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1740",
                        "name": "GPT-ImgEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/PicoTrex/GPT-ImgEval",
                        "paperLink": "https://arxiv.org/abs/2504.02782",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "71",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:16:12",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:16:12",
                        "createDate": "2025-04-11 16:58:50",
                        "desc": {
                            "cn": "GPT-ImgEval，从三个关键维度对GPT-4o的性能进行定量和定性诊断：（1）生成质量，（2）编辑能力，以及（3）基于世界知识的语义合成能力。",
                            "en": "GPT-ImgEval, quantitatively and qualitatively diagnoses GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gpt_imgeval'. Error: Path opencompass/gpt_imgeval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1755",
                    "name": "REAL",
                    "version": "1.0.0",
                    "description": "Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites",
                    "url": "opencompass/opencompass_1755.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1755",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1755",
                        "name": "REAL",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/agi-inc/agisdk",
                        "paperLink": "https://arxiv.org/abs/2504.11543",
                        "officialWebsiteLink": "https://www.realevals.xyz/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "71",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-21 12:34:45",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-21 12:34:45",
                        "createDate": "2025-04-21 12:31:05",
                        "desc": {
                            "cn": "在真实网站的确定性模拟上对自主代理进行基准测试",
                            "en": "Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/real'. Error: Path opencompass/real is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1623",
                    "name": "MJ-Bench",
                    "version": "1.0.0",
                    "description": "MJ-Bench incorporates a comprehensive preference dataset to evaluate multimodal judges in providing feedback for image generation models across four key perspectives: alignment, safety, image quality, and bias. ",
                    "url": "opencompass/opencompass_1623.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1623",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1623",
                        "name": "MJ-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MJ-Bench/MJ-Bench",
                        "paperLink": "https://arxiv.org/abs/2407.04842",
                        "officialWebsiteLink": "https://mj-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "69",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-12 11:07:50",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-12 11:07:50",
                        "createDate": "2025-03-11 15:46:38",
                        "desc": {
                            "cn": "MJ-Bench，它包含了一个综合的偏好数据集，用于从四个关键角度评估多模态评委在为图像生成模型提供反馈方面的能力：对齐、安全性、图像质量和偏见。",
                            "en": "MJ-Bench incorporates a comprehensive preference dataset to evaluate multimodal judges in providing feedback for image generation models across four key perspectives: alignment, safety, image quality, and bias. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mj_bench'. Error: Path opencompass/mj_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1668",
                    "name": "TimeTravel",
                    "version": "1.0.0",
                    "description": "TimeTravel Taxonomy maps artifacts from 10 civilizations, 266 cultures, and 10k+ verified samples for AI-driven historical analysis.",
                    "url": "opencompass/opencompass_1668.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1668",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1668",
                        "name": "TimeTravel",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/mbzuai-oryx/TimeTravel",
                        "paperLink": "https://arxiv.org/abs/2502.14865",
                        "officialWebsiteLink": "https://mbzuai-oryx.github.io/TimeTravel/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "69",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-24 18:48:21",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-24 18:48:21",
                        "createDate": "2025-03-24 17:05:23",
                        "desc": {
                            "cn": "时间旅行分类将来自 10 个文明、266 个文化以及 10k+个验证样本的文物映射，用于 AI 驱动的历史分析。",
                            "en": "TimeTravel Taxonomy maps artifacts from 10 civilizations, 266 cultures, and 10k+ verified samples for AI-driven historical analysis."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/timetravel'. Error: Path opencompass/timetravel is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1801",
                    "name": "OmniGIRL",
                    "version": "1.0.0",
                    "description": "A multi-modal, multi-language benchmark dataset for the GitHub Issue Resolution task.",
                    "url": "opencompass/opencompass_1801.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1801",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Code",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1801",
                        "name": "OmniGIRL",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "代码",
                                "en": "Code"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Github Issue Resolution",
                                "en": "Github Issue Resolution"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/DeepSoftwareAnalytics/OmniGIRL",
                        "paperLink": "https://arxiv.org/abs/2505.04606",
                        "officialWebsiteLink": "https://deepsoftwareanalytics.github.io/omnigirl_leaderboard.html",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "26208047",
                            "name": null,
                            "avatar": null,
                            "nickname": "gnohgnailoug"
                        },
                        "lookNum": "68",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-05-09 21:26:49",
                        "supportOnlineEval": false,
                        "updateDate": "2025-05-09 21:26:49",
                        "createDate": "2025-05-09 20:52:15",
                        "desc": {
                            "cn": "一个面向 GitHub Issue ResoLution任务的多语言、多模态基准数据集，包含以下特点: 1.支持 Python、Java、JS、TS 四种主流编程语言，2. 输入信息涵盖文本、图像、网页等多种模态，3. 提供可复现的 Docker 评估环境。",
                            "en": "A multi-modal, multi-language benchmark dataset for the GitHub Issue Resolution task."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/omnigirl'. Error: Path opencompass/omnigirl is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1832",
                    "name": "MedBrowseComp",
                    "version": "1.0.0",
                    "description": "the first benchmark that systematically tests an agent’s ability to reliably retrieve and synthesize multi-hop medical facts from live, domain-specific knowledge bases. MedBrowseComp holds 1,000+ human-curated questions that mirror clinical scenarios\n﻿",
                    "url": "opencompass/opencompass_1832.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1832",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1832",
                        "name": "MedBrowseComp",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Medical",
                                "en": "Medical"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/shan23chen/MedBrowseComp",
                        "paperLink": "https://arxiv.org/pdf/2505.14963",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "68",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:23",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:23",
                        "createDate": "2025-06-04 11:17:23",
                        "desc": {
                            "cn": "the first benchmark that systematically tests an agent’s ability to reliably retrieve and synthesize multi-hop medical facts from live, domain-specific knowledge bases. MedBrowseComp holds 1,000+ human-curated questions that mirror clinical scenarios\n﻿",
                            "en": "the first benchmark that systematically tests an agent’s ability to reliably retrieve and synthesize multi-hop medical facts from live, domain-specific knowledge bases. MedBrowseComp holds 1,000+ human-curated questions that mirror clinical scenarios\n﻿"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medbrowsecomp'. Error: Path opencompass/medbrowsecomp is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1667",
                    "name": "MicroVQA",
                    "version": "1.0.0",
                    "description": "MicroVQA is a benchmark that evaluates LLM reasoning on multiple-choice questions about microscopy images, created by expert biologists.",
                    "url": "opencompass/opencompass_1667.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1667",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1667",
                        "name": "MicroVQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/jmhb0/microvqa",
                        "paperLink": "https://arxiv.org/abs/2503.13399",
                        "officialWebsiteLink": "https://jmhb0.github.io/microvqa/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "68",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-24 18:48:38",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-24 18:48:38",
                        "createDate": "2025-03-24 16:37:21",
                        "desc": {
                            "cn": "MicroVQA，一个评估关于显微镜图像的多选题推理基准，由专家生物学家创建，旨在反映生物研究中能够有意义地协助的任务，每个问题都需要多模态推理。",
                            "en": "MicroVQA is a benchmark that evaluates LLM reasoning on multiple-choice questions about microscopy images, created by expert biologists."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/microvqa'. Error: Path opencompass/microvqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1675",
                    "name": "PokerBench",
                    "version": "1.0.0",
                    "description": "PokerBench contains natural language game scenarios and optimal decisions computed by solvers in No Limit Texas Hold’em. It is divided into pre-flop and post-flop datasets, each with training and test splits. ",
                    "url": "opencompass/opencompass_1675.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1675",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1675",
                        "name": "PokerBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/pokerllm/pokerbench",
                        "paperLink": "https://arxiv.org/abs/2501.08328",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "68",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-25 10:42:49",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-25 10:42:49",
                        "createDate": "2025-03-25 10:41:14",
                        "desc": {
                            "cn": "PokerBench包含自然语言游戏场景和由求解器在无限制德州扑克中计算出的最优决策。它分为前注和后注数据集，每个数据集都包含训练集和测试集。",
                            "en": "PokerBench contains natural language game scenarios and optimal decisions computed by solvers in No Limit Texas Hold’em. It is divided into pre-flop and post-flop datasets, each with training and test splits. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/pokerbench'. Error: Path opencompass/pokerbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1847",
                    "name": "Massive-STEPS",
                    "version": "1.0.0",
                    "description": "Massive-STEPS is a large-scale semantic trajectories dataset designed for understanding and predicting Point-of-Interest (POI) check-ins. ",
                    "url": "opencompass/opencompass_1847.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1847",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1847",
                        "name": "Massive-STEPS",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            },
                            {
                                "cn": "Embodied Decision Making",
                                "en": "Embodied Decision Making"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/cruiseresearchgroup/Massive-STEPS",
                        "paperLink": "https://arxiv.org/pdf/2505.11239",
                        "officialWebsiteLink": "https://huggingface.co/collections/CRUISEResearchGroup/massive-steps-point-of-interest-check-in-dataset-682716f625d74c2569bc7a73",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "67",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:44",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:44",
                        "createDate": "2025-06-04 11:15:18",
                        "desc": {
                            "cn": "Massive-STEPS是一个大规模的语义轨迹数据集，旨在理解和预测兴趣点（POI）签到行为。该数据集基于Semantic Trails数据集构建，覆盖12个全球不同地区的城市，包含2012-2013年和2017-2018年的签到数据，提供了更现代和多样化的POI签到信息。Massive-STEPS不仅丰富了签到数据的语义信息，还通过与Foursquare Open Source Places数据集对齐，增加了POI的地理坐标、名称和地址等元数据。",
                            "en": "Massive-STEPS is a large-scale semantic trajectories dataset designed for understanding and predicting Point-of-Interest (POI) check-ins. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/massive_steps'. Error: Path opencompass/massive_steps is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2075",
                    "name": "Arena-Hard-Auto",
                    "version": "1.0.0",
                    "description": "Arena-Hard-Auto is an automated benchmark for instruction-tuned LLMs, designed to efficiently approximate human preferences. ",
                    "url": "opencompass/opencompass_2075.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2075",
                    "sample_count": 1000,
                    "traits": [
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2075",
                        "name": "Arena-Hard-Auto",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/lmarena/arena-hard-auto",
                        "paperLink": "https://icml.cc/virtual/2025/poster/45630",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "张富才"
                        },
                        "lookNum": "66",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-23 14:49:02",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-23 14:49:02",
                        "createDate": "2025-07-23 14:48:52",
                        "desc": {
                            "cn": "Arena-Hard-Auto 是一个用于评估 LLM 的基准，自动甄选 500 条高难度开放式提示，从模型区分度、人类偏好一致性与提示质量三维度进行严苛评测。依托 BenchBuilder 管道、主题建模与 LLM 裁判，实现众包数据→筛选→评分的全自动闭环。",
                            "en": "Arena-Hard-Auto is an automated benchmark for instruction-tuned LLMs, designed to efficiently approximate human preferences. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/arena_hard_auto'. Error: Path opencompass/arena_hard_auto is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1626",
                    "name": "CRAG",
                    "version": "1.0.0",
                    "description": "The Comprehensive RAG Benchmark (CRAG) is a rich and comprehensive factual question answering benchmark designed to advance research in RAG. Besides question-answer pairs, CRAG provides mock APIs to simulate web and knowledge graph search. ",
                    "url": "opencompass/opencompass_1626.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1626",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1626",
                        "name": "CRAG",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "RAG",
                                "en": "RAG"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/facebookresearch/CRAG",
                        "paperLink": "https://arxiv.org/abs/2406.04744",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "66",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-12 11:07:30",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-12 11:07:30",
                        "createDate": "2025-03-11 16:16:34",
                        "desc": {
                            "cn": "CRAG是一个丰富且全面的基于事实的问题回答基准，旨在推进 RAG 研究。除了问答对之外，CRAG 还提供了模拟网页和知识图谱搜索的模拟 API。",
                            "en": "The Comprehensive RAG Benchmark (CRAG) is a rich and comprehensive factual question answering benchmark designed to advance research in RAG. Besides question-answer pairs, CRAG provides mock APIs to simulate web and knowledge graph search. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/crag'. Error: Path opencompass/crag is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1738",
                    "name": "RUListening",
                    "version": "1.0.0",
                    "description": "RUListening: Robust Understanding through Listening, an automated QA generation framework for evaluating multimodal perception.",
                    "url": "opencompass/opencompass_1738.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1738",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1738",
                        "name": "RUListening",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2504.00369",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "66",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:15:12",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:15:12",
                        "createDate": "2025-04-11 16:07:18",
                        "desc": {
                            "cn": "RUListening：通过聆听的稳健理解，这是一个用于评估多模态感知的自动问答生成框架。",
                            "en": "RUListening: Robust Understanding through Listening, an automated QA generation framework for evaluating multimodal perception."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/rulistening'. Error: Path opencompass/rulistening is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1915",
                    "name": "RewardBench",
                    "version": "1.0.0",
                    "description": "RewardBench is a benchmark designed to evaluate the capabilities and safety of reward models (including those trained with Direct Preference Optimization, DPO). ",
                    "url": "opencompass/opencompass_1915.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1915",
                    "sample_count": 1000,
                    "traits": [
                        "Instruct",
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1915",
                        "name": "RewardBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "指令跟随",
                                "en": "Instruct"
                            },
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/allenai/reward-bench",
                        "paperLink": "https://arxiv.org/pdf/2506.01937",
                        "officialWebsiteLink": "https://huggingface.co/datasets/allenai/reward-bench-2",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "64",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-01 09:59:19",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-01 09:59:19",
                        "createDate": "2025-07-01 09:46:35",
                        "desc": {
                            "cn": "RewardBench是一个用于评估奖励模型（包括通过直接偏好优化（DPO）训练的模型）能力和安全性的基准。",
                            "en": "RewardBench is a benchmark designed to evaluate the capabilities and safety of reward models (including those trained with Direct Preference Optimization, DPO). "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/rewardbench'. Error: Path opencompass/rewardbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1975",
                    "name": "CausalVQA",
                    "version": "1.0.0",
                    "description": "CausalVQA tests causal reasoning in videos across five question types, and state-of-the-art multimodal models still trail human performance.",
                    "url": "opencompass/opencompass_1975.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1975",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1975",
                        "name": "CausalVQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/facebookresearch/CausalVQA",
                        "paperLink": "https://arxiv.org/abs/2506.09943",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "64",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-25 15:30:44",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-25 15:30:44",
                        "createDate": "2025-06-25 10:25:30",
                        "desc": {
                            "cn": "CausalVQA 是面向视频问答的因果推理基准，涵盖反事实、假设、预判、规划、描述五类问题，强调真实物理场景。",
                            "en": "CausalVQA tests causal reasoning in videos across five question types, and state-of-the-art multimodal models still trail human performance."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/causalvqa'. Error: Path opencompass/causalvqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1737",
                    "name": "FortisAVQA",
                    "version": "1.0.0",
                    "description": " FortisAVQA is the first dataset designed to assess the robustness of AVQA models. ",
                    "url": "opencompass/opencompass_1737.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1737",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1737",
                        "name": "FortisAVQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "AVQA",
                                "en": "AVQA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/reml-group/fortisavqa",
                        "paperLink": "https://arxiv.org/abs/2504.00487",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "64",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:14:21",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:14:21",
                        "createDate": "2025-04-11 15:55:15",
                        "desc": {
                            "cn": "FortisAVQA，这是首个用于评估AVQA模型鲁棒性的数据集。",
                            "en": " FortisAVQA is the first dataset designed to assess the robustness of AVQA models. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/fortisavqa'. Error: Path opencompass/fortisavqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1781",
                    "name": "MIEB",
                    "version": "1.0.0",
                    "description": "Massive Image Embedding Benchmark (MIEB) is to evaluate the performance of image and image-text embedding models across the broadest spectrum to date. ",
                    "url": "opencompass/opencompass_1781.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1781",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1781",
                        "name": "MIEB",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/embeddings-benchmark/mteb",
                        "paperLink": "https://arxiv.org/abs/2504.10471",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "64",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-25 17:00:52",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-25 17:00:52",
                        "createDate": "2025-04-25 16:52:48",
                        "desc": {
                            "cn": "MIEB用于评估图像和图像文本嵌入模型在迄今为止最广泛的范围内的性能。",
                            "en": "Massive Image Embedding Benchmark (MIEB) is to evaluate the performance of image and image-text embedding models across the broadest spectrum to date. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mieb'. Error: Path opencompass/mieb is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2141",
                    "name": "LEXam",
                    "version": "1.0.0",
                    "description": "LEXam is a novel benchmark derived from 340 law exams spanning 116 law school courses across a range of subjects and degree levels. The dataset comprises 4,886 law exam questions in English and German, including 2,841 long-form, open-ended questions and 2,045 multiple-choice questions.",
                    "url": "opencompass/opencompass_2141.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2141",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Examination",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2141",
                        "name": "LEXam",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "学科",
                                "en": "Examination"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "长文本",
                                "en": "长文本"
                            },
                            {
                                "cn": "推理",
                                "en": "推理"
                            },
                            {
                                "cn": "法律",
                                "en": "法律"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/LEXam-Benchmark/LEXam",
                        "paperLink": "https://arxiv.org/abs/2505.12864",
                        "officialWebsiteLink": "https://lexam-benchmark.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "91903579",
                            "name": "LEXam",
                            "avatar": null,
                            "nickname": "LEXam"
                        },
                        "lookNum": "62",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-10-15 16:28:11",
                        "supportOnlineEval": false,
                        "updateDate": "2025-10-15 16:28:11",
                        "createDate": "2025-10-13 02:40:42",
                        "desc": {
                            "cn": "LEXam是一项全新的基准，来源于340份法学院考试，涵盖了116门不同学科和学位层级的课程。该数据集包含4,886道英德双语的法学考试题目，其中包括2,841道长篇开放性问题和2,045道选择题。除参考答案外，开放性问题还配有明确的指导，说明了预期的法律推理方式，例如问题识别、规则回忆或规则适用。我们在开放性问题和选择题上的评估表明，当前的LLM在这一任务上面临重大挑战，尤其是在需要结构化、多步骤法律推理的开放性问题上表现不佳。此外，我们的结果强调了该数据集在区分不同能力模型方面的有效性。",
                            "en": "LEXam is a novel benchmark derived from 340 law exams spanning 116 law school courses across a range of subjects and degree levels. The dataset comprises 4,886 law exam questions in English and German, including 2,841 long-form, open-ended questions and 2,045 multiple-choice questions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/lexam'. Error: Path opencompass/lexam is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1917",
                    "name": "Orak",
                    "version": "1.0.0",
                    "description": "Orak (오락) is a foundational benchmark for evaluating Large Language Model (LLM) agents in diverse popular video games. ",
                    "url": "opencompass/opencompass_1917.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1917",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1917",
                        "name": "Orak",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/krafton-ai/Orak",
                        "paperLink": "https://arxiv.org/abs/2506.03610",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "62",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-16 09:52:11",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-16 09:52:11",
                        "createDate": "2025-06-10 14:48:51",
                        "desc": {
                            "cn": "Orak是一个基础性的基准,用于评估在各种流行视频游戏中的大型语言模型(LLM)代理。",
                            "en": "Orak (오락) is a foundational benchmark for evaluating Large Language Model (LLM) agents in diverse popular video games. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/orak'. Error: Path opencompass/orak is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1704",
                    "name": "Mono2Stereo",
                    "version": "1.0.0",
                    "description": "For evaluating stereo image conversion, it provides test data for five different scenarios: animation, indoor, outdoor, complex, and simple, with a total of approximately 2,500 test sample pairs. It also offers evaluation metrics for assessing the stereo effect.",
                    "url": "opencompass/opencompass_1704.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1704",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1704",
                        "name": "Mono2Stereo",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Stereo Conversion",
                                "en": "Stereo Conversion"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/song2yu/Mono2Stereo",
                        "paperLink": "https://arxiv.org/abs/2503.22262",
                        "officialWebsiteLink": "https://mono2stereo-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "52300297",
                            "name": null,
                            "avatar": null,
                            "nickname": "迷藏"
                        },
                        "lookNum": "62",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-07 10:30:36",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-07 10:30:36",
                        "createDate": "2025-04-02 17:37:32",
                        "desc": {
                            "cn": "用于测评立体影像转换，提供了动画，室内，室外，复杂，简单共五种场景的测试数据，总共约2500对测试样本。并提供了用于测评立体效果的评价指标-立体交并比。",
                            "en": "For evaluating stereo image conversion, it provides test data for five different scenarios: animation, indoor, outdoor, complex, and simple, with a total of approximately 2,500 test sample pairs. It also offers evaluation metrics for assessing the stereo effect."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mono2stereo'. Error: Path opencompass/mono2stereo is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1844",
                    "name": "MIRACL-VISION",
                    "version": "1.0.0",
                    "description": "MIRACL-VISION is a large-scale, multilingual visual document retrieval benchmark built by the NVIDIA team, extending the popular MIRACL multilingual text retrieval benchmark. It covers 18 languages and contains 211 original questions.",
                    "url": "opencompass/opencompass_1844.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1844",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1844",
                        "name": "MIRACL-VISION",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            },
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "Strong Reasoning",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/pdf/2505.11651",
                        "officialWebsiteLink": "https://huggingface.co/datasets/nvidia/miracl-vision",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "61",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-03 10:50:34",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-03 10:50:34",
                        "createDate": "2025-05-27 10:50:53",
                        "desc": {
                            "cn": "MIRACL-VISION是一个大规模的多语言视觉文档检索基准测试，由NVIDIA团队构建，扩展了流行的MIRACL多语言文本检索基准。该数据集覆盖18种语言，包含211个原创问题，涵盖边界层分析、WKB方法、非线性偏微分方程的渐近解和振荡积分的渐近性等核心主题。",
                            "en": "MIRACL-VISION is a large-scale, multilingual visual document retrieval benchmark built by the NVIDIA team, extending the popular MIRACL multilingual text retrieval benchmark. It covers 18 languages and contains 211 original questions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/miracl_vision'. Error: Path opencompass/miracl_vision is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1682",
                    "name": "MotionBench",
                    "version": "1.0.0",
                    "description": "MotionBench, is a comprehensive evaluation benchmark designed to assess the fine-grained motion comprehension of video understanding models.",
                    "url": "opencompass/opencompass_1682.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1682",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1682",
                        "name": "MotionBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/THUDM/MotionBench",
                        "paperLink": "https://arxiv.org/abs/2501.02955",
                        "officialWebsiteLink": "https://motion-bench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "61",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-03-26 15:56:14",
                        "supportOnlineEval": false,
                        "updateDate": "2025-03-26 15:56:14",
                        "createDate": "2025-03-26 15:35:43",
                        "desc": {
                            "cn": "MotionBench是一个综合性的评估基准，旨在评估视频理解模型的细粒度运动理解能力。",
                            "en": "MotionBench, is a comprehensive evaluation benchmark designed to assess the fine-grained motion comprehension of video understanding models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/motionbench'. Error: Path opencompass/motionbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1706",
                    "name": "KOFFVQA",
                    "version": "1.0.0",
                    "description": "KOFFVQA is a carefully crafted free-form visual question answering(VQA) benchmark in the Korean language consisting of 275 questions across 10 different tasks. ",
                    "url": "opencompass/opencompass_1706.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1706",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1706",
                        "name": "KOFFVQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/maum-ai/KOFFVQA",
                        "paperLink": "https://arxiv.org/abs/2503.23730",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "60",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-03 19:47:01",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-03 19:47:01",
                        "createDate": "2025-04-03 14:31:30",
                        "desc": {
                            "cn": "KOFFVQA是一个精心设计的韩语自由形式视觉问答（VQA）基准测试，包含10个不同任务中的275个问题。",
                            "en": "KOFFVQA is a carefully crafted free-form visual question answering(VQA) benchmark in the Korean language consisting of 275 questions across 10 different tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/koffvqa'. Error: Path opencompass/koffvqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1986",
                    "name": "SEC-bench",
                    "version": "1.0.0",
                    "description": "SEC-bench is a benchmark designed to evaluate large language model (LLM) agents on real-world software security tasks.",
                    "url": "opencompass/opencompass_1986.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1986",
                    "sample_count": 1000,
                    "traits": [
                        "Safety",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1986",
                        "name": "SEC-bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "安全",
                                "en": "Safety"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SEC-bench/SEC-bench",
                        "paperLink": "https://arxiv.org/abs/2506.11791",
                        "officialWebsiteLink": "https://huggingface.co/datasets/SEC-bench/SEC-bench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "60",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-27 18:13:24",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-27 18:13:24",
                        "createDate": "2025-06-26 14:35:04",
                        "desc": {
                            "cn": "SEC-bench 是一个面向大型语言模型（LLM）智能体的软件安全任务评测基准，旨在自动化评估模型在真实漏洞环境中的能力。 该基准通过多智能体框架自动构建代码仓库、复现漏洞并生成修复补丁，涵盖漏洞验证（PoC 生成）和补丁修复两个关键任务，包含数百个真实安全案例。",
                            "en": "SEC-bench is a benchmark designed to evaluate large language model (LLM) agents on real-world software security tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/sec_bench'. Error: Path opencompass/sec_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1732",
                    "name": "SCAM",
                    "version": "1.0.0",
                    "description": "SCAM is the largest and most diverse dataset of real-world typographic attack images to date, containing 1,162 images across hundreds of object categories and attack words.",
                    "url": "opencompass/opencompass_1732.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1732",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1732",
                        "name": "SCAM",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Bliss-e-V/SCAM",
                        "paperLink": "https://arxiv.org/abs/2504.04893",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "60",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:12:53",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:12:53",
                        "createDate": "2025-04-11 14:29:41",
                        "desc": {
                            "cn": "SCAM，是迄今为止规模最大、多样性最丰富的真实世界排版攻击图像数据集，包含数百个对象类别和攻击词汇的1,162张图像。",
                            "en": "SCAM is the largest and most diverse dataset of real-world typographic attack images to date, containing 1,162 images across hundreds of object categories and attack words."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/scam'. Error: Path opencompass/scam is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1907",
                    "name": "CFinBench",
                    "version": "1.0.0",
                    "description": "We present CFinBench: a meticulously crafted, the most comprehensive evaluation benchmark to date, for assessing the financial knowledge of LLMs under Chinese context. ",
                    "url": "opencompass/opencompass_1907.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1907",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1907",
                        "name": "CFinBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "NAACL 2025",
                                "en": "NAACL 2025"
                            },
                            {
                                "cn": "金融",
                                "en": "金融"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://cfinbench.github.io/",
                        "paperLink": "https://aclanthology.org/2025.naacl-long.40.pdf",
                        "officialWebsiteLink": "https://cfinbench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "52307116",
                            "name": null,
                            "avatar": null,
                            "nickname": "Bball"
                        },
                        "lookNum": "59",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-09 11:30:13",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-09 11:30:13",
                        "createDate": "2025-06-06 17:13:27",
                        "desc": {
                            "cn": "为了更加全面地探究大语言模型在中文财经领域的能力，本工作提出了目前为止量级最大的中文财经评测基准（CFinBench）。该数据集共包含99,100个评测样本，并包含单选题、多选题和判断题在内的三种题型。该工作对当前主流的大模型从四个维度进行了详细评测：财经学科基础、财经资格认证、财经从业实践、财经法律法规。数据集和测评代码均已开源。",
                            "en": "We present CFinBench: a meticulously crafted, the most comprehensive evaluation benchmark to date, for assessing the financial knowledge of LLMs under Chinese context. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cfinbench'. Error: Path opencompass/cfinbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1707",
                    "name": "RXRX3-CORE",
                    "version": "1.0.0",
                    "description": "RxRx3-core dataset is a challenge dataset in phenomics optimized for the research\ncommunity. ",
                    "url": "opencompass/opencompass_1707.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1707",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1707",
                        "name": "RXRX3-CORE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Xavi3398/fer_benchmark",
                        "paperLink": "https://arxiv.org/abs/2503.20428",
                        "officialWebsiteLink": "https://www.rxrx.ai/rxrx3-core",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "59",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-03 19:47:14",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-03 19:47:14",
                        "createDate": "2025-04-03 15:13:31",
                        "desc": {
                            "cn": "RxRx3-core数据集是Recursion为研究社区优化的表型组学挑战数据集。",
                            "en": "RxRx3-core dataset is a challenge dataset in phenomics optimized for the research\ncommunity. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/rxrx3_core'. Error: Path opencompass/rxrx3_core is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1734",
                    "name": "Thai_local_benchmark",
                    "version": "1.0.0",
                    "description": "Thai local dialect benchmark covers Northern (Lanna), Northeastern (Isan), and Southern (Dambro) Thai, evaluating LLMs on five NLP tasks: summarization, question answering, translation, conversation, and food-related tasks.",
                    "url": "opencompass/opencompass_1734.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1734",
                    "sample_count": 1000,
                    "traits": [
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1734",
                        "name": "Thai_local_benchmark",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/mrpeerat/Thai_local_benchmark",
                        "paperLink": "https://arxiv.org/abs/2504.05898",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "59",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:13:26",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:13:26",
                        "createDate": "2025-04-11 14:57:01",
                        "desc": {
                            "cn": "这是一个涵盖泰国北部（兰纳）、东北部（伊森）和南部（丹布罗）方言的泰国地方方言基准测试，评估大型语言模型在五项自然语言处理任务上的表现：总结、问答、翻译、对话以及与食物相关的任务。",
                            "en": "Thai local dialect benchmark covers Northern (Lanna), Northeastern (Isan), and Southern (Dambro) Thai, evaluating LLMs on five NLP tasks: summarization, question answering, translation, conversation, and food-related tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/thai_local_benchmark'. Error: Path opencompass/thai_local_benchmark is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1787",
                    "name": "LiveLongBench",
                    "version": "1.0.0",
                    "description": "LiveLongBench is the first spoken long-text benchmark designed to address the challenges of long-context understanding in real-world dialogues, characterized by speech-specific features, high redundancy, and uneven information density. Existing benchmarks fail to capture these complexities, limiting",
                    "url": "opencompass/opencompass_1787.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1787",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1787",
                        "name": "LiveLongBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Spoken text",
                                "en": "Spoken text"
                            },
                            {
                                "cn": "Long context",
                                "en": "Long context"
                            },
                            {
                                "cn": "Live streams",
                                "en": "Live streams"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Yarayx/livelongbench",
                        "paperLink": "https://arxiv.org/abs/2504.17366",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "10207810",
                            "name": null,
                            "avatar": null,
                            "nickname": "Yarayx"
                        },
                        "lookNum": "59",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-05-06 11:02:03",
                        "supportOnlineEval": false,
                        "updateDate": "2025-05-06 11:02:03",
                        "createDate": "2025-04-28 16:54:00",
                        "desc": {
                            "cn": "LiveLongBench 是首个面向口语长文本理解的基准测试，基于直播内容构建，涵盖检索类、推理类及混合类三种任务类型，针对现实对话中存在的语音特性、高冗余性和信息密度不均等挑战。",
                            "en": "LiveLongBench is the first spoken long-text benchmark designed to address the challenges of long-context understanding in real-world dialogues, characterized by speech-specific features, high redundancy, and uneven information density. Existing benchmarks fail to capture these complexities, limiting"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/livelongbench'. Error: Path opencompass/livelongbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1918",
                    "name": "RDB2G-Bench",
                    "version": "1.0.0",
                    "description": "RDB2G-Bench provides comprehensive performance evaluation data for graph neural network models applied to relational database tasks. The dataset contains extensive experiments across multiple graph configurations and architectures.",
                    "url": "opencompass/opencompass_1918.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1918",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1918",
                        "name": "RDB2G-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/chlehdwon/RDB2G-Bench?tab=readme-ov-file",
                        "paperLink": "https://arxiv.org/abs/2506.01360",
                        "officialWebsiteLink": "https://huggingface.co/datasets/kaistdata/RDB2G-Bench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "58",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-30 15:32:24",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-30 15:32:24",
                        "createDate": "2025-06-30 15:06:09",
                        "desc": {
                            "cn": "RDB2G-Bench提供了针对关系数据库任务应用的图神经网络模型的全面性能评估数据。该数据集包含了跨多种图配置和架构的广泛实验。",
                            "en": "RDB2G-Bench provides comprehensive performance evaluation data for graph neural network models applied to relational database tasks. The dataset contains extensive experiments across multiple graph configurations and architectures."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/rdb2g_bench'. Error: Path opencompass/rdb2g_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1954",
                    "name": "ClimateViz",
                    "version": "1.0.0",
                    "description": "ClimateViz is a large-scale multimodal benchmark designed to evaluate the scientific fact-checking and statistical reasoning capabilities of large language and vision-language models. It focuses on real-world climate science data, with over 49,000 high quality natural language claims.",
                    "url": "opencompass/opencompass_1954.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1954",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1954",
                        "name": "ClimateViz",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal Reasoning ",
                                "en": "Multimodal Reasoning "
                            },
                            {
                                "cn": "Fact-Checking",
                                "en": "Fact-Checking"
                            },
                            {
                                "cn": "Charts",
                                "en": "Charts"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Albasu120491/ClimateViz",
                        "paperLink": "https://arxiv.org/abs/2506.08700",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "90307327",
                            "name": "Alba",
                            "avatar": null,
                            "nickname": "Alba"
                        },
                        "lookNum": "58",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-20 15:05:24",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-20 15:05:24",
                        "createDate": "2025-06-18 17:56:23",
                        "desc": {
                            "cn": "ClimateViz 是一个多模态基准数据集，用于评估大模型在气候科学图表上的事实核查与统计推理能力。数据来源于 NOAA、英国气象局等权威机构，共包含约 2,800 张科学图表与近 5 万条主张，标注为支持（support）、反驳（refute）或信息不足（NEI）。\n\n该数据集支持三种输入格式：图表+主张、表格+主张、图表标题+表格+主张，涵盖趋势识别、时空推理与科学对比等核心任务。ClimateViz 适用于多模态大模型和语言模型的系统性评估。\n图表转表格 + 图表标题 + 主张（Caption + Table",
                            "en": "ClimateViz is a large-scale multimodal benchmark designed to evaluate the scientific fact-checking and statistical reasoning capabilities of large language and vision-language models. It focuses on real-world climate science data, with over 49,000 high quality natural language claims."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/climateviz'. Error: Path opencompass/climateviz is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2078",
                    "name": "AutoAdvExBench",
                    "version": "1.0.0",
                    "description": "AutoAdvExBench is a benchmark designed to evaluate large language models' (LLMs) ability to autonomously exploit adversarial example defenses, directly measuring LLMs' success on tasks regularly performed by machine learning security experts.",
                    "url": "opencompass/opencompass_2078.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2078",
                    "sample_count": 1000,
                    "traits": [
                        "Safety",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2078",
                        "name": "AutoAdvExBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "安全",
                                "en": "Safety"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ethz-spylab/AutoAdvExBench",
                        "paperLink": "https://icml.cc/virtual/2025/poster/45896",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "57",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:10",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:10",
                        "createDate": "2025-07-22 16:51:34",
                        "desc": {
                            "cn": "AutoAdvExBench 是一个评估大型语言模型（LLMs）自主利用对抗性样本防御能力的基准，直接衡量LLMs在机器学习安全专家任务上的成功率。它主要评估模型理解学术论文、代码实现及生成对抗性攻击的能力。测试集包含75个对抗性样本防御实现。",
                            "en": "AutoAdvExBench is a benchmark designed to evaluate large language models' (LLMs) ability to autonomously exploit adversarial example defenses, directly measuring LLMs' success on tasks regularly performed by machine learning security experts."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/autoadvexbench'. Error: Path opencompass/autoadvexbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1840",
                    "name": "IQBench",
                    "version": "1.0.0",
                    "description": "IQBench is a novel benchmark designed to evaluate the fluid intelligence of VisionLanguage Models (VLMs) using standardized visual IQ tests. It consists of 500 manually collected and annotated visual IQ questions covering various domains.",
                    "url": "opencompass/opencompass_1840.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1840",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1840",
                        "name": "IQBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            },
                            {
                                "cn": "Strong Reasoning",
                                "en": "Strong Reasoning"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://anonymous.4open.science/r/IQBench_anonymous-3515/README.md",
                        "paperLink": "https://arxiv.org/pdf/2505.12000",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "57",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:46",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:46",
                        "createDate": "2025-06-04 11:15:30",
                        "desc": {
                            "cn": "IQBench是一个新基准测试，旨在通过标准化视觉智商测试评估视觉语言模型（VLMs）的流体智力。该基准包含500个手动收集和注释的视觉智商问题，涵盖模式识别、类比推理、视觉算术、空间理解等多个领域。与以往仅关注最终答案准确性的基准不同，IQBench强调对模型推理能力的评估，采用双重评估框架：准确性评分和推理评分。实验表明，即使是性能最高的模型（如o4mini、gemini2.5flash和claude3.7sonnet），在3D空间和字母重组任务上也表现出明显不足，凸显了当前VLMs在通用推理能力上的局限性。IQBench为开发更透明、更具认知能力的多模态系统奠定了基础。",
                            "en": "IQBench is a novel benchmark designed to evaluate the fluid intelligence of VisionLanguage Models (VLMs) using standardized visual IQ tests. It consists of 500 manually collected and annotated visual IQ questions covering various domains."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/iqbench'. Error: Path opencompass/iqbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1731",
                    "name": "DOVE",
                    "version": "1.0.0",
                    "description": "DOVE (Dataset Of Variation Evaluation) is a large-scale dataset containing prompt perturbations of various evaluation benchmarks.",
                    "url": "opencompass/opencompass_1731.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1731",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1731",
                        "name": "DOVE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "LLM敏感性评估",
                                "en": "LLM敏感性评估"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SLAB-NLP/DOVE",
                        "paperLink": "https://arxiv.org/abs/2503.01622",
                        "officialWebsiteLink": "https://slab-nlp.github.io/DOVE/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "57",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-11 17:12:41",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-11 17:12:41",
                        "createDate": "2025-04-11 14:22:36",
                        "desc": {
                            "cn": "DOVE（变异评估数据集），这是一个大规模数据集，包含了各种评估基准的提示扰动。",
                            "en": "DOVE (Dataset Of Variation Evaluation) is a large-scale dataset containing prompt perturbations of various evaluation benchmarks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/dove'. Error: Path opencompass/dove is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1753",
                    "name": "AgMMU",
                    "version": "1.0.0",
                    "description": "A Comprehensive Agricultural Multimodal Understanding and Reasoning Benchmark",
                    "url": "opencompass/opencompass_1753.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1753",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1753",
                        "name": "AgMMU",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "农业",
                                "en": "农业"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AgMMU/AgMMU",
                        "paperLink": "https://arxiv.org/abs/2504.10568",
                        "officialWebsiteLink": "https://agmmu.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "57",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-21 12:34:40",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-21 12:34:40",
                        "createDate": "2025-04-21 12:12:24",
                        "desc": {
                            "cn": "农业综合多模态理解和推理基准。",
                            "en": "A Comprehensive Agricultural Multimodal Understanding and Reasoning Benchmark"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/agmmu'. Error: Path opencompass/agmmu is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1834",
                    "name": "CLEVER",
                    "version": "1.0.0",
                    "description": "CLEVER is a benchmark suite for end-to-end code generation and formal verification in Lean 4, adapted from the HumanEval dataset. It requires models to generate implementations, formal specifications, and proofs—all verifiable by Lean's type checker, moving beyond test-case-driven evaluation.",
                    "url": "opencompass/opencompass_1834.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1834",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1834",
                        "name": "CLEVER",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/trishullab/clever",
                        "paperLink": "https://arxiv.org/abs/2505.13938",
                        "officialWebsiteLink": "https://huggingface.co/datasets/amitayusht/clever",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "56",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:57",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:57",
                        "createDate": "2025-06-04 11:17:08",
                        "desc": {
                            "cn": "CLEVER is a benchmark suite for end-to-end code generation and formal verification in Lean 4, adapted from the HumanEval dataset. It requires models to generate implementations, formal specifications, and proofs—all verifiable by Lean's type checker, moving beyond test-case-driven evaluation.",
                            "en": "CLEVER is a benchmark suite for end-to-end code generation and formal verification in Lean 4, adapted from the HumanEval dataset. It requires models to generate implementations, formal specifications, and proofs—all verifiable by Lean's type checker, moving beyond test-case-driven evaluation."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/clever'. Error: Path opencompass/clever is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2025",
                    "name": "TableEval",
                    "version": "1.0.0",
                    "description": "TableEval is the first cross-lingual benchmark for tabular question answering, supporting Simplified Chinese, Traditional Chinese, and English. It is designed to evaluate model performance on real-world, complex table understanding tasks across multiple languages.",
                    "url": "opencompass/opencompass_2025.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2025",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2025",
                        "name": "TableEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "表格问答",
                                "en": "表格问答"
                            },
                            {
                                "cn": "跨语言",
                                "en": "跨语言"
                            },
                            {
                                "cn": "真实世界数据",
                                "en": "真实世界数据"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/wenge-research/TableEval",
                        "paperLink": "https://arxiv.org/abs/2506.03949",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "085564",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-QmRrrAZeT"
                        },
                        "lookNum": "56",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-11 10:29:43",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-11 10:29:43",
                        "createDate": "2025-07-04 16:32:40",
                        "desc": {
                            "cn": "TableEval 是首个支持简体中文、繁体中文和英文的跨语言表格问答基准数据集，旨在系统评估大模型在真实复杂表格理解任务中的表现。",
                            "en": "TableEval is the first cross-lingual benchmark for tabular question answering, supporting Simplified Chinese, Traditional Chinese, and English. It is designed to evaluate model performance on real-world, complex table understanding tasks across multiple languages."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/tableeval'. Error: Path opencompass/tableeval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1777",
                    "name": "ColorBench",
                    "version": "1.0.0",
                    "description": "ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. ",
                    "url": "opencompass/opencompass_1777.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1777",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1777",
                        "name": "ColorBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/tianyi-lab/ColorBench",
                        "paperLink": "https://arxiv.org/abs/2504.10514",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "55",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-25 17:00:41",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-25 17:00:41",
                        "createDate": "2025-04-25 16:14:54",
                        "desc": {
                            "cn": "ColorBench，这是一个创新且精心设计的基准测试，旨在评估VLMs在颜色理解方面的能力，包括颜色感知、推理和鲁棒性。",
                            "en": "ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/colorbench'. Error: Path opencompass/colorbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1833",
                    "name": "CSTS",
                    "version": "1.0.0",
                    "description": "CSTS is a synthetic benchmark for evaluating correlation structure discovery in multivariate time series. It features 23 distinct correlation structures with systematic data variations (distribution shifts, sparsification, downsampling) and provides ground truth labels for validation.",
                    "url": "opencompass/opencompass_1833.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1833",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1833",
                        "name": "CSTS",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/isabelladegen/corrclust-validation",
                        "paperLink": "https://arxiv.org/abs/2505.14596",
                        "officialWebsiteLink": "https://huggingface.co/datasets/idegen/csts",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "54",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-04 11:18:27",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-04 11:18:27",
                        "createDate": "2025-06-04 11:16:54",
                        "desc": {
                            "cn": "CSTS is a synthetic benchmark for evaluating correlation structure discovery in multivariate time series. It features 23 distinct correlation structures with systematic data variations (distribution shifts, sparsification, downsampling) and provides ground truth labels for validation.",
                            "en": "CSTS is a synthetic benchmark for evaluating correlation structure discovery in multivariate time series. It features 23 distinct correlation structures with systematic data variations (distribution shifts, sparsification, downsampling) and provides ground truth labels for validation."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/csts'. Error: Path opencompass/csts is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2027",
                    "name": "GroundingSuite",
                    "version": "1.0.0",
                    "description": "GroundingSuite is designed to test the localization capabilities of multimodal models. It created 3,720 pixel-level data entries based on COCO Unlabeled images. This dataset covers four dimensions: Stuff Class Object, Multi-Object, Part-Level Object, and Single Object. ",
                    "url": "opencompass/opencompass_2027.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2027",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2027",
                        "name": "GroundingSuite",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/hustvl/GroundingSuite",
                        "paperLink": "https://arxiv.org/abs/2503.10596",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "10207026",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-sBd5aPOD1"
                        },
                        "lookNum": "54",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-07 11:45:13",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-07 11:45:13",
                        "createDate": "2025-07-05 17:59:14",
                        "desc": {
                            "cn": "GroundingSuite 用来测试多模态模型的定位能力。它通过半自动标注和人工筛选在COCO Unlabel的图片基础上创建了3720条pixel-level的数据，覆盖Stuff Class Object, Multi Object, Part Level Object, Single Object四个维度，是一个全面评测多模态模型定位能力的测试集。",
                            "en": "GroundingSuite is designed to test the localization capabilities of multimodal models. It created 3,720 pixel-level data entries based on COCO Unlabeled images. This dataset covers four dimensions: Stuff Class Object, Multi-Object, Part-Level Object, and Single Object. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/groundingsuite'. Error: Path opencompass/groundingsuite is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2047",
                    "name": "LLMThinkBench",
                    "version": "1.0.0",
                    "description": "LLMThinkBench is a benchmark framework designed to evaluate large language models (LLMs) on basic math reasoning and “overthinking” behaviors, targeting code-executing language models.",
                    "url": "opencompass/opencompass_2047.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2047",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning",
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2047",
                        "name": "LLMThinkBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ctrl-gaurav/LLMThinkBench",
                        "paperLink": "https://arxiv.org/abs/2507.04023",
                        "officialWebsiteLink": "https://ctrl-gaurav.github.io/llmthinkbench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "54",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-14 09:39:28",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-14 09:39:28",
                        "createDate": "2025-07-11 14:17:58",
                        "desc": {
                            "cn": "LLMThinkBench 是一个用于评估大语言模型（LLM）在基础数学推理和“过度思考”行为方面的基准框架，支持对具备代码执行能力的语言模型进行全面评估。",
                            "en": "LLMThinkBench is a benchmark framework designed to evaluate large language models (LLMs) on basic math reasoning and “overthinking” behaviors, targeting code-executing language models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/llmthinkbench'. Error: Path opencompass/llmthinkbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1874",
                    "name": "MedArabiQ",
                    "version": "1.0.0",
                    "description": "MedArabiQ introduces a new benchmark dataset consisting of seven Arabic medical tasks, covering multiple specialties and question formats:\n✅ Multiple-choice questions\n✏️ Fill-in-the-blank (with and without choices)\n💬 Patient-doctor question answering",
                    "url": "opencompass/opencompass_1874.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1874",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1874",
                        "name": "MedArabiQ",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Medical",
                                "en": "Medical"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/nyuad-cai/MedArabiQ",
                        "paperLink": "https://arxiv.org/pdf/2505.03427",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "53",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-03 17:23:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-03 17:23:33",
                        "createDate": "2025-06-03 17:18:16",
                        "desc": {
                            "cn": "大型语言模型（LLMs）在医疗保健应用中显示出了显著的前景，但由于缺乏高质量的领域特定数据集，它们在阿拉伯语医学领域的表现在很大程度上仍未得到探索。MedArabiQ引入了一个新的基准数据集，包含七个阿拉伯语医学任务，涵盖多个专业和问题格式：\n✅ 多项选择题\n✏️ 填空题（有选项和无选项）\n💬 患者-医生问答\n\n该数据集使用过往医学考试和公开可用资源构建，并进行了修改以评估LLMs在各种能力方面的表现，包括偏见缓解。\n",
                            "en": "MedArabiQ introduces a new benchmark dataset consisting of seven Arabic medical tasks, covering multiple specialties and question formats:\n✅ Multiple-choice questions\n✏️ Fill-in-the-blank (with and without choices)\n💬 Patient-doctor question answering"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medarabiq'. Error: Path opencompass/medarabiq is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1991",
                    "name": "AssetOpsBench",
                    "version": "1.0.0",
                    "description": "AssetOpsBench is a comprehensive benchmark designed to evaluate the performance of large language models (LLMs) and AI agents in complex asset operation and maintenance tasks.",
                    "url": "opencompass/opencompass_1991.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1991",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1991",
                        "name": "AssetOpsBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/IBM/AssetOpsBench",
                        "paperLink": "https://arxiv.org/abs/2506.03828",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "53",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-27 18:12:23",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-27 18:12:23",
                        "createDate": "2025-06-27 14:46:33",
                        "desc": {
                            "cn": "AssetOpsBench 是一个专注于评估大语言模型（LLM）和智能体在资产运维领域复杂任务中实际表现的多维度评测基准。该基准旨在检验模型在工业场景下的任务规划、多步推理、工具调用、安全合规性以及领域知识理解等核心能力，覆盖设备维护、异常诊断、风险评估等典型运维场景。测试集包含 1,000 个高质量样本，涉及 5 大类任务和 20 余种细分领域，数据来源于真实运维手册、工单记录及专家验证案例。",
                            "en": "AssetOpsBench is a comprehensive benchmark designed to evaluate the performance of large language models (LLMs) and AI agents in complex asset operation and maintenance tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/assetopsbench'. Error: Path opencompass/assetopsbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1779",
                    "name": "MLRC-Bench",
                    "version": "1.0.0",
                    "description": "MLRC-Bench, a benchmark designed to quantify how effectively language agents can tackle challenging Machine Learning (ML) Research Competitions. ",
                    "url": "opencompass/opencompass_1779.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1779",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1779",
                        "name": "MLRC-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/yunx-z/MLRC-Bench",
                        "paperLink": "https://arxiv.org/abs/2504.09702",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "53",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-25 17:00:47",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-25 17:00:47",
                        "createDate": "2025-04-25 16:34:27",
                        "desc": {
                            "cn": " MLRC-Bench旨在量化大模型代理如何有效地应对具有挑战性的机器学习 （ML） 研究竞赛。",
                            "en": "MLRC-Bench, a benchmark designed to quantify how effectively language agents can tackle challenging Machine Learning (ML) Research Competitions. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mlrc_bench'. Error: Path opencompass/mlrc_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1754",
                    "name": "OpenTuringBench",
                    "version": "1.0.0",
                    "description": "OpenTuringBench, a new benchmark based on OLLMs, designed to train and evaluate machine-generated text detectors on the Turing Test and Authorship Attribution problems. ",
                    "url": "opencompass/opencompass_1754.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1754",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1754",
                        "name": "OpenTuringBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2504.11369",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "52",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-21 12:34:42",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-21 12:34:42",
                        "createDate": "2025-04-21 12:26:14",
                        "desc": {
                            "cn": "OpenTuringBench，这是一个基于OLLMs的新基准测试，旨在训练和评估机器生成文本检测器在图灵测试和作者归属问题上的性能。",
                            "en": "OpenTuringBench, a new benchmark based on OLLMs, designed to train and evaluate machine-generated text detectors on the Turing Test and Authorship Attribution problems. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/openturingbench'. Error: Path opencompass/openturingbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1846",
                    "name": "TGLG",
                    "version": "1.0.0",
                    "description": "Temporally-Grounded Language Generation (TGLG) is a benchmark for real-time vision-language models (VLMs) that focus on two key capabilities: perceptual updating and contingency awareness. ",
                    "url": "opencompass/opencompass_1846.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1846",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Language"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1846",
                        "name": "TGLG",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "语言",
                                "en": "Language"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Multimodal",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "Strong Reasoning",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/yukw777/tglg",
                        "paperLink": "https://arxiv.org/pdf/2505.11326",
                        "officialWebsiteLink": "https://huggingface.co/datasets/kpyu/tglg",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "51",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-03 10:02:19",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-03 10:02:19",
                        "createDate": "2025-05-27 11:15:19",
                        "desc": {
                            "cn": "基于时间的语言生成（TGLG）是实时视觉语言模型（VLM）的基准，侧重于两个关键功能：感知更新和应急意识。该存储库还包含TGLG的基线实时VLM代码，即具有时间同步交织的视觉语言模型（VLM-TSI）。",
                            "en": "Temporally-Grounded Language Generation (TGLG) is a benchmark for real-time vision-language models (VLMs) that focus on two key capabilities: perceptual updating and contingency awareness. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/tglg'. Error: Path opencompass/tglg is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1851",
                    "name": "MMLongBench",
                    "version": "1.0.0",
                    "description": "MMLongBench is a benchmark that evaluates long-context vision-language models across various tasks, image types, and input lengths, revealing that single-task performance is insufficient for gauging overall vision-language long-context capability.",
                    "url": "opencompass/opencompass_1851.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1851",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Knowledge",
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1851",
                        "name": "MMLongBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            },
                            {
                                "cn": "长文本",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "multimodal",
                                "en": "multimodal"
                            },
                            {
                                "cn": "Long-Context",
                                "en": "Long-Context"
                            },
                            {
                                "cn": "Vision-Language",
                                "en": "Vision-Language"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/EdinburghNLP/MMLongBench",
                        "paperLink": "https://arxiv.org/abs/2505.10610",
                        "officialWebsiteLink": "https://zhaowei-wang-nlp.github.io/MMLongBench-page/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "43305250",
                            "name": null,
                            "avatar": null,
                            "nickname": "zhaowei-wang-nlp"
                        },
                        "lookNum": "51",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-24 18:08:10",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-24 18:08:10",
                        "createDate": "2025-05-27 18:50:31",
                        "desc": {
                            "cn": "MMLongBench 是一个针对长上下文视觉-语言模型的基准，覆盖多种任务、图像类型和输入长度。评测结果表明，单一任务的表现不足以衡量模型的整体长上下文能力。",
                            "en": "MMLongBench is a benchmark that evaluates long-context vision-language models across various tasks, image types, and input lengths, revealing that single-task performance is insufficient for gauging overall vision-language long-context capability."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mmlongbench'. Error: Path opencompass/mmlongbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1783",
                    "name": "NPPC",
                    "version": "1.0.0",
                    "description": "Nondeterministic Polynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark for LLMs. ",
                    "url": "opencompass/opencompass_1783.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1783",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1783",
                        "name": "NPPC",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SMU-DIGA/nppc",
                        "paperLink": "https://arxiv.org/abs/2504.11239",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "51",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-28 11:36:56",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-28 11:36:56",
                        "createDate": "2025-04-28 11:16:50",
                        "desc": {
                            "cn": "非确定性多项式时间问题挑战 （NPPC），这是一个不断扩展的 LLM 推理基准。",
                            "en": "Nondeterministic Polynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark for LLMs. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/nppc'. Error: Path opencompass/nppc is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2039",
                    "name": "TransLaw",
                    "version": "1.0.0",
                    "description": "本文提出TransLaw——专为香港判例翻译设计的协同交互式多智能体框架。该框架创新性地将传统翻译流程拆解为翻译、错误标注及校对修正三大子任务，并分配三个智能体协同执行。为评估框架性能，我们构建了大规模双语基准数据集BJC Judgments，对13个开源与商业大语言模型（作为智能体）展开评测。实验结果验证了协同策略的有效性：在多智能体协作显著提升效果的同时，提供了具有参考价值的LLM性能横向对比。通过错误类型学分析，本研究进一步揭示了亟待解决的关键翻译挑战。未来工作将聚焦于优化智能体架构以应对这些挑战，同时开发更全面、低成本的评估基准。",
                    "url": "opencompass/opencompass_2039.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2039",
                    "sample_count": 1000,
                    "traits": [
                        "Language",
                        "Agent",
                        "Instruct"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2039",
                        "name": "TransLaw",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "语言",
                                "en": "Language"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            },
                            {
                                "cn": "指令跟随",
                                "en": "Instruct"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Legal AI",
                                "en": "Legal AI"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/pdf/2507.00875",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "18909912",
                            "name": null,
                            "avatar": null,
                            "nickname": "Oss"
                        },
                        "lookNum": "51",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-11 10:29:52",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-11 10:29:52",
                        "createDate": "2025-07-09 02:45:37",
                        "desc": {
                            "cn": "本文提出TransLaw——专为香港判例翻译设计的协同交互式多智能体框架。该框架创新性地将传统翻译流程拆解为翻译、错误标注及校对修正三大子任务，并分配三个智能体协同执行。为评估框架性能，我们构建了大规模双语基准数据集BJC Judgments，对13个开源与商业大语言模型（作为智能体）展开评测。实验结果验证了协同策略的有效性：在多智能体协作显著提升效果的同时，提供了具有参考价值的LLM性能横向对比。通过错误类型学分析，本研究进一步揭示了亟待解决的关键翻译挑战。未来工作将聚焦于优化智能体架构以应对这些挑战，同时开发更全面、低成本的评估基准。",
                            "en": "本文提出TransLaw——专为香港判例翻译设计的协同交互式多智能体框架。该框架创新性地将传统翻译流程拆解为翻译、错误标注及校对修正三大子任务，并分配三个智能体协同执行。为评估框架性能，我们构建了大规模双语基准数据集BJC Judgments，对13个开源与商业大语言模型（作为智能体）展开评测。实验结果验证了协同策略的有效性：在多智能体协作显著提升效果的同时，提供了具有参考价值的LLM性能横向对比。通过错误类型学分析，本研究进一步揭示了亟待解决的关键翻译挑战。未来工作将聚焦于优化智能体架构以应对这些挑战，同时开发更全面、低成本的评估基准。"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/translaw'. Error: Path opencompass/translaw is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2048",
                    "name": "RISEBench",
                    "version": "1.0.0",
                    "description": "RISEBench is a benchmark for evaluating large multimodal models (LMMs) on reasoning-informed visual editing tasks, targeting models with image understanding and generation capabilities. ",
                    "url": "opencompass/opencompass_2048.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2048",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning",
                        "Instruct"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2048",
                        "name": "RISEBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "指令跟随",
                                "en": "Instruct"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/PhoenixZ810/RISEBench",
                        "paperLink": "https://arxiv.org/pdf/2504.02826",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "50",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-11 14:28:48",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-11 14:28:48",
                        "createDate": "2025-07-11 14:25:32",
                        "desc": {
                            "cn": "RISEBench 是一个用于评估多模态大模型（LMMs）在推理驱动视觉编辑任务中能力的基准，面向具备图像理解与生成能力的模型。",
                            "en": "RISEBench is a benchmark for evaluating large multimodal models (LMMs) on reasoning-informed visual editing tasks, targeting models with image understanding and generation capabilities. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/risebench'. Error: Path opencompass/risebench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1926",
                    "name": "VideoMathQA",
                    "version": "1.0.0",
                    "description": "VideoMathQA is a benchmark designed to evaluate mathematical reasoning in real-world educational videos. It requires models to interpret and integrate information from three modalities, visuals, audio, and text, across time. ",
                    "url": "opencompass/opencompass_1926.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1926",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1926",
                        "name": "VideoMathQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/mbzuai-oryx/VideoMathQA",
                        "paperLink": "https://arxiv.org/abs/2506.05349",
                        "officialWebsiteLink": "https://mbzuai-oryx.github.io/VideoMathQA/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "张富才"
                        },
                        "lookNum": "50",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-16 09:50:53",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-16 09:50:53",
                        "createDate": "2025-06-12 11:57:53",
                        "desc": {
                            "cn": "VideoMathQA是一个旨在评估实际教育视频中数学推理能力的基准。它要求模型解释和整合来自三种模态(视觉、音频和文本)随时间变化的信息。该基准解决了\"多模态针堆\"问题,即关键信息稀疏且分散在视频的不同模态和时刻。",
                            "en": "VideoMathQA is a benchmark designed to evaluate mathematical reasoning in real-world educational videos. It requires models to interpret and integrate information from three modalities, visuals, audio, and text, across time. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/videomathqa'. Error: Path opencompass/videomathqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1985",
                    "name": "CVDP",
                    "version": "1.0.0",
                    "description": "CVDP is a next-generation benchmark for evaluating large language models (LLMs) and agents in hardware design and verification, comprising 783 problems across 13 task categories, including RTL generation, verification, debugging, specification alignment, and technical Q&A. ",
                    "url": "opencompass/opencompass_1985.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1985",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1985",
                        "name": "CVDP",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/NVlabs/cvdp_benchmark",
                        "paperLink": "https://arxiv.org/abs/2506.14074",
                        "officialWebsiteLink": "https://huggingface.co/datasets/nvidia/cvdp-benchmark-dataset",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "50",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-27 18:13:12",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-27 18:13:12",
                        "createDate": "2025-06-26 14:21:49",
                        "desc": {
                            "cn": "CVDP 是一个面向大型语言模型（LLM）和智能体的下一代硬件设计与验证评测基准，涵盖 13 类任务共 783 个问题，涉及 RTL 生成、验证、调试、规范对齐和技术问答等。",
                            "en": "CVDP is a next-generation benchmark for evaluating large language models (LLMs) and agents in hardware design and verification, comprising 783 problems across 13 task categories, including RTL generation, verification, debugging, specification alignment, and technical Q&A. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/cvdp'. Error: Path opencompass/cvdp is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1969",
                    "name": "ALE-Bench",
                    "version": "1.0.0",
                    "description": "ALE-Bench is a benchmark for evaluating AI systems on score-based algorithmic programming contests.",
                    "url": "opencompass/opencompass_1969.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1969",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1969",
                        "name": "ALE-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/SakanaAI/ALE-Bench",
                        "paperLink": "https://arxiv.org/abs/2506.09050",
                        "officialWebsiteLink": "https://huggingface.co/datasets/SakanaAI/ALE-Bench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "49",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-23 20:41:05",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-23 20:41:05",
                        "createDate": "2025-06-23 15:33:02",
                        "desc": {
                            "cn": "ALE-Bench 是一个用于评估 AI 系统在基于分数的算法编程竞赛中的基准测试。",
                            "en": "ALE-Bench is a benchmark for evaluating AI systems on score-based algorithmic programming contests."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/ale_bench'. Error: Path opencompass/ale_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1970",
                    "name": "EditInspector",
                    "version": "1.0.0",
                    "description": "EditInspector is a novel benchmark for evaluation of text-guided image edits, based on human annotations collected using an extensive template for edit verification. We leverage EditInspector to evaluate the performance of state-of-the-art (SoTA) vision and language models.",
                    "url": "opencompass/opencompass_1970.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1970",
                    "sample_count": 1000,
                    "traits": [
                        "Creation",
                        "Instruct"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1970",
                        "name": "EditInspector",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "创作",
                                "en": "Creation"
                            },
                            {
                                "cn": "指令跟随",
                                "en": "Instruct"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/editinspector/EditInspector",
                        "paperLink": "https://arxiv.org/abs/2506.09988",
                        "officialWebsiteLink": "https://editinspector.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "49",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-23 20:40:59",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-23 20:40:59",
                        "createDate": "2025-06-23 15:40:22",
                        "desc": {
                            "cn": "EditInspector 评估最先进（SoTA）视觉和语言模型在多个维度上评估编辑的性能，包括准确性、瑕疵检测、视觉质量、与图像场景的无缝融合、遵循常识以及描述编辑引起变化的能力。",
                            "en": "EditInspector is a novel benchmark for evaluation of text-guided image edits, based on human annotations collected using an extensive template for edit verification. We leverage EditInspector to evaluate the performance of state-of-the-art (SoTA) vision and language models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/editinspector'. Error: Path opencompass/editinspector is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1992",
                    "name": "HtFLlib",
                    "version": "1.0.0",
                    "description": "HtFLlib is a benchmark for heterogeneous federated learning that examines how 40 vision, NLP and sensor models and 10 algorithms collaborate under non-IID data. ",
                    "url": "opencompass/opencompass_1992.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1992",
                    "sample_count": 1000,
                    "traits": [
                        "Safety",
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1992",
                        "name": "HtFLlib",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "安全",
                                "en": "Safety"
                            },
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/TsingZ0/HtFLlib",
                        "paperLink": "https://arxiv.org/abs/2506.03954",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "49",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-27 18:12:30",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-27 18:12:30",
                        "createDate": "2025-06-27 14:50:47",
                        "desc": {
                            "cn": "HtFLlib 是一个面向异构联邦学习算法的综合评测基准，旨在衡量不同模型架构在非 IID 数据环境中的协同学习能力。评测对象覆盖图像、文本与传感信号三类模型，总计 40 个架构及 10 种代表性方法。",
                            "en": "HtFLlib is a benchmark for heterogeneous federated learning that examines how 40 vision, NLP and sensor models and 10 algorithms collaborate under non-IID data. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/htfllib'. Error: Path opencompass/htfllib is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1784",
                    "name": "xVerify",
                    "version": "1.0.0",
                    "description": "xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions",
                    "url": "opencompass/opencompass_1784.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1784",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1784",
                        "name": "xVerify",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/IAAR-Shanghai/xVerify",
                        "paperLink": "https://arxiv.org/abs/2504.10481",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "49",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-28 11:36:52",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-28 11:36:52",
                        "createDate": "2025-04-28 11:24:15",
                        "desc": {
                            "cn": "xVerify，这是一种用于推理模型评估的高效答案验证器。xVerify 在等价判断方面表现出强大的能力，使其能够有效地确定推理模型生成的答案是否等同于各种类型客观问题的参考答案。",
                            "en": "xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/xverify'. Error: Path opencompass/xverify is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1786",
                    "name": "HypoEval",
                    "version": "1.0.0",
                    "description": "HypoEval, Hypothesis-guided Evaluation framework, which first uses a small corpus of human evaluations to generate more detailed rubrics for human judgments and then incorporates a checklist-like approach to combine LLM's assigned scores on each decomposed dimension to acquire overall scores. ",
                    "url": "opencompass/opencompass_1786.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1786",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1786",
                        "name": "HypoEval",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ChicagoHAI/HypoEval",
                        "paperLink": "https://arxiv.org/abs/2504.07174",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "49",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-28 11:36:49",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-28 11:36:49",
                        "createDate": "2025-04-28 11:34:44",
                        "desc": {
                            "cn": "HypoEval，即假设指导的评估框架，该框架首先使用一小部分人工评估来生成更详细的人类判断量规，然后采用类似清单的方法，将 LLM 在每个分解维度上的分配分数结合起来，以获得总分。",
                            "en": "HypoEval, Hypothesis-guided Evaluation framework, which first uses a small corpus of human evaluations to generate more detailed rubrics for human judgments and then incorporates a checklist-like approach to combine LLM's assigned scores on each decomposed dimension to acquire overall scores. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/hypoeval'. Error: Path opencompass/hypoeval is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2077",
                    "name": "ITBench",
                    "version": "1.0.0",
                    "description": "ITBench is a benchmark designed to evaluate the performance of AI agents in real-world IT automation tasks. ",
                    "url": "opencompass/opencompass_2077.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2077",
                    "sample_count": 1000,
                    "traits": [
                        "Code",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2077",
                        "name": "ITBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/itbench-hub/ITBench",
                        "paperLink": "https://icml.cc/virtual/2025/poster/44303",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "48",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:04",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:04",
                        "createDate": "2025-07-22 16:51:52",
                        "desc": {
                            "cn": "ITBench 是一个旨在评估 AI 智能体在真实世界 IT 自动化任务中表现的基准。它涵盖了站点可靠性工程、合规与安全运营以及财务运营等关键维度，并包含 102 个真实场景。该基准提供了一个开源框架和多种基线智能体实现，并集成了CrewAI等工具，以促进AI驱动的IT自动化发展。",
                            "en": "ITBench is a benchmark designed to evaluate the performance of AI agents in real-world IT automation tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/itbench'. Error: Path opencompass/itbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2079",
                    "name": "MER-UniBench",
                    "version": "1.0.0",
                    "description": "MER-UniBench is a benchmark designed to evaluate multimodal large language models (MLLMs) for their emotion understanding capabilities across typical multimodal emotion recognition (MER) tasks. ",
                    "url": "opencompass/opencompass_2079.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2079",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2079",
                        "name": "MER-UniBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "MER",
                                "en": "MER"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/zeroQiaoba/AffectGPT",
                        "paperLink": "https://icml.cc/virtual/2025/poster/43565",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "47",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:16",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:16",
                        "createDate": "2025-07-22 16:51:18",
                        "desc": {
                            "cn": "MER-UniBench 是一个旨在评估多模态大语言模型（MLLM）在典型多模态情感识别（MER）任务中情感理解能力的评测基准。它涵盖了细粒度情感识别、基本情感识别和情感分析三个主要维度。该基准利用了包括 MER-Caption 在内的多个数据集，其中 MER-Caption 拥有超过 2000 种细粒度情感类别和 11.5 万个样本。",
                            "en": "MER-UniBench is a benchmark designed to evaluate multimodal large language models (MLLMs) for their emotion understanding capabilities across typical multimodal emotion recognition (MER) tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mer_unibench'. Error: Path opencompass/mer_unibench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2014",
                    "name": "UTBoost",
                    "version": "1.0.0",
                    "description": "UTBoost generates unit tests using LLMs to augment the test cases for certain instances in SWE-Bench, enabling a more rigorous use of SWE-Bench to evaluate the performance of Code Agents.",
                    "url": "opencompass/opencompass_2014.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2014",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2014",
                        "name": "UTBoost",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Code Agent",
                                "en": "Code Agent"
                            },
                            {
                                "cn": "SWE-Bench",
                                "en": "SWE-Bench"
                            },
                            {
                                "cn": "Automatic test augmentation",
                                "en": "Automatic test augmentation"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/CUHK-Shenzhen-SE/UTBoost",
                        "paperLink": "https://arxiv.org/abs/2506.09289",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "85501967",
                            "name": "BoxiYu",
                            "avatar": null,
                            "nickname": "行者"
                        },
                        "lookNum": "47",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-01 15:40:25",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-01 15:40:25",
                        "createDate": "2025-07-01 15:27:00",
                        "desc": {
                            "cn": "UTBoost通过LLM生成的单元测试，增强了SWE-Bench中一些instances的测试用例，能严谨的使用SWE-Bench来评估Code Agents的表现。",
                            "en": "UTBoost generates unit tests using LLMs to augment the test cases for certain instances in SWE-Bench, enabling a more rigorous use of SWE-Bench to evaluate the performance of Code Agents."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/utboost'. Error: Path opencompass/utboost is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1944",
                    "name": "LoopNav",
                    "version": "1.0.0",
                    "description": "A video-action dataset containing many loop-based navigation dataset in Minecraft environment, aiming to boost the spatial consistency and providing insight for the design of memory module",
                    "url": "opencompass/opencompass_1944.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1944",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Long-Context",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1944",
                        "name": "LoopNav",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "长文本",
                                "en": "Long-Context"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "navigation",
                                "en": "navigation"
                            },
                            {
                                "cn": "consistency",
                                "en": "consistency"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Kevin-lkw/LoopNav",
                        "paperLink": "https://arxiv.org/abs/2505.22976",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "29904400",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-no7Zd0NrI"
                        },
                        "lookNum": "46",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-20 15:14:30",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-20 15:14:30",
                        "createDate": "2025-06-17 09:30:27",
                        "desc": {
                            "cn": "一个视频-动作导航数据集，包括了在Minecraft环境下大量基于回环的导航数据，能够促进世界模型等视频模型空间一致性的训练，启发记忆模块的设计。",
                            "en": "A video-action dataset containing many loop-based navigation dataset in Minecraft environment, aiming to boost the spatial consistency and providing insight for the design of memory module"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/loopnav'. Error: Path opencompass/loopnav is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1980",
                    "name": "FalseReject",
                    "version": "1.0.0",
                    "description": "Safety alignment approaches in large language models (LLMs) often lead\nto the over-refusal of benign queries, significantly diminishing their utility\nin sensitive scenarios. To address this challenge, we introduce FalseReject,\na comprehensive resource containing 16k seemingly toxic queries accompani",
                    "url": "opencompass/opencompass_1980.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1980",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning",
                        "Safety"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1980",
                        "name": "FalseReject",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "安全",
                                "en": "Safety"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://false-reject.github.io/",
                        "paperLink": "https://arxiv.org/pdf/2505.08054",
                        "officialWebsiteLink": "https://false-reject.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "38705030",
                            "name": null,
                            "avatar": null,
                            "nickname": "胥伟杰"
                        },
                        "lookNum": "46",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-26 14:32:37",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-26 14:32:37",
                        "createDate": "2025-06-26 07:09:15",
                        "desc": {
                            "cn": "FalseReject 构建了包含 16 000 条表面“有毒”但实为良性的查询样本，覆盖 44 个安全相关类别，并提出一种基于图信息的对抗多智能体交互框架，用以生成多样且复杂的提示–响应对，并在响应中引入显式推理链，帮助模型更准确地区分安全与不安全上下文；该工作还为标准指令调优模型和推理导向模型分别准备了专项训练集，并附带人工标注的基准测试集，针对 29 款最先进 LLM 进行了大规模评估，结果表明经 FalseReject 监督微调后，模型在显著减少对良性查询的过度拒绝的同时，不仅未损失整体安全性，也保持了语言生成能力，为敏感场景下提升 LLM 可用性提供了首个系统化、可复现的资源与方法框",
                            "en": "Safety alignment approaches in large language models (LLMs) often lead\nto the over-refusal of benign queries, significantly diminishing their utility\nin sensitive scenarios. To address this challenge, we introduce FalseReject,\na comprehensive resource containing 16k seemingly toxic queries accompani"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/falsereject'. Error: Path opencompass/falsereject is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1993",
                    "name": "ViStoryBench",
                    "version": "1.0.0",
                    "description": "ViStoryBench is a benchmark designed to rigorously evaluate the capabilities of multimodal generative models (e.g., diffusion models, LLM-based agents) in synthesizing visually coherent image sequences from textual narratives and reference images.",
                    "url": "opencompass/opencompass_1993.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1993",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Creation"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1993",
                        "name": "ViStoryBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "创作",
                                "en": "Creation"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/vistorybench/vistorybench",
                        "paperLink": "https://arxiv.org/abs/2505.24862",
                        "officialWebsiteLink": "https://huggingface.co/datasets/ViStoryBench/ViStoryBench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "46",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-30 14:40:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-30 14:40:55",
                        "createDate": "2025-06-30 14:27:22",
                        "desc": {
                            "cn": "ViStoryBench 是一个面向故事可视化任务的综合性评测基准，旨在评估多模态生成模型（如扩散模型视频生成模型等）根据给定叙事文本和参考图像生成视觉连贯且情节一致的图像序列的能力。",
                            "en": "ViStoryBench is a benchmark designed to rigorously evaluate the capabilities of multimodal generative models (e.g., diffusion models, LLM-based agents) in synthesizing visually coherent image sequences from textual narratives and reference images."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/vistorybench'. Error: Path opencompass/vistorybench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1782",
                    "name": "HypoBench",
                    "version": "1.0.0",
                    "description": "HypoBench, a novel benchmark designed to evaluate LLMs and hypothesis generation methods across multiple aspects, including practical utility, generalizability, and hypothesis discovery rate. HypoBench includes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets. ",
                    "url": "opencompass/opencompass_1782.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1782",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1782",
                        "name": "HypoBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ChicagoHAI/HypoBench",
                        "paperLink": "https://arxiv.org/abs/2504.11524",
                        "officialWebsiteLink": "https://chicagohai.github.io/HypoBench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "46",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-04-28 11:36:59",
                        "supportOnlineEval": false,
                        "updateDate": "2025-04-28 11:36:59",
                        "createDate": "2025-04-28 11:08:05",
                        "desc": {
                            "cn": "HypoBench，这是一种新颖的基准，旨在从多个方面评估 LLM 和假设生成方法，包括实用性、泛化性和假设发现率。HypoBench 包括 7 个真实任务和 5 个合成任务，具有 194 个不同的数据集。",
                            "en": "HypoBench, a novel benchmark designed to evaluate LLMs and hypothesis generation methods across multiple aspects, including practical utility, generalizability, and hypothesis discovery rate. HypoBench includes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/hypobench'. Error: Path opencompass/hypobench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1924",
                    "name": "LaMP-QA",
                    "version": "1.0.0",
                    "description": "The benchmark covers questions from three major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal Development, and (3) Society & Culture, encompassing over 45 subcategories in total. ",
                    "url": "opencompass/opencompass_1924.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1924",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1924",
                        "name": "LaMP-QA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/LaMP-Benchmark/LaMP-QA",
                        "paperLink": "https://arxiv.org/abs/2506.00137",
                        "officialWebsiteLink": "https://huggingface.co/datasets/alireza7/LaMP-QA",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "45",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-16 09:51:39",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-16 09:51:39",
                        "createDate": "2025-06-12 11:07:04",
                        "desc": {
                            "cn": "基准旨在评估个性化长篇答案生成。该基准涵盖三大类问题:(1)艺术与娱乐,(2)生活与个人发展,(3)社会与文化,共包含45个以上的子类别。",
                            "en": "The benchmark covers questions from three major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal Development, and (3) Society & Culture, encompassing over 45 subcategories in total. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/lamp_qa'. Error: Path opencompass/lamp_qa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1983",
                    "name": "ByteMorph",
                    "version": "1.0.0",
                    "description": "ByteMorph is a benchmark for instruction-guided image editing, focusing on evaluating models’ capabilities in handling non-rigid motions such as camera viewpoint changes, object deformations, human articulations, and complex interactions. ",
                    "url": "opencompass/opencompass_1983.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1983",
                    "sample_count": 1000,
                    "traits": [
                        "Creation",
                        "Instruct"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1983",
                        "name": "ByteMorph",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "创作",
                                "en": "Creation"
                            },
                            {
                                "cn": "指令跟随",
                                "en": "Instruct"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ByteDance-Seed/BM-code",
                        "paperLink": "https://arxiv.org/abs/2506.03107",
                        "officialWebsiteLink": "https://huggingface.co/datasets/ByteDance-Seed/BM-Bench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "45",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-30 11:44:45",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-30 11:44:45",
                        "createDate": "2025-06-30 11:44:24",
                        "desc": {
                            "cn": "ByteMorph 是一个面向指令驱动图像编辑的基准，专注于评估模型在处理非刚性运动（如相机视角变化、物体变形、人类动作和复杂交互）方面的能力。 该基准包括超过 600 万对高分辨率图像编辑样本，涵盖多种动态编辑场景，支持对模型在多种非刚性运动类型下的表现进行细粒度评估。",
                            "en": "ByteMorph is a benchmark for instruction-guided image editing, focusing on evaluating models’ capabilities in handling non-rigid motions such as camera viewpoint changes, object deformations, human articulations, and complex interactions. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/bytemorph'. Error: Path opencompass/bytemorph is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1989",
                    "name": "SWE-Factory",
                    "version": "1.0.0",
                    "description": "SWE-Factory is a benchmark for evaluating large language models on software issue fixing tasks. ",
                    "url": "opencompass/opencompass_1989.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1989",
                    "sample_count": 1000,
                    "traits": [
                        "Code",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1989",
                        "name": "SWE-Factory",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/DeepSoftwareAnalytics/swe-factory",
                        "paperLink": "https://arxiv.org/abs/2506.10954",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "45",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-27 18:13:21",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-27 18:13:21",
                        "createDate": "2025-06-26 16:21:27",
                        "desc": {
                            "cn": "SWE-Factory 是一个面向大型语言模型的软件问题修复评测基准，旨在提升构建效率与评估准确性。该基准集成多智能体系统 SWE-Builder 自动搭建任务环境，采用退出码自动评分，并通过 fail2pass 流程验证修复有效性，确保评测可靠。SWE-Factory 覆盖四种语言共 671 个问题，支持高效、自动化、可扩展的 LLM 评估流程。",
                            "en": "SWE-Factory is a benchmark for evaluating large language models on software issue fixing tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/swe_factory'. Error: Path opencompass/swe_factory is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1938",
                    "name": "WorldGenBench",
                    "version": "1.0.0",
                    "description": "Evaluation dataset: WorldGenBench",
                    "url": "opencompass/opencompass_1938.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1938",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1938",
                        "name": "WorldGenBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2505.01490",
                        "officialWebsiteLink": "https://dwanzhang-ai.github.io/WorldGenBench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "38703035",
                            "name": "JiangC1233",
                            "avatar": null,
                            "nickname": "JiangC1233"
                        },
                        "lookNum": "44",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-20 15:08:20",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-20 15:08:20",
                        "createDate": "2025-06-16 13:51:32",
                        "desc": {
                            "cn": null,
                            "en": null
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/worldgenbench'. Error: Path opencompass/worldgenbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1990",
                    "name": "OPT-BENCH",
                    "version": "1.0.0",
                    "description": "OPT-BENCH is a comprehensive benchmark designed to evaluate large language model (LLM) agents on large-scale search space optimization problems, focusing on their iterative reasoning and problem-solving capabilities.",
                    "url": "opencompass/opencompass_1990.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1990",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1990",
                        "name": "OPT-BENCH",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OliverLeeXZ/OPT-BENCH",
                        "paperLink": "https://arxiv.org/abs/2506.10764",
                        "officialWebsiteLink": "https://huggingface.co/datasets/OPT-Bench/OPT-Bench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "44",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-01 09:44:01",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-01 09:44:01",
                        "createDate": "2025-07-01 09:43:53",
                        "desc": {
                            "cn": "OPT-BENCH 是一个面向大型语言模型（LLM）智能体的大规模搜索空间优化评测基准，旨在系统评估模型在迭代推理和解决复杂优化问题中的能力。 该基准包含 30 个任务，包括 20 个来自 Kaggle 的真实机器学习任务和 10 个经典 NP 问题，涵盖预测建模、图论和组合优化等领域。",
                            "en": "OPT-BENCH is a comprehensive benchmark designed to evaluate large language model (LLM) agents on large-scale search space optimization problems, focusing on their iterative reasoning and problem-solving capabilities."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/opt_bench'. Error: Path opencompass/opt_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2126",
                    "name": "KMMLU-Redux",
                    "version": "1.0.0",
                    "description": "KMMLU-Redux is a reconstructed version of the existing KMMLU, comprising 2,587 problems from Korean National Technical Qualification (KNTQ) exams. ",
                    "url": "opencompass/opencompass_2126.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2126",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2126",
                        "name": "KMMLU-Redux",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2507.08924",
                        "officialWebsiteLink": "https://huggingface.co/datasets/LGAI-EXAONE/KMMLU-Redux",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "张富才"
                        },
                        "lookNum": "42",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-28 10:05:11",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-28 10:05:11",
                        "createDate": "2025-07-28 09:45:22",
                        "desc": {
                            "cn": "KMMLU-Redux是现有 KMMLU 的一个重建版本，包含来自韩国国家技术资格（KNTQ）考试的 2,587 个问题。我们发现了 KMMLU 中的一些关键问题，包括泄露的答案、缺乏清晰度、问题表述不当、符号错误和污染风险。",
                            "en": "KMMLU-Redux is a reconstructed version of the existing KMMLU, comprising 2,587 problems from Korean National Technical Qualification (KNTQ) exams. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/kmmlu_redux'. Error: Path opencompass/kmmlu_redux is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1900",
                    "name": "VideoReasonBench",
                    "version": "1.0.0",
                    "description": "VideoReasonBench is designed to evaluate vision-centric complex video reasoning.",
                    "url": "opencompass/opencompass_1900.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1900",
                    "sample_count": 1000,
                    "traits": [
                        "Strong Reasoning",
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1900",
                        "name": "VideoReasonBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "强推理",
                                "en": "Strong Reasoning"
                            },
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "video reasoning",
                                "en": "video reasoning"
                            },
                            {
                                "cn": "MLLMs",
                                "en": "MLLMs"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/llyx97/video_reason_bench?tab=readme-ov-file",
                        "paperLink": "https://huggingface.co/papers/2505.23359",
                        "officialWebsiteLink": "https://llyx97.github.io/video_reason_bench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "53003130",
                            "name": null,
                            "avatar": null,
                            "nickname": "llyx97"
                        },
                        "lookNum": "42",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-06 14:46:13",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-06 14:46:13",
                        "createDate": "2025-06-05 11:49:20",
                        "desc": {
                            "cn": "VideoReasonBench是一个用于评测视觉为中心、复杂视频推理的基准。",
                            "en": "VideoReasonBench is designed to evaluate vision-centric complex video reasoning."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/videoreasonbench'. Error: Path opencompass/videoreasonbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1920",
                    "name": "MedBookVQA",
                    "version": "1.0.0",
                    "description": "MedBookVQA is a medical visual question answering (VQA) benchmark constructed from open-access medical textbooks. It includes 5,000 questions across five clinical task types and is hierarchically organized by imaging modality, anatomical structure, and clinical specialty.",
                    "url": "opencompass/opencompass_1920.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1920",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1920",
                        "name": "MedBookVQA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Medical",
                                "en": "Medical"
                            },
                            {
                                "cn": "VQA",
                                "en": "VQA"
                            },
                            {
                                "cn": "MLLM",
                                "en": "MLLM"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/slyipae1/MedBookVQA",
                        "paperLink": "https://arxiv.org/abs/2506.00855",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50213650",
                            "name": null,
                            "avatar": null,
                            "nickname": "slyipae1"
                        },
                        "lookNum": "42",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-16 09:51:52",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-16 09:51:52",
                        "createDate": "2025-06-10 16:22:39",
                        "desc": {
                            "cn": "MedBookVQA 是一个基于开放获取医学教科书构建的医学视觉问答（VQA）基准数据集。它包含 5,000 个问题，涵盖五种临床任务类型，并按照影像模态、解剖结构和临床专科进行分层组织。",
                            "en": "MedBookVQA is a medical visual question answering (VQA) benchmark constructed from open-access medical textbooks. It includes 5,000 questions across five clinical task types and is hierarchically organized by imaging modality, anatomical structure, and clinical specialty."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medbookvqa'. Error: Path opencompass/medbookvqa is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2081",
                    "name": "General-Bench",
                    "version": "1.0.0",
                    "description": "General Bench is a set of universal evaluation benchmarks for multimodal large models, covering language, image, video, audio, and 3D five modalities, with a total of 145 skills, over 700 tasks, and 325800 samples.",
                    "url": "opencompass/opencompass_2081.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2081",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2081",
                        "name": "General-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/path2generalist/General-Level",
                        "paperLink": "https://icml.cc/virtual/2025/poster/45047",
                        "officialWebsiteLink": "https://generalist.top/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "41",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:28",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:28",
                        "createDate": "2025-07-22 16:50:17",
                        "desc": {
                            "cn": "General-Bench 是一套面向多模态大模型的通用评测基准，涵盖语言、图像、视频、音频和 3D 五大模态，共计 145 项技能、700 余个任务，包含 325 800 条样本。",
                            "en": "General Bench is a set of universal evaluation benchmarks for multimodal large models, covering language, image, video, audio, and 3D five modalities, with a total of 145 skills, over 700 tasks, and 325800 samples."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/general_bench'. Error: Path opencompass/general_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1892",
                    "name": "ER-Reason",
                    "version": "1.0.0",
                    "description": "ER-Reason is a large-scale benchmark suite for evaluating the clinical reasoning capabilities of large language models (LLMs) in the emergency room (ER) — a high-stakes environment where clinicians make rapid, life-critical decisions.",
                    "url": "opencompass/opencompass_1892.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1892",
                    "sample_count": 1000,
                    "traits": [
                        "Reasoning"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1892",
                        "name": "ER-Reason",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Medical",
                                "en": "Medical"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AlaaLab/ER-Reason",
                        "paperLink": "https://arxiv.org/pdf/2505.22919",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "41",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-05 10:00:58",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-05 10:00:58",
                        "createDate": "2025-06-04 17:53:21",
                        "desc": {
                            "cn": "ER-Reason 是一个大规模基准套件，用于评估大语言模型（LLMs）在急诊室（ER）中的临床推理能力。急诊室是一个高风险环境，临床医生需要快速做出关乎生命的关键决策。",
                            "en": "ER-Reason is a large-scale benchmark suite for evaluating the clinical reasoning capabilities of large language models (LLMs) in the emergency room (ER) — a high-stakes environment where clinicians make rapid, life-critical decisions."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/er_reason'. Error: Path opencompass/er_reason is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1893",
                    "name": "MEDAL",
                    "version": "1.0.0",
                    "description": "MEDAL is a framework for generating and evaluating multilingual open-domain chatbots and their evaluators. This framework supports various language models and provides a structured approach to creating conversational datasets.\n\n",
                    "url": "opencompass/opencompass_1893.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1893",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1893",
                        "name": "MEDAL",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/johndmendonca/medal",
                        "paperLink": "https://arxiv.org/pdf/2505.22777",
                        "officialWebsiteLink": "https://huggingface.co/datasets/Johndfm/medal",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "41",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-25 15:30:50",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-25 15:30:50",
                        "createDate": "2025-06-25 15:30:29",
                        "desc": {
                            "cn": "MEDAL是一个用于生成和评估多语言开放域聊天机器人及其评估器的框架。该框架支持各种语言模型,并提供了一种结构化的方法来创建对话数据集。",
                            "en": "MEDAL is a framework for generating and evaluating multilingual open-domain chatbots and their evaluators. This framework supports various language models and provides a structured approach to creating conversational datasets.\n\n"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/medal'. Error: Path opencompass/medal is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1943",
                    "name": "CombiBench",
                    "version": "1.0.0",
                    "description": "We introduce CombiBench, a comprehensive benchmark comprising 100 combinatorial problems, each formalized in Lean4 and paired with its corresponding informal statement. The problem set covers a wide spectrum of difficulty levels, ranging from middle school to IMO and university level.",
                    "url": "opencompass/opencompass_1943.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1943",
                    "sample_count": 1000,
                    "traits": [
                        "Math"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1943",
                        "name": "CombiBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "数学",
                                "en": "Math"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Combinatorics",
                                "en": "Combinatorics"
                            },
                            {
                                "cn": "Lean4",
                                "en": "Lean4"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MoonshotAI/CombiBench",
                        "paperLink": "https://arxiv.org/abs/2505.03171",
                        "officialWebsiteLink": "https://moonshotai.github.io/CombiBench/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "38702983",
                            "name": null,
                            "avatar": null,
                            "nickname": "192016"
                        },
                        "lookNum": "41",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-20 15:11:44",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-20 15:11:44",
                        "createDate": "2025-06-16 18:39:52",
                        "desc": {
                            "cn": "CombiBench是一个包含100个组合问题的综合基准测试，每个问题都用Lean4进行了形式化，并附有其对应的非形式化表述。这些问题涵盖了从中学生到国际数学奥林匹克竞赛（IMO）以及大学水平的广泛难度范围。",
                            "en": "We introduce CombiBench, a comprehensive benchmark comprising 100 combinatorial problems, each formalized in Lean4 and paired with its corresponding informal statement. The problem set covers a wide spectrum of difficulty levels, ranging from middle school to IMO and university level."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/combibench'. Error: Path opencompass/combibench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1995",
                    "name": "AIRTBench",
                    "version": "1.0.0",
                    "description": "AIRTBench is a benchmark designed to evaluate large language models (LLMs) on their autonomous AI red teaming capabilities.",
                    "url": "opencompass/opencompass_1995.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1995",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1995",
                        "name": "AIRTBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/dreadnode/AIRTBench-Code",
                        "paperLink": "https://arxiv.org/abs/2506.14682",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "41",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-30 10:06:16",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-30 10:06:16",
                        "createDate": "2025-06-27 19:55:37",
                        "desc": {
                            "cn": "AIRTBench 是一个专为评估大型语言模型（LLM）在“红队”安全任务中的自主攻击能力而设计的评测基准。本基准包含 70 个黑盒 CTF（夺旗赛）挑战，模拟真实 AI/ML 系统漏洞环境，要求模型独立编写 Python 代码进行漏洞发现、利用与夺旗操作，体现其计划、推理与系统操控等综合能力。",
                            "en": "AIRTBench is a benchmark designed to evaluate large language models (LLMs) on their autonomous AI red teaming capabilities."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/airtbench'. Error: Path opencompass/airtbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2088",
                    "name": "LMAct",
                    "version": "1.0.0",
                    "description": "LMAct is a benchmark for evaluating frontier multimodal models' in-context imitation learning capabilities in long contexts. ",
                    "url": "opencompass/opencompass_2088.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2088",
                    "sample_count": 1000,
                    "traits": [
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2088",
                        "name": "LMAct",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "长文本",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/google-deepmind/lm_act",
                        "paperLink": "https://icml.cc/virtual/2025/poster/46274",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "40",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:25",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:25",
                        "createDate": "2025-07-22 16:47:51",
                        "desc": {
                            "cn": "LMAct是评估前沿多模态模型在长上下文中的上下文模仿学习能力的基准。它评估了诸如井字棋、国际象棋和雅达利等交互式任务的多模式决策。该测试集包含多达100万个令牌上下文和512个专家演示集。",
                            "en": "LMAct is a benchmark for evaluating frontier multimodal models' in-context imitation learning capabilities in long contexts. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/lmact'. Error: Path opencompass/lmact is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2128",
                    "name": "lm-evaluation-harness",
                    "version": "1.0.0",
                    "description": "AI Language Proficiency Monitor is a multilingual benchmark platform that systematically assesses LLM performance across up to 200 languages.",
                    "url": "opencompass/opencompass_2128.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2128",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2128",
                        "name": "lm-evaluation-harness",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OpenGPTX/lm-evaluation-harness",
                        "paperLink": "https://arxiv.org/abs/2507.08538",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "张富才"
                        },
                        "lookNum": "40",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-28 10:05:17",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-28 10:05:17",
                        "createDate": "2025-07-28 09:58:05",
                        "desc": {
                            "cn": "AI Language Proficiency Monitor是一个多语言大语言模型评测平台，系统性评估模型在多达200种语言上的性能表现，特别关注低资源语言，整合FLORES+、MMLU、GSM8K、TruthfulQA和ARC等数据集评测翻译、问答、数学推理和事实性等能力，提供开源自动更新排行榜和交互式仪表板，覆盖全球80-95%人口使用的语言。",
                            "en": "AI Language Proficiency Monitor is a multilingual benchmark platform that systematically assesses LLM performance across up to 200 languages."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/lm_evaluation_harness'. Error: Path opencompass/lm_evaluation_harness is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1876",
                    "name": "Aneumo",
                    "version": "1.0.0",
                    "description": "Based on 427 real aneurysm geometries, we synthesized 10,660 3D shapes via controlled deformation to simulate aneurysm evolution. CFD computations were performed on each shape under eight steady-state mass flow conditions, generating a total of 85,280 blood flow dynamics data covering key parameters",
                    "url": "opencompass/opencompass_1876.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1876",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1876",
                        "name": "Aneumo",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "Medical Imagery",
                                "en": "Medical Imagery"
                            },
                            {
                                "cn": "Aneurysm",
                                "en": "Aneurysm"
                            },
                            {
                                "cn": "Computational Fluid Dynamics",
                                "en": "Computational Fluid Dynamics"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Xigui-Li/Aneumo",
                        "paperLink": "https://arxiv.org/pdf/2505.14717",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "52305425",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-nkWkVBdJ1"
                        },
                        "lookNum": "40",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-03 19:48:15",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-03 19:48:15",
                        "createDate": "2025-06-03 19:25:50",
                        "desc": {
                            "cn": "Based on 427 real aneurysm geometries, we synthesized 10,660 3D shapes via controlled deformation to simulate aneurysm evolution. CFD computations were performed on each shape under eight steady-state mass flow conditions, generating a total of 85,280 blood flow dynamics data covering key parameters",
                            "en": "Based on 427 real aneurysm geometries, we synthesized 10,660 3D shapes via controlled deformation to simulate aneurysm evolution. CFD computations were performed on each shape under eight steady-state mass flow conditions, generating a total of 85,280 blood flow dynamics data covering key parameters"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/aneumo'. Error: Path opencompass/aneumo is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1966",
                    "name": "AMSbench",
                    "version": "1.0.0",
                    "description": "AMSbench is a benchmark of ~8000 questions to evaluate multi-modal LLMs on analog/mixed-signal circuit tasks like schematic recognition, analysis, and design. ",
                    "url": "opencompass/opencompass_1966.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1966",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding",
                        "Creation"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1966",
                        "name": "AMSbench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            },
                            {
                                "cn": "创作",
                                "en": "Creation"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "AMS",
                                "en": "AMS"
                            },
                            {
                                "cn": "Circuit",
                                "en": "Circuit"
                            },
                            {
                                "cn": "EDA",
                                "en": "EDA"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Why0912/AMSBench",
                        "paperLink": "https://arxiv.org/abs/2505.24138",
                        "officialWebsiteLink": "https://amsbench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "85502520",
                            "name": null,
                            "avatar": null,
                            "nickname": "脆皮烤土豆"
                        },
                        "lookNum": "40",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-23 10:04:59",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-23 10:04:59",
                        "createDate": "2025-06-21 16:15:54",
                        "desc": {
                            "cn": "AMSbench 是一个包含约8000道题目的基准测试集，用于评估多模态大语言模型在模拟/混合信号电路任务中的表现，包括识图、分析与设计。",
                            "en": "AMSbench is a benchmark of ~8000 questions to evaluate multi-modal LLMs on analog/mixed-signal circuit tasks like schematic recognition, analysis, and design. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/amsbench'. Error: Path opencompass/amsbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2089",
                    "name": "LaRA",
                    "version": "1.0.0",
                    "description": "LaRA is a focused benchmark for testing Retrieval-Augmented Generation (RAG) and long-context LLMs. ",
                    "url": "opencompass/opencompass_2089.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2089",
                    "sample_count": 1000,
                    "traits": [
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2089",
                        "name": "LaRA",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "长文本",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Alibaba-NLP/LaRA",
                        "paperLink": "https://icml.cc/virtual/2025/poster/46069",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "39",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:14",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:14",
                        "createDate": "2025-07-22 16:47:29",
                        "desc": {
                            "cn": "LaRA 是专为评估检索增强生成（RAG）和长上下文大型语言模型（LLM）而打造的基准。它围绕信息定位、片段对比、内容推理和幻觉检测四大能力，通过 2 326 条测试用例，对四类问答任务和三种长文本场景（小说、学术论文、财务报表）进行全面测评。",
                            "en": "LaRA is a focused benchmark for testing Retrieval-Augmented Generation (RAG) and long-context LLMs. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/lara'. Error: Path opencompass/lara is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1976",
                    "name": "IntPhys2",
                    "version": "1.0.0",
                    "description": " IntPhys2 tests intuitive physics—permanence, immutability, continuity and solidity—using synthetic videos. SOTA models perform around chance (~50 %), far below human level.",
                    "url": "opencompass/opencompass_1976.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1976",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1976",
                        "name": "IntPhys2",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "视频生成",
                                "en": "视频生成"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/facebookresearch/IntPhys2",
                        "paperLink": "https://arxiv.org/abs/2506.09849",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "38",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-25 15:30:59",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-25 15:30:59",
                        "createDate": "2025-06-25 10:34:04",
                        "desc": {
                            "cn": "IntPhys 2，一个用于评估深度学习模型直观物理理解能力的视频基准。它围绕永恒性、不可变性、时空连续性和实体性四个核心原则，测试模型区分可能与不可能事件的能力。",
                            "en": " IntPhys2 tests intuitive physics—permanence, immutability, continuity and solidity—using synthetic videos. SOTA models perform around chance (~50 %), far below human level."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/intphys2'. Error: Path opencompass/intphys2 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2045",
                    "name": "DRAGON",
                    "version": "1.0.0",
                    "description": "DRAGON 是一个用于评估检索增强生成（RAG）系统在俄语新闻语境中事实性与检索能力的动态基准，支持对检索器与生成器组件的全面评估。",
                    "url": "opencompass/opencompass_2045.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2045",
                    "sample_count": 1000,
                    "traits": [
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2045",
                        "name": "DRAGON",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "RAG",
                                "en": "RAG"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/RussianNLP/DRAGON",
                        "paperLink": "https://arxiv.org/abs/2507.05713",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "38",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-14 09:39:22",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-14 09:39:22",
                        "createDate": "2025-07-11 11:00:03",
                        "desc": {
                            "cn": "DRAGON is a dynamic benchmark for evaluating Retrieval-Augmented Generation (RAG) systems in Russian news contexts, supporting comprehensive assessment of both retriever and generator components. ",
                            "en": "DRAGON 是一个用于评估检索增强生成（RAG）系统在俄语新闻语境中事实性与检索能力的动态基准，支持对检索器与生成器组件的全面评估。"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/dragon'. Error: Path opencompass/dragon is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2061",
                    "name": "AgentHazard",
                    "version": "1.0.0",
                    "description": "移动端 GUI Agent 通过与设备环境的交互来完成任务，在完成任务的过程中会遇到一些未知或不可信的信息来源，这些信息可能含有攻击性的内容，致使 Agent 无法正常完成任务，甚至对用户的隐私和财产带来危害。本评测集兼具动态执行环境和静态评测数据集，旨在为移动端 GUI Agent 提供一个仿真度高的模拟环境，以评估其在真实场景下执行的行为和安全性。",
                    "url": "opencompass/opencompass_2061.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2061",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Safety",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2061",
                        "name": "AgentHazard",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "安全",
                                "en": "Safety"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Zsbyqx20/AgentHazard",
                        "paperLink": "https://arxiv.org/abs/2507.04227",
                        "officialWebsiteLink": "https://agenthazard.github.io",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "20905480",
                            "name": null,
                            "avatar": null,
                            "nickname": "Zsbyqx20"
                        },
                        "lookNum": "37",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-17 09:29:55",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-17 09:29:55",
                        "createDate": "2025-07-16 22:06:35",
                        "desc": {
                            "cn": "移动端 GUI Agent 通过与设备环境的交互来完成任务，在完成任务的过程中会遇到一些未知或不可信的信息来源，这些信息可能含有攻击性的内容，致使 Agent 无法正常完成任务，甚至对用户的隐私和财产带来危害。本评测集兼具动态执行环境和静态评测数据集，旨在为移动端 GUI Agent 提供一个仿真度高的模拟环境，以评估其在真实场景下执行的行为和安全性。",
                            "en": "移动端 GUI Agent 通过与设备环境的交互来完成任务，在完成任务的过程中会遇到一些未知或不可信的信息来源，这些信息可能含有攻击性的内容，致使 Agent 无法正常完成任务，甚至对用户的隐私和财产带来危害。本评测集兼具动态执行环境和静态评测数据集，旨在为移动端 GUI Agent 提供一个仿真度高的模拟环境，以评估其在真实场景下执行的行为和安全性。"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/agenthazard'. Error: Path opencompass/agenthazard is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2083",
                    "name": "SAEBench",
                    "version": "1.0.0",
                    "description": "SAEBench is a comprehensive benchmark designed to evaluate and compare the performance of language model sparse autoencoders (SAEs). This benchmark provides over 200 SAE models covering seven architectures for systematic comparison, and has open-source code and models.",
                    "url": "opencompass/opencompass_2083.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2083",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2083",
                        "name": "SAEBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/adamkarvonen/SAEBench",
                        "paperLink": "https://icml.cc/virtual/2025/poster/43918",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "37",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:07",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:07",
                        "createDate": "2025-07-22 16:49:23",
                        "desc": {
                            "cn": "SAEBench 是一个旨在评估和比较语言模型稀疏自动编码器（SAEs）性能的综合性基准。该基准提供超过200个SAE模型，涵盖七种架构，以实现系统性比较，并且开源了代码和模型。",
                            "en": "SAEBench is a comprehensive benchmark designed to evaluate and compare the performance of language model sparse autoencoders (SAEs). This benchmark provides over 200 SAE models covering seven architectures for systematic comparison, and has open-source code and models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/saebench'. Error: Path opencompass/saebench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2086",
                    "name": "OR-Bench",
                    "version": "1.0.0",
                    "description": "OR Bench is a large-scale benchmark designed to evaluate the excessive rejection behavior of large language models.",
                    "url": "opencompass/opencompass_2086.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2086",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2086",
                        "name": "OR-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/justincui03/or-bench",
                        "paperLink": "https://icml.cc/virtual/2025/poster/46052",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "37",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:40",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:40",
                        "createDate": "2025-07-22 16:48:34",
                        "desc": {
                            "cn": "OR-Bench 是一个旨在评估大型语言模型过度拒绝行为的大规模基准。它衡量LLM在过度拒绝和有害提示拒绝方面的表现，涵盖暴力、隐私等10个类别。基准包含8万个、1千个困难及6百个有害提示，通过Mixtral等工具自动化生成。它为未来安全对齐研究提供强大测试平台。",
                            "en": "OR Bench is a large-scale benchmark designed to evaluate the excessive rejection behavior of large language models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/or_bench'. Error: Path opencompass/or_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1875",
                    "name": "VIBE",
                    "version": "1.0.0",
                    "description": "SVRPBench is an open and extensible benchmark for the Stochastic Vehicle Routing Problem (SVRP). It includes 500+ instances spanning small to large scales (10–1000 customers), designed to evaluate algorithms under realistic urban logistics conditions with uncertainty and operational constraints.",
                    "url": "opencompass/opencompass_1875.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1875",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1875",
                        "name": "VIBE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/yehias21/vrp-benchmarks",
                        "paperLink": "https://arxiv.org/pdf/2505.21887",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "37",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-03 20:02:21",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-03 20:02:21",
                        "createDate": "2025-06-03 18:28:29",
                        "desc": {
                            "cn": "SVRPBench是一个针对随机车辆路径问题（SVRP）的开放且可扩展的基准测试平台。它包含500多个实例，涵盖小到大规模（10-1000个客户），旨在评估算法在具有不确定性和操作约束的现实城市物流条件下的表现。",
                            "en": "SVRPBench is an open and extensible benchmark for the Stochastic Vehicle Routing Problem (SVRP). It includes 500+ instances spanning small to large scales (10–1000 customers), designed to evaluate algorithms under realistic urban logistics conditions with uncertainty and operational constraints."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/vibe'. Error: Path opencompass/vibe is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1927",
                    "name": "GMAIMMBench",
                    "version": "1.0.0",
                    "description": "GMAI-MMBench is the most comprehensive and structured benchmark developed to evaluate Large Vision-Language Models (LVLMs) in general medical artificial intelligence (GMAI) applications. It addresses the limitations of existing benchmarks that typically focus on narrow domains and lack perceptual di",
                    "url": "opencompass/opencompass_1927.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1927",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1927",
                        "name": "GMAIMMBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "多模态",
                                "en": "多模态"
                            },
                            {
                                "cn": "医疗",
                                "en": "医疗"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/uni-medical/GMAI-MMBench",
                        "paperLink": "https://arxiv.org/pdf/2408.03361",
                        "officialWebsiteLink": "https://uni-medical.github.io/GMAI-MMBench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "155355",
                            "name": "kennyutc",
                            "avatar": null,
                            "nickname": "kennyutc"
                        },
                        "lookNum": "34",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-16 09:50:40",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-16 09:50:40",
                        "createDate": "2025-06-12 16:05:21",
                        "desc": {
                            "cn": "GMAI-MMBench 是目前最全面、结构化最完善的通用医学人工智能（GMAI）基准数据集，专为评估大规模视觉语言模型（LVLMs）在医疗领域中的表现而设计。针对现有医学基准通常局限于特定学科、感知粒度单一的问题，GMAI-MMBench 从285个真实医学数据集构建而成，涵盖39种医学影像模态、18个临床任务、18个医学科室，以及4种不同的感知粒度。所有任务采用视觉问答（VQA）形式组织，具备良好的交互性和通用性。其独特的词汇树结构允许用户针对具体研究需求灵活定制评估路径，支持多样化的评测场景。我们对50个主流LVLMs进行了系统测试，发现即使是目前最先进的 GPT-4o，其准确率也仅为5",
                            "en": "GMAI-MMBench is the most comprehensive and structured benchmark developed to evaluate Large Vision-Language Models (LVLMs) in general medical artificial intelligence (GMAI) applications. It addresses the limitations of existing benchmarks that typically focus on narrow domains and lack perceptual di"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gmaimmbench'. Error: Path opencompass/gmaimmbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2046",
                    "name": "CREW-WILDFIRE",
                    "version": "1.0.0",
                    "description": "CREW-WILDFIRE is a benchmark designed to evaluate large language model-based multi-agent systems’ collaboration capabilities in complex, dynamic tasks, targeting agentic frameworks with perception, planning, and execution abilities.",
                    "url": "opencompass/opencompass_2046.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2046",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2046",
                        "name": "CREW-WILDFIRE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/generalroboticslab/CREW/tree/main/crew-algorithms/crew_algorithms/wildfire_alg",
                        "paperLink": "https://arxiv.org/abs/2507.05178",
                        "officialWebsiteLink": "http://www.generalroboticslab.com/blogs/blog/2025-07-05-crewwildfire/index.html",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "33",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-14 09:39:25",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-14 09:39:25",
                        "createDate": "2025-07-11 14:15:45",
                        "desc": {
                            "cn": "CREW-WILDFIRE 是一个用于评估基于大语言模型的多智能体系统在复杂动态任务中协作能力的基准，面向具备感知、规划与执行能力的智能体框架。",
                            "en": "CREW-WILDFIRE is a benchmark designed to evaluate large language model-based multi-agent systems’ collaboration capabilities in complex, dynamic tasks, targeting agentic frameworks with perception, planning, and execution abilities."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/crew_wildfire'. Error: Path opencompass/crew_wildfire is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2052",
                    "name": "ArtifactsBench",
                    "version": "1.0.0",
                    "description": "ArtifactsBench is a benchmark with 1,825 tasks for evaluating LLM-generated visual and interactive code. It addresses the gap of traditional benchmarks that focus only on algorithmic correctness by assessing visual fidelity and user interaction.",
                    "url": "opencompass/opencompass_2052.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2052",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2052",
                        "name": "ArtifactsBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "ArtifactsBench",
                                "en": "ArtifactsBench"
                            },
                            {
                                "cn": "代码可视化",
                                "en": "代码可视化"
                            },
                            {
                                "cn": "代码生成",
                                "en": "代码生成"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Tencent-Hunyuan/ArtifactsBenchmark",
                        "paperLink": "https://arxiv.org/abs/2507.04952",
                        "officialWebsiteLink": "https://artifactsbenchmark.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "90307965",
                            "name": null,
                            "avatar": null,
                            "nickname": "xxzcc"
                        },
                        "lookNum": "32",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-14 09:39:37",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-14 09:39:37",
                        "createDate": "2025-07-12 13:22:30",
                        "desc": {
                            "cn": "rtifactsBench 是一个专注于弥合传统代码评测中“视觉-交互”鸿沟的新型基准。它旨在全面评估大语言模型（LLM）生成动态可视化与交互式代码的能力，而非仅仅考核算法正确性。\n该基准包含1825个真实世界的任务，并开创了一套自动化多模态评估流程：系统会自动渲染代码、捕捉其视觉与交互行为，再由一个多模态大模型（MLLM）依据详细清单进行评分。该流程与人类专家判断的一致性高达94.4%，证明了其高度可靠性。\nArtifactsBench 已将数据集、评估框架及工具链完全开源，为社区提供了一个可扩展且精准的工具，以推动能创造丰富用户体验的新一代生成模型的发展。",
                            "en": "ArtifactsBench is a benchmark with 1,825 tasks for evaluating LLM-generated visual and interactive code. It addresses the gap of traditional benchmarks that focus only on algorithmic correctness by assessing visual fidelity and user interaction."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/artifactsbench'. Error: Path opencompass/artifactsbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2063",
                    "name": "VISCO",
                    "version": "1.0.0",
                    "description": "VISCO aims to evaluate the critique and correction capabilities of VLMs, which are two essential building blocks towards VLM self-improvement. VISCO requires VLMs to critique the correctness of each step in CoT, provide natural language explanation, and corrects the CoT based on the critique.",
                    "url": "opencompass/opencompass_2063.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2063",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Knowledge"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2063",
                        "name": "VISCO",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "知识",
                                "en": "Knowledge"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "self-critique",
                                "en": "self-critique"
                            },
                            {
                                "cn": "VLM",
                                "en": "VLM"
                            },
                            {
                                "cn": "VLM reasoning",
                                "en": "VLM reasoning"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/PlusLabNLP/VISCO",
                        "paperLink": "https://arxiv.org/abs/2412.02172",
                        "officialWebsiteLink": "https://visco-benchmark.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "20905070",
                            "name": null,
                            "avatar": null,
                            "nickname": "shirley-wu"
                        },
                        "lookNum": "32",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-17 10:17:50",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-17 10:17:50",
                        "createDate": "2025-07-17 09:56:30",
                        "desc": {
                            "cn": "VISCO 旨在评估 VLM 的评判（critique）和纠正（correction）能力，这两个能力是 VLM 自主提升推理性能的基础。对于一个视觉推理问题，给定模型生成的 CoT，VISCO 评测集要求 VLM 评判 CoT 中每个步骤的正确性，提供自然语言解释，并根据评判结果对 CoT 进行修正。",
                            "en": "VISCO aims to evaluate the critique and correction capabilities of VLMs, which are two essential building blocks towards VLM self-improvement. VISCO requires VLMs to critique the correctness of each step in CoT, provide natural language explanation, and corrects the CoT based on the critique."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/visco'. Error: Path opencompass/visco is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2085",
                    "name": "MIB",
                    "version": "1.0.0",
                    "description": "MIB is a benchmark designed to evaluate mechanistic interpretability methods for neural language models.",
                    "url": "opencompass/opencompass_2085.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2085",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2085",
                        "name": "MIB",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/aaronmueller/mib",
                        "paperLink": "https://icml.cc/virtual/2025/poster/43836",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "31",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:19",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:19",
                        "createDate": "2025-07-22 16:48:53",
                        "desc": {
                            "cn": "MIB 是一个旨在评估神经网络语言模型中机械可解释性方法的基准。它主要评估方法在精确地定位因果路径（电路定位）和特定概念（因果变量定位）方面的能力，侧重于忠实度和最小化电路规模。该基准涵盖了IOI和算术等四个任务，并评估了Llama-3.1和Gemma-2等四种模型。\n",
                            "en": "MIB is a benchmark designed to evaluate mechanistic interpretability methods for neural language models."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mib'. Error: Path opencompass/mib is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2091",
                    "name": "IS-Bench",
                    "version": "1.0.0",
                    "description": "IS-Bench is the first evaluation benchmark dedicated to assessing the safety of embodied agents during their interaction with home environments. It encompasses 161 high-risk domestic scenarios, spanning 10 hazard categories, including food poisoning, fire, electric shock, and more. ",
                    "url": "opencompass/opencompass_2091.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2091",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Safety",
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2091",
                        "name": "IS-Bench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "安全",
                                "en": "Safety"
                            },
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "task planning",
                                "en": "task planning"
                            },
                            {
                                "cn": "safety",
                                "en": "safety"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/AI45Lab/IS-Bench",
                        "paperLink": "https://www.arxiv.org/abs/2506.16402",
                        "officialWebsiteLink": "https://ursulalujun.github.io/isbench.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "62403124",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-PwR4JLkwp"
                        },
                        "lookNum": "31",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-21 10:18:50",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-21 10:18:50",
                        "createDate": "2025-07-19 20:51:53",
                        "desc": {
                            "cn": "IS-Bench 是首个专注于具身智能体与家用环境交互过程安全性的评测基准。它包含 161 个高风险家居场景及相关日常任务，覆盖食物中毒、火灾、触电等 10 大类常见风险。通过贯穿整个交互过程的动态评测框架，全方位评估具身智能体的安全素养。",
                            "en": "IS-Bench is the first evaluation benchmark dedicated to assessing the safety of embodied agents during their interaction with home environments. It encompasses 161 high-risk domestic scenarios, spanning 10 hazard categories, including food poisoning, fire, electric shock, and more. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/is_bench'. Error: Path opencompass/is_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2154",
                    "name": "MotionMillion",
                    "version": "1.0.0",
                    "description": "MotionMillion: The largest open-sourced 3D human motion dataset with text annotation, including 2.5k hours 1.9M episodes.",
                    "url": "opencompass/opencompass_2154.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2154",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2154",
                        "name": "MotionMillion",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "",
                        "officialWebsiteLink": "https://huggingface.co/datasets/InternRobotics/MotionMillion",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50207630",
                            "name": null,
                            "avatar": null,
                            "nickname": "fak111"
                        },
                        "lookNum": "31",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-08-15 16:40:09",
                        "supportOnlineEval": false,
                        "updateDate": "2025-08-15 16:40:09",
                        "createDate": "2025-08-15 16:38:27",
                        "desc": {
                            "cn": "MotionMillion：目前最大的开源带文本标注的 3D 人体动作数据集，包含 2500 小时、190 万个片段。",
                            "en": "MotionMillion: The largest open-sourced 3D human motion dataset with text annotation, including 2.5k hours 1.9M episodes."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/motionmillion'. Error: Path opencompass/motionmillion is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2080",
                    "name": "PhyGenBench",
                    "version": "1.0.0",
                    "description": "PhyGenBench is a benchmark designed to evaluate physical commonsense correctness in text-to-video generation models. I",
                    "url": "opencompass/opencompass_2080.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2080",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2080",
                        "name": "PhyGenBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OpenGVLab/PhyGenBench",
                        "paperLink": "https://icml.cc/virtual/2025/poster/44642",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "30",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:22",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:22",
                        "createDate": "2025-07-22 16:50:31",
                        "desc": {
                            "cn": "PhyGenBench 是一个评估T2V模型物理常识的基准。它涵盖力学、光学、热学和材料特性四大领域27种物理定律，包含160个提示。结合PhyGenEval框架，利用视听和大语言模型自动化评估模型的物理理解能力。",
                            "en": "PhyGenBench is a benchmark designed to evaluate physical commonsense correctness in text-to-video generation models. I"
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/phygenbench'. Error: Path opencompass/phygenbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2084",
                    "name": "AXBENCH",
                    "version": "1.0.0",
                    "description": "AXBENCH is a benchmark for large-scale evaluation of Language Model (LLM) control methods using synthetic data, focusing on fine-grained steering for safety and reliability.",
                    "url": "opencompass/opencompass_2084.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2084",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2084",
                        "name": "AXBENCH",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/stanfordnlp/axbench",
                        "paperLink": "https://icml.cc/virtual/2025/poster/45658",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "30",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:37",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:37",
                        "createDate": "2025-07-22 16:49:08",
                        "desc": {
                            "cn": "AXBENCH 是一个旨在评估LLM控制能力的基准。它通过概念检测和模型操控（包含概念、指令、流畅度）评估，旨在实现安全可靠的细粒度操控。基准使用大规模合成数据集，并集成了多种基线方法。",
                            "en": "AXBENCH is a benchmark for large-scale evaluation of Language Model (LLM) control methods using synthetic data, focusing on fine-grained steering for safety and reliability."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/axbench'. Error: Path opencompass/axbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2129",
                    "name": "LangNavBench",
                    "version": "1.0.0",
                    "description": "LangNavBench is a benchmark for evaluating natural language understanding in semantic navigation, built upon the manually verified LangNav open-set dataset.",
                    "url": "opencompass/opencompass_2129.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2129",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2129",
                        "name": "LangNavBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/3dlg-hcvc/langmonmap",
                        "paperLink": "https://arxiv.org/abs/2507.07299",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "张富才"
                        },
                        "lookNum": "29",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-28 10:05:09",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-28 10:05:09",
                        "createDate": "2025-07-28 10:01:42",
                        "desc": {
                            "cn": "LangNavBench是一个语义导航中自然语言理解评测基准，基于手工验证的LangNav开放集数据集，评测具身智能体在自然语言指令引导下的目标定位能力，涵盖类别层次理解、对象属性识别和空间关系推理等维度.",
                            "en": "LangNavBench is a benchmark for evaluating natural language understanding in semantic navigation, built upon the manually verified LangNav open-set dataset."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/langnavbench'. Error: Path opencompass/langnavbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2032",
                    "name": "DICE-BENCH",
                    "version": "1.0.0",
                    "description": "DICE-BENCH is a benchmark for evaluating large language models’ tool-use capabilities in multi-round, multi-party dialogues, focusing on function selection, parameter filling, and dialogue context comprehension.",
                    "url": "opencompass/opencompass_2032.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2032",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2032",
                        "name": "DICE-BENCH",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/snuhcc/DICE-Bench",
                        "paperLink": "https://arxiv.org/abs/2506.22853",
                        "officialWebsiteLink": "https://huggingface.co/datasets/OfficerChul/DICE-BENCH",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "张富才"
                        },
                        "lookNum": "29",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-07 11:44:26",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-07 11:44:26",
                        "createDate": "2025-07-07 10:40:34",
                        "desc": {
                            "cn": "DICE-BENCH 是一个评估大型语言模型在多轮、多方对话中工具调用能力的基准，涵盖函数选择、参数填充和对话上下文理解等维度。",
                            "en": "DICE-BENCH is a benchmark for evaluating large language models’ tool-use capabilities in multi-round, multi-party dialogues, focusing on function selection, parameter filling, and dialogue context comprehension."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/dice_bench'. Error: Path opencompass/dice_bench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_1979",
                    "name": "OpenUnlearning",
                    "version": "1.0.0",
                    "description": "OpenUnlearning is an efficient and modular benchmark platform designed to support and drive research on \"unlearning\" in large language models (LLMs).",
                    "url": "opencompass/opencompass_1979.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_1979",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "1979",
                        "name": "OpenUnlearning",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/locuslab/open-unlearning",
                        "paperLink": "https://arxiv.org/abs/2506.12618",
                        "officialWebsiteLink": "https://huggingface.co/open-unlearning",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "4006165",
                            "name": "opencompass",
                            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
                            "nickname": "OpenCompass 司南"
                        },
                        "lookNum": "28",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-06-27 18:12:41",
                        "supportOnlineEval": false,
                        "updateDate": "2025-06-27 18:12:41",
                        "createDate": "2025-06-25 17:24:57",
                        "desc": {
                            "cn": "OpenUnlearning 是一个高效且模块化的基准平台，旨在支持和推动大型语言模型（LLM）中的“遗忘”（unlearning）研究。",
                            "en": "OpenUnlearning is an efficient and modular benchmark platform designed to support and drive research on \"unlearning\" in large language models (LLMs)."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/openunlearning'. Error: Path opencompass/openunlearning is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2087",
                    "name": "PertEval-scFM",
                    "version": "1.0.0",
                    "description": "PertEval-scFM is a standardized framework to evaluate single-cell foundation models (scFMs) for predicting cellular perturbation effects. ",
                    "url": "opencompass/opencompass_2087.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2087",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2087",
                        "name": "PertEval-scFM",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/aaronwtr/PertEval",
                        "paperLink": "https://icml.cc/virtual/2025/poster/43799",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "27",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:34",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:34",
                        "createDate": "2025-07-22 16:48:13",
                        "desc": {
                            "cn": "PertEval-scFM 是一个评估单细胞基础模型（scFM）扰动效应预测的标准化框架。它从分布偏移泛化性、扰动强度及上下文对齐三方面评测。测试集包含Norman、Replogle等数据集，涵盖数千扰动样本和基因。\n",
                            "en": "PertEval-scFM is a standardized framework to evaluate single-cell foundation models (scFMs) for predicting cellular perturbation effects. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/perteval_scfm'. Error: Path opencompass/perteval_scfm is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2082",
                    "name": "StructTokenBench",
                    "version": "1.0.0",
                    "description": "StructTokenBench is a benchmark framework designed to comprehensively evaluate the quality and efficiency of protein structure tokenization methods, particularly focusing on fine-grained local substructures. ",
                    "url": "opencompass/opencompass_2082.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2082",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2082",
                        "name": "StructTokenBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/KatarinaYuan/StructTokenBench",
                        "paperLink": "https://icml.cc/virtual/2025/poster/44084",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "26",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-22 16:53:31",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-22 16:53:31",
                        "createDate": "2025-07-22 16:49:57",
                        "desc": {
                            "cn": "StructTokenBench 是一个评估蛋白质结构标记化方法质量效率的基准。它关注细粒度局部子结构，评测维度包括下游有效性、敏感性、独特性和码本利用效率。\n",
                            "en": "StructTokenBench is a benchmark framework designed to comprehensively evaluate the quality and efficiency of protein structure tokenization methods, particularly focusing on fine-grained local substructures. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/structtokenbench'. Error: Path opencompass/structtokenbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2127",
                    "name": "clembench",
                    "version": "1.0.0",
                    "description": "clembench is a benchmark framework for evaluating LLMs through dialogue game-based interactions, assessing chat-optimized models as conversational agents via multi-turn interactive game scenario.",
                    "url": "opencompass/opencompass_2127.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2127",
                    "sample_count": 1000,
                    "traits": [
                        "Creation"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2127",
                        "name": "clembench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "创作",
                                "en": "Creation"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/clembench/clembench",
                        "paperLink": "https://arxiv.org/abs/2507.08491",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "张富才"
                        },
                        "lookNum": "26",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-28 10:05:14",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-28 10:05:14",
                        "createDate": "2025-07-28 09:49:10",
                        "desc": {
                            "cn": "clembench是一个基于对话游戏的大语言模型评测框架，通过多回合交互式游戏场景评测聊天优化模型的会话智能体能力，包含Wordle、Taboo等多种游戏任务，评测指令遵循、目标导向行为和精细化交互理解等核心维度，提供可重复、可控制的自玩评测环境和综合得分机制，支持英语及多语言扩展的开源评测平台。",
                            "en": "clembench is a benchmark framework for evaluating LLMs through dialogue game-based interactions, assessing chat-optimized models as conversational agents via multi-turn interactive game scenario."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/clembench'. Error: Path opencompass/clembench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2151",
                    "name": "InternData-A1",
                    "version": "1.0.0",
                    "description": "InternData-A1: A hybrid synthetic-real manipulation dataset integrating 5 heterogeneous robots, 15 skills, and 200+ scenes, emphasizing multi-robot collaboration under dynamic scenarios.",
                    "url": "opencompass/opencompass_2151.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2151",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2151",
                        "name": "InternData-A1",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "video",
                                "en": "video"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "",
                        "officialWebsiteLink": "https://huggingface.co/datasets/InternRobotics/InternData-A1",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50207630",
                            "name": null,
                            "avatar": null,
                            "nickname": "fak111"
                        },
                        "lookNum": "26",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-08-15 16:40:00",
                        "supportOnlineEval": false,
                        "updateDate": "2025-08-15 16:40:00",
                        "createDate": "2025-08-15 16:39:27",
                        "desc": {
                            "cn": "InternData-A1：一个融合了 5 种异构机器人、15 项技能和 200+ 场景的混合合成-真实操作数据集，重点关注动态场景下的多机器人协作。",
                            "en": "InternData-A1: A hybrid synthetic-real manipulation dataset integrating 5 heterogeneous robots, 15 skills, and 200+ scenes, emphasizing multi-robot collaboration under dynamic scenarios."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/interndata_a1'. Error: Path opencompass/interndata_a1 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2029",
                    "name": "SMMILE",
                    "version": "1.0.0",
                    "description": "SMMILE is a benchmark for evaluating multimodal large language models (MLLMs) in medical in-context learning tasks, constructed by medical experts. ",
                    "url": "opencompass/opencompass_2029.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2029",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2029",
                        "name": "SMMILE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "医学",
                                "en": "医学"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/eth-medical-ai-lab/smmile",
                        "paperLink": "https://arxiv.org/abs/2506.21355",
                        "officialWebsiteLink": "https://smmile-benchmark.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "张富才"
                        },
                        "lookNum": "26",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-07 11:44:42",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-07 11:44:42",
                        "createDate": "2025-07-07 10:06:59",
                        "desc": {
                            "cn": "SMMILE 是一个由医学专家主导构建的多模态医疗上下文学习评测基准，旨在评估多模态大语言模型（MLLMs）在医学任务中的上下文学习能力。该基准包含111个问题（共517个图文问答三元组），涵盖6个医学专科和13种影像模态，另提供扩展版本 SMMILE++，包含1038个变换问题。",
                            "en": "SMMILE is a benchmark for evaluating multimodal large language models (MLLMs) in medical in-context learning tasks, constructed by medical experts. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/smmile'. Error: Path opencompass/smmile is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2030",
                    "name": "DABstep",
                    "version": "1.0.0",
                    "description": "DABstep is a benchmark for evaluating AI agents’ multi-step reasoning and planning abilities in realistic data analysis tasks, targeting code-executing language model agents. ",
                    "url": "opencompass/opencompass_2030.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2030",
                    "sample_count": 1000,
                    "traits": [
                        "Agent"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2030",
                        "name": "DABstep",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "智能体",
                                "en": "Agent"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2506.23719",
                        "officialWebsiteLink": "https://huggingface.co/datasets/adyen/DABstep",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "张富才"
                        },
                        "lookNum": "26",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-07 11:44:38",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-07 11:44:38",
                        "createDate": "2025-07-07 10:26:30",
                        "desc": {
                            "cn": "DABstep 是一个用于评估 AI 智能体在现实多步骤数据分析任务中推理与规划能力的基准，面向具备代码执行能力的语言模型代理。该基准包含450多个任务，源自金融分析平台，涵盖结构化数据处理、非结构化文档理解和跨源信息整合。",
                            "en": "DABstep is a benchmark for evaluating AI agents’ multi-step reasoning and planning abilities in realistic data analysis tasks, targeting code-executing language model agents. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/dabstep'. Error: Path opencompass/dabstep is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2067",
                    "name": "LongVALE",
                    "version": "1.0.0",
                    "description": "LongVALE is the first long video benchmark integrating fine-grained omni modalities (video, audio, speech) information in videos. It comprises 105K omni-modal events with precise temporal boundaries and detailed omni-modal captions, aiming to advance comprehensive multi-modal video understanding.",
                    "url": "opencompass/opencompass_2067.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2067",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Understanding",
                        "Long-Context"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2067",
                        "name": "LongVALE",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            },
                            {
                                "cn": "长文本",
                                "en": "Long-Context"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "long video understanding",
                                "en": "long video understanding"
                            },
                            {
                                "cn": "omni-modal",
                                "en": "omni-modal"
                            },
                            {
                                "cn": "fine-grained temporal understanding",
                                "en": "fine-grained temporal understanding"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/ttgeng233/LongVALE",
                        "paperLink": "https://openaccess.thecvf.com/content/CVPR2025/papers/Geng_LongVALE_Vision-Audio-Language-Event_Benchmark_Towards_Time-Aware_Omni-Modal_Perception_of_Long_Videos_CVPR_2025_paper.pdf",
                        "officialWebsiteLink": "https://ttgeng233.github.io/LongVALE/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "20905098",
                            "name": null,
                            "avatar": null,
                            "nickname": "ttgeng233"
                        },
                        "lookNum": "25",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-18 14:31:33",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-18 14:31:33",
                        "createDate": "2025-07-17 16:31:11",
                        "desc": {
                            "cn": "LongVALE 是首个集成视频中细粒度全模态（视频、音频、语音）信息的长视频理解基准。它包含 10.5 万个具有精确时间边界和详细的全模态描述的事件标注，致力于全面提升多模态视频大语言模型的跨模态推理与细粒度时间感知能力。",
                            "en": "LongVALE is the first long video benchmark integrating fine-grained omni modalities (video, audio, speech) information in videos. It comprises 105K omni-modal events with precise temporal boundaries and detailed omni-modal captions, aiming to advance comprehensive multi-modal video understanding."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/longvale'. Error: Path opencompass/longvale is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2031",
                    "name": "HERB",
                    "version": "1.0.0",
                    "description": "HERB comprises 39,190 artifacts—including documents, meeting transcripts, Slack messages, GitHub content, and URLs—simulating business workflows across product planning, development, and support stages, with noisy, multi-hop QA tasks featuring both answerable and unanswerable queries. ",
                    "url": "opencompass/opencompass_2031.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2031",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2031",
                        "name": "HERB",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "RAG",
                                "en": "RAG"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://arxiv.org/abs/2506.23139",
                        "officialWebsiteLink": "https://huggingface.co/datasets/Salesforce/HERB",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "张富才"
                        },
                        "lookNum": "25",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-07 11:44:29",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-07 11:44:29",
                        "createDate": "2025-07-07 10:31:04",
                        "desc": {
                            "cn": "HERB基准包含39,190个企业文档、会议记录、Slack消息、GitHub内容和网页链接，模拟产品规划、开发与支持等业务流程，生成包含噪声的多跳问答任务，涵盖可回答与不可回答查询。",
                            "en": "HERB comprises 39,190 artifacts—including documents, meeting transcripts, Slack messages, GitHub content, and URLs—simulating business workflows across product planning, development, and support stages, with noisy, multi-hop QA tasks featuring both answerable and unanswerable queries. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/herb'. Error: Path opencompass/herb is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2033",
                    "name": "RExBench",
                    "version": "1.0.0",
                    "description": "RExBench is a benchmark for evaluating large language model agents’ ability to autonomously implement AI research extensions, focusing on code generation, experimental design, and research comprehension. ",
                    "url": "opencompass/opencompass_2033.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2033",
                    "sample_count": 1000,
                    "traits": [
                        "Code"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2033",
                        "name": "RExBench",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "代码",
                                "en": "Code"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/tinlaboratory/RexBench",
                        "paperLink": "https://arxiv.org/abs/2506.22598",
                        "officialWebsiteLink": "https://huggingface.co/datasets/tin-lab/RExBench",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "张富才"
                        },
                        "lookNum": "25",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-07 11:44:23",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-07 11:44:23",
                        "createDate": "2025-07-07 10:46:09",
                        "desc": {
                            "cn": "RExBench 是一个评估大型语言模型代理在自动实现 AI 研究扩展能力的基准，涵盖代码生成、实验设计和研究理解等维度。该基准包含12个基于真实论文和代码库的任务，每项任务由领域专家提供扩展指令，并通过自动化基础设施执行代理输出以验证成功标准。",
                            "en": "RExBench is a benchmark for evaluating large language model agents’ ability to autonomously implement AI research extensions, focusing on code generation, experimental design, and research comprehension. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/rexbench'. Error: Path opencompass/rexbench is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2034",
                    "name": "MTEB",
                    "version": "1.0.0",
                    "description": "MTEB is a benchmark for evaluating text embedding models, aiming to enhance long-term usability and reproducibility, focusing on task extensibility, data integrity validation, and result generalizability. ",
                    "url": "opencompass/opencompass_2034.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2034",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2034",
                        "name": "MTEB",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "文本嵌入",
                                "en": "文本嵌入"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/embeddings-benchmark/mteb",
                        "paperLink": "https://arxiv.org/abs/2506.21182",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "张富才"
                        },
                        "lookNum": "25",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-07 11:44:19",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-07 11:44:19",
                        "createDate": "2025-07-07 10:50:59",
                        "desc": {
                            "cn": "MTEB 是一个评估文本嵌入模型的基准，旨在提升其长期可用性和可复现性，涵盖任务扩展性、数据完整性验证和结果通用性等维度。该基准包含包括分类、检索和聚类在内的多种任务，覆盖多个语言和领域。MTEB 引入了自动化测试管道、持续集成框架和社区贡献机制，以支持基准的可扩展性和质量控制。",
                            "en": "MTEB is a benchmark for evaluating text embedding models, aiming to enhance long-term usability and reproducibility, focusing on task extensibility, data integrity validation, and result generalizability. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/mteb'. Error: Path opencompass/mteb is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2153",
                    "name": "InternData-M1",
                    "version": "1.0.0",
                    "description": "InternData-M1: A large-scale synthetic dataset for generalizable pick-and-place over 80K objects, with open-ended instructions covering object recognition, spatial and commonsense reasoning, and long-horizon tasks.",
                    "url": "opencompass/opencompass_2153.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2153",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2153",
                        "name": "InternData-M1",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "",
                        "officialWebsiteLink": "https://huggingface.co/datasets/InternRobotics/InternData-M1",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50207630",
                            "name": null,
                            "avatar": null,
                            "nickname": "fak111"
                        },
                        "lookNum": "24",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-08-15 16:40:04",
                        "supportOnlineEval": false,
                        "updateDate": "2025-08-15 16:40:04",
                        "createDate": "2025-08-15 16:38:52",
                        "desc": {
                            "cn": "InternData-M1：一个大规模合成数据集，用于可泛化的抓取与放置任务，涵盖 8 万+ 对象，配有开放式指令，涉及物体识别、空间与常识推理以及长时任务。",
                            "en": "InternData-M1: A large-scale synthetic dataset for generalizable pick-and-place over 80K objects, with open-ended instructions covering object recognition, spatial and commonsense reasoning, and long-horizon tasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/interndata_m1'. Error: Path opencompass/interndata_m1 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2035",
                    "name": "Gym4ReaL",
                    "version": "1.0.0",
                    "description": "Gym4ReaL is a benchmark suite designed to evaluate reinforcement learning algorithms in real-world scenarios, addressing challenges such as non-stationarity, partial observability, and large state-action spaces. ",
                    "url": "opencompass/opencompass_2035.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2035",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2035",
                        "name": "Gym4ReaL",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "强化学习",
                                "en": "强化学习"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/Daveonwave/gym4ReaL",
                        "paperLink": "https://arxiv.org/abs/2507.00257",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "张富才"
                        },
                        "lookNum": "24",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-07 11:44:15",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-07 11:44:15",
                        "createDate": "2025-07-07 10:54:22",
                        "desc": {
                            "cn": "Gym4ReaL 是一个用于评估强化学习算法在真实世界场景中表现的基准套件，涵盖非平稳性、部分可观测性和大状态-动作空间等挑战。",
                            "en": "Gym4ReaL is a benchmark suite designed to evaluate reinforcement learning algorithms in real-world scenarios, addressing challenges such as non-stationarity, partial observability, and large state-action spaces. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/gym4real'. Error: Path opencompass/gym4real is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2050",
                    "name": "THUNDER",
                    "version": "1.0.0",
                    "description": "THUNDER is a benchmark designed to evaluate digital pathology foundation models on tile-level image understanding tasks, facilitating comparative analysis across various downstream tasks and models. ",
                    "url": "opencompass/opencompass_2050.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2050",
                    "sample_count": 1000,
                    "traits": [
                        "Understanding",
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2050",
                        "name": "THUNDER",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            },
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/MICS-Lab/thunder",
                        "paperLink": "https://arxiv.org/abs/2507.07860",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "006873",
                            "name": "vansin",
                            "avatar": null,
                            "nickname": "OpenXLab-in2MUhKRV"
                        },
                        "lookNum": "23",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-14 09:39:19",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-14 09:39:19",
                        "createDate": "2025-07-11 17:55:24",
                        "desc": {
                            "cn": "THUNDER 是一个用于评估数字病理学基础模型在切片级图像理解任务中表现的基准，旨在支持多种下游任务和模型的对比分析。",
                            "en": "THUNDER is a benchmark designed to evaluate digital pathology foundation models on tile-level image understanding tasks, facilitating comparative analysis across various downstream tasks and models. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/thunder'. Error: Path opencompass/thunder is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2076",
                    "name": "K-Sort-Arena",
                    "version": "1.0.0",
                    "description": "K-Sort Arena employs K-wise comparison, allowing K models to participate in a free-for-all, providing richer information than pairwise comparison. It also designs a matching algorithm based on exploration-exploitation and probabilistic modeling to achieve more efficient and reliable model ranking. ",
                    "url": "opencompass/opencompass_2076.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2076",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2076",
                        "name": "K-Sort-Arena",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "图像评估",
                                "en": "图像评估"
                            },
                            {
                                "cn": "视频评估",
                                "en": "视频评估"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "",
                        "paperLink": "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_K-Sort_Arena_Efficient_and_Reliable_Benchmarking_for_Generative_Models_via_CVPR_2025_paper.pdf",
                        "officialWebsiteLink": "https://huggingface.co/spaces/ksort/K-Sort-Arena",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "62402831",
                            "name": "lizhikai",
                            "avatar": null,
                            "nickname": "lizhikai"
                        },
                        "lookNum": "23",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-18 14:31:04",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-18 14:31:04",
                        "createDate": "2025-07-18 10:43:15",
                        "desc": {
                            "cn": "本项目提出K-Sort Arena，采用 K-wise 比较，允许 K 个模型参与自由混战，提供比成对比较更丰富的信息，并设计基于探索-利用的匹配算法和概率建模，从而实现更高效和更可靠的模型排名。目前，K-Sort Arena 已收集几千次高质量投票并构建了全面的模型排行榜，已用于评估几十种最先进的视觉生成模型，包括文生图和文生视频模型。K-Sort Arena已经历数月的项目内测，期间收到来自加州大学伯克利分校, 新加坡国立大学, 卡内基梅隆大学, 斯坦福大学, 普林斯顿大学, 北京大学等数十家机构的专业人员的技术反馈，现已公开线上发布。",
                            "en": "K-Sort Arena employs K-wise comparison, allowing K models to participate in a free-for-all, providing richer information than pairwise comparison. It also designs a matching algorithm based on exploration-exploitation and probabilistic modeling to achieve more efficient and reliable model ranking. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/k_sort_arena'. Error: Path opencompass/k_sort_arena is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2142",
                    "name": "OmniMMI",
                    "version": "1.0.0",
                    "description": "OmniMMI is a cutting-edge benchmark for multi-modal interaction, specifically designed for OmniLLMs in streaming video environments. It includes 1,121 videos and 2,290 questions, tackling the challenges of streaming video understanding and proactive reasoning across six unique subtasks.",
                    "url": "opencompass/opencompass_2142.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2142",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2142",
                        "name": "OmniMMI",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/OmniMMI/OmniMMI",
                        "paperLink": "https://arxiv.org/abs/2503.22952",
                        "officialWebsiteLink": "https://omnimmi.github.io/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "000196",
                            "name": null,
                            "avatar": null,
                            "nickname": "OpenXLab-WxyZUppDS"
                        },
                        "lookNum": "23",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-08-06 14:59:17",
                        "supportOnlineEval": false,
                        "updateDate": "2025-08-06 14:59:17",
                        "createDate": "2025-08-05 13:52:24",
                        "desc": {
                            "cn": "OmniMMI is a cutting-edge benchmark for multi-modal interaction, specifically designed for OmniLLMs in streaming video environments. It includes 1,121 videos and 2,290 questions, tackling the challenges of streaming video understanding and proactive reasoning across six unique subtasks.",
                            "en": "OmniMMI is a cutting-edge benchmark for multi-modal interaction, specifically designed for OmniLLMs in streaming video environments. It includes 1,121 videos and 2,290 questions, tackling the challenges of streaming video understanding and proactive reasoning across six unique subtasks."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/omnimmi'. Error: Path opencompass/omnimmi is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2130",
                    "name": "JOB-Complex",
                    "version": "1.0.0",
                    "description": "JOB-Complex is a challenging benchmark for traditional and learned query optimizers, containing 30 SQL queries and a plan-selection benchmark with nearly 6000 execution plans.",
                    "url": "opencompass/opencompass_2130.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2130",
                    "sample_count": 1000,
                    "traits": [
                        "Other"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2130",
                        "name": "JOB-Complex",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "其他",
                                "en": "Other"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/DataManagementLab/JOB-Complex",
                        "paperLink": "https://arxiv.org/abs/2507.07471",
                        "officialWebsiteLink": "",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "50206186",
                            "name": "godzhang233",
                            "avatar": null,
                            "nickname": "张富才"
                        },
                        "lookNum": "21",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-07-28 10:05:03",
                        "supportOnlineEval": false,
                        "updateDate": "2025-07-28 10:05:03",
                        "createDate": "2025-07-28 10:04:11",
                        "desc": {
                            "cn": "JOB-Complex是一个面向传统与学习式查询优化器的挑战性数据库基准，包含30个SQL查询和近6000个执行计划的计划选择基准。",
                            "en": "JOB-Complex is a challenging benchmark for traditional and learned query optimizers, containing 30 SQL queries and a plan-selection benchmark with nearly 6000 execution plans."
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/job_complex'. Error: Path opencompass/job_complex is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                },
                {
                    "id": "opencompass_2092",
                    "name": "Spatial457",
                    "version": "1.0.0",
                    "description": "Spatial457 systematically introduces four key capabilities—multi-object understanding, 2D and 3D localization, and 3D orientation—across five difficulty levels and seven question types, from simple object recognition to complex 6DoF spatial reasoning tasks. ",
                    "url": "opencompass/opencompass_2092.jsonl",
                    "size_mb": 1.0,
                    "checksum": "sha256:placeholder_opencompass_2092",
                    "sample_count": 1000,
                    "traits": [
                        "Multimodal",
                        "Reasoning",
                        "Understanding"
                    ],
                    "eval_type": {},
                    "metadata": {
                        "format": "jsonl",
                        "language": "English",
                        "domain": "General",
                        "difficulty": "medium",
                        "requires_auth": false,
                        "estimated_input_tokens": 1000,
                        "estimated_output_tokens": 500
                    },
                    "original_data": {
                        "id": "2092",
                        "name": "Spatial457",
                        "emoji": "",
                        "dimensions": [
                            {
                                "cn": "多模态",
                                "en": "Multimodal"
                            },
                            {
                                "cn": "推理",
                                "en": "Reasoning"
                            },
                            {
                                "cn": "理解",
                                "en": "Understanding"
                            }
                        ],
                        "subDimensions": [],
                        "tags": [
                            {
                                "cn": "3D Spatial Reasoning",
                                "en": "3D Spatial Reasoning"
                            },
                            {
                                "cn": "Spatial Intelligence",
                                "en": "Spatial Intelligence"
                            }
                        ],
                        "topicTags": [],
                        "benchCertificateLevel": 0,
                        "githubLink": "https://github.com/XingruiWang/Spatial457",
                        "paperLink": "https://arxiv.org/abs/2502.08636",
                        "officialWebsiteLink": "https://xingruiwang.github.io/projects/Spatial457/",
                        "leaderboardLink": false,
                        "creatorInfo": {
                            "uid": "20903864",
                            "name": null,
                            "avatar": null,
                            "nickname": "XingruiWang"
                        },
                        "lookNum": "18",
                        "top": false,
                        "state": 3,
                        "publicFlag": 1,
                        "reviewBlockError": null,
                        "reviewSuccessDate": "2025-08-15 16:37:31",
                        "supportOnlineEval": false,
                        "updateDate": "2025-08-15 16:37:31",
                        "createDate": "2025-07-20 08:57:13",
                        "desc": {
                            "cn": "Spatial457 聚焦于空间推理的四项核心能力：多物体理解、二维位置识别、三维位置识别，以及三维朝向判断。这些能力对现实世界中复杂场景的认知和理解至关重要。我们构建了一种层级递进的评估结构，将问题划分为 7 类问题类型，覆盖从基础的单物体识别任务，到我们首次提出的更具挑战性的 6DoF 空间推理任务。整个数据集按 5 个难度等级组织，帮助系统性地评估模型在不同层次空间理解任务中的表现。\n\nSpatial457 不仅提升了空间推理评测的覆盖面和挑战性，也为后续研究提供了统一的基准，有助于推动多模态模型在真实世界空间认知任务中的发展。",
                            "en": "Spatial457 systematically introduces four key capabilities—multi-object understanding, 2D and 3D localization, and 3D orientation—across five difficulty levels and seven question types, from simple object recognition to complex 6DoF spatial reasoning tasks. "
                        },
                        "modalities": [
                            "text"
                        ],
                        "sample_questions_answers": {
                            "examples": [],
                            "total_questions": 1000,
                            "question_format": "Various formats",
                            "difficulty_levels": [
                                "Beginner",
                                "Intermediate",
                                "Advanced"
                            ]
                        },
                        "advantages_disadvantages": {
                            "advantages": [],
                            "disadvantages": []
                        }
                    },
                    "analysis_file": null,
                    "analysis_summary": {
                        "error": "Could not load dataset 'opencompass/spatial457'. Error: Path opencompass/spatial457 is neither a directory nor a file\nPlease ensure the dataset name is correct and the dataset exists."
                    }
                }
            ]
        }
    },
    "update_policy": {
        "check_interval_hours": 24,
        "auto_update": false,
        "notify_on_update": true,
        "backup_before_update": true,
        "rollback_on_failure": true
    },
    "authentication": {
        "required_for": [
            "bud_custom"
        ],
        "method": "oauth2",
        "token_endpoint": "https://auth.bud.eco/token"
    },
    "migration": null,
    "changelog": {
        "2025.01.05": [
            "Added eval_type support for datasets to specify evaluation configurations",
            "GSM8K dataset now includes gen evaluation type configuration"
        ],
        "2025.01.04": [
            "Added GSM8K dataset for mathematical reasoning",
            "Added HumanEval dataset for code generation",
            "Added MMLU dataset for broad knowledge evaluation",
            "Added Bud Multilingual QA dataset",
            "Added Bud Safety Evaluation dataset",
            "Enhanced metadata with token estimates"
        ]
    }
}
