---
title: "Models"
description: "Cloud and Local Models capabilities and configuration"
---

## 1. Description

The Model module in Bud Admin centralizes how teams register, evaluate, and govern models across cloud and on-prem environments. It brings GPU-optional GenAI to enterprises by mixing elastic cloud endpoints with highly optimized local runtimes, so teams can ship production-grade AI without waiting for scarce accelerator capacity. The module spans LLM, multi-model LLM, embeddings, speech to text, text to speech, action transformers, and document workloads.

As enterprises scale GenAI, common pain points emerge:

- How do we blend cloud APIs with local checkpoints without rewriting every app?
- How do we avoid GPU queues while still meeting latency and quality targets?
- How do we govern on-prem artifacts with the same rigor as managed cloud endpoints?
- How do we give developers and auditors one source of truth for versions, workflows, and policies?

Bud AI Foundry addresses these challenges with:

- A unified catalog that treats cloud and local models as first-class citizens with consistent metadata and approval gates,
- Routing and scaling policies that favor CPU-first placements while bursting to cloud GPUs/HPUs etc when SLOs demand it,
- Safety, signing, and audit controls that travel with the model regardless of where it runs,
- Built-in analytics so platform and app teams can pick the right backend for cost, latency, and compliance.

## 2. USPs (Unique Selling Propositions)

### 1. GPU-optional architecture

Run models on cost-efficient CPU-first stacks while still bursting to cloud GPUs when needed. Configurable placement and workflow options keep setups consistent across deployment targets.

### 2. Unified catalog for every model type

Keep cloud APIs, Hugging Face downloads, URL-based downloads, and disk-mounted models in one governed inventory with consistent metadata and policies across LLM, multi-model LLM, embeddings, speech, action transformers, and document tasks.

### 3. Built-in safety and compliance controls

Enforce guardrails, content filters, and signing/attestation for on-prem artifacts before they are exposed to apps or external clients.

### 4. Production-ready reliability

Health checks, auto-failover between replicas/backends, and traffic shifting let teams roll out new model versions without downtime.

### 5. Cost and performance visibility

Analytics show latency, throughput, token usage, and cost so you can pick the right model or backend for each workload.

## 3. Features

### 3.1 Model onboarding & catalog

#### 3.1.1 Cloud models

- Register managed endpoints from major clouds with keys.
- Map SKUs and capabilities (LLM, multi-model LLM, embeddings, speech, action transformers, document) for fast discovery.

#### 3.1.2 Hugging Face models (download from Hugging Face)

- Pull repositories directly with checksum validation and support for LLM, embeddings, speech, action transformers, and document pipelines.
- Capture hardware preferences (CPU/GPU etc), memory constraints, quantization settings, and tokenizer configs.

#### 3.1.3 URL-based models (provide a URL to download model)

- Import checkpoints from signed or pre-approved URLs with expiry controls for air-gapped or controlled transfers.
- Capture hardware preferences (CPU/GPU etc), memory constraints, quantization settings, and tokenizer configs.

#### 3.1.4 Disk-based models (add from disk or server folder path)

- Register checkpoints from mounted disks or server paths with checksum/signature verification and lineage notes.
- Capture hardware preferences (CPU/GPU etc), memory constraints, quantization settings, and tokenizer configs.

#### 3.1.5 Metadata and governance

- Standard fields for owner, project, safety tags, license, and rollout status.
- Attach evaluation baselines and approval workflows before promotion to production.

### 3.2 Workflow orchestration and readiness

- Cloud and local onboarding workflows coordinate validation, downloads, and extraction through orchestration.
- Workflow steps emit progress updates so teams can resume or audit long-running imports.
- Request-level controls: max tokens, temperature, stop sequences, retry budgets, and timeout guards.

### 3.3 Safety, compliance, and auditability

- Content filters and jailbreak protection for managed endpoints.
- Mandatory signing/attestation for local artifacts with checksum validation for supply-chain visibility.
- Audit trails for model imports and policy changes exportable for compliance reviews.

### 3.4 Observability and optimization

- Evaluation traces and metrics (latency, token counts).
- Benchmark results to compare backends and hardware settings.
- Analytics that highlight regressions after model upgrades.

### 3.5 Quantized models and adapters

- Track quantized variants (GGUF, GPTQ, AWQ, bits-and-bytes) alongside full-precision checkpoints with shared metadata and approvals.
- Capture adapter lineage (LoRA/QLoRA/PEFT) including base model, training data tags, and compatible deployment targets.
- Validate tokenizer and quantization configs to ensure deterministic rollouts between staging and production.

### 3.6 Security scan and supply chain integrity

- Run integrity checks (signatures, checksums) plus vulnerability scans on imported artifacts before activation.
- Enforce policy gates so only signed and scanned models can be routed to projects.
- Re-run scans after updates to confirm checksums and configurations remain consistent.

### 3.7 Evaluations

- First-class evaluation runs with scenario tags (safety, hallucination, latency) that attach to each model version.
- Baseline comparisons across providers to quantify quality vs cost trade-offs.
- Store datasets for repeatable runs.

### 3.8 Performance benchmarks

- Benchmark suites for throughput (tokens/s), latency percentiles, and memory footprint across CPU and GPU targets.
- Compare cloud vs local vs quantized backends side-by-side to inform routing weights and autoscaling settings.
- Publish reports to projects so app teams can select the best-fit model for their workload profile.

## 4. How-to Guides

### 4.1 Accessing the Model Module

1. Log in to your Bud AI Foundry dashboard using SSO or your credentials.
2. Click on Models menu from the side menu.
3. View the model grid with counts of deployments, model type, and recommended cluster etc.

### 4.2 Add a cloud model

1. Click +Model and choose Modality.
2. Choose Cloud.
3. Select cloud provider and a model from the provider.
4. Type the model name and select tags.
5. Model is added to the catalog.

### 4.3 Onboard a local Hugging Face model

1. Click +Model and choose Modality.
2. Choose Hugging Face.
3. Submit model name, URL, author, and tags.
4. Model is added to the catalog.

### 4.4 Onboard a local URL-based model

1. Click +Model and choose Modality.
2. Choose URL.
3. Submit model name, URL, author, and tags.
4. Model is added to the catalog.

### 4.5 Onboard a local disk-based model

1. Click +Model and choose Modality.
2. Choose Disk.
3. Submit model name, folder path, author, and tags.
4. Model is added to the catalog.

### 4.6 Edit a model

1. Open the model detail page from the catalog.
2. Update metadata (name, tags, tasks, description, model public URLs and License URL/file etc.).
3. Save changes to refresh the catalog entry and downstream routes.

### 4.7 Delete a model

1. Open the model detail page and choose Delete.
2. Confirm removal to detach the model from active routes and archives.
3. Ensure dependent routes or applications are redirected before finalizing deletion.

### 4.8 Run a security scan

1. Open the local model detail page and select Security Scan.
2. Trigger a scan to validate checksums, signatures, and vulnerability results.
3. Review findings and apply required remediations before approving the model for routing.

### 4.9 Run an evaluation

1. Open the model detail page and select Evaluations tab.
2. Choose traits and datasets to run an evaluation.
3. Execute the run and review baseline deltas before promoting the model or updating routes.

### 4.10 Run a performance benchmark

1. Open the local model detail page and select Performance tab.
2. Choose Run Performance Benchmark.
3. Type Benchmark name, tags, description, concurrent request, eval with dataset/configuration.
4. Choose datasets and cluster for dataset based benchmark run.
5. Provide Max Input Tokens, Max Output Tokens and select cluster for configuration based benchmark run.

### 4.11 Add a quantized model variant

1. Open the local model detail page and select Adapters tab.
1. Click See More for Quantised Models section.
1. Click Add Quantizations to add Quantised Models.
1. Provide the quantization name, type and target hardware profile.
1. Choose Quantization method and provide quantization recipe (weight bits, weight granularity, activation bits, activation granularity and symmetric).
3. Save the variant; it will appear alongside the base model for routing.

### 4.12 View adapters

1. Open the local model detail page and select Adapters tab.
2. Review available adapters, their lineage (base model, dataset), and activation status.
3. Enable or disable adapters for specific routes or projects as needed.

### 4.13 Modify permissions for models

1. Open the user management page and select user.
2. Assign view access for users who should only browse model listings.
3. Grant manage permissions to users who can add models and perform edits, deletions updates.
4. Save updates to enforce access across catalog and routing actions.

## 5. FAQ

**Q1. Which model types are supported?**

Cloud APIs, Hugging Face imports, signed URL downloads, and disk-mounted checkpoints are all first-class citizens. The module supports LLM, multi-model LLM, embeddings, speech to text, text to speech, action transformers, and document workloads with the same onboarding flow.

**Q2. How do we keep local models secure?**

Artifacts require checksum or signature verification and policy-based approvals before activation. Safety filters and audit logs remain enforced even when running entirely on-prem.

**Q3. Can we prioritize CPU to avoid GPU queues?**

Yes. You can proceed with CPU-first preferences and set routing rules that favor CPU until SLOs require GPU burst capacity.

**Q4. How do we roll out a new model version safely?**

Run evaluations and benchmarks against the new version, review the results with approvers, and promote only after latency, quality, and safety signals meet thresholds.

**Q5. Does the module support multi-team isolation?**

Yes. Models carry project ownership, and permissions restrict who can view, edit, or delete catalog entries.

**Q6. What happens if a backend fails?**

Health checks and circuit breakers trigger automatic failover to the next eligible backend. Retries and timeout guards ensure requests either succeed or fail fast without hanging clients.

**Q7. Can we audit who changed a route or model config?**

Every onboarding, policy change, deployment, and routing edit is recorded with actor, timestamp, and diff, exportable for compliance or incident reviews.