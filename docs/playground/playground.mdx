---
title: "Playground"
description: "Interactive workspace to experiment with cloud and local models across Bud projects"
---

## 1. Description

The Playground module in Bud AI Foundry dashboard gives product teams, data scientists, and business stakeholders a safe environment to try cloud and local models (Hugging Face, URL-based, and disk-based) before routing traffic. It mirrors project context—deployments, routes, guardrails, and permissions—so experiments stay aligned with production governance while highlighting GPU-optional execution.

The tab appears in the primary navigation alongside Projects, Models, and Clusters, giving one-click access to a secure session. It provides real-time inference, model comparison, and streaming output, and configurable settings to keep experimentation fast and repeatable across supported providers.

## 2. USPs (Unique Selling Propositions)

### 1. Unified surface for cloud and local models

Test managed cloud endpoints cloud endpoints and local checkpoints (Hugging Face, URL downloads, disk mounts) from the same console with identical controls for parameters, temperature, and safety toggles.

### 2. Interactive testing with streaming feedback

Run live inference with streaming responses, adjustable decoding parameters (temperature etc), and a chat interface so users can iterate quickly on prompts and observe results in real time.

### 3. Built-in comparison workflow

Compare models side-by-side in dedicated panes to spot quality or latency differences using the same prompts and independent parameter settings.

### 4. Message-level controls

Copy prompts or responses for reuse, edit and resend user prompts, provide thumbs-up/thumbs-down feedback on assistant replies, and trigger a retry to regenerate outputs while keeping the same context and metrics visible.

## 3. Features

### 3.1 Access and navigation

- Launch Playground from the global side menu.
- Use header search to locate models.
- Quick actions include New chat window, Chat History, Settings and Delete Chat etc.

### 3.2 Chat and single-model testing

- Use the chat interface to send prompts to a selected model, with controls for temperature and other generation parameters.
- View streaming responses, inspect generated content, and refine prompts iteratively.

### 3.3 Model comparison

- Open the new chat window workflow to place two or more models side-by-side.
- Send the same prompt to each model and review outputs together to judge quality or latency differences.

### 3.4 Session settings and safety

- Configure presets, temperature, response limits, repeat penalties, stop strings, structured output, and conversation notes from the Settings panel to keep runs consistent.
- Use stop strings and structured output toggles to align responses with downstream constraints.

### 3.5 Response analysis and conversation history

- Review response details and performance insights surfaced alongside model outputs.
- Chat history drawer lists recent sessions with token usage etc.

### 3.6 Message controls and feedback

- Copy prompts or responses directly from each bubble to reuse them in new chats or templates.
- Edit sent prompts inline to refine them and resend without losing conversation history. - Apply thumbs-up/down ratings on assistant replies to log quick qualitative feedback.
- Use the retry control on assistant messages to regenerate the latest response while keeping the same model and context.

## 4. How-to Guides

### 4.1 Access the Playground

1. Log in to your Bud AI Foundry dashboard using SSO or your credentials.
2. Click Playground in the side navigation to open the interface.

### 4.2 Run a chat against a single model

1. Open the Playground and select a model from the model selector.
2. Adjust decoding parameters as needed.
3. Enter your prompt in the chat and send to view streaming output.
4. Refine prompts and resend until you achieve the desired response.

### 4.3 Compare multiple models

1. Open new chat window.
2. Load different models to the each chat window.
3. Enter a same prompt and send to evaluate outputs.

### 4.4 Use prompt templates and code-focused modes

1. Open the templates section and choose a template for your task.
2. Load the template into the editor and adjust variables or parameters.
3. For code use cases, switch to the code generation view to tailor completions before running.

### 4.5 Export conversations or comparisons

1. After finishing a chat or comparison, select the export option.
2. Choose the format offered by the interface to save the conversation or comparison results.

## 5. FAQs

**Q1. How do I turn on the Playground in Bud Admin?**

Set `NEXT_PUBLIC_ENABLE_PLAYGROUND=true` and provide a `NEXT_PUBLIC_PLAYGROUND_URL` so the Playground link is available in the Bud Admin navigation.

**Q2. Which experiments does Playground support?**

Playground includes interactive chat testing, side-by-side model comparisons, prompt templates, code generation views, and exportable results.

**Q3. Can I tune model parameters while testing?**

Yes. Temperature, top-k, and top-p controls are available so you can adjust generation settings between runs.

**Q4. How do I capture results for stakeholders?**

Use the export option in the Playground to save conversations or comparison outputs for sharing or downstream analysis.