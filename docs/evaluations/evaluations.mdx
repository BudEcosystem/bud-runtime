---
title: "Evaluations"
description: "Designing, running, and comparing automated model evaluations in Bud AI Foundry."
---

## 1. Description

The Evaluation module in Bud AI Foundry centralizes how teams validate model quality across Bud-hosted cloud deployments and local checkpoints (Hugging Face, URL-download, and disk-mounted). It bundles curated public benchmarks with organization-defined traits so evaluators can score latency, accuracy, and safety before promoting routes to production. Listing, detail, and experiment views are designed to be self-service with governance, giving product owners, red-teamers, and MLOps engineers a consistent way to compare candidates across clusters.

## 2. USPs (Unique Selling Propositions)

### 1. Unified catalog of evaluations and traits

Browse curated evaluations with modality, domain, and trait tags; deep links to GitHub, papers, and websites give reviewers the full context behind every benchmark.

### 2. Cloud and local model readiness

Run benchmarks against cloud endpoints and local deployments (Hugging Face, URL-based, or disk-based models) using the same workflows, so hardware location never blocks quality checks.

### 3. Rich evaluation detail workspace

Detail pages include Details, Leaderboard, and Evaluations Explorer tabs to inspect datasets, ranking tables, and run histories without leaving the page.

### 4. Governance and exportability

Experiments, runs, and workflows respect Bud RBAC. Project level evaluation tables surface traits, dataset references, and scores while offering buttons to run another evaluation against a deployment or export CSV results for offline review and downstream reporting.

### 5. Experiment lists with flexible filters

The Experiments tab provides search, pagination, optional status/tag/model/date filters, and sortable columns for experiment names, models, traits, status, tags, and created dates, giving operators a concise way to track benchmarking activity.

### 6. Reusable run context and metadata

Evaluation cards and detail pages expose tags, modality badges, and metadata links so teams can understand provenance and rerun studies with consistent context.

### 7. Dataset-level transparency in Evaluations Explorer

The Evaluations Explorer tab preserves each dataset item with its metric, prompt, and captured model response, pulling exemplar prompt and response context from the dataset’s source site so reviewers can audit behavior at a glance rather than inferring results from aggregate scores alone.

## 3. Features

### 3.1 Evaluation catalog

- Search bar with placeholder guidance (e.g., finance, problem solving) for quick discovery, plus a live counter showing how many evaluations match.
- Horizontal trait filters (multi-select pills) for narrowing by skill or domain.
- Cards show modality badges (text, image, video, actions, embeddings), trait tags, trimmed descriptions with “See more”, creator avatars, creation dates, and metadata links (GitHub, paper, website).
- Card click opens the evaluation detail page.

### 3.2 Evaluation detail page

- **Details tab:** Dataset summary, descriptive tags, advantages, constraints and provenance to understand scope and requirements.
- **Leaderboard tab:** Ranked table of model results for the evaluation with comparative scores.
- **Evaluations Explorer tab:** Table of historical evaluation runs and datasets, with prompt text, model responses, and metrics to trace regressions or improvements over time. Each row aligns a dataset item with its evaluated metric and includes exemplar request and response context sourced from the dataset’s website so teams can confirm model behavior against prompts.

### 3.3 Experiments and runs

- Experiments list shows experiment name, models, traits, tags, status, and created date with pagination, sorting, and search.
- Runs history captures run IDs, model names, trait names, status, start/end timestamps, duration, and per-dataset scores.
- Benchmark details section summarize average score, duartion and status etc for datasets included in a evaluation run.
- Current metrics section captures trait-level performance for each dataset.

### 3.4 Project-scoped evaluations

- Project evaluation pages list traits, dataset names, and scores for the selected deployment with search and pagination.
- Run Another Evaluation opens the evaluation drawer for the active deployment, helping teams launch another run without leaving the project context.
- Export downloads a CSV of evaluations for the project so results can be shared offline.

### 3.5 Dataset and trait management

- Trait chips are fetched centrally to keep filter lists consistent across catalog and experiment screens.
- Modality-aware icons clarify whether an evaluation measures text, image, video, action, or embedding capabilities.

## 4. How-to Guides

### 4.1 Access the Evaluation module

1. Log in to the Bud AI Foundry dashboard using SSO or your credentials..
2. From the side menu, select Evaluations to open the experiment view.
3. Click on Evaluations Hub tab to view listing with counts showing how many datasets match current filters.

### 4.2 Evaluations Hub

1. Click on Evaluations Hub tab.
2. View dataset catalog with modality, trait tags; deep links to GitHub, papers, and websites etc.
3. In the catalog header, type keywords (e.g., “finance”, “reasoning”) into the search bar.
4. Toggle trait pills to include or exclude skills or domains. The dataset count updates immediately.

### 4.3 Evaluations details

1. Click a dataset to open detail page.
2. On the detail page, view the description, tags and mutiple tabs.
3. Switch to **Details** to see datasets and scope, **Leaderboard** for ranked results, or **Evaluations Explorer** for run timelines.
4. Use breadcrumbs or the back control to return to the catalog.

#### 4.3.1 Introduction, reasons, expectations, advantages, and constraints

1. Read the **Introduction** summary describing the dataset or evaluation scope.

2. Expand the bullet lists under **Why Run this Eval?** and **What to Expect?** to understand goals and coverage.

3. Check **Advantages** for strengths and **Constraints** for domain or requirement caveats.

#### 4.3.2 Evaluation values and token estimates

1. Review **Evaluation Values** to see bar chart scores sourced from leaderboard accuracy for the available models.

2. Note **Total Input tokens** and **Expected Output** values to estimate usage before running or comparing models.

#### 4.3.3 Metadata links and provenance

1. Use the **GitHub**, **Paper**, or **Website** chips when present to open supporting references in new tabs.

2. Keep the detail page open to avoid losing context while reviewing external material.

#### 4.3.4 Modalities

1. Scan the **Input** and **Output** modality panels to verify whether the evaluation covers text, image, or audio.

2. Confirm the modality chips match the domains you plan to assess before launching new runs.

#### 4.3.5 Domains, concepts, qualifications, language, skills, task types, and age distribution

1. Review the domain, concept, human qualification, language, skills identified, and task type tag groups to validate scope.

2. If provided, inspect the **Age Distribution** chart to see the evaluator demographic for the dataset.

### 4.4 Run an evaluation for a project

1. Open **Projects → [Project] → Evaluations**.
2. Click **Run Another Evaluation** to open the drawer for the active deployment. Ensure a valid deployment ID is selected.
3. Follow the drawer steps to choose the experiment, traits, and deployment, then start the run.

### 4.5 Compare models with leaderboards

1. Open an evaluation detail page and select the **Leaderboard** tab.
2. Sort or scan rows to compare benchmark scores across models and deployments.
3. Use the **Evaluations Explorer** tab to correlate leaderboard positions with specific runs and timestamps.

### 4.6 Export evaluation results

1. From a project’s evaluations page, click **Export**.
2. Confirm the CSV download. The file includes evaluation details for offline review.

### 4.7 Manage experiments and runs

1. Navigate to **Evaluations → Experiments**.
2. Use search and optional status or tag filters to locate an experiment.
3. Review experiment rows for models, traits, status, tags, and created dates.
4. Open the **New experiment** drawer from the tab bar when you need to add another study.

## 5. FAQs

**Q1. How do I start a new experiment?**

Use the **New experiment** button in the Evaluations tab bar to open the run drawer and add a study without leaving the page.

**Q2. How do I find evaluations relevant to my domain?**

Use the search bar and trait pills on the catalog page to filter by domain or skill. Counts adjust instantly so you know how many fit your criteria.

**Q3. What tabs should I check on the evaluation detail page?**

Start with **Details** for scope, **Leaderboard** for rankings, and **Evaluations Explorer** for run history and dataset context before making routing decisions.

**Q4. Can I export evaluation outcomes?**

Yes. Project evaluation pages include an **Export** button that downloads CSV data for audits or custom reporting.

**Q5. How do I launch another evaluation for a deployment?**

Open the project’s Evaluations page, ensure the deployment ID is set in the route, and click **Run Another Evaluation** to start a new run.

**Q6. Can I target both cloud and local models in evaluations?**

Yes. The run drawer supports cloud deployments alongside local Hugging Face, URL-based, and disk-based models so benchmarking matches where models are hosted.

**Q7. How do I check what traits or modalities an evaluation covers?**

Review the trait chips and modality badges on catalog cards or the detail page. Use the **\+X more** expander to see the full list when tags are truncated.