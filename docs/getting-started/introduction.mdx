---
title: "Introduction to Bud AI Foundry"
description: "Learn about Bud AI Foundry - the comprehensive platform for AI model deployment, governance, and observability."
---

Bud AI Foundry is a full-stack platform for deploying, governing, and optimizing GenAI workloads across cloud and on-prem infrastructure. It brings model management, secure runtime controls, and OpenAI-compatible APIs into a single workflow so teams can ship AI features without stitching together multiple tools.

## What is Bud AI Foundry?

Bud AI Foundry was created to make GenAI accessible and sustainable. Instead of locking teams into expensive GPU-only stacks, the platform optimizes inference and routing on commodity hardware while still allowing burst-to-accelerator when latency or throughput requires it. This approach reduces cost, avoids hardware scarcity, and helps enterprises move from prototype to production faster.

## Key Benefits

### 2.1 GPU-optional deployment

Run on CPU-first infrastructure with the ability to burst to GPUs when workloads demand higher performance. Bud AI Foundry optimizes placement, routing, and scaling based on cost and latency targets.

### 2.2 Unified model lifecycle

Register, evaluate, and version cloud and local models in one catalog with consistent metadata and approvals.

### 2.3 Built-in governance

Apply guardrails, signing, and audit controls to models, routes, and deployments so teams stay compliant.

### 2.4 OpenAI-compatible APIs

Expose deployments through familiar request formats so application teams can integrate quickly while still using Bud-native extensions for routing and safety.

### 2.5 Performance and cost visibility

Track usage, latency, and spend across providers and clusters to optimize workloads continuously.

### 2.6 Built-in observability

Monitor latency, throughput, token usage, and safety signals from a single dashboard, and connect metrics to deployments for fast troubleshooting.

## 3. Primary use cases

### 3.1 Production inference for applications

Deploy and scale model endpoints for customer-facing apps, internal copilots, or workflow automation with consistent routing and uptime guarantees.

### 3.2 Model governance and evaluation

Compare models and configurations with evaluations and benchmarks, then promote the best-performing versions to production.

### 3.3 Hybrid AI infrastructure

Blend managed cloud APIs with on-prem or private deployments while keeping a single catalog, access layer, and monitoring stack.

## 4. How Bud AI Foundry is organized

1. **Workspaces and projects:** Define teams, access, and billing boundaries.

2. **Model catalog:** Register cloud models and local checkpoints in a unified hub.

3. **Deployments and routing:** Launch models on clusters and expose endpoints.

4. **Observability and guardrails:** Monitor usage and enforce safety policies.

## 5. Next steps

- [Platform overview](/getting-started/platform-overview) to understand the architecture.

- [Account setup](/getting-started/account-setup) to configure your workspace and roles.

- [Quick start](/getting-started/quickstart) to deploy a model and call your first endpoint.

## Primary Use Cases

- **Enterprise GenAI enablement:** Deliver a governed model hub and standardized deployment workflows for multiple teams.
- **Hybrid inference:** Balance on-prem models with cloud APIs while keeping routing and safety controls centralized.
- **Cost optimization:** Use routing and observability to shift workloads to the most cost-efficient backend.
- **Production readiness:** Validate, benchmark, and monitor models before promoting them to critical applications.