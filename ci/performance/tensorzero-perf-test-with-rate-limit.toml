# Performance test configuration with rate limiting enabled

[gateway]
authentication.enabled = false
observability.enabled = false

# Rate limiting configuration
[gateway.rate_limits]
enabled = true
# Note: Redis connection pool size and local cache size 
# need to be configured in the actual implementation
default_requests_per_minute = 100000  # Very high limit for perf testing
default_burst_size = 10000
default_algorithm = "fixed_window"  # Fastest algorithm for performance

# Models
[models."gpt-3.5-turbo"]
routing = ["openai"]
endpoints = ["chat"]

# High rate limits for performance testing - shouldn't trigger during test
[models."gpt-3.5-turbo".rate_limits]
algorithm = "fixed_window"
requests_per_second = 5000  # 5000 req/s - much higher than test rate
burst_size = 10000
enabled = true
cache_ttl_ms = 1000         # Longer cache for performance
redis_timeout_ms = 5        # 5ms timeout
local_allowance = 0.9       # High local allowance for performance
sync_interval_ms = 1000     # Less frequent sync

[models."gpt-3.5-turbo".providers.openai]
type = "openai"
model_name = "gpt-3.5-turbo"
api_base = "http://localhost:3030/openai/"
api_key_location = "none"

# Functions (for legacy /inference endpoint)
[functions.function1]
type = "chat"

[functions.function1.variants.variant1]
type = "chat_completion"
model = "gpt-3.5-turbo"
max_tokens = 100